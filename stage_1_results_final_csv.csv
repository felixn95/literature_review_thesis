Review_Stage;Database;Title;Author;Year;DOI;DOI_Link;Abstract
NOT_RELEVANT;IEEE Xplore;The NLP Cookbook: Modern Recipes for Transformer Based Deep Learning Architectures;Singh, Sushant and Mahmood, Ausif;2021;10.1109/ACCESS.2021.3077350;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;SIFRank: A New Baseline for Unsupervised Keyphrase Extraction Based on Pre-Trained Language Model;Sun, Yi and Qiu, Hangping and Zheng, Yu and Wang, Zhongwei and Zhang, Chaoran;2020;10.1109/ACCESS.2020.2965087;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;ChatTf: A Knowledge Graph-Enhanced Intelligent Q&A System for Mitigating Factuality Hallucinations in Traditional Folklore;Xu, Jun and Zhang, Hao and Zhang, Haijing and Lu, Jiawei and Xiao, Gang;2024;10.1109/ACCESS.2024.3485877;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;Summarization of Text and Image Captioning in Information Retrieval Using Deep Learning Techniques;Mahalakshmi, P. and Fatima, N. Sabiyath;2022;10.1109/ACCESS.2022.3150414;NOT_FOUND;Automated information retrieval and text summarization concept is a difficult process in natural language processing because of the infrequent structure and high complexity of the documents. The text summarization process creates a summary by paraphrasing a long text. Earlier models on information retrieval and summarization are based on a massive labeled dataset by the use of handcrafted features, leveraging on knowledge for a particular domain, and concentrated on the narrow sub-domain to improve efficiency. This paper presents a new deep learning (DL) based information retrieval with a text summarization model. The proposed model involves three major processes namely information retrieval, template generation, and text summarization. Initially, the bidirectional long short term memory (BiLSTM) approach is employed for retrieving the textual data, which assumes each word in a sentence, extracts the information, and embeds it into the semantic vector. Next, the template generation process takes place using the DL model. The deep belief network (DBN) model is employed as a text summarization tool to summarize the textual content. In addition, the image description is generated for the visualized entities that exist in the images. The design of BiLSTM with the DBN model for the text summarization and image captioning process shows the novelty of the work. The performance of the presented method is validated using Giga word corpus and DUC corpus. The experimental results referred that the proposed DBN model outperformed the compared methods with the maximum precision, recall and F-score. The image captions are compared with a predefined set of captions that exists for the image and the performance is evaluated using the BLEU metric.
NOT_RELEVANT;IEEE Xplore;Generation of Asset Administration Shell With Large Language Model Agents: Toward Semantic Interoperability in Digital Twins in the Context of Industry 4.0;Xia, Yuchen and Xiao, Zhewen and Jazdi, Nasser and Weyrich, Michael;2024;10.1109/ACCESS.2024.3415470;NOT_FOUND;NOT_FOUND
MAYBE_RELEVANT;IEEE Xplore;UniRaG: Unification, Retrieval, and Generation for Multimodal Question Answering With Pre-Trained Language Models;Zhi Lim, Qi and Poo Lee, Chin and Ming Lim, Kian and Kamsani Samingan, Ahmad;2024;10.1109/ACCESS.2024.3403101;NOT_FOUND;NOT_FOUND
MAYBE_RELEVANT;IEEE Xplore;LegalReasoner: A Multi-Stage Framework for Legal Judgment Prediction via Large Language Models and Knowledge Integration;Wang, Xuran and Zhang, Xinguang and Hoo, Vanessa and Shao, Zhouhang and Zhang, Xuguang;2024;10.1109/ACCESS.2024.3496666;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;DAKRS: Domain Adaptive Knowledge-Based Retrieval System for Natural Language-Based Vehicle Retrieval;Ha, Synh Viet-Uyen and Le, Huy Dinh-Anh and Nguyen, Quang Qui-Vinh and Chung, Nhat Minh;2023;10.1109/ACCESS.2023.3260149;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;Large Language Models for Clinical Text Cleansing Enhance Medical Concept Normalization;Abdulnazar, Akhila and Roller, Roland and Schulz, Stefan and Kreuzthaler, Markus;2024;10.1109/ACCESS.2024.3472500;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;Building a Multilevel Inflection Handling Stemmer to Improve Search Effectiveness for Urdu Language;Jabbar, Abdul and Iqbal, Sajid and Alaulamie, Abdullah Abdulrhman and Ilahi, Manzoor;2024;10.1109/ACCESS.2024.3373714;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;MCQGen: A Large Language Model-Driven MCQ Generator for Personalized Learning;Hang, Ching Nam and Wei Tan, Chee and Yu, Pei-Duo;2024;10.1109/ACCESS.2024.3420709;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;Plain Template Insertion: Korean-Prompt-Based Engineering for Few-Shot Learners;Seo, Jaehyung and Moon, Hyeonseok and Lee, Chanhee and Eo, Sugyeong and Park, Chanjun and Kim, Jihoon and Chun, Changwoo and Lim, Heuiseok;2022;10.1109/ACCESS.2022.3213027;NOT_FOUND;NOT_FOUND
MAYBE_RELEVANT;IEEE Xplore;Harnessing the Power of Metadata for Enhanced Question Retrieval in Community Question Answering;Ghasemi, Shima and Shakery, Azadeh;2024;10.1109/ACCESS.2024.3395449;NOT_FOUND;NOT_FOUND
MAYBE_RELEVANT;IEEE Xplore;Retrieval-Augmented Response Generation for Knowledge-Grounded Conversation in the Wild;Ahn, Yeonchan and Lee, Sang-Goo and Shim, Junho and Park, Jaehui;2022;10.1109/ACCESS.2022.3228964;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;Autonomous Robots and the SP Theory of Intelligence;Wolff, James Gerard;2014;10.1109/ACCESS.2014.2382753;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;Effective Natural Language Processing and Interpretable Machine Learning for Structuring CT Liver-Tumor Reports;Chuang, Yi-Hsuan and Su, Ja-Hwung and Han, Ding-Hong and Liao, Yi-Wen and Lee, Yeong-Chyi and Cheng, Yu-Fan and Hong, Tzung-Pei and Li, Katherine Shu-Min and Ou, Hsin-You and Lu, Yi and Wang, Chih-Chi;2022;10.1109/ACCESS.2022.3218646;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;WirelessLLM: Empowering Large Language Models Towards Wireless Intelligence;Shao, Jiawei and Tong, Jingwen and Wu, Qiong and Guo, Wei and Li, Zijian and Lin, Zehong and Zhang, Jun;2024;10.23919/JCIN.2024.10582827;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;Document-Level Text Classification Using Single-Layer Multisize Filters Convolutional Neural Network;Akhter, Muhammad Pervez and Jiangbin, Zheng and Naqvi, Irfan Raza and Abdelmajeed, Mohammed and Mehmood, Atif and Sadiq, Muhammad Tariq;2020;10.1109/ACCESS.2020.2976744;NOT_FOUND;NOT_FOUND
MAYBE_RELEVANT;IEEE Xplore;An Innovative Solution to Design Problems: Applying the Chain-of-Thought Technique to Integrate LLM-based Agents with Concept Generation Methods;Ge, Shijun and Sun, Yuanbo and Cui, Yin and Wei, Dapeng;2024;10.1109/ACCESS.2024.3494054;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;Knowledge Based Deep Inception Model for Web Page Classification;Gupta, Amit and Bhatia, Rajesh;2021;10.13052/jwe1540-9589.2075;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;A History and Theory of Textual Event Detection and Recognition;Chen, Yanping and Ding, Zehua and Zheng, Qinghua and Qin, Yongbin and Huang, Ruizhang and Shah, Nazaraf;2020;10.1109/ACCESS.2020.3034907;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;An Eclectic Approach for Enhancing Language Models Through Rich Embedding Features;Aldana-Bobadilla, Edwin and Sosa-Sosa, Victor Jesus and Molina-Villegas, Alejandro and Gazca-Hernandez, Karina and Olivas, Jose Angel;2024;10.1109/ACCESS.2024.3422971;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;Enhancing Semantic Code Search With Deep Graph Matching;Bibi, Nazia and Maqbool, Ayesha and Rana, Tauseef and Afzal, Farkhanda and Akgül, Ali and Eldin, Sayed M.;2023;10.1109/ACCESS.2023.3263878;NOT_FOUND;NOT_FOUND
MAYBE_RELEVANT;IEEE Xplore;RDguru: A Conversational Intelligent Agent for Rare Diseases;Yang, Jian and Shu, Liqi and Duan, Huilong and Li, Haomin;2024;10.1109/JBHI.2024.3464555;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;Architectural Synergies in Bi-Modal and Bi-Contrastive Learning;Gu, Yujia and Liu, Brian and Zhang, Tianlong and Sha, Xinye and Chen, Shiyong;2024;10.1109/ACCESS.2024.3457586;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;Classical Arabic Named Entity Recognition Using Variant Deep Neural Network Architectures and BERT;Alsaaran, Norah and Alrabiah, Maha;2021;10.1109/ACCESS.2021.3092261;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;Urdu Wikification and Its Application in Urdu News Recommendation System;Kanwal, Safia and Malik, Muhammad Kamran and Nawaz, Zubair and Mehmood, Khawar;2022;10.1109/ACCESS.2022.3208666;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;Enhancing Parameter Efficiency in Model Inference Using an Ultralight Inter-Transformer Linear Structure;Shi, Haoxiang and Sakai, Tetsuya;2024;10.1109/ACCESS.2024.3378518;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;Ground Camera Image and Large-Scale 3-D Image-Based Point Cloud Registration Based on Learning Domain Invariant Feature Descriptors;Liu, Weiquan and Lai, Baiqi and Wang, Cheng and Cai, Guorong and Su, Yanfei and Bian, Xuesheng and Li, Yongchuan and Chen, Shuting and Li, Jonathan;2021;10.1109/JSTARS.2020.3035359;NOT_FOUND;NOT_FOUND
MAYBE_RELEVANT;IEEE Xplore;Entity Linking: An Issue to Extract Corresponding Entity With Knowledge Base;Wu, Gongqing and He, Ying and Hu, Xuegang;2018;10.1109/ACCESS.2017.2787787;NOT_FOUND;NOT_FOUND
MAYBE_RELEVANT;IEEE Xplore;Enhanced Natural Language Interface for Web-Based Information Retrieval;Bai, Tian and Ge, Yan and Guo, Shuyu and Zhang, Zhenting and Gong, Leiguang;2021;10.1109/ACCESS.2020.3048164;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;Short Text Understanding Combining Text Conceptualization and Transformer Embedding;Li, Jun and Huang, Guimin and Chen, Jianheng and Wang, Yabing;2019;10.1109/ACCESS.2019.2938303;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;An Analytical Analysis of Text Stemming Methodologies in Information Retrieval and Natural Language Processing Systems;Jabbar, Abdul and Iqbal, Sajid and Tamimy, Manzoor Ilahi and Rehman, Amjad and Bahaj, Saeed Ali and Saba, Tanzila;2023;10.1109/ACCESS.2023.3332710;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;Visual Analysis of Citation Context-Based Article Influence Ranking;Shi, Chen and Wang, Haoxuan and Chen, Binjie and Liu, Yuhua and Zhou, Zhiguang;2019;10.1109/ACCESS.2019.2932051;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;Geospatial Big Data: Survey and Challenges;Wu, Jiayang and Gan, Wensheng and Chao, Han-Chieh and Yu, Philip S.;2024;10.1109/JSTARS.2024.3438376;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;Consumer Document Analytical Accelerator Hardware;Radhakrishnan, Aswani and Mahapatra, Dibyasha and James, Alex;2023;10.1109/ACCESS.2023.3237463;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;Exploring Topic Coherence With PCC-LDA and BERT for Contextual Word Generation;Rachamadugu, Sandeep Kumar and Pushphavathi, T. P. and Bhatia Khan, Surbhi and Alojail, Mohammad;2024;10.1109/ACCESS.2024.3477992;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;A Novel Joint Extraction Model for Entity Relations Using Interactive Encoding and Visual Attention;Yu, Youren and Zhang, Yangsen and Liu, Xueyang and Zhu, Siwen;2023;10.1109/ACCESS.2023.3335623;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;An Overview of Data Extraction From Invoices;Saout, Thomas and Lardeux, Frédéric and Saubion, Frédéric;2024;10.1109/ACCESS.2024.3360528;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;Evaluating the Effectiveness of GPT Large Language Model for News Classification in the IPTC News Ontology;Fatemi, Bahareh and Rabbi, Fazle and Opdahl, Andreas L.;2023;10.1109/ACCESS.2023.3345414;NOT_FOUND;NOT_FOUND
MAYBE_RELEVANT;IEEE Xplore;Automated Question-Answering for Interactive Decision Support in Operations & Maintenance of Wind Turbines;Chatterjee, Joyjit and Dethlefs, Nina;2022;10.1109/ACCESS.2022.3197167;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;An Optimized LSTM-Based Augmented Language Model (FLSTM-ALM) Using Fox Algorithm for Automatic Essay Scoring Prediction;Chassab, Ridha Hussein and Zakaria, Lailatul Qadri and Tiun, Sabrina;2024;10.1109/ACCESS.2024.3381619;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;Research on Topic Recognition of Network Sensitive Information Based on SW-LDA Model;Xu, Guixian and Wu, Xu and Yao, Haishen and Li, Fan and Yu, Ziheng;2019;10.1109/ACCESS.2019.2897475;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;Improving Pseudo-Relevance Feedback With Neural Network-Based Word Representations;Xu, Bo and Lin, Hongfei and Lin, Yuan and Yang, Liang and Xu, Kan;2018;10.1109/ACCESS.2018.2876425;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;A Novel Class-Center Vector Model for Text Classification Using Dependencies and a Semantic Dictionary;Zhu, Xinhua and Xu, Qingting and Chen, Yishan and Chen, Hongchao and Wu, Tianjun;2020;10.1109/ACCESS.2019.2954106;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;Collocation Extraction Using Web Feedback Data;Lin, Jianfang and Li, Sheng and Cai, Yuhan;2009;NOT_FOUND;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;ANEC: An Amharic Named Entity Corpus and Transformer Based Recognizer;Jibril, Ebrahim Chekol and Tantuğ, A. Cüneyd;2023;10.1109/ACCESS.2023.3243468;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;Large Scale Category-Structured Image Retrieval for Object Identification Through Supervised Learning of CNN and SURF-Based Matching;Li, Xiaoqing and Yang, Jiansheng and Ma, Jinwen;2020;10.1109/ACCESS.2020.2982560;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;Attention Retrieval Model for Entity Relation Extraction From Biological Literature;Srivastava, Prashant and Bej, Saptarshi and Schultz, Kristian and Yordanova, Kristina and Wolkenhauer, Olaf;2022;10.1109/ACCESS.2022.3154820;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;READSUM: Retrieval-Augmented Adaptive Transformer for Source Code Summarization;Choi, Yunseok and Na, Cheolwon and Kim, Hyojun and Lee, Jee-Hyong;2023;10.1109/ACCESS.2023.3271992;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;Graph-Based Text Representation and Matching: A Review of the State of the Art and Future Challenges;Osman, Ahmed Hamza and Barukub, Omar Mohammed;2020;10.1109/ACCESS.2020.2993191;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;VB-PTC: Visual Block Multi-Record Text Extraction Based on Sensor Network Page Type Conversion;Gong, Jibing and Zhang, Hekai and Du, Weixia and Li, Huanhuan and Wen, Hongnian;2020;10.1109/ACCESS.2020.3024194;NOT_FOUND;NOT_FOUND
MAYBE_RELEVANT;IEEE Xplore;A Review of Question Answering Systems;Ojokoh, Bolanle and Adebisi, Emmanuel;2018;10.13052/jwe1540-9589.1785;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;A Non-Exclusive Multi-Class Convolutional Neural Network for the Classification of Functional Requirements in AUTOSAR Software Requirement Specification Text;Jp, Sanjanasri and Menon, Vijay Krishna and Soman, KP and Ojha, Atul K. R.;2022;10.1109/ACCESS.2022.3217752;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;A Study of the Effects of Stemming Strategies on Arabic Document Classification;Alhaj, Yousif A. and Xiang, Jianwen and Zhao, Dongdong and Al-Qaness, Mohammed A. A. and Abd Elaziz, Mohamed and Dahou, Abdelghani;2019;10.1109/ACCESS.2019.2903331;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;FoleyGAN: Visually Guided Generative Adversarial Network-Based Synchronous Sound Generation in Silent Videos;Ghose, Sanchita and Prevost, John J.;2023;10.1109/TMM.2022.3177894;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;BaNeP: An End-to-End Neural Network Based Model for Bangla Parts-of-Speech Tagging;Ovi, Jesan Ahammed and Islam, Md. Ashraful and Karim, Md. Rezaul;2022;10.1109/ACCESS.2022.3208269;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;On the Effectiveness of Pre-Trained Language Models for Legal Natural Language Processing: An Empirical Study;Song, Dezhao and Gao, Sally and He, Baosheng and Schilder, Frank;2022;10.1109/ACCESS.2022.3190408;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;Transformer-Based Discriminative and Strong Representation Deep Hashing for Cross-Modal Retrieval;Zhou, Suqing and Han, Yu and Chen, Ning and Huang, Siyu and Igorevich, Kostromitin Konstantin and Luo, Jia and Zhang, Peiying;2023;10.1109/ACCESS.2023.3339581;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;MediGPT: Exploring Potentials of Conventional and Large Language Models on Medical Data;Abu Tareq Rony, Mohammad and Shariful Islam, Mohammad and Sultan, Tipu and Alshathri, Samah and El-Shafai, Walid;2024;10.1109/ACCESS.2024.3428918;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;MPSC: A Multiple-Perspective Semantics-Crossover Model for Matching Sentences;Peng, Dunlu and Wu, Shaohong and Liu, Cong;2019;10.1109/ACCESS.2019.2915937;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;A Review of the Analytics Techniques for an Efficient Management of Online Forums: An Architecture Proposal;Peral, Jesús and Ferrández, Antonio and Mora, Higinio and Gil, David and Kauffmann, Erick;2019;10.1109/ACCESS.2019.2892987;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;Grounded Vocabulary for Image Retrieval Using a Modified Multi-Generator Generative Adversarial Network;Kim, Kuekyeng and Park, Chanjun and Seo, Jaehyung and Lim, Heuiseok;2021;10.1109/ACCESS.2021.3122547;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;A Semantic Based Approach for Topic Evaluation in Information Filtering;Xu, Yue and Nguyen, Hanh and Li, Yuefeng;2020;10.1109/ACCESS.2020.2985079;NOT_FOUND;NOT_FOUND
MAYBE_RELEVANT;IEEE Xplore;Medical Information Extraction With NLP-Powered QABots: A Real-World Scenario;Crema, Claudio and Verde, Federico and Tiraboschi, Pietro and Marra, Camillo and Arighi, Andrea and Fostinelli, Silvia and Giuffré, Guido Maria and Maschio, Vera Pacoova Dal and L'Abbate, Federica and Solca, Federica and Poletti, Barbara and Silani, Vincenzo and Rotondo, Emanuela and Borracci, Vittoria and Vimercati, Roberto and Crepaldi, Valeria and Inguscio, Emanuela and Filippi, Massimo and Caso, Francesca and Rosati, Alessandra Maria and Quaranta, Davide and Binetti, Giuliano and Pagnoni, Ilaria and Morreale, Manuela and Burgio, Francesca and Stanzani-Maserati, Michelangelo and Capellari, Sabina and Pardini, Matteo and Girtler, Nicola and Piras, Federica and Piras, Fabrizio and Lalli, Stefania and Perdixi, Elena and Lombardi, Gemma and Tella, Sonia Di and Costa, Alfredo and Capelli, Marco and Fundarò, Cira and Manera, Marina and Muscio, Cristina and Pellencin, Elisa and Lodi, Raffaele and Tagliavini, Fabrizio and Redolfi, Alberto;2024;10.1109/JBHI.2024.3450118;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;Case based reasoning in conflict negotiation in concurrent engineering;Xu, Wensheng and Xiong, Guangleng and Gao, Feng and Zhang, Xinfang;1999;NOT_FOUND;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;Mapping Bug Reports to Relevant Source Code Files Based on the Vector Space Model and Word Embedding;Liu, Guangliang and Lu, Yang and Shi, Ke and Chang, Jingfei and Wei, Xing;2019;10.1109/ACCESS.2019.2922686;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;ADAGENT: Anomaly Detection Agent With Multimodal Large Models in Adverse Environments;Zhang, Miao and Shen, Yiqing and Yin, Jun and Lu, Shuai and Wang, Xueqian;2024;10.1109/ACCESS.2024.3480250;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;Impact of Stemming and Word Embedding on Deep Learning-Based Arabic Text Categorization;Almuzaini, Huda Abdulrahman and Azmi, Aqil M.;2020;10.1109/ACCESS.2020.3009217;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;Face Detection in Security Monitoring Based on Artificial Intelligence Video Retrieval Technology;Dong, Zuolin and Wei, Jiahong and Chen, Xiaoyu and Zheng, Pengfei;2020;10.1109/ACCESS.2020.2982779;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;A Deep Learning Approach for Robust Detection of Bots in Twitter Using Transformers;Martín-Gutiérrez, David and Hernández-Peñaloza, Gustavo and Hernández, Alberto Belmonte and Lozano-Diez, Alicia and Álvarez, Federico;2021;10.1109/ACCESS.2021.3068659;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;An Empirical Study on Compliance with Ranking Transparency in the Software Documentation of EU Online Platforms;Sovrano, Francesco and Lognoul, Michaël and Bacchelli, Alberto;2024;NOT_FOUND;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;Transfer Learning for Arabic Named Entity Recognition With Deep Neural Networks;Al-Smadi, Mohammad and Al-Zboon, Saad and Jararweh, Yaser and Juola, Patrick;2020;10.1109/ACCESS.2020.2973319;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;Automatic Methods and Neural Networks in Arabic Texts Diacritization: A Comprehensive Survey;Almanea, Manar M.;2021;10.1109/ACCESS.2021.3122977;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;Make It Directly: Event Extraction Based on Tree-LSTM and Bi-GRU;Yu, Wentao and Yi, Mianzhu and Huang, Xiaohui and Yi, Xiaoyu and Yuan, Qingjun;2020;10.1109/ACCESS.2020.2965964;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;A Survey of Person Name Disambiguation on the Web;Delgado, Agustín D. and Montalvo, Soto and Martínez Unanue, Raquel and Fresno, Víctor;2018;10.1109/ACCESS.2018.2874891;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;Extending Embedding Representation by Incorporating Latent Relations;Yang, Gao and Wenbo, Wang and Qian, Liu and Heyan, Huang and Li, Yuefeng;2018;10.1109/ACCESS.2018.2866531;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;Leveraging the Synergy of IPv6, Generative AI, and Web Engineering to Create a Big Data-Driven Education Platform;Yongli, Gao and Qi, Dong and Zhipeng, Chen;2024;10.13052/jwe1540-9589.2321;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;A Bootstrapping Approach With CRF and Deep Learning Models for Improving the Biomedical Named Entity Recognition in Multi-Domains;Kim, Juae and Ko, Youngjoong and Seo, Jungyun;2019;10.1109/ACCESS.2019.2914168;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;Improving Ranking Using Hybrid Custom Embedding Models on Persian Web;Bostan, Shekoofe and Zareh Bidoki, Ali Mohammad and Pajoohan, Mohammad-Reza;2023;10.13052/jwe1540-9589.2253;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;Optimizing Satellite-Based Latent Heating Rate Profiling Using a Convolutional Neural Network Heating (CNNH) Algorithm;Zhao, Hongwei and Yang, Shuping and Wu, Qiong and Chen, Lin and Zhang, Peng and Li, Rui;2024;10.1109/TGRS.2024.3466952;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;Interactive Self-Attentive Siamese Network for Biomedical Sentence Similarity;Li, Zhengguang and Lin, Hongfei and Zheng, Wei and Tadesse, Michael M. and Yang, Zhihao and Wang, Jian;2020;10.1109/ACCESS.2020.2985685;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;A Comparative Assessment of Unsupervised Keyword Extraction Tools;Nadim, Mohammad and Akopian, David and Matamoros, Adolfo;2023;10.1109/ACCESS.2023.3344032;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;Information-Centric Mobile Networks: A Survey, Discussion, and Future Research Directions;Fayyaz, Sana and Atif Ur Rehman, Muhammad and Salah Ud Din, Muhammad and Biswas, Md. Israfil and Bashir, Ali Kashif and Kim, Byung-Seo;2023;10.1109/ACCESS.2023.3268775;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;IEEE Xplore;A Novel Shape Representation Method for Complex Trademark Image;Ye, Ben and Cai, Zhanchuan and Lan, Ting and Xiao, Youqing;2019;10.1109/ACCESS.2019.2912230;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;ProQuest;Nanjing Yunjin intelligent question-answering system based on knowledge graphs and retrieval augmented generation technology;"Xu, Liang;Lu, Lu;Liu, Minglu;Song, Chengxuan;Wu, Lizhen";2024;10.1186/s40494-024-01231-3;NOT_FOUND;Nanjing Yunjin, a traditional Chinese silk weaving craft, is celebrated globally for its unique local characteristics and exquisite workmanship, forming an integral part of the world's intangible cultural heritage. However, with the advancement of information technology, the experiential knowledge of the Nanjing Yunjin production process is predominantly stored in text format. As a highly specialized and vertical domain, this information is not readily convert into usable data. Previous studies on a knowledge graph-based Nanjing Yunjin Question-Answering System have partially addressed this issue. However, knowledge graphs need to be constantly updated and rely on predefined entities and relationship types. Faced with ambiguous or complex natural language problems, knowledge graph information retrieval faces some challenges. Therefore, this study proposes a Nanjing Yunjin Question-Answering System that integrates Knowledge Graphs and Retrieval Augmented Generation techniques. In this system, the ROBERTA model is first utilized to vectorize Nanjing Yunjin textual information, delving deep into textual semantics to unveil its profound cultural connotations. Additionally, the FAISS vector database is employed for efficient storage and retrieval of Nanjing Yunjin information, achieving a deep semantic match between questions and answers. Ultimately, related retrieval results are fed into the Large Language Model for enhanced generation, aiming for more accurate text generation outcomes and improving the interpretability and logic of the Question-Answering System. This research merges technologies like text embedding, vectorized retrieval, and natural language generation, aiming to overcome the limitations of knowledge graphs-based Question-Answering System in terms of graph updating, dependency on predefined types, and semantic understanding. System implementation and testing have shown that the Nanjing Yunjin Intelligent Question-Answering System, constructed on the basis of Knowledge Graphs and Retrieval Augmented Generation, possesses a broader knowledge base that considers context, resolving issues of polysemy, vague language, and sentence ambiguity, and efficiently and accurately generates answers to natural language queries. This significantly facilitates the retrieval and utilization of Yunjin knowledge, providing a paradigm for constructing Question-Answering System for other intangible cultural heritages, and holds substantial theoretical and practical significance for the deep exploration and discovery of the knowledge structure of human intangible heritage, promoting cultural inheritance and protection.
NOT_RELEVANT;ProQuest;Generative AI and the future of higher education: a threat to academic integrity or reformation? Evidence from multicultural perspectives: Revista de Universidad y Sociedad del Conocimiento;"Yusuf, Abdullahi;Pervin, Nasrin;Román-González, Marcos";2024;10.1186/s41239-024-00453-6;NOT_FOUND;In recent years, higher education (HE) globally has witnessed extensive adoption of technology, particularly in teaching and research. The emergence of generative Artificial Intelligence (GenAI) further accelerates this trend. However, the increasing sophistication of GenAI tools has raised concerns about their potential to automate teaching and research processes. Despite widespread research on GenAI in various fields, there is a lack of multicultural perspectives on its impact and concerns in HE. This study addresses this gap by examining the usage, benefits, and concerns of GenAI in higher education from a multicultural standpoint. We employed an online survey that collected responses from 1217 participants across 76 countries, encompassing a broad range of gender categories, academic disciplines, geographical locations, and cultural orientations. Our findings revealed a high level of awareness and familiarity with GenAI tools among respondents. A significant portion had prior experience and expressed the intention to continue using these tools, primarily for information retrieval and text paraphrasing. The study emphasizes the importance of GenAI integration in higher education, highlighting both its potential benefits and concerns. Notably, there is a strong correlation between cultural dimensions and respondents’ views on the benefits and concerns related to GenAI, including its potential as academic dishonesty and the need for ethical guidelines. We, therefore, argued that responsible use of GenAI tools can enhance learning processes, but addressing concerns may require robust policies that are responsive to cultural expectations. We discussed the findings and offered recommendations for researchers, educators, and policymakers, aiming to promote the ethical and effective integration of GenAI tools in higher education.
NOT_RELEVANT;ProQuest;A chat about actinic keratosis: Examining capabilities and user experience of ChatGPT as a digital health technology in dermato‐oncology;"Lent, Heather C.;Ortner, Vinzent K.;Karmisholt, Katrine E.;Wiegell, Stine R.;Nissen, Christoffer V.;Omland, Silje H.;Kamstrup, Maria R.;Togsverd‐Bo, Katrine;Haedersdal, Merete";2024;10.1002/jvc2.263;NOT_FOUND;Background The potential applications of artificial intelligence (AI) in dermatology are evolving rapidly. Chatbots are an emerging trend in healthcare that rely on large language models (LLMs) to generate answers to prompts from users. However, the factuality and user experience (UX) of such chatbots remain to be evaluated in the context of dermato‐oncology. Objectives To examine the potential of Chat Generative Pretrained Transformer (ChatGPT) as a reliable source of information in the context of actinic keratosis (AK) and to evaluate clinicians' attitudes and UX with regard to the chatbot. Methods A set of 38 clinical questions were compiled and entered as natural language queries in separate, individual conversation threads in ChatGPT (OpenAI, default GPT 3.5). Questions pertain to patient education, diagnosis, and treatment. ChatGPT's responses were presented to a panel of 7 dermatologists for rating of factual accuracy, currency of information, and completeness of the response. Attitudes towards ChatGTP were explored qualitatively and quantitatively using a validated user experience questionnaire (UEQ). Results ChatGPT answered 12 questions (31.6%) with an accurate, current, and complete response. ChatGPT performed best for questions on patient education, including pathogenesis of AK and potential risk factors, but struggled with diagnosis and treatment. Major deficits were seen in grading AK, providing up‐to‐date treatment guidance, and asserting incorrect information with unwarranted confidence. Further, responses were considered verbose with an average word count of 198 (SD 55) and overly alarming of the risk of malignant transformation. Based on UEQ responses, the expert panel considered ChatGPT an attractive and efficient tool, scoring highest for speed of information retrieval, but deemed the chatbot inaccurate and verbose, scoring lowest for clarity . Conclusions While dermatologists rated ChatGPT high in UX, the underlying LLMs that enable such chatbots require further development to guarantee accuracy and concision required in a clinical setting.
NOT_RELEVANT;ProQuest;exKidneyBERT: a language model for kidney transplant pathology reports and the crucial role of extended vocabularies;"Yang, Tiancheng;Sucholutsky, Ilia;Kuang-Yu, Jen;Schonlau, Matthias";2024;10.7717/peerj-cs.1888;NOT_FOUND;"Background Pathology reports contain key information about the patient’s diagnosis as well as important gross and microscopic findings. These information-rich clinical reports offer an invaluable resource for clinical studies, but data extraction and analysis from such unstructured texts is often manual and tedious. While neural information retrieval systems (typically implemented as deep learning methods for natural language processing) are automatic and flexible, they typically require a large domain-specific text corpus for training, making them infeasible for many medical subdomains. Thus, an automated data extraction method for pathology reports that does not require a large training corpus would be of significant value and utility. Objective To develop a language model-based neural information retrieval system that can be trained on small datasets and validate it by training it on renal transplant-pathology reports to extract relevant information for two predefined questions: (1) “What kind of rejection does the patient show?”; (2) “What is the grade of interstitial fibrosis and tubular atrophy (IFTA)?” Methods Kidney BERT was developed by pre-training Clinical BERT on 3.4K renal transplant pathology reports and 1.5M words. Then, exKidneyBERT was developed by extending Clinical BERT’s tokenizer with six technical keywords and repeating the pre-training procedure. This extended the model’s vocabulary. All three models were fine-tuned with information retrieval heads. Results The model with extended vocabulary, exKidneyBERT, outperformed Clinical BERT and Kidney BERT in both questions. For rejection, exKidneyBERT achieved an 83.3% overlap ratio for antibody-mediated rejection (ABMR) and 79.2% for T-cell mediated rejection (TCMR). For IFTA, exKidneyBERT had a 95.8% exact match rate. Conclusion ExKidneyBERT is a high-performing model for extracting information from renal pathology reports. Additional pre-training of BERT language models on specialized small domains does not necessarily improve performance. Extending the BERT tokenizer’s vocabulary library is essential for specialized domains to improve performance, especially when pre-training on small corpora."
NOT_RELEVANT;ProQuest;Deep learning-based information retrieval with normalized dominant feature subset and weighted vector model;"Eswaraiah, Poluru;Hussain, Syed";2024;10.7717/peerj-cs.1805;NOT_FOUND;Multimedia data, which includes textual information, is employed in a variety of practical computer vision applications. More than a million new records are added to social media and news sites every day, and the text content they contain has gotten increasingly complex. Finding a meaningful text record in an archive might be challenging for computer vision researchers. Most image searches still employ the tried and true language-based techniques of query text and metadata. Substantial work has been done in the past two decades on content-based text retrieval and analysis that still has limitations. The importance of feature extraction in search engines is often overlooked. Web and product search engines, recommendation systems, and question-answering activities frequently leverage these features. Extracting high-quality machine learning features from large text volumes is a challenge for many open-source software packages. Creating an effective feature set manually is a time-consuming process, but with deep learning, new actual feature demos from training data are analyzed. As a novel feature extraction method, deep learning has made great strides in text mining. Automatically training a deep learning model with the most pertinent text attributes requires massive datasets with millions of variables. In this research, a Normalized Dominant Feature Subset with Weighted Vector Model (NDFS-WVM) is proposed that is used for feature extraction and selection for information retrieval from big data using natural language processing models. The suggested model outperforms the conventional models in terms of text retrieval. The proposed model achieves 98.6% accuracy in information retrieval.
NOT_RELEVANT;ProQuest;Sentiment Analysis of African American Young Adult Novels: Detecting Emotion Patterns;"Haghanikar, Taraneh Matloob, PhD;Filatova, Elena, PhD";2024;NOT_FOUND;NOT_FOUND;Emotion detection is valuable for various practical applications such as customer service, social media, product analysis, human-computer interaction, information retrieval, as well as for social and literary analysis. In this paper, we concentrate on identifying emotions in the text of three young adult (YA) novels for the purpose of literary analysis. This initial step showcases the capabilities of sentiment analysis and natural language processing (NLP) in literature. Emotions play a significant role in the interaction between fictional characters, their experiences, and the events in the story. In this study, we identify representations of Plutchik's eight basic emotions in three award-winning African American YA novels and focus on the sentiment associated with the emotion words in the selected books and demonstrate the distribution of positive and negative sentiment words across the novels. We adopt Mohammad's emotion analysis technique1 and use manually created lexicons that contain the eight primary emotions. This work provides a new perspective on reading and analyzing diverse YA books and offers methods that can easily be applied to literary texts to reveal their deeper meanings.
MAYBE_RELEVANT;ProQuest;TravelRAG: A Tourist Attraction Retrieval Framework Based on Multi-Layer Knowledge Graph;"Song, Sihan;Yang, Chuncheng;Xu, Li;Shang, Haibin;Li, Zhuo;Chang, Yinghui";2024;10.3390/ijgi13110414;NOT_FOUND;A novel framework called TravelRAG is introduced in this paper, which is built upon a large language model (LLM) and integrates Retrieval-Augmented Generation (RAG) with knowledge graphs to create a retrieval system framework designed for the tourism domain. This framework seeks to address the challenges LLMs face in providing precise and contextually appropriate responses to domain-specific queries in the tourism field. TravelRAG extracts information related to tourist attractions from User-Generated Content (UGC) on social media platforms and organizes it into a multi-layer knowledge graph. The travel knowledge graph serves as the core retrieval source for the LLM, enhancing the accuracy of information retrieval and significantly reducing the generation of erroneous or fabricated responses, often termed as “hallucinations”. As a result, the accuracy of the LLM’s output is enhanced. Comparative analyses with traditional RAG pipelines indicate that TravelRAG significantly boosts both the retrieval efficiency and accuracy, while also greatly reducing the computational cost of model fine-tuning. The experimental results show that TravelRAG not only outperforms traditional methods in terms of retrieval accuracy but also better meets user needs for content generation.
MAYBE_RELEVANT;ProQuest;Integrating a Virtual Assistant by Using the RAG Method and VERTEX AI Framework at Algebra University;"Morić, Zlatan;Mršić, Leo;Filjak, Mario;Đambić, Goran";2024;10.3390/app142210748;NOT_FOUND;The development and testing of a virtual assistant (VA) designed to enhance information retrieval and support in an academic environment are presented in this paper, with the Retrieval-Augmented Generation (RAG) approach being utilized alongside Google’s VERTEX AI Palm-2 model. A novel integration of RAG with contextual learning is introduced in this study, specifically for applications in university contact centers, where accuracy and relevance are considered paramount. The effectiveness of the VA was evaluated through user testing, focusing on two primary hypotheses: first, that the VA can achieve accurate interpretation and response to queries with context-based information, and second, that the VA minimizes potential harm from erroneous responses. In total, 187 participants were involved in the testing, and a diverse set of inquiries was utilized, resulting in 561 query–response interactions that were analyzed. It was shown that contextual data significantly reduced hallucinations and increased response accuracy, thereby underscoring the value of the RAG method in applications requiring high levels of specificity. Furthermore, the study provides empirical insights into the impact of AI-generated hallucinations and response inconsistencies, particularly about structured or procedural data. A framework for mitigating these challenges in future implementations is also offered. The scalability and adaptability of the RAG method in specialized academic contexts are demonstrated in this work, with broader implications for integrating AI-driven VAs across educational and professional domains being highlighted.
NOT_RELEVANT;ProQuest;Optimizing Text Summarization with Sentence Clustering and Natural Language Processing;PDF;2024;10.14569/IJACSA.2024.01510115;NOT_FOUND;Text summarization is an important task in natural language processing (NLP), with significant implications for information retrieval and content management. Traditional summarization methods often struggle with issues like redundancy, loss of key information, and inability to capture the underlying semantic structure of the text. This paper addresses these challenges by presenting an advanced approach to extractive summarization, which integrates clustering-based sentence selection with the BART model. The proposed method tackles the problem of redundancy by using Term Frequency-Inverse Document Frequency (TF-IDF) for feature extraction, followed by K-means clustering to group similar sentences. This clustering step is designed to reduce redundancy by ensuring that each cluster represents a distinct theme or topic. Representative sentences are then selected from these clusters based on their cosine similarity to a user query, which helps in retaining the most relevant information. These selected sentences are then fed into the BART model to generate the final abstractive summary. This combination of extractive and abstractive techniques addresses the common problem of information loss, ensuring that the summary is both comprehensive and coherent. The approach is evaluated using the CNN/DailyMail and XSum datasets, which are widely recognized benchmarks in the summarization domain. Results assessed through ROUGE metrics demonstrate that the proposed model substantially improves summarization quality compared to existing benchmarks.
NOT_RELEVANT;ProQuest;CPEQA: A Large Language Model Based Knowledge Base Retrieval System for Chinese Confidentiality Knowledge Question Answering;"Cao, Jian;Cao, Jiuxin";2024;10.3390/electronics13214195;NOT_FOUND;Large language models (LLMs) have exhibited remarkable performance on various natural language processing (NLP) tasks, particularly in the construction of the intelligent question-answering system. These systems, especially in specialized fields, usually rely on NLP through the retrieval of corpus and answering databases to efficiently provide accurate and concise answers. This paper focuses on the national confidentiality publicity and education field, aiming to address the dilemma of inaccurate knowledge retrieval in this field. Therefore, we design an intelligent confidentiality question-answering system CPEQA by comprehensively utilizing the LLMs platform and information retrieval technique. CPEQA is capable of providing professional answers to questions about Chinese confidentiality publicity and education raised by users. Additionally, we also integrate the conventional database retrieval technique and LLMs into the database query construction, enabling CPEQA to perform real-time queries and data analysis for both single-table and multi-table querying tasks. Through extensive experiments with generated query sentences, we show both methodological comparisons and empirical evaluations of CPEQA’s performance. Experimental results indicate that CPEQA has achieved competitive results on answering precision, recall rate and other metrics. Finally, we explore the challenges of the CPEQA system associated with these techniques and outline potential avenues for future research in this emerging field.
NOT_RELEVANT;ProQuest;Automating Systematic Literature Reviews with Retrieval-Augmented Generation: A Comprehensive Overview;"Han, Binglan;Teo Susnjak;Mathrani, Anuradha";2024;10.3390/app14199103;NOT_FOUND;This study examines Retrieval-Augmented Generation (RAG) in large language models (LLMs) and their significant application for undertaking systematic literature reviews (SLRs). RAG-based LLMs can potentially automate tasks like data extraction, summarization, and trend identification. However, while LLMs are exceptionally proficient in generating human-like text and interpreting complex linguistic nuances, their dependence on static, pre-trained knowledge can result in inaccuracies and hallucinations. RAG mitigates these limitations by integrating LLMs’ generative capabilities with the precision of real-time information retrieval. We review in detail the three key processes of the RAG framework—retrieval, augmentation, and generation. We then discuss applications of RAG-based LLMs to SLR automation and highlight future research topics, including integration of domain-specific LLMs, multimodal data processing and generation, and utilization of multiple retrieval sources. We propose a framework of RAG-based LLMs for automating SRLs, which covers four stages of SLR process: literature search, literature screening, data extraction, and information synthesis. Future research aims to optimize the interaction between LLM selection, training strategies, RAG techniques, and prompt engineering to implement the proposed framework, with particular emphasis on the retrieval of information from individual scientific papers and the integration of these data to produce outputs addressing various aspects such as current status, existing gaps, and emerging trends.
NOT_RELEVANT;ProQuest;DAFE-MSGAT: Dual-Attention Feature Extraction and Multi-Scale Graph Attention Network for Polyphonic Piano Transcription;"Cao, Rui;Liang, Zushuang;Zheng, Yan;Liu, Bing";2024;10.3390/electronics13193939;NOT_FOUND;Automatic music transcription (AMT) aims to convert raw audio signals into symbolic music. This is a highly challenging task in the fields of signal processing and artificial intelligence, and it holds significant application value in music information retrieval (MIR). Existing methods based on convolutional neural networks (CNNs) often fall short in capturing the time-frequency characteristics of audio signals and tend to overlook the interdependencies between notes when processing polyphonic piano with multiple simultaneous notes. To address these issues, we propose a dual attention feature extraction and multi-scale graph attention network (DAFE-MSGAT). Specifically, we design a dual attention feature extraction module (DAFE) to enhance the frequency and time-domain features of the audio signal, and we utilize a long short-term memory network (LSTM) to capture the temporal features within the audio signal. We introduce a multi-scale graph attention network (MSGAT), which leverages the various implicit relationships between notes to enhance the interaction between different notes. Experimental results demonstrate that our model achieves high accuracy in detecting the onset and offset of notes on public datasets. In both frame-level and note-level metrics, DAFE-MSGAT achieves performance comparable to the state-of-the-art methods, showcasing exceptional transcription capabilities.
NOT_RELEVANT;ProQuest;An Open-Domain Search Quiz Engine Based on Transformer;PDF;2024;10.14569/IJACSA.2024.01509103;NOT_FOUND;"As the volume of information on the Internet continues to grow exponentially, efficient retrieval of relevant data has become a significant challenge. Traditional keyword matching techniques, while useful, often fall short in addressing the complex and varied queries users present. This paper introduces a novel approach to automated question and answer systems by integrating deep learning and natural language processing (NLP) technologies. Specifically, it combines the Transformer model with the HowNet knowledge base to enhance semantic understanding and contextual relevance of responses. The proposed system architecture includes layers for word embedding, Transformer encoding, attention mechanisms, and Bi-directional Long Short-Term Memory (Bi-LSTM) processing, enabling sophisticated semantic matching and implication recognition. Using the BQ Corpus dataset in the banking and finance domain, the system demonstrated substantial improvements in accuracy and F1-score over existing models. The primary contributions of this research are threefold: (1) the introduction of a semantic fusion approach using HowNet for enhanced contextual understanding, (2) the optimization of Transformer-based deep learning techniques for Q&amp;A systems, and (3) a comprehensive evaluation using the BQ Corpus dataset, demonstrating significant improvements in accuracy and F1-score over baseline models. These contributions have important implications for improving the handling of complex and synonym-rich queries in automated Q&amp;A systems. The experimental results highlight that the integrated approach significantly enhances the performance of automated Q&amp;A systems, offering a more efficient and accurate means of information retrieval. This advancement is particularly crucial in the era of big data and Web 3.0, where the ability to quickly and accurately access relevant information is essential for both users and organizations."
MAYBE_RELEVANT;ProQuest;Interactive ChatBot for PDF Content Conversation Using an LLM Language Model;PDF;2024;10.14569/IJACSA.2024.01509105;NOT_FOUND;Natural Language Processing (NLP) leverages Artificial Intelligence (AI) to enable computer programs to understand and generate human language. ChatGPT has recently become popular in assignment accomplishment. This project aims to develop and improve an interactive PDF chat application using OpenAI's language model (LLM), specifically GPT-3.5, integrated with Streamlit and LangChain frameworks to assist in learning process. The application enhances user interaction with documents by providing real-time text extraction, summarization, translation, and user-defined question-answering to increase learning opportunities. Key features include obtaining document summaries, multilingual support for improved accessibility, and a document preview section with features such as zoom, rotation, and download. Although it currently faces limitations in handling image-rich PDFs, future enhancements include better image rendering, conversation history, and query download features. Overall, this interactive chatbot model aims to streamline document interaction, making information retrieval efficient and user-friendly.
NOT_RELEVANT;ProQuest;A Hybrid Approach to Ontology Construction for the Badini Kurdish Language;"Media Azzat;Karwan Jacksi;Ali, Ismael";2024;10.3390/info15090578;NOT_FOUND;Semantic ontologies have been widely utilized as crucial tools within natural language processing, underpinning applications such as knowledge extraction, question answering, machine translation, text comprehension, information retrieval, and text summarization. While the Kurdish language, a low-resource language, has been the subject of some ontological research in other dialects, a semantic web ontology for the Badini dialect remains conspicuously absent. This paper addresses this gap by presenting a methodology for constructing and utilizing a semantic web ontology for the Badini dialect of the Kurdish language. A Badini annotated corpus (UOZBDN) was created and manually annotated with part-of-speech (POS) tags. Subsequently, an HMM-based POS tagger model was developed using the UOZBDN corpus and applied to annotate additional text for ontology extraction. Ontology extraction was performed by employing predefined rules to identify nouns and verbs from the model-annotated corpus and subsequently forming semantic predicates. Robust methodologies were adopted for ontology development, resulting in a high degree of precision. The POS tagging model attained an accuracy of 95.04% when applied to the UOZBDN corpus. Furthermore, a manual evaluation conducted by Badini Kurdish language experts yielded a 97.42% accuracy rate for the extracted ontology.
NOT_RELEVANT;ProQuest;Cross-Lingual Short-Text Semantic Similarity for Kannada–English Language Pair;"Muralikrishna, S N;Raghurama Holla;Harivinod, N;Ganiga, Raghavendra";2024;10.3390/computers13090236;NOT_FOUND;Analyzing the semantic similarity of cross-lingual texts is a crucial part of natural language processing (NLP). The computation of semantic similarity is essential for a variety of tasks such as evaluating machine translation systems, quality checking human translation, information retrieval, plagiarism checks, etc. In this paper, we propose a method for measuring the semantic similarity of Kannada–English sentence pairs that uses embedding space alignment, lexical decomposition, word order, and a convolutional neural network. The proposed method achieves a maximum correlation of 83% with human annotations. Experiments on semantic matching and retrieval tasks resulted in promising results in terms of precision and recall.
NOT_RELEVANT;ProQuest;Combining Semantic Matching, Word Embeddings, Transformers, and LLMs for Enhanced Document Ranking: Application in Systematic Reviews;"Mitrov, Goran;Stanoev, Boris;Gievska, Sonja;Mirceva, Georgina;Zdravevski, Eftim";2024;10.3390/bdcc8090110;NOT_FOUND;The rapid increase in scientific publications has made it challenging to keep up with the latest advancements. Conducting systematic reviews using traditional methods is both time-consuming and difficult. To address this, new review formats like rapid and scoping reviews have been introduced, reflecting an urgent need for efficient information retrieval. This challenge extends beyond academia to many organizations where numerous documents must be reviewed in relation to specific user queries. This paper focuses on improving document ranking to enhance the retrieval of relevant articles, thereby reducing the time and effort required by researchers. By applying a range of natural language processing (NLP) techniques, including rule-based matching, statistical text analysis, word embeddings, and transformer- and LLM-based approaches like Mistral LLM, we assess the article’s similarities to user-specific inputs and prioritize them according to relevance. We propose a novel methodology, Weighted Semantic Matching (WSM) + MiniLM, combining the strengths of the different methodologies. For validation, we employ global metrics such as precision at K, recall at K, average rank, median rank, and pairwise comparison metrics, including higher rank count, average rank difference, and median rank difference. Our proposed algorithm achieves optimal performance, with an average recall at 1000 of 95% and an average median rank of 185 for selected articles across the five datasets evaluated. These findings give promising results in pinpointing the relevant articles and reducing the manual work.
MAYBE_RELEVANT;ProQuest;Design and Implementation of an Interactive Question-Answering System with Retrieval-Augmented Generation for Personalized Databases;"Byun, Jaeyeon;Kim, Bokyeong;Cha, Kyung-Ae;Lee, Eunhyung";2024;10.3390/app14177995;NOT_FOUND;This study introduces a novel approach to personalized information retrieval by integrating retrieval augmentation generation (RAG) with a personalized database system. Recent advancements in large language models (LLMs) have shown impressive text generation capabilities but face limitations in knowledge accuracy and hallucinations. Our research addresses these challenges by combining LLMs with structured, personalized data to enhance search precision and relevance. By tagging keywords within personal documents and organizing information into context-based categories, users can conduct efficient searches within their data repositories. We conducted experiments using the GPT-3.5 and text-embedding-ada-002 models and evaluated the RAG assessment framework with five different language models and two embedding models. Our results indicate that the combination of GPT-3.5 and text-embedding-ada-002 is effective for a personalized database question-answering system, with potential for various language models depending on the application. Our approach offers improved accuracy, real-time data updates, and enhanced user experience, making a significant contribution to information retrieval by LLMs and impacting various artificial intelligence applications.
NOT_RELEVANT;ProQuest;Leveraging Generative AI in Short Document Indexing;"Bouzid, Sara;Piron, Loïs";2024;10.3390/electronics13173563;NOT_FOUND;The efficiency of information retrieval systems primarily depends on the effective representation of documents during query processing. This representation is mainly constructed from relevant document terms identified and selected during their indexing, which are then used for retrieval. However, when documents contain only a few features, such as in short documents, the resulting representation may be information-poor due to a lack of index terms and their lack of relevance. Although document representation can be enriched using techniques like word embeddings, these techniques require large pre-trained datasets, which are often unavailable in the context of domain-specific short documents. This study investigates a new approach to enrich document representation during indexing using generative AI. In the proposed approach, relevant terms extracted from documents and preprocessed for indexing are enriched with a list of key terms suggested by a large language model (LLM). After conducting a small benchmark of several renowned LLM models for key term suggestions from a set of short texts, the GPT-4o model was chosen to experiment with the proposed indexing approach. The findings of this study yielded notable results, demonstrating that generative AI can efficiently fill the knowledge gap in document representation, regardless of the retrieval technique used.
NOT_RELEVANT;ProQuest;Help-Seeking Situations Related to Visual Interactions on Mobile Platforms and Recommended Designs for Blind and Visually Impaired Users;"Xie, Iris;Choi, Wonchan;Wang, Shengang;Lee, Hyun Seung;Hong, Bo Hyun;Ning-Chiao, Wang;Cudjoe, Emmanuel Kwame";2024;10.3390/jimaging10080205;NOT_FOUND;While it is common for blind and visually impaired (BVI) users to use mobile devices to search for information, little research has explored the accessibility issues they encounter in their interactions with information retrieval systems, in particular digital libraries (DLs). This study represents one of the most comprehensive research projects, investigating accessibility issues, especially help-seeking situations BVI users face in their DL search processes. One hundred and twenty BVI users were recruited to search for information in six DLs on four types of mobile devices (iPhone, iPad, Android phone, and Android tablet), and multiple data collection methods were employed: questionnaires, think-aloud protocols, transaction logs, and interviews. This paper reports part of a large-scale study, including the categories of help-seeking situations BVI users face in their interactions with DLs, focusing on seven types of help-seeking situations related to visual interactions on mobile platforms: difficulty finding a toggle-based search feature, difficulty understanding a video feature, difficulty navigating items on paginated sections, difficulty distinguishing collection labels from thumbnails, difficulty recognizing the content of images, difficulty recognizing the content of graphs, and difficulty interacting with multilayered windows. Moreover, corresponding design recommendations are also proposed: placing meaningful labels for icon-based features in an easy-to-access location, offering intuitive and informative video descriptions for video players, providing structure information about a paginated section, separating collection/item titles from thumbnail descriptions, incorporating artificial intelligence image/graph recognition mechanisms, and limiting screen reader interactions to active windows. Additionally, the limitations of the study and future research are discussed.
NOT_RELEVANT;ProQuest;Enhancing Biomedical Question Answering with Large Language Models;"Yang, Hua;Li, Shilong;Gonçalves, Teresa";2024;10.3390/info15080494;NOT_FOUND;"In the field of Information Retrieval, biomedical question answering is a specialized task that focuses on answering questions related to medical and healthcare domains. The goal is to provide accurate and relevant answers to the posed queries related to medical conditions, treatments, procedures, medications, and other healthcare-related topics. Well-designed models should efficiently retrieve relevant passages. Early retrieval models can quickly retrieve passages but often with low precision. In contrast, recently developed Large Language Models can retrieve documents with high precision but at a slower pace. To tackle this issue, we propose a two-stage retrieval approach that initially utilizes BM25 for a preliminary search to identify potential candidate documents; subsequently, a Large Language Model is fine-tuned to evaluate the relevance of query–document pairs. Experimental results indicate that our approach achieves comparative performances on the BioASQ and the TREC-COVID datasets."
NOT_RELEVANT;ProQuest;Review of River Ice Observation and Data Analysis Technologies;"Zakharov, Igor;Puestow, Thomas;Amir Ali Khan;Briggs, Robert;Barrette, Paul";2024;10.3390/hydrology11080126;NOT_FOUND;This paper provides a comprehensive review of the available literature on the observation and characterization of river ice using remote sensing technologies. Through an analysis of 200 publications spanning from 1919 to June 2024, we reviewed different observation technologies deployed on in situ, aerial and satellite platforms for their utility in monitoring and characterizing river ice covers. River ice information, captured by 51 terms extracted from the literature, holds significant value in enhancing infrastructure resilience in the face of climate change. Satellite technologies, in particular the multispectral optical and multi-polarimetric synthetic aperture radar (SAR), provide a number of advantages, such as ice features discrimination, better ice characterization, and reliable delineation of open water and ice, with both current and upcoming sensors. The review includes data analysis methods employed for the monitoring and characterization of river ice, including ice information retrieval methods and corresponding accuracies. The need for further research on artificial intelligence and, in particular, deep learning (DL) techniques has been recognized as valuable for enhancing the accuracy of automated systems. The growing availability of freely available and commercial satellites, UAVs, and in situ data with improved characteristics suggests significant operational potential for river ice observation in the near future. Our study also identifies gaps in the current capabilities for river ice observation and provides suggestions for improved data analysis and interpretation.
NOT_RELEVANT;ProQuest;Second-Order Text Matching Algorithm for Agricultural Text;"Sun, Xiaoyang;Song, Yunsheng;Huang, Jianing";2024;10.3390/app14167012;NOT_FOUND;Text matching promotes the research and application of deep understanding of text information, and it provides the basis for information retrieval, recommendation systems and natural language processing by exploring the similar structures in text data. Owning to the outstanding performance and automatically extract text features for the target, the methods based-pre-training models gradually become the mainstream. However, such models usually suffer from the disadvantages of slow retrieval speed and low running efficiency. On the other hand, previous text matching algorithms have mainly focused on horizontal domain research, and there are relatively few vertical domain algorithms for agricultural text, which need to be further investigated. To address this issue, a second-order text matching algorithm has been developed. This paper first obtains a large amount of text about typical agricultural crops and constructs a database by using web crawlers and querying relevant textbooks, etc. Then BM25 algorithm is used to generate a candidate set and BERT model is used to filter the optimal match based on the candidate set. Experiments have shown that the Precision@1 of this second-order algorithm can reach 88.34% on the dataset constructed in this paper, and the average time to match a piece of text is only 2.02 s. Compared with BERT model and BM25 algorithm, there is an increase of 8.81% and 13.73% in Precision@1 respectively. In terms of the average time required for matching a text, it is 55.2 s faster than BERT model and only 2 s slower than BM25 algorithm. It can improve the efficiency and accuracy of agricultural information retrieval, agricultural decision support, agricultural market analysis, etc., and promote the sustainable development of agriculture.
NOT_RELEVANT;ProQuest;Web Search or Conversation with an Artificial Intelligence? Analysis of Misinformation and Relevance in the Case of Radon Gas;"Pascual-Presa, Noel;Fernández-Pichel, Marcos;Losada, David E;García-Orosa, Berta";2024;10.3145/epi.2024.0220;NOT_FOUND;Health-related information plays a crucial role in public health management, empowering individuals to make informed decisions and adopt behaviours that mitigate the effects of potential risks. The internet and the emergence of new technologies, such as conversational models equipped with Artificial Intelligence, present opportunities, and challenges in this field. This research focuses specifically on the risk of radon, a natural radioactive gas recognised worldwide as one of the leading causes of lung cancer and a persistent threat over time. The aim of this study is to analyse the information provided for this specific risk by two key information access tools: web search engines and AI-based conversational agents (ChatGPT). To carry out this interdisciplinary research (journalism-communication-computer science) we employ a mixed methodological design (quantitative and qualitative) and apply methods from the areas of Information Retrieval (IR), Big Data, and Artificial Intelligence (AI). The results of this study show that information on the internet about the risk of radon often lacks relevance and does not meet the information needs of users. We also found that some websites provide a significant amount of good quality information but there are often some misleading contents. ChatGPT proves to be more accurate in providing relevant and good quality information but contains a higher proportion of misinformation. Consequently, this raises concerns about the integrity of the information provided and emphasises the need to monitor and improve the accuracy of these computational tools.
MAYBE_RELEVANT;ProQuest;Revolutionizing Campus Communication: NLP-Powered University Chatbots;PDF;2024;10.14569/IJACSA.2024.0150606;NOT_FOUND;Artificial intelligence (AI) based chatbots leverage programmed software instructions to simulate human speech and user interaction. These versatile tools can be employed in various domains, from managing smart home devices to providing personal virtual assistants. They can also be useful in responding to common queries and can make information easier to access. In response to this need, we developed a specialized chatbot tailored for the academic environment by training an NLP model to answer frequently asked questions (FAQs) the need of searching through the university website. The main goal is to optimize user engagement and streamline information retrieval within a university setting. By employing ML and NLP techniques, we enhance the chatbot's capabilities, enabling it to provide effective and precise answers, contributing to a more seamless and efficient experience for users seeking information about the university. The study discusses the pivotal decision-making process between implementing a custom neural network and the BERT model. Through a comparative analysis, the custom neural network emerges as the preferred solution, displaying efficiency, quick deployment, and superior accuracy in handling task-specific queries. While BERT presents unparalleled versatility in natural language processing, its resource-intensive pre-training, and challenges in adapting to the intricacies of the university-specific dataset limit its efficiency in this application. This research emphasizes the importance of customization to meet the unique demands of a university chatbot, providing valuable insights for developers seeking to strike a balance between efficiency and specialization in similar applications.
NOT_RELEVANT;ProQuest;Bayes R-CNN: An Uncertainty-Aware Bayesian Approach to Object Detection in Remote Sensing Imagery for Enhanced Scene Interpretation;"Sagar A S M Sharifuzzaman;Jawad Tanveer;Chen, Yu;Chan, Jun Hoong;Kim, Hyung Seok;Karam Dad Kallu;Shahzad, Ahmed";2024;10.3390/rs16132405;NOT_FOUND;Remote sensing technology has been modernized by artificial intelligence, which has made it possible for deep learning algorithms to extract useful information from images. However, overfitting and lack of uncertainty quantification, high-resolution images, information loss in traditional feature extraction, and background information retrieval for detected objects limit the use of deep learning models in various remote sensing applications. This paper proposes a Bayes by backpropagation (BBB)-based system for scene-driven identification and information retrieval in order to overcome the above-mentioned problems. We present the Bayes R-CNN, a two-stage object detection technique to reduce overfitting while also quantifying uncertainty for each object recognized within a given image. To extract features more successfully, we replace the traditional feature extraction model with our novel Multi-Resolution Extraction Network (MRENet) model. We propose the multi-level feature fusion module (MLFFM) in the inner lateral connection and a Bayesian Distributed Lightweight Attention Module (BDLAM) to reduce information loss in the feature pyramid network (FPN). In addition, our system incorporates a Bayesian image super-resolution model which enhances the quality of the image to improve the prediction accuracy of the Bayes R-CNN. Notably, MRENet is used to classify the background of the detected objects to provide detailed interpretation of the object. Our proposed system is comprehensively trained and assessed utilizing the state-of-the-art DIOR and HRSC2016 datasets. The results demonstrate our system’s ability to detect and retrieve information from remote sensing scene images.
NOT_RELEVANT;ProQuest;Evaluating Neural Networks’ Ability to Generalize against Adversarial Attacks in Cross-Lingual Settings;"Mathur, Vidhu;Dadu, Tanvi;Aggarwal, Swati";2024;10.3390/app14135440;NOT_FOUND;Featured Application The application of this research is to create better multilingual datasets by utilizing the insights gained from our investigation into mBART and XLM-Roberta. These improved datasets will support the creation of more robust and accurate AI NLP models that can effectively handle various languages, enhancing performance in tasks like machine translation, sentiment analysis, text categorization, and information retrieval. This research addresses biases and limitations in the current translation methods. Abstract Cross-lingual transfer learning using multilingual models has shown promise for improving performance on natural language processing tasks with limited training data. However, translation can introduce superficial patterns that negatively impact model generalization. This paper evaluates two state-of-the-art multilingual models, Cross-Lingual Model-Robustly Optimized BERT Pretraining Approach (XLM-Roberta) and Multilingual Bi-directional Auto-Regressive Transformer (mBART), on the cross-lingual natural language inference (XNLI) natural language inference task using both original and machine-translated evaluation sets. Our analysis demonstrates that translation can facilitate cross-lingual transfer learning, but maintaining linguistic patterns is critical. The results provide insights into the strengths and limitations of state-of-the-art multilingual natural language processing architectures for cross-lingual understanding.
NOT_RELEVANT;ProQuest;Financial Statement Text Information Mining and Key Information Extraction Model Construction;Xu, Yi;2024;NOT_FOUND;NOT_FOUND;- Financial statement text information mining and key information extraction model design are critical areas of research that aim to use advanced computational approaches to extract important insights from textual data contained in financial documents. In this work, they look at methodologies, techniques, and applications that combine natural language processing (NLP) and machine learning to automate financial statement interpretation. To lay the groundwork for the research, researchers first conduct a thorough examination of existing literature in interdisciplinary domains such as computational linguistics, information retrieval, and finance. Building on insights from earlier studies, they design and use unique NLP approaches, such as named entity identification, syntactic parsing, sentiment analysis, and topic modelling, to extract essential financial metrics from textual data. Additionally, they create machine learning models that are suited to the peculiarities of financial terminology and reporting standards, combining domain-specific knowledge with linguistic experience to improve accuracy and reliability. They demonstrate the efficacy and scalability of the technique in automating the extraction of crucial financial information, such as revenue trends, cost patterns, and risk factors, through rigorous testing on real-world financial data. These results highlight the transformative power of natural language processing and machine learning in financial analysis, providing stakeholders in finance and accounting with actionable intelligence for informed decision-making, risk assessment, and compliance monitoring. By bridging the gap between computational linguistics and financial analysis, this study advances financial text analysis and provides the framework for future research and innovation in this emerging field.
NOT_RELEVANT;ProQuest;An Extensive study of Symantic and Syntatic Approaches to Automatic Text Summarization;"Gaikwad, Manisha;Shinde, Gitanjali;Mahalle, Parikshit;Sable, Nilesh;Ate, Namrata Khar";2024;NOT_FOUND;NOT_FOUND;Automatic text summarization (ATS) has emerged as a crucial research domain in the discipline of natural language processing (NLP) and information retrieval. The exponential growth of digital content has necessitated the need for efficient techniques that can automatically generate concise and informative summaries from lengthy documents. This article provided a comprehensive recap of automatic text summarization, covering both abstractive and extractive methods. Using extractive techniques, prime phrases or keywords from the original text are identified and chosen, while abstractive methods involve producing summaries by paraphrasing and synthesizing content in a more human-like manner. Discussed the advantages and limitations of each approach, including the challenge of ATS, which arises when summarizing content from external sources. Furthermore, reviews common evaluation metrics used for assessing the quality of summaries and discusses recent advancements in neural network-based approaches for text summarization. This survey aims to provide an overview of automatic text summarization which acts as a useful resource for researchers and practitioners in the fields of information retrieval and NLP.
NOT_RELEVANT;ProQuest;Homonym Detection Using WordNet and Modified Lesk Approach for English Language;"Karnik, Madhuri P;Kodavade, D V";2024;NOT_FOUND;NOT_FOUND;Word sense disambiguation (WSD) is a basic and persistent problem that has existed since its inception in the natural language processing (NLP) area. The process of determining the accurate meaning of a word within a specific context is referred to as word sense disambiguation, commonly known as WSD. In NLP, a single word can have two or more meanings, with each meaning being distinguished by its context. This is known as word polysemy. Its applications span a wide range of fields, such as question answering systems, machine translation, information retrieval (IR) etc. Ontology and NLP are still struggling with ambiguity. Homonyms, which are ubiquitous in most languages, are words that have the same spelling but a different meaning. This method's fundamental premise is to select the appropriate sense by comparing a word's context in a sentence to contexts generated from WordNet. The primary goal of this study is to employ WordNet and the Lesk algorithm for WSD. After the algorithm was put into practice and tested on a collection of sentences that included ambiguous words, the synset was able to determine the proper interpretation for most of the sentences. The Lesk algorithm relies on finding the highest number of shared words (maximum overlap) between a word's context, prepositions and the definitions for its different meanings(glosses). This approach helps in identifying the most accurate interpretation for a given word within a specific context. According to experimental findings, the suggested strategy considerably boosts performance while identifying homonyms.
NOT_RELEVANT;ProQuest;A Survey of Text-Matching Techniques;"Jiang, Peng;Cai, Xiaodong";2024;10.3390/info15060332;NOT_FOUND;Text matching, as a core technology of natural language processing, plays a key role in tasks such as question-and-answer systems and information retrieval. In recent years, the development of neural networks, attention mechanisms, and large-scale language models has significantly contributed to the advancement of text-matching technology. However, the rapid development of the field also poses challenges in fully understanding the overall impact of these technological improvements. This paper aims to provide a concise, yet in-depth, overview of the field of text matching, sorting out the main ideas, problems, and solutions for text-matching methods based on statistical methods and neural networks, as well as delving into matching methods based on large-scale language models, and discussing the related configurations, API applications, datasets, and evaluation methods. In addition, this paper outlines the applications and classifications of text matching in specific domains and discusses the current open problems that are being faced and future research directions, to provide useful references for further developments in the field.
NOT_RELEVANT;ProQuest;Enhancing Medical Image Retrieval with UMLS-Integrated CNN-Based Text Indexing;"Gasmi, Karim;Ayadi, Hajer;Torjmen, Mouna";2024;10.3390/diagnostics14111204;NOT_FOUND;In recent years, Convolutional Neural Network (CNN) models have demonstrated notable advancements in various domains such as image classification and Natural Language Processing (NLP). Despite their success in image classification tasks, their potential impact on medical image retrieval, particularly in text-based medical image retrieval (TBMIR) tasks, has not yet been fully realized. This could be attributed to the complexity of the ranking process, as there is ambiguity in treating TBMIR as an image retrieval task rather than a traditional information retrieval or NLP task. To address this gap, our paper proposes a novel approach to re-ranking medical images using a Deep Matching Model (DMM) and Medical-Dependent Features (MDF). These features incorporate categorical attributes such as medical terminologies and imaging modalities. Specifically, our DMM aims to generate effective representations for query and image metadata using a personalized CNN, facilitating matching between these representations. By using MDF, a semantic similarity matrix based on Unified Medical Language System (UMLS) meta-thesaurus, and a set of personalized filters taking into account some ranking features, our deep matching model can effectively consider the TBMIR task as an image retrieval task, as previously mentioned. To evaluate our approach, we performed experiments on the medical ImageCLEF datasets from 2009 to 2012. The experimental results show that the proposed model significantly enhances image retrieval performance compared to the baseline and state-of-the-art approaches.
NOT_RELEVANT;ProQuest;Integrating Lesk Algorithm with Cosine Semantic Similarity to Resolve Polysemy for Setswana Language;PDF;2024;10.14569/IJACSA.2024.0150479;NOT_FOUND;Word Sense Disambiguation (WSD) serves as an intermediate task for enhancing text understanding in Natural Language Processing (NLP) applications, including machine translation, information retrieval, and text summarization. Its role is to enhance the effectiveness and efficiency of these applications by ensuring the accurate selection of the appropriate sense for polysemous words in diverse contexts. This task is recognized as an AI-complete problem, indicating its longstanding complexity since the 1950s. One of the earliest proposed solutions to address polysemy in NLP is the Lesk algorithm, which has seen various adaptations by researchers for different languages over the years. This study proposes a simplified, Lesk-based algorithm to resolve polysemy for Setswana. Instead of combinatorial comparisons among candidate senses that Lesk is based on that cause computational complexity, this study models word sense glosses using Bidirectional Encoder Representations from Transformers (BERT) and Cosine similarity measure, which have been proven to achieve optimal performance in WSD. The proposed algorithm was evaluated on Setswana and obtained an accuracy of 86.66 and an error rate of 14.34, surpassing the accuracy of other Lesk-based algorithms for other languages.
NOT_RELEVANT;ProQuest;A Question and Answering Service of Typhoon Disasters Based on the T5 Large Language Model;"Xia, Yongqi;Huang, Yi;Qiu, Qianqian;Zhang, Xueying;Miao, Lizhi;Chen, Yixiang";2024;10.3390/ijgi13050165;NOT_FOUND;"A typhoon disaster is a common meteorological disaster that seriously impacts natural ecology, social economy, and even human sustainable development. It is crucial to access the typhoon disaster information, and the corresponding disaster prevention and reduction strategies. However, traditional question and answering (Q&amp;A) methods exhibit shortcomings like low information retrieval efficiency and poor interactivity. This makes it difficult to satisfy users’ demands for obtaining accurate information. Consequently, this work proposes a typhoon disaster knowledge Q&amp;A approach based on LLM (T5). This method integrates two technical paradigms of domain fine-tuning and retrieval-augmented generation (RAG) to optimize user interaction experience and improve the precision of disaster information retrieval. The process specifically includes the following steps. First, this study selects information about typhoon disasters from open-source databases, such as Baidu Encyclopedia and Wikipedia. Utilizing techniques such as slicing and masked language modeling, we generate a training set and 2204 Q&amp;A pairs specifically focused on typhoon disaster knowledge. Second, we continuously pretrain the T5 model using the training set. This process involves encoding typhoon knowledge as parameters in the neural network’s weights and fine-tuning the pretrained model with Q&amp;A pairs to adapt the T5 model for downstream Q&amp;A tasks. Third, when responding to user queries, we retrieve passages from external knowledge bases semantically similar to the queries to enhance the prompts. This action further improves the response quality of the fine-tuned model. Finally, we evaluate the constructed typhoon agent (Typhoon-T5) using different similarity-matching approaches. Furthermore, the method proposed in this work lays the foundation for the cross-integration of large language models with disaster information. It is expected to promote the further development of GeoAI."
NOT_RELEVANT;ProQuest;Biomedical semantic text summarizer;"Kirmani, Mahira;Kour, Gagandeep;Mudasir Mohd;Sheikh, Nasrullah;Dawood Ashraf Khan;Maqbool, Zahid;Mohsin Altaf Wani;Abid Hussain Wani";2024;10.1186/s12859-024-05712-x;NOT_FOUND;Background Text summarization is a challenging problem in Natural Language Processing, which involves condensing the content of textual documents without losing their overall meaning and information content, In the domain of bio-medical research, summaries are critical for efficient data analysis and information retrieval. While several bio-medical text summarizers exist in the literature, they often miss out on an essential text aspect: text semantics. Results This paper proposes a novel extractive summarizer that preserves text semantics by utilizing bio-semantic models. We evaluate our approach using ROUGE on a standard dataset and compare it with three state-of-the-art summarizers. Our results show that our approach outperforms existing summarizers. Conclusion The usage of semantics can improve summarizer performance and lead to better summaries. Our summarizer has the potential to aid in efficient data analysis and information retrieval in the field of biomedical research.
NOT_RELEVANT;ProQuest;Integration and Assessment of ChatGPT in Medical Case Reporting: A Multifaceted Approach;"Kuan-Chen, Lin;Tsung-An, Chen;Lin, Ming-Hwai;Yu-Chun, Chen;Tzeng-Ji, Chen";2024;10.3390/ejihpe14040057;NOT_FOUND;ChatGPT, a large language model, has gained significance in medical writing, particularly in case reports that document the course of an illness. This article explores the integration of ChatGPT and how ChatGPT shapes the process, product, and politics of medical writing in the real world. We conducted a bibliometric analysis on case reports utilizing ChatGPT and indexed in PubMed, encompassing publication information. Furthermore, an in-depth analysis was conducted to categorize the applications and limitations of ChatGPT and the publication trend of application categories. A total of 66 case reports utilizing ChatGPT were identified, with a predominant preference for the online version and English input by the authors. The prevalent application categories were information retrieval and content generation. Notably, this trend remained consistent across different months. Within the subset of 32 articles addressing ChatGPT limitations in case report writing, concerns related to inaccuracies and a lack of clinical context were prominently emphasized. This pointed out the important role of clinical thinking and professional expertise, representing the foundational tenets of medical education, while also accentuating the distinction between physicians and generative artificial intelligence.
NOT_RELEVANT;ProQuest;Remote sensing image information extraction based on Compensated Fuzzy Neural Network and big data analytics;"Sun, Rui;Zhang, Zhengyin;Liu, Yajun;Niu, Xiaohang;Yuan, Jie";2024;10.1186/s12880-024-01266-9;NOT_FOUND;Medical imaging AI systems and big data analytics have attracted much attention from researchers of industry and academia. The application of medical imaging AI systems and big data analytics play an important role in the technology of content based remote sensing (CBRS) development. Environmental data, information, and analysis have been produced promptly using remote sensing (RS). The method for creating a useful digital map from an image data set is called image information extraction. Image information extraction depends on target recognition (shape and color). For low-level image attributes like texture, Classifier-based Retrieval(CR) techniques are ineffective since they categorize the input images and only return images from the determined classes of RS. The issues mentioned earlier cannot be handled by the existing expertise based on a keyword/metadata remote sensing data service model. To get over these restrictions, Fuzzy Class Membership-based Image Extraction (FCMIE), a technology developed for Content-Based Remote Sensing (CBRS), is suggested. The compensation fuzzy neural network (CFNN) is used to calculate the category label and fuzzy category membership of the query image. Use a basic and balanced weighted distance metric. Feature information extraction (FIE) enhances remote sensing image processing and autonomous information retrieval of visual content based on time-frequency meaning, such as color, texture and shape attributes of images. Hierarchical nested structure and cyclic similarity measure produce faster queries when searching. The experiment’s findings indicate that applying the proposed model can have favorable outcomes for assessment measures, including Ratio of Coverage, average means precision, recall, and efficiency retrieval that are attained more effectively than the existing CR model. In the areas of feature tracking, climate forecasting, background noise reduction, and simulating nonlinear functional behaviors, CFNN has a wide range of RS applications. The proposed method CFNN-FCMIE achieves a minimum range of 4–5% for all three feature vectors, sample mean and comparison precision-recall ratio, which gives better results than the existing classifier-based retrieval model. This work provides an important reference for medical imaging artificial intelligence system and big data analysis.
NOT_RELEVANT;ProQuest;An Update on the Use of Artificial Intelligence in Cardiovascular Medicine;"Rao, Shiavax J;Iqbal, Shaikh B;Isath, Ameesh;Hafeez Ul Hassan Virk;Wang, Zhen;Glicksberg, Benjamin S;Krittanawong, Chayakrit";2024;10.3390/hearts5010007;NOT_FOUND;Artificial intelligence, specifically advanced language models such as ChatGPT, have the potential to revolutionize various aspects of healthcare, medical education, and research. In this review, we evaluate the myriad applications of artificial intelligence in diverse healthcare domains. We discuss its potential role in clinical decision-making, exploring how it can assist physicians by providing rapid, data-driven insights for diagnosis and treatment. We review the benefits of artificial intelligence such as ChatGPT in personalized patient care, particularly in geriatric care, medication management, weight loss and nutrition, and physical activity guidance. We further delve into its potential to enhance medical research, through the analysis of large datasets, and the development of novel methodologies. In the realm of medical education, we investigate the utility of artificial intelligence as an information retrieval tool and personalized learning resource for medical students and professionals.
NOT_RELEVANT;ProQuest;Vox Populi, Vox ChatGPT: Large Language Models, Education and Democracy;"Zuber, Niina;Gogoll, Jan";2024;10.3390/philosophies9010013;NOT_FOUND;In the era of generative AI and specifically large language models (LLMs), exemplified by ChatGPT, the intersection of artificial intelligence and human reasoning has become a focal point of global attention. Unlike conventional search engines, LLMs go beyond mere information retrieval, entering into the realm of discourse culture. Their outputs mimic well-considered, independent opinions or statements of facts, presenting a pretense of wisdom. This paper explores the potential transformative impact of LLMs on democratic societies. It delves into the concerns regarding the difficulty in distinguishing ChatGPT-generated texts from human output. The discussion emphasizes the essence of authorship, rooted in the unique human capacity for reason—a quality indispensable for democratic discourse and successful collaboration within free societies. Highlighting the potential threats to democracy, this paper presents three arguments: the Substitution argument, the Authenticity argument, and the Facts argument. These arguments highlight the potential risks that are associated with an overreliance on LLMs. The central thesis posits that widespread deployment of LLMs may adversely affect the fabric of a democracy if not comprehended and addressed proactively and properly. In proposing a solution, we advocate for an emphasis on education as a means to mitigate risks. We suggest cultivating thinking skills in children, fostering coherent thought formulation, and distinguishing between machine-generated output and genuine, i.e., human, reasoning. The focus should be on the responsible development and usage of LLMs, with the goal of augmenting human capacities in thinking, deliberating and decision-making rather than substituting them.
NOT_RELEVANT;ProQuest;English Sentence Semantic Feature Extraction Method Based on Fuzzy Logic Algorithm;Qi, Zhenzhen;2024;NOT_FOUND;NOT_FOUND;Semantic features play a pivotal role in natural language processing, providing a deeper understanding of the meaning and context within textual data, hr the realm of machme learning and artificial intelligence, semantic feature extraction involves translating linguistic elements mto numerical representations, often utilizing advanced techniques like word embeddings and deep learning models. The integration of semantic features enhances the precision and context-awareness of language models, enabling applications such as sentiment analysis, document categorization, and information retrieval to operate with greater accuracy and relevance. The paper introduces a novel approach, Hierarchical Mandhami Optimized Semantic Feature Extraction (HMOSFE), designed to enhance semantic feature extraction fam English sentences. The proposed HMOSFE model comprises fusion of hierarchical clustering and fuzzy-based feature extraction, HMOSFE aims to capture intricate semantic relationships within sentences, providing nuanced insights mto the underlying meaning of textual content. The model employs pre-trained word embeddings for term representation, calculates a similarity matrix ushig coshie similarity, and utilizes hierarchical clustering for document grouping. Fuzzy logic contributes to assigning weights to features, enabling a more refined understandhig of semantic significance. The paper presents comprehensive results, including semantic similarity estimations, clustering distances, and fuzzy memberships, demonstrating the effectiveness of HMOSFE across diverse documents.
NOT_RELEVANT;ProQuest;Deep learning-based idiomatic expression recognition for the Amharic language;"Demeke Endalie;Haile, Getamesay;Taye, Wondmagegn";2023;10.1371/journal.pone.0295339;NOT_FOUND;Idiomatic expressions are built into all languages and are common in ordinary conversation. Idioms are difficult to understand because they cannot be deduced directly from the source word. Previous studies reported that idiomatic expression affects many Natural language processing tasks in the Amharic language. However, most natural language processing models used with the Amharic language, such as machine translation, semantic analysis, sentiment analysis, information retrieval, question answering, and next-word prediction, do not consider idiomatic expressions. As a result, in this paper, we proposed a convolutional neural network (CNN) with a FastText embedding model for detecting idioms in an Amharic text. We collected 1700 idiomatic and 1600 non-idiomatic expressions from Amharic books to test the proposed model’s performance. The proposed model is then evaluated using this dataset. We employed an 80 by 10,10 splitting ratio to train, validate, and test the proposed idiomatic recognition model. The proposed model’s learning accuracy across the training dataset is 98%, and the model achieves 80% accuracy on the testing dataset. We compared the proposed model to machine learning models like K-Nearest Neighbor (KNN), Support Vector Machine (SVM), and Random Forest classifiers. According to the experimental results, the proposed model produces promising results.
NOT_RELEVANT;ProQuest;Harmonizing minds and machines: survey on transformative power of machine learning in music;Liang, Jing;2023;10.3389/fnbot.2023.1267561;NOT_FOUND;This survey explores the symbiotic relationship between Machine Learning (ML) and music, focusing on the transformative role of Artificial Intelligence (AI) in the musical sphere. Beginning with a historical contextualization of the intertwined trajectories of music and technology, the paper discusses the progressive use of ML in music analysis and creation. Emphasis is placed on present applications and future potential. A detailed examination of music information retrieval, automatic music transcription, music recommendation, and algorithmic composition presents state-of-the-art algorithms and their respective functionalities. The paper underscores recent advancements, including ML-assisted music production and emotion-driven music generation. The survey concludes with a prospective contemplation of future directions of ML within music, highlighting the ongoing growth, novel applications, and anticipation of deeper integration of ML across musical domains. This comprehensive study asserts the profound potential of ML to revolutionize the musical landscape and encourages further exploration and advancement in this emerging interdisciplinary field.
NOT_RELEVANT;ProQuest;Preface;NOT_FOUND;2023;10.1088/1742-6596/2562/1/011001;NOT_FOUND;The International Conference on Artificial Intelligence and Industrial Technology Applications originates from 2021 and has attracted various promising researchers who are developing new directions in the field of artificial intelligence and industrial technology. The 2023 3 rd International Conference on Artificial Intelligence and Industrial Technology Applications (AIITA 2023), held in Suzhou, China from March 24 th to 26 th , 2023, was meant to unite researchers, experts, professors and academic staff as well as a wide range of participants of professional community from China and other countries concerned. AIITA 2023, with about 150 participants, was held to further integrate artificial intelligence with industrial technologies, and promoted research and developmental activities in related fields as well as scientific information interchange between researchers, developers, engineers, students, and practitioners working all around the world. Highly eventful program of the Conference, the list of well-established organizations-participants and devoted engagement of many colleagues throughout all the stages of Conference preparation instill confidence in practical importance of mutual initiative. The framework of the Conference comprises keynote speeches, oral presentations, and academic investigation, discussing open and significant problems facing industrial technologies, proposing innovative ideas and approaches to solution of the problems, and considering the new possibilities of application and development of cutting-edge artificial intelligence with industrial technologies. The Conference has become one of the venues for demonstrating scientific achievements of international class by leading scientists and young scholars and would facilitate the strengthening of academic cooperation, meanwhile the Conference proceedings would be expressed in the form of international scientific publishing. Various topics of the papers, gone through a vigorous peer review process, are covered in the proceedings, including: Artificial Intelligence Algorithm, Information Retrieval and Fusion, Intelligent Mechatronics, Integrated Manufacturing System, Intelligent Service Machines, etc. On behalf of the Conference Organizing Committee, we would like to thank the referees for their efficient and thoughtful actions. We are grateful to the members of the Technical Program Committee for their efforts in making and shaping the program for AIITA 2023. Particularly, we acknowledge the publishing support from the members of Journal of Physics: Conference Series. We hope that the future AIITA will be as successful and stimulating, as indicated with the contributions presented in this volume. The Committee of AIITA 2023 List of Committee Member are available in this pdf.
NOT_RELEVANT;ProQuest;Biomedical-named entity recognition using CUDA accelerated KNN algorithm;"Bali, Manish;Pichandi, Anandaraj Shanthi;Duraisamy, Jude Hemanth";2023;10.12928/TELKOMNIKA.v21i4.24065;NOT_FOUND;Biomedical named entity recognition (Bio-NER) is a highly complex and time-consuming research domain using natural language processing (NLP). It's widely used in information retrieval, knowledge summarization, biomolecular event extraction, and discovery applications. This paper proposes a method for the recognition and classification of named entities in the biomedical domain using machine learning (ML) techniques. Support vector machine (SVM), decision trees (DT), K-nearest neighbor (KNN), and its kernel versions are used. However, recent advancements in programmable, massively parallel graphics processing units (GPU) hold promise in terms of increased computational capacity at a lower cost to address multi-dimensional data and time complexity. We implement a novel parallel version of KNN by porting the distance computation step on GPU using the compute unified device architecture (CUDA) and compare the performance of all the algorithms using the BioNLP/NLPBA 2004 corpus. Results demonstrate that CUDA-KNN takes full advantage of the GPU's computational capacity and multi-leveled memory architecture, resulting in a 35x performance enhancement over the central processing unit (CPU). In a comparative study with existing research, the proposed model provides an option for a faster NER system for higher dimensionality and larger datasets as it offers balanced performance in terms of accuracy and speed-up, thus providing critical design insights into developing a robust BioNLP system.
MAYBE_RELEVANT;ProQuest;Investigating the Potential Impact of Artificial Intelligence in Librarianship;"Affum, Mark Quaye;Dwomoh, Oliver Kofi";2023;NOT_FOUND;NOT_FOUND;This research aims to investigate the potential impact of artificial intelligence (AI) in the field of librarianship. With advancements in AI technology, there is a growing interest in exploring how it can enhance various aspects of librarianship, including information retrieval, cataloging, user services, and knowledge management. This study will examine the current state of AI applications in libraries, analyze the benefits and challenges associated with its implementation, and explore potential future developments in the field. By understanding the potential impact of AI in librarianship, librarians, researchers, and stakeholders can make informed decisions regarding the integration of AI technologies to improve library services and meet the evolving needs of users.
MAYBE_RELEVANT;ProQuest;Analyzing the Strengths, Weaknesses, Opportunities, and Threats of AI in Libraries;Chhetri, Parbat;2023;NOT_FOUND;NOT_FOUND;This article provides a comprehensive analysis of the strengths, weaknesses, opportunities, and threats (SWOT) associated with the integration of Artificial Intelligence (AI) in libraries. AI has the potential to transform library and information science, revolutionizing processes, and services. The strengths of AI in libraries include efficient information retrieval and management, enhanced user experiences through personalization, automation of routine tasks, and improved decision-making through data analysis. However, the weaknesses of AI in libraries encompass ethical considerations and biases, the potential lack of human touch and personalized assistance, technical challenges, and concerns about job displacement. The article also explores the opportunities presented by AI, such as advanced search capabilities, expanded accessibility of digital collections, support for diverse user needs, and collaboration among libraries. On the other hand, the threats and challenges of AI in libraries involve privacy and security risks, dependence on technology and potential system failures, user acceptance and trust issues, and the impact on traditional library services and roles. By considering these factors, libraries can make informed decisions and strategically implement AI to maximize its benefits while addressing the associated challenges. The findings of this analysis emphasize the importance of thoughtful implementation and human-AI collaboration to ensure the best outcomes for library users and stakeholders in the future.
NOT_RELEVANT;ProQuest;Named Entity Recognition: a Survey for the Portuguese Language;"Albuquerque, Hidelberg O;Souza, Ellen;Gomes, Carlos;Matheus Henrique de C. Pinto;Filho, Ricardo P S;Costa, Rosimeire;Vinícius Teixeira de M. Lopes;Nádia F. F. da Silva;André C. P. L. F. de Carvalho;Oliveira, Adriano L I";2023;NOT_FOUND;NOT_FOUND;Named Entity Recognition (NER) is an important task in Natural Language Processing, as it is a key information extraction sub-task with numerous applications, such as information retrieval and machine learning. However, resources are still scarce for some languages, as it is the case of Portuguese. Thus, the objective of this research is to map NER techniques, methods and resources for the Portuguese language. Manual and automated searches were applied, retrieving 447 primary studies, of which 45 were included in our review. The growing number of studies reveal a greater interest of researchers in the area. 21 studies focused on the comparative analysis between techniques and tools. 24 new or updated NER corpora were mapped, in several domains. The most used text pre-processing techniques were tokenization, embeddings, and PoS Tagging, while the most used methods/algorithms were based on BiLSTM, CRF, and BERT models. The most relevant researchers, institutions and countries were also mapped, as well as the evolution of publications.
NOT_RELEVANT;ProQuest;Evaluation of Language Models on Romanian XQuAD and RoITD datasets;"Constantin Dragos Nicolae;Yadav, Rohan Kumar;Tufiş, Dan";2023;10.15837/ijccc.2023.1.5111;NOT_FOUND;Natural language processing (NLP) has become a vital requirement in a wide range of applications, including machine translation, information retrieval, and text classification. The development and evaluation of NLP models for various languages have received significant attention in recent years, but there has been relatively little work done on comparing the performance of different language models on Romanian data. In particular, the introduction and evaluation of various Romanian language models with multilingual models have barely been comparatively studied. In this paper, we address this gap by evaluating eight NLP models on two Romanian datasets, XQuAD and RoITD. Our experiments and results show that bert-base-multilingual-cased and bertbase- multilingual-uncased, perform best on both XQuAD and RoITD tasks, while RoBERT-small model and DistilBERT models perform the worst. We also discuss the implications of our findings and outline directions for future work in this area.
NOT_RELEVANT;ProQuest;A Comparative Study of Stemming Techniques on the Malay Text;PDF;2023;10.14569/IJACSA.2023.0141213;NOT_FOUND;Text stemming, an essential preprocessing step in the development of Natural Language Processing (NLP) applications, involves the transformation of various word forms into their root words. Stemming plays a critical role in decreasing the volume of text, thereby enhancing the efficiency of various computational tasks such as information retrieval, text classification, and text clustering. Stemming is a rule-based approach. On the other hand, it frequently suffers affixation errors that result in under-stemming, over-stemming, or both, as well as unstemmed or spelling exceptions. Every language has different stemming techniques, and among the most well-known Malay stemming algorithms are the Othman and Ahmad algorithms. Therefore, this study aims to compare the performance of the stemming errors between the Othman and Ahmad algorithms in stemming Malay text, particularly on two different domains of textual datasets, which are the course summaries of the education domain and housebreaking crime reports of the crime domain. The Othman algorithm presents a set of 121 stemming rules (set A). In the meantime, Ahmad's algorithm proposes two distinct sets of stemming rules, comprising 432 (set B) and 561 rules (set C), respectively. Based on the experiment results with 100 course summaries, the Ahmad algorithm (Set B) obtained a higher accuracy rate of 93.61%. The second highest is the Ahmad algorithm (Set C) with 93.53%. The Othman algorithm achieved the lowest accuracy with 86.04% compared to the other two algorithms. Meanwhile, findings from the experiment with 100 housebreaking crime reports show similar results, with the Ahmad algorithm (Set C) achieving the highest stemming accuracy of approximately 93.80% and the Othman algorithm producing the lowest stemming accuracy (83.09%). The result indicates that stemming accuracy is consistent across different types of datasets.
NOT_RELEVANT;ProQuest;Identifying Patient Populations in Texts Describing Drug Approvals Through Deep Learning–Based Information Extraction: Development of a Natural Language Processing Algorithm;"Gendrin, Aline;Souliotis, Leonidas;Loudon-Griffiths, James;Aggarwal, Ravisha;Amoako, Daniel;Desouza, Gregory;Dimitrievska, Sashka;Metcalfe, Paul;Louvet, Emilie;Sahni, Harpreet";2023;10.2196/44876;NOT_FOUND;Background: New drug treatments are regularly approved, and it is challenging to remain up-to-date in this rapidly changing environment. Fast and accurate visualization is important to allow a global understanding of the drug market. Automation of this information extraction provides a helpful starting point for the subject matter expert, helps to mitigate human errors, and saves time. Objective: We aimed to semiautomate disease population extraction from the free text of oncology drug approval descriptions from the BioMedTracker database for 6 selected drug targets. More specifically, we intended to extract (1) line of therapy, (2) stage of cancer of the patient population described in the approval, and (3) the clinical trials that provide evidence for the approval. We aimed to use these results in downstream applications, aiding the searchability of relevant content against related drug project sources. Methods: We fine-tuned a state-of-the-art deep learning model, Bidirectional Encoder Representations from Transformers, for each of the 3 desired outputs. We independently applied rule-based text mining approaches. We compared the performances of deep learning and rule-based approaches and selected the best method, which was then applied to new entries. The results were manually curated by a subject matter expert and then used to train new models. Results: The training data set is currently small (433 entries) and will enlarge over time when new approval descriptions become available or if a choice is made to take another drug target into account. The deep learning models achieved 61% and 56% 5-fold cross-validated accuracies for line of therapy and stage of cancer, respectively, which were treated as classification tasks. Trial identification is treated as a named entity recognition task, and the 5-fold cross-validated F 1 -score is currently 87%. Although the scores of the classification tasks could seem low, the models comprise 5 classes each, and such scores are a marked improvement when compared to random classification. Moreover, we expect improved performance as the input data set grows, since deep learning models need to be trained on a large enough amount of data to be able to learn the task they are taught. The rule-based approach achieved 60% and 74% 5-fold cross-validated accuracies for line of therapy and stage of cancer, respectively. No attempt was made to define a rule-based approach for trial identification. Conclusions: We developed a natural language processing algorithm that is currently assisting subject matter experts in disease population extraction, which supports health authority approvals. This algorithm achieves semiautomation, enabling subject matter experts to leverage the results for deeper analysis and to accelerate information retrieval in a crowded clinical environment such as oncology.
NOT_RELEVANT;ProQuest;Semantic Embeddings for Arabic Retrieval Augmented Generation (ARAG);PDF;2023;10.14569/IJACSA.2023.01411135;NOT_FOUND;In recent times, Retrieval Augmented Generation (RAG) models have garnered considerable attention, primarily due to the impressive capabilities exhibited by Large Language Models (LLMs). Nevertheless, the Arabic language, despite its significance and widespread use, has received relatively less research emphasis in this field. A critical element within RAG systems is the Information Retrieval component, and at its core lies the vector embedding process commonly referred to as “semantic embedding”. This study encompasses an array of multilingual semantic embedding models, intending to enhance the model’s ability to comprehend and generate Arabic text effec-tively. We conducted an extensive evaluation of the performance of ten cutting-edge Multilingual Semantic embedding models, employing a publicly available ARCD dataset as a benchmark and assessing their performance using the average Recall@k metric. The results showed that the Microsoft E5 sentence embedding model outperformed all other models on the ARCD dataset, with Recall@10 exceeding 90%.
NOT_RELEVANT;ProQuest;AI and semantic ontology for personalized activity eCoaching in healthy lifestyle recommendations: a meta-heuristic approach;"Chatterjee, Ayan;Pahari, Nibedita;Prinz, Andreas;Riegler, Michael";2023;10.1186/s12911-023-02364-4;NOT_FOUND;"Background Automated coaches (eCoach) can help people lead a healthy lifestyle (e.g., reduction of sedentary bouts) with continuous health status monitoring and personalized recommendation generation with artificial intelligence (AI). Semantic ontology can play a crucial role in knowledge representation, data integration, and information retrieval. Methods This study proposes a semantic ontology model to annotate the AI predictions, forecasting outcomes, and personal preferences to conceptualize a personalized recommendation generation model with a hybrid approach. This study considers a mixed activity projection method that takes individual activity insights from the univariate time-series prediction and ensemble multi-class classification approaches. We have introduced a way to improve the prediction result with a residual error minimization (REM) technique and make it meaningful in recommendation presentation with a Naïve-based interval prediction approach. We have integrated the activity prediction results in an ontology for semantic interpretation. A SPARQL query protocol and RDF Query Language (SPARQL) have generated personalized recommendations in an understandable format. Moreover, we have evaluated the performance of the time-series prediction and classification models against standard metrics on both imbalanced and balanced public PMData and private MOX2-5 activity datasets. We have used Adaptive Synthetic (ADASYN) to generate synthetic data from the minority classes to avoid bias. The activity datasets were collected from healthy adults (n = 16 for public datasets; n = 15 for private datasets). The standard ensemble algorithms have been used to investigate the possibility of classifying daily physical activity levels into the following activity classes: sedentary (0), low active (1), active (2), highly active (3), and rigorous active (4). The daily step count, low physical activity (LPA), medium physical activity (MPA), and vigorous physical activity (VPA) serve as input for the classification models. Subsequently, we re-verify the classifiers on the private MOX2-5 dataset. The performance of the ontology has been assessed with reasoning and SPARQL query execution time. Additionally, we have verified our ontology for effective recommendation generation. Results We have tested several standard AI algorithms and selected the best-performing model with optimized configuration for our use case by empirical testing. We have found that the autoregression model with the REM method outperforms the autoregression model without the REM method for both datasets. Gradient Boost (GB) classifier outperforms other classifiers with a mean accuracy score of 98.00%, and 99.00% for imbalanced PMData and MOX2-5 datasets, respectively, and 98.30%, and 99.80% for balanced PMData and MOX2-5 datasets, respectively. Hermit reasoner performs better than other ontology reasoners under defined settings. Our proposed algorithm shows a direction to combine the AI prediction forecasting results in an ontology to generate personalized activity recommendations in eCoaching. Conclusion The proposed method combining step-prediction, activity-level classification techniques, and personal preference information with semantic rules is an asset for generating personalized recommendations."
NOT_RELEVANT;ProQuest;BIR: Biomedical Information Retrieval System for Cancer Treatment in Electronic Health Record Using Transformers;"Pir Noman Ahmad;Liu, Yuanchao;Khan, Khalid;Jiang, Tao;Umama Burhan";2023;10.3390/s23239355;NOT_FOUND;The rapid growth of electronic health records (EHRs) has led to unprecedented biomedical data. Clinician access to the latest patient information can improve the quality of healthcare. However, clinicians have difficulty finding information quickly and easily due to the sheer data mining volume. Biomedical information retrieval (BIR) systems can help clinicians find the information required by automatically searching EHRs and returning relevant results. However, traditional BIR systems cannot understand the complex relationships between EHR entities. Transformers are a new type of neural network that is very effective for natural language processing (NLP) tasks. As a result, transformers are well suited for tasks such as machine translation and text summarization. In this paper, we propose a new BIR system for EHRs that uses transformers for predicting cancer treatment from EHR. Our system can understand the complex relationships between the different entities in an EHR, which allows it to return more relevant results to clinicians. We evaluated our system on a dataset of EHRs and found that it outperformed state-of-the-art BIR systems on various tasks, including medical question answering and information extraction. Our results show that Transformers are a promising approach for BIR in EHRs, reaching an accuracy and an F1-score of 86.46%, and 0.8157, respectively. We believe that our system can help clinicians find the information they need more quickly and easily, leading to improved patient care.
NOT_RELEVANT;ProQuest;Keyphrase Distance Analysis Technique from News Articles as a Feature for Keyphrase Extraction: An Unsupervised Approach;"Mohammad Badrul Alam Miah;Awang, Suryanti;Rahman, Md Mustafizur;A. S. M. Sanwar Hosen";2023;10.14569/IJACSA.2023.01410104;NOT_FOUND;Due to the rapid expansion of information and online sources, automatic keyphrase extraction remains an important and challenging problem in the field of current study. The use of keyphrases is extremely beneficial for many tasks, including information retrieval (IR) systems and natural language processing (NLP). It is essential to extract the features of those keyphrases for extracting the most significant keyphrases as well as summarizing the texts to the highest standard. In order to analyze the distance between keyphrases in news articles as a feature of keyphrases, this research proposed a region-based unsupervised keyphrase distance analysis (KDA) technique. The proposed method is broken down into eight steps: gathering data, data preprocessing, data processing, searching keyphrases, distance calculation, averaging distance, curve plotting, and lastly, the curve fitting technique. The proposed approach begins by gathering two distinct datasets containing the news items, which are then used in the data preprocessing step, which makes use of a few preprocessing techniques. This preprocessed data is then employed in the data processing phase, where it is routed to the keyphrase searching, distance computation, and distance averaging phases. Finally, the curve fitting method is used after applying a curve plotting analysis. These two benchmark datasets are then used to evaluate and test the performance of the proposed approach. The proposed approach is then contrasted with different approaches to show how effective, advantageous, and significant it is. The results of the evaluation also proved that the proposed technique considerably improved the efficiency of keyphrase extraction techniques. It produces an F1-score value of 96.91% whereas its present keyphrases are 94.55%.
NOT_RELEVANT;ProQuest;Towards a Method to Enable the Selection of Physical Models within the Systems Engineering Process: A Case Study with Simulink Models;"Cibrián, Eduardo;Álvarez-Rodríguez, Jose María;Mendieta, Roy;Llorens, Juan";2023;10.3390/app132111999;NOT_FOUND;The use of different techniques and tools is a common practice to cover all stages in the development life-cycle of systems generating a significant number of work products. These artefacts are frequently encoded using diverse formats, and often require access through non-standard protocols and formats. In this context, Model-Based Systems Engineering (MBSE) emerges as a methodology to shift the paradigm of Systems Engineering practice from a document-oriented environment to a model-intensive environment. To achieve this major goal, a formalised application of modelling is employed throughout the life-cycle of systems to generate various system artefacts represented as models, such as requirements, logical models, and multi-physics models. However, the mere use of models does not guarantee one of the main challenges in the Systems Engineering discipline, namely, the reuse of system artefacts. Considering the fact that models are becoming the main type of system artefact, it is necessary to provide the capability to properly and efficiently represent and retrieve the generated models. In light of this, traditional information retrieval techniques have been widely studied to match existing software assets according to a set of capabilities or restrictions. However, there is much more at stake than the simple retrieval of models or even any piece of knowledge. An environment for model reuse must provide the proper mechanisms to (1) represent any piece of data, information, or knowledge under a common and shared data model, and (2) provide advanced retrieval mechanisms to elevate the meaning of information resources from text-based descriptions to concept-based ones. This need has led to novel methods using word embeddings and vector-based representations to semantically encode information. Such methods are applied to encode the information of physical models while preserving their underlying semantics. In this study, a text corpus from MATLAB Simulink models was preprocessed using Natural Language Processing (NLP) techniques and trained to generate word vector representations. Then, the presented method was validated using a testbed of MATLAB Simulink physical models in which verbalisations of models are transformed into vectors. The effectiveness of the proposed solution was assessed through a use case study. Evaluation of the results demonstrates a precision value of 0.925, a recall value of 0.865, and an F1 score of 0.884.
NOT_RELEVANT;ProQuest;Building a search tool for compositely annotated entities using Transformer-based approach: Case study in Biosimulation Model Search Engine (BMSE);"Yuda, Munarko;Rampadarath Anand;Nickerson, David";2023;10.12688/f1000research.128982.1;NOT_FOUND;The Transformer-based approaches to solving natural language processing (NLP) tasks such as BERT and GPT are gaining popularity due to their ability to achieve high performance. These approaches benefit from using enormous data sizes to create pre-trained models and the ability to understand the context of words in a sentence. Their use in the information retrieval domain is thought to increase effectiveness and efficiency. This paper demonstrates a BERT-based method (CASBERT) implementation to build a search tool over data annotated compositely using ontologies. The data was a collection of biosimulation models written using the CellML standard in the Physiome Model Repository (PMR). A biosimulation model structurally consists of basic entities of constants and variables that construct higher-level entities such as components, reactions, and the model. Finding these entities specific to their level is beneficial for various purposes regarding variable reuse, experiment setup, and model audit. Initially, we created embeddings representing compositely-annotated entities for constant and variable search (lowest level entity). Then, these low-level entity embeddings were vertically and efficiently combined to create higher-level entity embeddings to search components, models, images, and simulation setups. Our approach was general, so it can be used to create search tools with other data semantically annotated with ontologies - biosimulation models encoded in the SBML format, for example. Our tool is named Biosimulation Model Search Engine (BMSE).
NOT_RELEVANT;ProQuest;Leveraging Generative AI and Large Language Models: A Comprehensive Roadmap for Healthcare Integration;"Yu, Ping;Xu, Hua;Hu, Xia;Deng, Chao";2023;10.3390/healthcare11202776;NOT_FOUND;Generative artificial intelligence (AI) and large language models (LLMs), exemplified by ChatGPT, are promising for revolutionizing data and information management in healthcare and medicine. However, there is scant literature guiding their integration for non-AI professionals. This study conducts a scoping literature review to address the critical need for guidance on integrating generative AI and LLMs into healthcare and medical practices. It elucidates the distinct mechanisms underpinning these technologies, such as Reinforcement Learning from Human Feedback (RLFH), including few-shot learning and chain-of-thought reasoning, which differentiates them from traditional, rule-based AI systems. It requires an inclusive, collaborative co-design process that engages all pertinent stakeholders, including clinicians and consumers, to achieve these benefits. Although global research is examining both opportunities and challenges, including ethical and legal dimensions, LLMs offer promising advancements in healthcare by enhancing data management, information retrieval, and decision-making processes. Continued innovation in data acquisition, model fine-tuning, prompt strategy development, evaluation, and system implementation is imperative for realizing the full potential of these technologies. Organizations should proactively engage with these technologies to improve healthcare quality, safety, and efficiency, adhering to ethical and legal guidelines for responsible application.
NOT_RELEVANT;ProQuest;Machine Learning Model for Automated Assessment of Short Subjective Answers;"Zaira Hassan Amur;Yew Kwang Hooi;Bhanbro, Hina;Mairaj Nabi Bhatti;Gul Muhammad Soomro";2023;10.14569/IJACSA.2023.0140812;NOT_FOUND;"Natural Language Processing (NLP) has recently gained significant attention; where, semantic similarity techniques are widely used in diverse applications, such as information retrieval, question-answering systems, and sentiment analysis. One promising area where NLP is being applied, is personalized learning, where assessment and adaptive tests are used to capture students' cognitive abilities. In this context, open-ended questions are commonly used in assessments due to their simplicity, but their effectiveness depends on the type of answer expected. To improve comprehension, it is essential to understand the underlying meaning of short text answers, which is challenging due to their length, lack of clarity, and structure. Researchers have proposed various approaches, including distributed semantics and vector space models, However, assessing short answers using these methods presents significant challenges, but machine learning methods, such as transformer models with multi-head attention, have emerged as advanced techniques for understanding and assessing the underlying meaning of answers. This paper proposes a transformer learning model that utilizes multi-head attention to identify and assess students' short answers to overcome these issues. Our approach improves the performance of assessing the assessments and outperforms current state-of-the-art techniques. We believe our model has the potential to revolutionize personalized learning and significantly contribute to improving student outcomes."
NOT_RELEVANT;ProQuest;Information and Media Literacy in the Age of AI: Options for the Future;"Tiernan, Peter;Costello, Eamon;Donlon, Enda;Parysz, Maria;Scriney, Michael";2023;10.3390/educsci13090906;NOT_FOUND;The concepts of information and media literacy have been central components of digital literacy since the digitization of information began. However, the increasing influence of artificial intelligence on how individuals locate, evaluate, and create content has significant implications for what it means to be information and media literate. This paper begins by exploring the role artificial intelligence plays at the various stages of information retrieval and creation processes. Following this, the paper reviews existing digital literacy frameworks to ascertain their definitions of information and media literacy and the potential impact of artificial intelligence on them. We find that digital literacy frameworks have been slow to react to artificial intelligence and its repercussions, and we recommend a number of strategies for the future. These strategies center around a more agile, responsive, and participatory approach to digital literacy framework development and maintenance.
NOT_RELEVANT;ProQuest;Evaluation of an Arabic Chatbot Based on Extractive Question-Answering Transfer Learning and Language Transformers;"Alruqi, Tahani N;Alzahrani, Salha M";2023;10.3390/ai4030035;NOT_FOUND;Chatbots are programs with the ability to understand and respond to natural language in a way that is both informative and engaging. This study explored the current trends of using transformers and transfer learning techniques on Arabic chatbots. The proposed methods used various transformers and semantic embedding models from AraBERT, CAMeLBERT, AraElectra-SQuAD, and AraElectra (Generator/Discriminator). Two datasets were used for the evaluation: one with 398 questions, and the other with 1395 questions and 365,568 documents sourced from Arabic Wikipedia. Extensive experimental works were conducted, evaluating both manually crafted questions and the entire set of questions by using confidence and similarity metrics. Our experimental results demonstrate that combining the power of transformer architecture with extractive chatbots can provide more accurate and contextually relevant answers to questions in Arabic. Specifically, our experimental results showed that the AraElectra-SQuAD model consistently outperformed other models. It achieved an average confidence score of 0.6422 and an average similarity score of 0.9773 on the first dataset, and an average confidence score of 0.6658 and similarity score of 0.9660 on the second dataset. The study concludes that the AraElectra-SQuAD showed remarkable performance, high confidence, and robustness, which highlights its potential for practical applications in natural language processing tasks for Arabic chatbots. The study suggests that the language transformers can be further enhanced and used for various tasks, such as specialized chatbots, virtual assistants, and information retrieval systems for Arabic-speaking users.
NOT_RELEVANT;ProQuest;An Extended AHP-Based Corpus Assessment Approach for Handling Keyword Ranking of NLP: An Example of COVID-19 Corpus Data;"Liang-Ching, Chen;Kuei-Hu, Chang";2023;10.3390/axioms12080740;NOT_FOUND;"The use of corpus assessment approaches to determine and rank keywords for corpus data is critical due to the issues of information retrieval (IR) in Natural Language Processing (NLP), such as when encountering COVID-19, as it can determine whether people can rapidly obtain knowledge of the disease. The algorithms used for corpus assessment have to consider multiple parameters and integrate individuals’ subjective evaluation information simultaneously to meet real-world needs. However, traditional keyword-list-generating approaches are based on only one parameter (i.e., the keyness value) to determine and rank keywords, which is insufficient. To improve the evaluation benefit of the traditional keyword-list-generating approach, this paper proposed an extended analytic hierarchy process (AHP)-based corpus assessment approach to, firstly, refine the corpus data and then use the AHP method to compute the relative weights of three parameters (keyness, frequency, and range). To verify the proposed approach, this paper adopted 53 COVID-19-related research environmental science research articles from the Web of Science (WOS) as an empirical example. After comparing with the traditional keyword-list-generating approach and the equal weights (EW) method, the significant contributions are: (1) using the machine-based technique to remove function and meaningless words for optimizing the corpus data; (2) being able to consider multiple parameters simultaneously; and (3) being able to integrate the experts’ evaluation results to determine the relative weights of the parameters."
NOT_RELEVANT;ProQuest;The <i>Cannabis sativa</i> genetics and therapeutics relationship network: automatically associating cannabis-related genes to therapeutic properties through chemicals from cannabis literature;NOT_FOUND;2023;10.1186/s42238-023-00182-z;NOT_FOUND;Background Understanding the genome of Cannabis sativa holds significant scientific value due to the multi-faceted therapeutic nature of the plant. Links from cannabis gene to therapeutic property are important to establish gene targets for the optimization of specific therapeutic properties through selective breeding of cannabis strains. Our work establishes a resource for quickly obtaining a complete set of therapeutic properties and genes associated with any known cannabis chemical constituent, as well as relevant literature. Methods State-of-the-art natural language processing (NLP) was used to automatically extract information from many cannabis-related publications, thus producing an undirected multipartite weighted-edge paragraph co-occurrence relationship network composed of two relationship types, gene-chemical and chemical property. We also developed an interactive application to visualize sub-graphs of manageable size. Results Two hundred thirty-four cannabis constituent chemicals, 352 therapeutic properties, and 124 genes from the Cannabis sativa genome form a multipartite network graph which transforms 29,817 cannabis-related research documents from PubMed Central into an easy to visualize and explore network format. Conclusion Use of our network replaces time-consuming and labor intensive manual extraction of information from the large amount of available cannabis literature. This streamlined information retrieval process will enhance the activities of cannabis breeders, cannabis researchers, organic biochemists, pharmaceutical researchers and scientists in many other disciplines.
MAYBE_RELEVANT;ProQuest;ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge;"Li, Yunxiang;Li Zihan;Zhang, Kai;Ruilong, Dan;Jiang, Steve;Zhang, You";2023;10.7759/cureus.40895;NOT_FOUND;"Objective The primary aim of this research was to address the limitations observed in the medical knowledge of prevalent large language models (LLMs) such as ChatGPT, by creating a specialized language model with enhanced accuracy in medical advice. Methods We achieved this by adapting and refining the large language model meta-AI (LLaMA)&#xa0;using a large dataset of 100,000 patient-doctor dialogues sourced from a widely used online medical consultation platform. These conversations were cleaned and anonymized to respect privacy concerns. In addition to the model refinement, we incorporated a self-directed information retrieval mechanism, allowing the model to access and utilize real-time information from online sources like Wikipedia and data from curated offline medical databases. Results The fine-tuning of the model with real-world patient-doctor interactions significantly improved the model's ability to understand patient needs and provide informed advice. By equipping the model with self-directed information retrieval from reliable online and offline sources, we observed substantial improvements in the accuracy of its responses. Conclusion Our proposed ChatDoctor, represents a significant advancement in medical LLMs, demonstrating a significant improvement in understanding patient inquiries and providing accurate advice. Given the high stakes and low error tolerance in the medical field, such enhancements in providing accurate and reliable information are not only beneficial but essential."
MAYBE_RELEVANT;ProQuest;An Intelligent Conversational Agent for the Legal Domain;"Amato, Flora;Fonisto, Mattia;Giacalone, Marco;Sansone, Carlo";2023;10.3390/info14060307;NOT_FOUND;"An intelligent conversational agent for the legal domain is an AI-powered system that can communicate with users in natural language and provide legal advice or assistance. In this paper, we present CREA2, an agent designed to process legal concepts and be able to guide users on legal matters. The conversational agent can help users navigate legal procedures, understand legal jargon, and provide recommendations for legal action. The agent can also give suggestions helpful in drafting legal documents, such as contracts, leases, and notices. Additionally, conversational agents can help reduce the workload of legal professionals by handling routine legal tasks. CREA2, in particular, will guide the user in resolving disputes between people residing within the European Union, proposing solutions in controversies between two or more people who are contending over assets in a divorce, an inheritance, or the division of a company. The conversational agent can later be accessed through various channels, including messaging platforms, websites, and mobile applications. This paper presents a retrieval system that evaluates the similarity between a user’s query and a given question. The system uses natural language processing (NLP) algorithms to interpret user input and associate responses by addressing the problem as a semantic search similar question retrieval. Although a common approach to question and answer (Q&amp;A) retrieval is to create labelled Q&amp;A pairs for training, we exploit an unsupervised information retrieval system in order to evaluate the similarity degree between a given query and a set of questions contained in the knowledge base. We used the recently proposed SBERT model for the evaluation of relevance. In the paper, we illustrate the effective design principles, the implemented details and the results of the conversational system and describe the experimental campaign carried out on it."
NOT_RELEVANT;ProQuest;An Intelligent Semi-Honest System for Secret Matching against Malicious Adversaries;"Liu, Xin;Kong, Jianwei;Luo, Dan;Xiong, Neal;Xu, Gang;Chen, Xiubo";2023;10.3390/electronics12122617;NOT_FOUND;With natural language processing as an important research direction in deep learning, the problems of text similarity calculation, natural language inference, question and answer systems, and information retrieval can be regarded as text matching applications for different data and scenarios. Secure matching computation of text string patterns can solve the privacy protection problem in the fields of biological sequence analysis, keyword search, and database query. In this paper, we propose an Intelligent Semi-Honest System (ISHS) for secret matching against malicious adversaries. Firstly, a secure computation protocol based on the semi-honest model is designed for the secret matching of text strings, which adopts a new digital encoding method and an ECC encryption algorithm and can provide a solution for honest participants. The text string matching protocol under the malicious model which uses the cut-and-choose method and zero-knowledge proof is designed for resisting malicious behaviors that may be committed by malicious participants in the semi-honest protocol. The correctness and security of the protocol are analyzed, which is more efficient and has practical value compared with the existing algorithms. The secure text matching has important engineering applications.
NOT_RELEVANT;ProQuest;Quantifying the Dissimilarity of Texts;"Shade, Benjamin;Altmann, Eduardo G";2023;10.3390/info14050271;NOT_FOUND;Quantifying the dissimilarity of two texts is an important aspect of a number of natural language processing tasks, including semantic information retrieval, topic classification, and document clustering. In this paper, we compared the properties and performance of different dissimilarity measures D using three different representations of texts—vocabularies, word frequency distributions, and vector embeddings—and three simple tasks—clustering texts by author, subject, and time period. Using the Project Gutenberg database, we found that the generalised Jensen–Shannon divergence applied to word frequencies performed strongly across all tasks, that D ’s based on vector embedding representations led to stronger performance for smaller texts, and that the optimal choice of approach was ultimately task-dependent. We also investigated, both analytically and numerically, the behaviour of the different D ’s when the two texts varied in length by a factor h . We demonstrated that the (natural) estimator of the Jaccard distance between vocabularies was inconsistent and computed explicitly the h -dependency of the bias of the estimator of the generalised Jensen–Shannon divergence applied to word frequencies. We also found numerically that the Jensen–Shannon divergence and embedding-based approaches were robust to changes in h , while the Jaccard distance was not.
NOT_RELEVANT;ProQuest;Householder Transformation-Based Temporal Knowledge Graph Reasoning;"Zhao, Xiaojuan;Li, Aiping;Jiang, Rong;Chen, Kai;Peng, Zhichao";2023;10.3390/electronics12092001;NOT_FOUND;Knowledge graphs’ reasoning is of great significance for the further development of artificial intelligence and information retrieval, especially for reasoning over temporal knowledge graphs. The rotation-based method has been shown to be effective at modeling entities and relations on a knowledge graph. However, due to the lack of temporal information representation capability, existing approaches can only model partial relational patterns and they cannot handle temporal combination reasoning. In this regard, we propose HTTR: Householder Transformation-based Temporal knowledge graph Reasoning, which focuses on the characteristics of relations that evolve over time. HTTR first fuses the relation and temporal information in the knowledge graph, then uses the Householder transformation to obtain an orthogonal matrix about the fused information, and finally defines the orthogonal matrix as the rotation of the head-entity to the tail-entity and calculates the similarity between the rotated vector and the vector representation of the tail entity. In addition, we compare three methods for fusing relational and temporal information. We allow other fusion methods to replace the current one as long as the dimensionality satisfies the requirements. We show that HTTR is able to outperform state-of-the-art methods in temporal knowledge graph reasoning tasks and has the ability to learn and infer all of the four relational patterns over time: symmetric reasoning, antisymmetric reasoning, inversion reasoning, and temporal combination reasoning.
NOT_RELEVANT;ProQuest;Developing an Urdu Lemmatizer Using a Dictionary-Based Lookup Approach;"Shaukat, Saima;Asad, Muhammad;Asmara Akram";2023;10.3390/app13085103;NOT_FOUND;Lemmatization aims at returning the root form of a word. The lemmatizer is envisioned as a vital instrument that can assist in many Natural Language Processing (NLP) tasks. These tasks include Information Retrieval, Word Sense Disambiguation, Machine Translation, Text Reuse, and Plagiarism Detection. Previous studies in the literature have focused on developing lemmatizers using rule-based approaches for English and other highly-resourced languages. However, there have been no thorough efforts for the development of a lemmatizer for most South Asian languages, specifically Urdu. Urdu is a morphologically rich language with many inflectional and derivational forms. This makes the development of an efficient Urdu lemmatizer a challenging task. A standardized lemmatizer would contribute towards establishing much-needed methodological resources for this low-resourced language, which are required to boost the performance of many Urdu NLP applications. This paper presents a lemmatization system for the Urdu language, based on a novel dictionary lookup approach. The contributions made through this research are the following: (1) the development of a large benchmark corpus for the Urdu language, (2) the exploration of the relationship between parts of speech tags and the lemmatizer, and (3) the development of standard approaches for an Urdu lemmatizer. Furthermore, we experimented with the impact of Part of Speech (PoS) on our proposed dictionary lookup approach. The empirical results showed that we achieved the best accuracy score of 76.44% through the proposed dictionary lookup approach.
NOT_RELEVANT;ProQuest;A General Paradigm for Retrieving Soil Moisture and Surface Temperature from Passive Microwave Remote Sensing Data Based on Artificial Intelligence;"Mao, Kebiao;Wang, Han;Shi, Jiancheng;Heggy, Essam;Wu, Shengli;Bateni, Sayed M;Du, Guoming";2023;10.3390/rs15071793;NOT_FOUND;Soil moisture (SM) and land surface temperature (LST) are entangled, and the retrieval of one of them requires a priori specification of the other one. Due to insufficient observational information, retrieval of LST and SM from passive microwave remote sensing data is often ill-posed, and the retrieval accuracy needs to be improved. In this study, a novel fully-coupled paradigm is developed to robustly retrieve SM and LST from passive microwave data, which integrates deep learning, physical methods, and statistical methods. The key condition of the general paradigm proposed by us is that the output parameters of deep learning can be uniquely determined by the input parameters theoretically through a certain mathematical equation. Firstly, the physical method is deduced based on the energy radiation balance equation. The nine unknowns require the brightness temperatures of nine channels to construct nine equations, and the solutions of the physical method equations are obtained by model simulation. Based on the derivation of the physical method, the solution of the statistical method is constructed using multi-source data. Secondly, the solutions of physical and statistical methods constitute the training and test data of deep learning, which is used to obtain the solution curve of physical and statistical methods. The retrieval accuracy of LST and SM is greatly improved by smartly utilizing the mutual prior knowledge of SM and LST and cross iterative optimization calculations. Finally, validation indicates that the mean absolute error of the retrieved SM and LST data are 0.027 m 3 /m 3 and 1.38 K, respectively, at an incidence angle of 0–65°. A model-data-knowledge-driven and deep learning method can overcome the shortcomings of traditional methods and provide a paradigm for retrieval of other geophysical variables. The proposed paradigm not only has physical meaning, but also makes deep learning physically interpretable, which is a milestone in the retrieval of geophysical remote sensing parameters based on artificial intelligence technology.
NOT_RELEVANT;ProQuest;Deep Learning-Based Framework for Soil Moisture Content Retrieval of Bare Soil from Satellite Data;"Dabboor, Mohammed;Atteia, Ghada;Meshoul, Souham;Alayed, Walaa";2023;10.3390/rs15071916;NOT_FOUND;Machine learning (ML) is a branch of artificial intelligence (AI) that has been successfully applied in a variety of remote sensing applications, including geophysical information retrieval such as soil moisture content (SMC). Deep learning (DL) is a subfield of ML that uses models with complex structures to solve prediction problems with higher performance than traditional ML. In this study, a framework based on DL was developed for SMC retrieval. For this purpose, a sample dataset was built, which included synthetic aperture radar (SAR) backscattering, radar incidence angle, and ground truth data. Herein, the performance of five optimized ML prediction models was evaluated in terms of soil moisture prediction. However, to boost the prediction performance of these models, a DL-based data augmentation technique was implemented to create a reconstructed version of the available dataset. This includes building a sparse autoencoder DL network for data reconstruction. The Bayesian optimization strategy was employed for fine-tuning the hyperparameters of the ML models in order to improve their prediction performance. The results of our study highlighted the improved performance of the five ML prediction models with augmented data. The Gaussian process regression (GPR) showed the best prediction performance with 4.05% RMSE and 0.81 R 2 on a 10% independent test subset.
NOT_RELEVANT;ProQuest;Ontology Learning Applications of Knowledge Base Construction for Microelectronic Systems Information;"Wawrzik, Frank;Khushnood Adil Rafique;Rahman, Farin;Grimm, Christoph";2023;10.3390/info14030176;NOT_FOUND;Knowledge base construction (KBC) using AI has been one of the key goals of this highly popular technology since its emergence, as it helps to comprehend everything, including relations, around us. The construction of knowledge bases can summarize a piece of text in a machine-processable and understandable way. This can prove to be valuable and assistive to knowledge engineers. In this paper, we present the application of natural language processing in the construction of knowledge bases. We demonstrate how a trained bidirectional long short-term memory or bi-LSTM neural network model can be used to construct knowledge bases in accordance with the exact ISO26262 definitions as defined in the GENIAL! Basic Ontology. We provide the system with an electronic text document from the microelectronics domain and the system attempts to create a knowledge base from the available information in textual format. This information is then expressed in the form of graphs when queried by the user. This method of information retrieval presents the user with a much more technical and comprehensive understanding of an expert piece of text. This is achieved by applying the process of named entity recognition (NER) for knowledge extraction. This paper provides a result report of the current status of our knowledge construction process and knowledge base content, as well as describes our challenges and experiences.
NOT_RELEVANT;ProQuest;Learned Text Representation for Amharic Information Retrieval and Natural Language Processing;"Yeshambel, Tilahun;Mothe, Josiane;Assabie, Yaregal";2023;10.3390/info14030195;NOT_FOUND;Over the past few years, word embeddings and bidirectional encoder representations from transformers (BERT) models have brought better solutions to learning text representations for natural language processing (NLP) and other tasks. Many NLP applications rely on pre-trained text representations, leading to the development of a number of neural network language models for various languages. However, this is not the case for Amharic, which is known to be a morphologically complex and under-resourced language. Usable pre-trained models for automatic Amharic text processing are not available. This paper presents an investigation on the essence of learned text representation for information retrieval and NLP tasks using word embeddings and BERT language models. We explored the most commonly used methods for word embeddings, including word2vec, GloVe, and fastText, as well as the BERT model. We investigated the performance of query expansion using word embeddings. We also analyzed the use of a pre-trained Amharic BERT model for masked language modeling, next sentence prediction, and text classification tasks. Amharic ad hoc information retrieval test collections that contain word-based, stem-based, and root-based text representations were used for evaluation. We conducted a detailed empirical analysis on the usability of word embeddings and BERT models on word-based, stem-based, and root-based corpora. Experimental results show that word-based query expansion and language modeling perform better than stem-based and root-based text representations, and fastText outperforms other word embeddings on word-based corpus.
NOT_RELEVANT;ProQuest;Short-Text Semantic Similarity (STSS): Techniques, Challenges and Future Perspectives;"Zaira Hassan Amur;Yew Kwang Hooi;Bhanbhro, Hina;Dahri, Kamran;Gul Muhammad Soomro";2023;10.3390/app13063911;NOT_FOUND;In natural language processing, short-text semantic similarity (STSS) is a very prominent field. It has a significant impact on a broad range of applications, such as question–answering systems, information retrieval, entity recognition, text analytics, sentiment classification, and so on. Despite their widespread use, many traditional machine learning techniques are incapable of identifying the semantics of short text. Traditional methods are based on ontologies, knowledge graphs, and corpus-based methods. The performance of these methods is influenced by the manually defined rules. Applying such measures is still difficult, since it poses various semantic challenges. In the existing literature, the most recent advances in short-text semantic similarity (STSS) research are not included. This study presents the systematic literature review (SLR) with the aim to (i) explain short sentence barriers in semantic similarity, (ii) identify the most appropriate standard deep learning techniques for the semantics of a short text, (iii) classify the language models that produce high-level contextual semantic information, (iv) determine appropriate datasets that are only intended for short text, and (v) highlight research challenges and proposed future improvements. To the best of our knowledge, we have provided an in-depth, comprehensive, and systematic review of short text semantic similarity trends, which will assist the researchers to reuse and enhance the semantic information.
NOT_RELEVANT;ProQuest;Prompt-Based Word-Level Information Injection BERT for Chinese Named Entity Recognition;"He, Qiang;Chen, Guowei;Song, Wenchao;Zhang, Pengzhou";2023;10.3390/app13053331;NOT_FOUND;Named entity recognition (NER) is a subfield of natural language processing (NLP) that identifies and classifies entities from plain text, such as people, organizations, locations, and other types. NER is a fundamental task in information extraction, information retrieval, and text summarization, as it helps to organize the relevant information in a structured way. The current approaches to Chinese named entity recognition do not consider the category information of matched Chinese words, which limits their ability to capture the correlation between words. This makes Chinese NER more challenging than English NER, which already has well-defined word boundaries. To improve Chinese NER, it is necessary to develop new approaches that take into account category features of matched Chinese words, and the category information would help to effectively capture the relationship between words. This paper proposes a Prompt-based Word-level Information Injection BERT (PWII-BERT) to integrate prompt-guided lexicon information into a pre-trained language model. Specifically, we engineer a Word-level Information Injection Adapter (WIIA) through the original Transformer encoder and prompt-guided Transformer layers. Thus, the ability of PWII-BERT to explicitly obtain fine-grained character-to-word relevant information according to the category prompt is one of its key advantages. In experiments on four benchmark datasets, PWII-BERT outperforms the baselines, demonstrating the significance of fully utilizing the advantages of fusing the category information and lexicon feature to implement Chinese NER.
NOT_RELEVANT;ProQuest;Toward a Multi-Column Knowledge-Oriented Neural Network for Web Corpus Causality Mining;"Wajid, Ali;Zuo, Wanli;Wang, Ying;Rahman, Ali";2023;10.3390/app13053047;NOT_FOUND;In the digital age, many sources of textual content are devoted to studying and expressing many sorts of relationships, including employer–employee, if–then, part–whole, product–producer, and cause–effect relations/causality. Mining cause–effect relations are a key topic in many NLP (natural language processing) applications, such as future event prediction, information retrieval, healthcare, scenario generation, decision making, commerce risk management, question answering, and adverse drug reaction. Many statistical and non-statistical methods have been developed in the past to address this topic. Most of them frequently used feature-driven supervised approaches and hand-crafted linguistic patterns. However, the implicit and ambiguous statement of causation prevented these methods from achieving great recall and precision. They cover a limited set of implicit causality and are difficult to extend. In this work, a novel MCKN (multi-column knowledge-oriented network) is introduced. This model includes various knowledge-oriented channels/columns (KCs), where each channel integrates prior human knowledge to capture language cues of causation. MCKN uses unique convolutional word filters (wf) generated automatically using WordNet and FrameNet. To reduce MCKN’s dimensionality, we use filter selection and clustering approaches. Our model delivers superior performance on the Alternative Lexicalization (AltLexes) dataset, proving that MCKN is a simpler and distinctive approach for informal datasets.
NOT_RELEVANT;ProQuest;Pattern-based hybrid book recommendation system using semantic relationships;"Wayesa, Fikadu;Leranso, Mesfin;Asefa, Girma;Kedir, Abduljebar";2023;10.1038/s41598-023-30987-0;NOT_FOUND;In the fields of machine learning and artificial intelligence, recommendation systems (RS) or recommended engines are commonly used. In today's world, recommendation systems based on user preferences assist consumers in making the best decisions without depleting their cognitive resources. They can be applied to a variety of things, including search engines, travel, music, movies, literature, news, gadgets, and dining. A lot of people utilize RS on social media sites like Facebook, Twitter, and LinkedIn, and it has proven beneficial in corporate settings like those at Amazon, Netflix, Pandora, and Yahoo. There have been numerous proposals for recommender system variations. However, certain techniques result in unfairly recommended things due to biased data because there are no established connections between the items and consumers. In order to solve the challenges mentioned above for new users, we propose in this work to employ Content-based Filtering (CBF) and Collaborative Filtering (CF) with semantic relationships to capture the relationships as knowledge-based book recommendations to readers in a digital library. When proposing things, patterns are more discriminative than single phrases. To capture the similarity of the books that the new user had retrieved, the patterns were grouped in a semantically equivalent manner using the Clustering method. The effectiveness of the suggested model is examined through a series of extensive tests employing Information Retrieval (IR) evaluation criteria. Recall Precision and F-Measure, two of the three widely used performance measuring metrics, were employed. The findings demonstrate that the suggested model performs noticeably better than cutting-edge models.
NOT_RELEVANT;ProQuest;Educational Psychology Aspects of Learning with Chatbots without Artificial Intelligence: Suggestions for Designers;Černý, Michal;2023;10.3390/ejihpe13020022;NOT_FOUND;Chatbots without artificial intelligence can play the role of practical and easy-to-implement learning objects in e-learning environments, allowing a reduction in social or psychological isolation. This research, with a sample of 79 students, explores the principles that need to be followed in designing this kind of chatbot in education in order to ensure an acceptable outcome for students. Research has shown that students interacting with a chatbot without artificial intelligence expect similar psychological and communicative responses to those of a live human, project the characteristics of the chatbot from the dialogue, and are taken aback when the chatbot does not understand or cannot help them sufficiently. The study is based on a design through research approach, in which students in information studies and library science interacted with a specific chatbot focused on information retrieval, and recorded their experiences and feelings in an online questionnaire. The study intends to find principles for the design of chatbots without artificial intelligence so that students feel comfortable interacting with them.
NOT_RELEVANT;ProQuest;Design, implementation, and evaluation of the computer-aided clinical decision support system based on learning-to-rank: collaboration between physicians and machine learning in the differential diagnosis process;"Miyachi, Yasuhiko;Ishii, Osamu;Torigoe, Keijiro";2023;10.1186/s12911-023-02123-5;NOT_FOUND;Background We are researching, developing, and publishing the clinical decision support system based on learning-to-rank. The main objectives are (1) To support for differential diagnoses performed by internists and general practitioners and (2) To prevent diagnostic errors made by physicians. The main features are that “A physician inputs a patient's symptoms, findings, and test results to the system, and the system outputs a ranking list of possible diseases”. Method The software libraries for machine learning and artificial intelligence are TensorFlow and TensorFlow Ranking. The prediction algorithm is Learning-to-Rank with the listwise approach. The ranking metric is normalized discounted cumulative gain (NDCG). The loss functions are Approximate NDCG (A-NDCG). We evaluated the machine learning performance on k-fold cross-validation. We evaluated the differential diagnosis performance with validated cases. Results The machine learning performance of our system was much higher than that of the conventional system. The differential diagnosis performance of our system was much higher than that of the conventional system. We have shown that the clinical decision support system prevents physicians' diagnostic errors due to confirmation bias. Conclusions We have demonstrated that the clinical decision support system is useful for supporting differential diagnoses and preventing diagnostic errors. We propose that differential diagnosis by physicians and learning-to-rank by machine has a high affinity. We found that information retrieval and clinical decision support systems have much in common (Target data, learning-to-rank, etc.). We propose that Clinical Decision Support Systems have the potential to support: (1) recall of rare diseases, (2) differential diagnoses for difficult-to-diagnoses cases, and (3) prevention of diagnostic errors. Our system can potentially evolve into an explainable clinical decision support system.
NOT_RELEVANT;ProQuest;Contextual Urdu Lemmatization Using Recurrent Neural Network Models;"Hafeez, Rabab;Muhammad, Waqas Anwar;Jamal, Muhammad Hasan;Fatima, Tayyaba;Martínez Espinosa, Julio César;Luis Alonso Dzul López;Ernesto Bautista Thompson;Imran Ashraf";2023;10.3390/math11020435;NOT_FOUND;In the field of natural language processing, machine translation is a colossally developing research area that helps humans communicate more effectively by bridging the linguistic gap. In machine translation, normalization and morphological analyses are the first and perhaps the most important modules for information retrieval (IR). To build a morphological analyzer, or to complete the normalization process, it is important to extract the correct root out of different words. Stemming and lemmatization are techniques commonly used to find the correct root words in a language. However, a few studies on IR systems for the Urdu language have shown that lemmatization is more effective than stemming due to infixes found in Urdu words. This paper presents a lemmatization algorithm based on recurrent neural network models for the Urdu language. However, lemmatization techniques for resource-scarce languages such as Urdu are not very common. The proposed model is trained and tested on two datasets, namely, the Urdu Monolingual Corpus (UMC) and the Universal Dependencies Corpus of Urdu (UDU). The datasets are lemmatized with the help of recurrent neural network models. The Word2Vec model and edit trees are used to generate semantic and syntactic embedding. Bidirectional long short-term memory (BiLSTM), bidirectional gated recurrent unit (BiGRU), bidirectional gated recurrent neural network (BiGRNN), and attention-free encoder–decoder (AFED) models are trained under defined hyperparameters. Experimental results show that the attention-free encoder-decoder model achieves an accuracy, precision, recall, and F-score of 0.96, 0.95, 0.95, and 0.95, respectively, and outperforms existing models.
NOT_RELEVANT;ProQuest;The New Version of the ANDDigest Tool with Improved AI-Based Short Names Recognition;"Ivanisenko, Timofey V;Demenkov, Pavel S;Kolchanov, Nikolay A;Ivanisenko, Vladimir A";2022;10.3390/ijms232314934;NOT_FOUND;The body of scientific literature continues to grow annually. Over 1.5 million abstracts of biomedical publications were added to the PubMed database in 2021. Therefore, developing cognitive systems that provide a specialized search for information in scientific publications based on subject area ontology and modern artificial intelligence methods is urgently needed. We previously developed a web-based information retrieval system, ANDDigest, designed to search and analyze information in the PubMed database using a customized domain ontology. This paper presents an improved ANDDigest version that uses fine-tuned PubMedBERT classifiers to enhance the quality of short name recognition for molecular-genetics entities in PubMed abstracts on eight biological object types: cell components, diseases, side effects, genes, proteins, pathways, drugs, and metabolites. This approach increased average short name recognition accuracy by 13%.
NOT_RELEVANT;ProQuest;Artificial intelligence for the prediction of acute kidney injury during the perioperative period: systematic review and Meta-analysis of diagnostic test accuracy;"Zhang, Hanfei;Wang, Amanda Y;Wu, Shukun;Ngo, Johnathan;Feng, Yunlin;He, Xin;Zhang, Yingfeng;Wu, Xingwei;Hong, Daqing";2022;10.1186/s12882-022-03025-w;NOT_FOUND;Background Acute kidney injury (AKI) is independently associated with morbidity and mortality in a wide range of surgical settings. Nowadays, with the increasing use of electronic health records (EHR), advances in patient information retrieval, and cost reduction in clinical informatics, artificial intelligence is increasingly being used to improve early recognition and management for perioperative AKI. However, there is no quantitative synthesis of the performance of these methods. We conducted this systematic review and meta-analysis to estimate the sensitivity and specificity of artificial intelligence for the prediction of acute kidney injury during the perioperative period. Methods Pubmed, Embase, and Cochrane Library were searched to 2nd October 2021. Studies presenting diagnostic performance of artificial intelligence in the early detection of perioperative acute kidney injury were included. True positives, false positives, true negatives and false negatives were pooled to collate specificity and sensitivity with 95% CIs and results were portrayed in forest plots. The risk of bias of eligible studies was assessed using the PROBAST tool. Results Nineteen studies involving 304,076 patients were included. Quantitative random-effects meta-analysis using the Rutter and Gatsonis hierarchical summary receiver operating characteristics (HSROC) model revealed pooled sensitivity, specificity, and diagnostic odds ratio of 0.77 (95% CI: 0.73 to 0.81),0.75 (95% CI: 0.71 to 0.80), and 10.7 (95% CI 8.5 to 13.5), respectively. Threshold effect was found to be the only source of heterogeneity, and there was no evidence of publication bias. Conclusions Our review demonstrates the promising performance of artificial intelligence for early prediction of perioperative AKI. The limitations of lacking external validation performance and being conducted only at a single center should be overcome. Trial registration This study was not registered with PROSPERO.
NOT_RELEVANT;ProQuest;From Text Representation to Financial Market Prediction: A Literature Review;"Farimani, Saeede Anbaee;Majid Vafaei Jahan;Amin Milani Fard";2022;10.3390/info13100466;NOT_FOUND;News dissemination in social media causes fluctuations in financial markets. (Scope) Recent advanced methods in deep learning-based natural language processing have shown promising results in financial market analysis. However, understanding how to leverage large amounts of textual data alongside financial market information is important for the investors’ behavior analysis. In this study, we review over 150 publications in the field of behavioral finance that jointly investigated natural language processing (NLP) approaches and a market data analysis for financial decision support. This work differs from other reviews by focusing on applied publications in computer science and artificial intelligence that contributed to a heterogeneous information fusion for the investors’ behavior analysis. (Goal) We study various text representation methods, sentiment analysis, and information retrieval methods from heterogeneous data sources. (Findings) We present current and future research directions in text mining and deep learning for correlation analysis, forecasting, and recommendation systems in financial markets, such as stocks, cryptocurrencies, and Forex (Foreign Exchange Market).
NOT_RELEVANT;ProQuest;Machine Reading at Scale: A Search Engine for Scientific and Academic Research;"Sousa, Norberto;Oliveira, Nuno;Praça, Isabel";2022;10.3390/systems10020043;NOT_FOUND;The Internet, much like our universe, is ever-expanding. Information, in the most varied formats, is continuously added to the point of information overload. Consequently, the ability to navigate this ocean of data is crucial in our day-to-day lives, with familiar tools such as search engines carving a path through this unknown. In the research world, articles on a myriad of topics with distinct complexity levels are published daily, requiring specialized tools to facilitate the access and assessment of the information within. Recent endeavors in artificial intelligence, and in natural language processing in particular, can be seen as potential solutions for breaking information overload and provide enhanced search mechanisms by means of advanced algorithms. As the advent of transformer-based language models contributed to a more comprehensive analysis of both text-encoded intents and true document semantic meaning, there is simultaneously a need for additional computational resources. Information retrieval methods can act as low-complexity, yet reliable, filters to feed heavier algorithms, thus reducing computational requirements substantially. In this work, a new search engine is proposed, addressing machine reading at scale in the context of scientific and academic research. It combines state-of-the-art algorithms for information retrieval and reading comprehension tasks to extract meaningful answers from a corpus of scientific documents. The solution is then tested on two current and relevant topics, cybersecurity and energy, proving that the system is able to perform under distinct knowledge domains while achieving competent performance.
NOT_RELEVANT;ProQuest;Student Suite+, A Closed Domain Question Answering System for Educational Domain;"Swathilakshmi, V;Satyanarayanamma, M Rama;Udayakumar, Vismaya;Sindhura, Satyavarapu Sai";2021;NOT_FOUND;NOT_FOUND;Natural Language Processing (NLP) is a branch of Artificial Intelligence (AI). It is a study of how machines understand the language of humans. It aims to build systems that can understand text and perform tasks like language translation and classifying a topic. NLP has manyapplications. NLP tools help to process unstructured data. Question Answering (QA) system or information retrieval system falls under the category of NLP. The QA system mainly involves two methods, question analysis and answer extraction. Intelligent QA systems have always outperformed the non-intelligent QA systems. It is a neural network that is incorporated with the QA system to make it intelligent. Neural networks(NN) are a series of algorithms that are used to mimic the operations of a human brain to understand the relationships between data. NN can learn by themselves and produce the output that is not limited to the input provided to them.
NOT_RELEVANT;ProQuest;Quantum Mathematics in Artificial Intelligence;"Widdows, Dominic;Kitto, Kirsty;Cohen, Trevor";2021;10.1613/jair.1.12702;NOT_FOUND;In the decade since 2010, successes in artificial intelligence have been at the forefront of computer science and technology, and vector space models have solidified a position at the forefront of artificial intelligence. At the same time, quantum computers have become much more powerful, and announcements of major advances are frequently in the news. The mathematical techniques underlying both these areas have more in common than is sometimes realized. Vector spaces took a position at the axiomatic heart of quantum mechanics in the 1930s, and this adoption was a key motivation for the derivation of logic and probability from the linear geometry of vector spaces. Quantum interactions between particles are modelled using the tensor product, which is also used to express objects and operations in artificial neural networks. This paper describes some of these common mathematical areas, including examples of how they are used in artificial intelligence (AI), particularly in automated reasoning and natural language processing (NLP). Techniques discussed include vector spaces, scalar products, subspaces and implication, orthogonal projection and negation, dual vectors, density matrices, positive operators, and tensor products. Application areas include information retrieval, categorization and implication, modelling word-senses and disambiguation, inference in knowledge bases, and semantic composition. Some of these approaches can potentially be implemented on quantum hardware. Many of the practical steps in this implementation are in early stages, and some are already realized. Explaining some of the common mathematical tools can help researchers in both AI and quantum computing further exploit these overlaps, recognizing and exploring new directions along the way.
NOT_RELEVANT;ProQuest;Improving Sentence Representations via Component Focusing;"Yin, Xiaoya;Zhang, Wu;Zhu, Wenhao;Liu, Shuang;Yao, Tengjun";2020;10.3390/app10030958;NOT_FOUND;Featured Application Natural language processing (NLP) is a crossing domain of computer science, artificial intelligence, and linguistics that focuses on the interaction between computers and human (natural) languages. NLP faces many challenges, including sentence representation. Appropriate sentence representation significantly improves the efficiency of NLP tasks such as text comprehension, text classification, machine translation, and information extraction. Abstract The efficiency of natural language processing (NLP) tasks, such as text classification and information retrieval, can be significantly improved with proper sentence representations. Neural networks such as convolutional neural network (CNN) and recurrent neural network (RNN) are gradually applied to learn the representations of sentences and are suitable for processing sequences. Recently, bidirectional encoder representations from transformers (BERT) has attracted much attention because it achieves state-of-the-art performance on various NLP tasks. However, these standard models do not adequately address a general linguistic fact, that is, different sentence components serve diverse roles in the meaning of a sentence. In general, the subject, predicate, and object serve the most crucial roles as they represent the primary meaning of a sentence. Additionally, words in a sentence are also related to each other by syntactic relations. To emphasize on these issues, we propose a sentence representation model, a modification of the pre-trained bidirectional encoder representations from transformers (BERT) network via component focusing (CF-BERT). The sentence representation consists of a basic part which refers to the complete sentence, and a component-enhanced part, which focuses on subject, predicate, object, and their relations. For the best performance, a weight factor is introduced to adjust the ratio of both parts. We evaluate CF-BERT on two different tasks: semantic textual similarity and entailment classification. Results show that CF-BERT yields a significant performance gain compared to other sentence representation methods.
NOT_RELEVANT;ProQuest;Evaluation of Information Retrieval Based Ontology Development Editors for Semantic Web;"Kaur, N;Aggarwal, H";2017;10.5815/ijmecs.2017.07.07;NOT_FOUND;Ontology is one of the central area in natural language processing (NLP), artificial intelligence (AI) information retrieval (IR) and semantic web (SW). If you are working on ontology project, this paper will give you the relevant information about ontology related terms and best ontology development editor. In this paper five ontology development editors are reviewed and compared with their updated versions. They are Apollo1.0, SWOOP 2.3Beta4, Protégé 5.0, Graffoo 1.0 and Neon 2.5.2. Comparison of two main data models ontology and RDBMS is also done. This paper also present the classification of ontology languages from those reported in the Literature, with a special attention accorded to the interoperability between them. Additionally, this paper presents the important terms related to ontology. The main criterion for comparison of these tools and languages was the user interest and their application in different kind of real world tasks. The primary goal of this study is to introduce these important tools, languages and data models to ensure more understanding from their use.
NOT_RELEVANT;ProQuest;N-GRAMS SOLUTION FOR ERROR DETECTION AND CORRECTION IN HINDI LANGUAGE;"Kanwar, Shailza;Sachan, Manoj Kumar;Singh, Gurpreet";2017;NOT_FOUND;NOT_FOUND;Hindi is the National language of India, which is still in its early stage of research and development regarding natural language processing applications in comparison to other languages like English, Chinese. Natural language processing is a field of Artificial Intelligence, which includes major tasks such as information retrieval, word segmentation, speech recognition, parsing, part of speech tagging, text classification, automatic text summarization etc. Spelling detection and correction in Hindi language is an important task of NLP which has not gotten sufficient attention till date. Spelling detection and correction for Indian languages such as Hindi is considered as a difficult task. Hindi Language is very different from English language in its phonetic properties and grammatical rules. Thus the existing techniques and methods that are being used to check the errors in English language can’t be used for Hindi Language. There are mainly two types of error: Non word error and real word error. Error detection for non-word error in Hindi language has been done but for real word error no work has been done till date. This paper focused on Real word spelling error detection and correction in Hindi text by using N Grams Model and Levensthein edit distance algorithm.
NOT_RELEVANT;ProQuest;Developments in The Field of Natural Language Processing;Goel, Bhargavi;2017;NOT_FOUND;NOT_FOUND;"Natural language processing involves computer science, artificial intelligence and computational linguistics concerned with interactions between computers and human (natural) languages. The paper attempts to critically analyse state of the art technology algorithms in the field of Information Extraction and Information Retrieval. Information Extraction is concerned in general with the extraction of semantic information from text. Retrieval, filtering, indexing and other such tools have been built which have been used to accomplish tasks such as named entity recognition, co-reference resolution, relationship extraction, etc. By collating important work systematically, the paper also aims to simplify the process of referencing and literature review for future researchers and developers in the field of Natural Language Processing. Major challenges in NLP including natural language understanding, enabling computers to derive meaning from human or natural language input; natural language generation among others have also been discussed."
MAYBE_RELEVANT;ProQuest;Question Answering Systems: A Review on Present Developments, Challenges and Trends;"Kodra, Lorena;Meçe, Elinda Kajo";2017;10.14569/IJACSA.2017.080931;NOT_FOUND;Question Answering Systems (QAS) are becoming a model for the future of web search. In this paper we present a study of the latest research in this area. We collected publications from top conferences and journals on information retrieval, knowledge management, artificial intelligence, web intelligence, natural language processing and the semantic web. We identified and classified the topics of Question Answering (QA) being researched on and the solutions that are being proposed. In this study we also identified the issues being most researched on, the most popular solutions being proposed and the newest trends to help researchers gain an insight on the latest developments and trends of the research being done in the area of question answering.
NOT_RELEVANT;ProQuest;A generic framework for ontology-based information retrieval and image retrieval in web data;"Vijayarajan, V;Dinakaran, M;Tejaswin, Priyam;Lohani, Mayank";2016;10.1186/s13673-016-0074-1;NOT_FOUND;In the internet era, search engines play a vital role in information retrieval from web pages. Search engines arrange the retrieved results using various ranking algorithms. Additionally, retrieval is based on statistical searching techniques or content-based information extraction methods. It is still difficult for the user to understand the abstract details of every web page unless the user opens it separately to view the web content. This key point provided the motivation to propose and display an ontology-based object-attribute-value (O-A-V) information extraction system as a web model that acts as a user dictionary to refine the search keywords in the query for subsequent attempts. This first model is evaluated using various natural language processing (NLP) queries given as English sentences. Additionally, image search engines, such as Google Images, use content-based image information extraction and retrieval of web pages against the user query. To minimize the semantic gap between the image retrieval results and the expected user results, the domain ontology is built using image descriptions. The second proposed model initially examines natural language user queries using an NLP parser algorithm that will identify the subject-predicate-object (S-P-O) for the query. S-P-O extraction is an extended idea from the ontology-based O-A-V web model. Using this S-P-O extraction and considering the complex nature of writing SPARQL protocol and RDF query language (SPARQL) from the user point of view, the SPARQL auto query generation module is proposed, and it will auto generate the SPARQL query. Then, the query is deployed on the ontology, and images are retrieved based on the auto-generated SPARQL query. With the proposed methodology above, this paper seeks answers to following two questions. First, how to combine the use of domain ontology and semantics to improve information retrieval and user experience? Second, does this new unified framework improve the standard information retrieval systems? To answer these questions, a document retrieval system and an image retrieval system were built to test our proposed framework. The web document retrieval was tested against three key-words/bag-of-words models and a semantic ontology model. Image retrieval was tested on IAPR TC-12 benchmark dataset. The precision, recall and accuracy results were then compared against standard information retrieval systems using TREC_EVAL. The results indicated improvements over the standard systems. A controlled experiment was performed by test subjects querying the retrieval system in the absence and presence of our proposed framework. The queries were measured using two metrics, time and click-count. Comparisons were made on the retrieval performed with and without our proposed framework. The results were encouraging.
NOT_RELEVANT;ProQuest;Introduction to the Special Issue on Cross-Language Algorithms and Applications;"Costa-jussà, Marta R;Bangalore, Srinivas;Lambert, Patrik;Màrquez, Lluís;Montiel-Ponsoda, Elena";2016;10.1613/jair.5022;NOT_FOUND;With the increasingly global nature of our everyday interactions, the need for multilin- gual technologies to support efficient and effective information access and communication cannot be overemphasized. Computational modeling of language has been the focus of Natural Language Processing, a subdiscipline of Artificial Intelligence. One of the current challenges for this discipline is to design methodologies and algorithms that are cross- language in order to create multilingual technologies rapidly. The goal of this JAIR special issue on Cross-Language Algorithms and Applications (CLAA) is to present leading re- search in this area, with emphasis on developing unifying themes that could lead to the development of the science of multi- and cross-lingualism. In this introduction, we provide the reader with the motivation for this special issue and summarize the contributions of the papers that have been included. The selected papers cover a broad range of cross-lingual technologies including machine translation, domain and language adaptation for sentiment analysis, cross-language lexical resources, dependency parsing, information retrieval and knowledge representation. We anticipate that this special issue will serve as an invaluable resource for researchers interested in topics of cross-lingual natural language processing.
NOT_RELEVANT;ProQuest;Dynamic summarization of bibliographic-based data;"Workman, T Elizabeth;Hurdle, John F";2011;10.1186/1472-6947-11-6;NOT_FOUND;"  Abstract Background: Traditional information retrieval techniques typically return excessive output when directed at large bibliographic databases. Natural Language Processing applications strive to extract salient content from the excessive data. Semantic MEDLINE, a National Library of Medicine (NLM) natural language processing application, highlights relevant information in PubMed data. However, Semantic MEDLINE implements manually coded schemas, accommodating few information needs. Currently, there are only five such schemas, while many more would be needed to realistically accommodate all potential users. The aim of this project was to develop and evaluate a statistical algorithm that automatically identifies relevant bibliographic data; the new algorithm could be incorporated into a dynamic schema to accommodate various information needs in Semantic MEDLINE, and eliminate the need for multiple schemas. Methods: We developed a flexible algorithm named Combo that combines three statistical metrics, the Kullback-Leibler Divergence (KLD), Riloff's RlogF metric (RlogF), and a new metric called PredScal, to automatically identify salient data in bibliographic text. We downloaded citations from a PubMed search query addressing the genetic etiology of bladder cancer. The citations were processed with SemRep, an NLM rule-based application that produces semantic predications. SemRep output was processed by Combo, in addition to the standard Semantic MEDLINE genetics schema and independently by the two individual KLD and RlogF metrics. We evaluated each summarization method using an existing reference standard within the task-based context of genetic database curation. Results: Combo asserted 74 genetic entities implicated in bladder cancer development, whereas the traditional schema asserted 10 genetic entities; the KLD and RlogF metrics individually asserted 77 and 69 genetic entities, respectively. Combo achieved 61% recall and 81% precision, with an F-score of 0.69. The traditional schema achieved 23% recall and 100% precision, with an F-score of 0.37. The KLD metric achieved 61% recall, 70% precision, with an F-score of 0.65. The RlogF metric achieved 61% recall, 72% precision, with an F-score of 0.66. Conclusions: Semantic MEDLINE summarization using the new Combo algorithm outperformed a conventional summarization schema in a genetic database curation task. It potentially could streamline information acquisition for other needs without having to hand-build multiple saliency schemas."
NOT_RELEVANT;ProQuest;Other 1 -- No Title;NOT_FOUND;1991;NOT_FOUND;NOT_FOUND;Basic Concepts and Tools Artificial Intelligence Fuzzy Neurocomputing Data and Information Processing Pattern Recognition Conaol Systems Decision Analysis and Optimization Fuzzy Hardware Engineering Applications In addition, there will be tutorials on Fuxzy Engineering: Basic Concepts, Fuzzy Control Theory and Applications, Basic Concepts: Fuzzy Sets and Logic, Fuzzy Development Tools: Hardware and Software, and Fuzzy Intelligent Systems. Research in IR touches upon fields as distinct as design and analysis of information systems, cognitive modelling, man-machine intta-action, natural language processing, hypertext, and multimedia data manage ment. Information Retrieval Theory--Models, Cognitive models and theory, Domain analysis Artificial latelligence and IR--Intelligent IR, Knowledge-based methods, Expert systems, Connec- tionism, Neural networirs and IR Advanced Methods in User Interface Design--Man-machine interaction, Design considerations and prototypes Evaluation of IR Systems--Evaluation theory and methods, evaluation of special systems Hypertext and Multimedia Systems--Integrated and cooperative systems, Hypermedia models, Retrieval strategies, Image processing Natural Language Processing and IR--Syntactical and semantic parsing, Lexicon applications, Mul tilingual systems IR Applications and Implementation Issues--Special information systems.
NOT_RELEVANT;ProQuest;Using Vector and Extended Boolean Matching in an Expert System for Selecting Foster Homes;"Fox, Edward A;Winett, Sheila G";1990;NOT_FOUND;NOT_FOUND;Since there are many similarities between the fields of information retrieval and artificial intelligence, it may be appropriate to adapt methods developed in one field to applications usually thought of as belonging to the other. While recent attention in this regard has focused on the use of artificial intelligence methods in the infor mation sciences, this project demonstrates the utility of information retrieval techniques in the expert systems area of artificial intelligence. In particular, it addresses the difficult task of building an expert system for the so cial sciences. FOCES (FOster Care Expert System) is a prototype assistant to social workers involved in select ing foster homes for children who are in need of place ment. The design of FOCES called for use of GUESS (General-pUrpose Expert System Shell) and tailored Prolog routines for extended Boolean matching and vector correlation. Evaluation of the implemented sys tem has shown that FOCES can perform its assigned task as well as trained social workers.
NOT_RELEVANT;ProQuest;Back Material 1 -- No Title;NOT_FOUND;1986;NOT_FOUND;NOT_FOUND;"B. Boolean Queries and Term Dependenties in Probabilistic Retrieval Models, 71 7&gt;e isoa, W. E. The International Teleconununications Environment, 418 De Caluwe, R. M. M. See Kerre, E. E., 341(C) Deboas, A. Review of Information Seeking: Assessing and Anticipating User Needs, 173 Desai, B. C. -; Goyal, P.; Sadri, F.: A Data Model for Use with Formatted and Textual Data, 158 Dick, R. Views on Telecommunications: Implications for the Future, 423 Doez&amp;oca, T. E. Natural Language Processing in Information Retrieval, 191 Drott, M. C. See Griffith, B. C., 261 Eggbe, L The Dual of Bradford's Law, 246 Ellie, D. European Research Letter: Social Science Information Research, 86 Fidel, R. Towards Expert Systems for the Selection of Search Keys, 37 Fiacbhoff, B. -; MacGregor, D.: Calibrating Databases, 222 For, C. Future Generation Information Systems, 215 Frakee, R'. Telecommunicntions: Principles, Developments, Prospects, 401 MncGregor, D. See Fischhoff, B., 222 Mnhapntra, M. -; Biswas, S. C.: Interdependence of PRECIS Role Operators: A Quantitative Analysis of Their Associations, 20 Main, R. G. See Grant, F. L., 12 McCain, K. W. Cocited Author Mapping as n Valid Representation of Intellectual Structure, 111 McGretb, W. E. Theory Building and Hypothesis Testing in Multidimensional Scaling Research, 355(L) Meadow, C. T. Networks and Distributed Information Services, 405 Moors, D. C. Review of The Wardbee, 92 Morgnn, M. G. See Henrion, M., 319 Mukhopndhyny, U. -; Stephens, L. M.; Huhns, M. N.; and Bonnell, R. F.: An Intelligent System for Document Retrieval in Distributed O(Iice Environments, 123 Muan, F. A. A Syatem for Processing Bilingual Arnbic/English Text, 288 Nndziejkn, D. E. Titular Colonicity: Some Comments, 177(L) Nair, I. See Henrion, M., 319 Neufeld,Ilf. L -: Cornog, M.: Database History: Ftom Dinosaurs to Compact Di; , 183 Ojala, M. Views on End-User Searching, 197 Pao, M. L An Empirical Examination of Lotlca's Law, 26 'Ilt Colonicity: Farther Developments, 177(L Phillipe, J. See Shepherd, M. A, 146 Picara, R. G. Review of Reporters Undvr FSre: U.S. Media Cooeraga o/ Conflicts in Lebanon and Central America, 276 Raghavan, V. V. -; Wong, S. K. If.: A Critical Analysis of Vector Space Model for Information Retrieval, 279 Sadri, F. See Desai, B. C., 158 Salvendy, G. Review of Measuring Prodttctluity: Trends and Comp orisons Jrom the First International Prctiui y Symposium, 49 Baye, J. D. See Griffith, B. C., 261 Sewell, W. -; Teitelbaum, S.: Observations of End-User Online Searching Behavior Over Eleven Yearn, 234 Sharma, G. Review of Spread Spectrum Communications, Vol.l, 273 -; Review of Elements Digital Satellite Communication, Vol. 274 Shaw, W. M. Jr. Academic curricula, 12 -, and the information age, 12 Arabic English texts, 288 Artificial intelligence, 97 Bibliographic databases, 261 -- weeding"", 2S6 Bibliometnes, 136 Bilingual texts, 288 Boolean queries, 71 Bradford s law, 246, 307 CD-ROM, 204 Citation motives, 34 Cocitation, 111 Cotited author mapping, 111 Comprehensive Employment and'Il aining Act (CETA) program, 377 Consensus passages, 97 Controlled vocabulary, 331 Data compilation, 26 Data storage, 341(C) Data-communications services, 418 Database historx, 183 Databesee, end information searching, 222 Decision modeling, 319 Demos system, 319 Diffusion theory, 377 Document distribution, I23 Document retrieval, 123, 146 Document retrieval systems, 3 End-user online searching, 234 End-user searching, 197 End-users, 37 Expert systems, 37 Ftee-text, 331 Fizzy set theory, 341(C) Gateways, 204 General retrieval effectiveness, 346(C) History, databases, 183 Information processing, population-dynamics model, 300 Information processes, 300 Information retrieval, 37, 191 279, 341(C) Information retrieval research, 71 Information systems, technological advances, 216 Interdocument similarity information, 3 Journal obsolescence, 136 Journal productivity, 136 Lotka's law, 26, 307 Manngerial support, and use of scientific information, 1u3 Menu selection systems, 57 -, use of semantic organization, 57 Morse, Philip, 45 Multiple Intelligent Node Document Servers (MINDS), 123 Na ve (end-user) searchers, 315 -, presearch experience 315 Nataonal Library of Medicine (NLM), evaluating databases, 261 National Library of Medicine (NLM), online databases, 234 Natural lenguape prose-sling 191 Network participants, adaptation to change, 40S Networks, 40S -, as cause of change, 405 New technology, 215 Non-tree structures, 166(C) Nonfirat normal form relations, 158 OnCme catalogs, 387 -, user behavior, 387 Online information retrieval, reading list, 220 Online searching, 197 Online subject catalog access, design principles 357 PRECIS, 20 Patron status, and single book renewals, 78 Perspectives on online retrieval, 180 Probabilistic retrieval models, 71 Profile-query relationship, 146 Programmers, -, expert, 294 -, novice, 294 Programming knowledge, 294 Regulatory development, United States, 409 Role operators, 20 Science Citation Index, 97 Single book renewals, 78 Social science information research, -, Europe, 86 -, United Kingdom, 86 Specialty narratives, 97 String indexing system, 166(C) Telecommunications, United States, 409, 414 -, and new technology, 414 global, 418 -- future growth 423 Teleprocessing networks, 429 Textual data, 158 Transparent information retrieval, 204 Uncertainty analysis 319 Universal relation model, 158 User behavior, and online catalogs, 387 Uset profile, 146 User query, 146 Vector space model, 279 Zipf's Iaw, 307"
NOT_RELEVANT;ProQuest;Natural Language Processing in Information Retrieval;Doszkocs, Tamas E;1986;NOT_FOUND;NOT_FOUND;The role and contributions of natural-language process- ing in information-retrieval and artificial-intelligence re- search is examined in the context of large operational information-retrieval systems and services. State-of- the-art information-retrieval systems are found to com- bine the functional capabilities of the conventional in- verted file--Boolean logic--term adjacency approach, commonly employed by commercial search services, with statistical-combinatoric techniques pioneered in experimental information-retrieval research and formal natural-language processing methods and tools bor- rowed from artificial intelligence.
NOT_RELEVANT;ProQuest;Back Material 2 -- No Title;NOT_FOUND;1984;NOT_FOUND;NOT_FOUND;Subject Index to Volume 35 Brief Communications, Letters, and Errata are denoted by the letters C, L, and E, respectively. Abstracting and indexing services, coverage, 56(L) Accreditation, curriculum mapping, 82 Aeronautics, dictionaries, compilation, 75 ANauigatoro/ NaturalLanguage0rganized Data (ANNOD), 235 Arabic in computerized information interchange, 204 Artificial intelligence, 280 -, bibliography, 317 -, eapert systems, 297 -, machine learning, 306 -, natural language processing, 291 Artificial intelligence: concepts, techniques, applications, promise, perspective on, 277 Automatic document classification, nearest neighbors,149(C) Automatic indeaing, 3, 136(L), 325 Bibliographic coupling, 29 Bibliographic records, displays, 344 Bibliometric scaling, research,194 Book reviews, 254, 373 Book use, models, 259 Catalog records, displays, 344 Chinese characters, structure, 372(L) Circulation patterns, library systems, 118(C), 122(C) Citation analysis, interdisciplinary research, 360 -, macroeconomics, 351 -, reading research, 332 -, social science literature, 11 Citation clnssics,interdisciplinary research,360 Clustering algorithms, 268 Cocitation, 29 -, macroeconomics, 351 CODAR-U/FD, Areb/Latin computer code, 204 Coding methods, information retrieval, 248 Cognition, in online searching, 57(L) Cognitive authority, 254 Computer-assisted instruction, online searching, 53,129(C) Computer displays, bibliographic records, 344 Computer science, and information science, 157 -, research activity, 369(C) Computer technology, and information science, 164 Computer technology for Arabic input and output, 204 Computer vision, 280 Copyright, 67 Curriculum mapping, library and information science, 82 Dictionaries, aeronautic, compilation, 75 Displays, bibliographic records, 344 Document retrieval, clustering, 268 -, clustering, nearest neighbors, 149(C) -, probabilistic models, 153(L) Editor's note, 203 Education, library and information science,185 Electronic delivery of documents and graphics, 45 Entropy principle, maximum, 153(L) Errata,139,138 Evaluation of information, 254 Expert systems, 280, 297 Fair use, copyright, 67 Federal policy (U.SJ, scientific and technical information, 179 File maintenance, subject authority, MEDLINE, 34 Full-teat indexing, indirect information retrieval, 19 Full-teat knowledge base, natural language retrieval, 235 Fully Automatic Syntactically Based Indexing of Teat(FASIT),136(L) Fuzzy set theory, modeling journal binding decisions, 228 Gotfman's indirect method of information retrieval, 19 Indexing, automatic, 3, 136(L), 325 Indirect method of information retrieval,19 Information, evaluation, 254 Information flow, research, 194 Information industry, integration, 170 Information need, 129(C) Information policies, United States.179 Information processing, research, 194 Information retrieval, coding schemes, 298 -, history,157,164 -, indirect method,19 -, maximum entropy principle, 153(L) -, natural language processing, 291 Information science, education, 185 -, expert systems, 297 -, history,157,164 -, research, 153(L), 194 -, theory and practice, 135(L), 137(L) Information science: retrospect and prospect, perspectives on,155 Information seeking expressions, 124(C) Information services, value, 136(L) Information technology, history, 164 -, integration,170 Infostructure, information industry,170 Interdisciplinary research, citation analysis, 360 Interlibrary lending, electronic, 45 International information flow, social sciences, Il International Meetings, admissibility of delegates, 56(L),135(L) Interviewing, online searching, 90 Invisible colleges, 29 JASIS,:eferees, 203 Journal article titles, information content, 222 Journal binding decisions, modeling, 228 Journelliterature,reading research,332 Keyword indexing, 325 Keywords, journal article titles, 222 Knowledge-based systems, 297 Knowledge representation, 280 Language acquisition, artificial intelligence, 306 Learning, artificial intelligence, 280, 306 Letters,56,57,135,136.137,153,253,372 Library and information science, curriculum mapping,82 Library science, education, 185 -, end information science,157 Library systems, circulation patterns, 118(C), 122(C) Machine learning, 306 Machine translation, 373 Macroeconomics, cocitation, 351 Mapping, library and information science curriculum, 82 llfarkov models, book use, 259 Maximum entropy principle, in information retrieval, 153(L) Medical Subject Headings (MeSH), 34 MEDLINE, subject authority, file maintenance, 34 Modeling, journal binding decisions, 228 Alodels, book use, 259 Alultidimensional scaling, curriculum mapping, 82 -, library circulation patterns, 118(C), 122(C) National Advisory Committee for Aeronautics, 75 Natural language, retrieve) systems, 235 Natural language processing, 280, 291 Nearest neighbors, document classification, 149(C) Networks, resource sharing, electronic, 45 Nomenclature /or Aeronautics, compilation, 75 Non-Boolean searching models, 19 Online information-seeking behavior, 2I1 Online searching, computer-assisted instruction, 53,129(C) -, loss of information, 56(L) -, pre-search interviews, 90 -, psychology, 57(L) -, scientific inquiry model,110 -, user satisfaction, 90 Online searching styles, 211 Perspectives on artificial intelligence: concepts, techniques, applications, promise, 277 Perspectives on information science: retrospect and prospect, 155 Pool, Ithiel de Sola, 253(L) Pragmatic Approach to Subject Indexing (PASI), 325 Price, Derek John de Solla, 147, 372(L) Psychology of online searching, 57(L) Question-answering systems, artificial intelligence, 291 Reading research,journallitereture, 332 Reference interviews, online searching, 90 Research, information science, 194 Rc :^arch activity, computer scientists, 369(C) Research policies in information science,153(L) Resource sharing, electronic, 45 Romanizatian standards, Arabic, 209 -- Scholarly migration, macroeconomics, 351 Scientific and scholarly text, spelling errors, 104 Scientific and technical information policies, United States, 179 Scientific inquiry, model for online searching,110 Searching patterns, online systems, 211 Social sciences, international information flow, 11 SPEEDCOP SPelling, Error Detection Correction Program, 104 Spelling errors, automatic detection and correction, 104 Statistical selection, index terms, 3 String coding, information retrieval, 248 Subject authority, file maintenance, MEDLINE, 34 Teat structuring, artificial intelligence, 291 Translation, 373 Typographic errors, automatic detection and rnrrection, 104 SUBJECTINDEX United Kipgdom, online aeardvmg, computer ssaisted imtruction, b3 Users, information seeking eapressiona,124(C) Value of information aervicea,136(L)
NOT_RELEVANT;ProQuest;Other 5 -- No Title;NOT_FOUND;1984;NOT_FOUND;NOT_FOUND;"Journal of the American Society for Information Service 35 1984 Contents January 1984, No. 1 Statistical Recognition of Content Terms in General Text M. Dillon and P. Federhart 3 Internationality of the Social Sciences: Implications for Information Transfer J. M. Brittain 11 Less Than Full-Text Indexing Using a Non-Boolean Searching Model D. B. Cleveland, A. D. Cleveland, and O. B. Wise 19 Co-Citation Analysis and the Invisible College E. Noma 29 File Maintenance of MeSH Headings in MEDLINE S. M. Humphrey 34 Application of Modern Technologies to Interlibrary Resource-Sharing Networks J. F. Reintjes 45 European Research Letter-Teaching Online Information Retrieval in United Kingdom Library Schools F. E. Wood 53 Letters to the Editor 56 Information for Contributors 58 March 1984, No. 2 REVIEW AND OPINION Fair Use versus Fair Return: Copyright Legislation and Its Consequences 1. L. Horowitz and M. E. Curtis 67 APPLICATION Dictionary Making by Conference and Committee: NACA and the American Aeronautical Language, 1916-1934 G. Johns 75 Mapping a Curriculum by Computer H. D. White and K. Calhoun 82 RESEARCH Search Interview Techniques and Information Gain as Antecedents of User Satisfaction with Online Bibliographic Retrieval 3c E. Auster and S. B. Gawton 90 System Design for Detection and Correction of Spelling Errors in Scientific and Scholarly Text J. J. Pollock and A. Zamora 104 Scientific Inquiry: A Model for Online Searching S. P. Harter 110 BRIEF COMMUNICATIONS A Method for Studying Intercorrelated Circulation Patterns in Library Systems C. H. Smirh 118 Applications of Multidimensional Scaling: Comment on ""A Method for Studying Intercorrelated Circulation Patterns in Library Systems"" W. E. McGrath 122 Information Seeking Expressions of Users R. L. Derr 124 A Drill and Practice Program for Online Retrieval B. R. Boyce, D. Martin, B. Francis, acrd M. E. Sieuert 129 LETTERS TO THE EDITOR 135 Erratum 138 May 1984, No. 3 Toward a Paradigm for Information Science: The Influence of Derek de Solla Price M. Kochert I47 BRIEF COMMUNICATIONS A Note on the Use of Nearest Neighbors for Implementing Single Linkage Document Classifications P. jVillett 149 LETTERS TO THE EDITOR 153 f PERSPECTIVES ON . . Artificial Intelligence: Concepts, Techniques, Applications, Promise L. F. Lupin and L. C. Smith 277 Artificial Intelligence: Underlying Assumptions and Basic Objectives N. Cercone and C. McCalla 280 Natural Language Processing R. Grishnran 291 Expert Systems: A Tutorial N. S. Yaghmai and J. A. Maxin 297 Approaches to Machine Learning P. Langley and J. G. Carbonell 306 Artificial Intelligence: A Selected Bibliography Compiled by L. C. Smith 317 November t984, No. 6 Editor's Note C. T. Meadow 323 APPLICATION Pragmatic Approach to Subject Indexing: A New Concept S. Dntta and P. K. Sinha 325 A Review and Application of Citation Analysis Methodology to Reading Research Journal Literature E. C. Snnnners 332 RESEARCH The Effect of Spatial Arrangement, Upper-Lower Case Letter Combinations, and Reverse Video on Patron Response to CRT Displayed Catalog Records B. S. Fryser and K. H. Stirling 344 Longitudinal Author Cocitation Mapping: The Changing Structure of Macroeconomics K. W. McCain 351 ""Citation Classics"" Analysis: An Approach to Characterizing Interdisciplinary Research D. E. Chnbin, A, L. Porter, and F. A. Rossini 360 BRIEF COMMUNICATION Research Productivity and Breadth of Interest of Computer Scientists K. Snbramanyaltt 369 LETTERS TO THE EDITOR 372 BOOK REVIEWS 373 Information for Contributors, 375 Author Index 377 Subjectlndex 379 Volume Contents 381"
NOT_RELEVANT;ProQuest;Natural Language Processing;Grishman, Ralph;1984;NOT_FOUND;NOT_FOUND;"Natural language processing has two primary roles to play in the storage and retrieval of large bodies of infor mation: providing a friendly, easily-learned interface to information retrieval systems, and automatically struc- turing texts so that their information can be more easily processed and retrieved. This article outlines the organi- zation of a natural language interface for data retrieval (a ""question--answering system's and some of the ap proaches being taken to text structuring. It closes by describing a few of the research issues in computational linguistics and a possibility for using interactive natural language processing for information acquisition."
NOT_RELEVANT;ProQuest;Automated Scoring of Constructed- Response Items Using Artificial Neural Networks in International Large-scale Assessment;"Jung, Ji Yoon;Tyack, Lillian;von Davier, Matthias";2022;NOT_FOUND;NOT_FOUND;"The results showed that human and automated scores were highly correlated on average (r=0.91). [...]this study found that a novel approach of adopting expected scores generated from item response theory (IRT) can be useful for quality control. Keywords: International large-scale assessment, eTIMSS, constructed-response items, automated scoring, artificial neural networks, natural language processing (ProQuest: ... denotes formulae omitted.) Introduction The move to computer-based assessment has enabled international large-scale assessments (ILSAs) to enhance the measurement of student achievement through novel items. CR items may elicit constructive cognitive processes by requiring students to produce their own answers, employing their knowledge and reasoning abilities (Lissitz et al., 2012), while MC items mostly focus on skills such as recognition, recall, or prompted information retrieval (DarlingHammond &amp; Adamson, 2010). [...]CR items may provide deeper insight into student thinking since they allow students to construct heterogeneous or even idiosyncratic answers rather than choosing from a set of responses provided on the test (Federer et al., 2015). Fortunately, a growing number of studies have shown that automated scoring can play a viable role in the scoring of CR items, suggesting that high levels of agreement between human rater- and computer-generated scores can be achieved (Ha, 2016; Kersting et al., 2014; Liu et al., 2016; Shermis et al., 2010; Shermis &amp; Burstein, 2013)."
NOT_RELEVANT;ProQuest;Deep learning-based approach for Arabic open domain question answering;"Alsubhi, Kholoud;Jamal, Amani;Alhothali, Areej";2022;10.7717/peerj-cs.952;NOT_FOUND;Open-domain question answering (OpenQA) is one of the most challenging yet widely investigated problems in natural language processing. It aims at building a system that can answer any given question from large-scale unstructured text or structured knowledge-base. To solve this problem, researchers traditionally use information retrieval methods to retrieve the most relevant documents and then use answer extractions techniques to extract the answer or passage from the candidate documents. In recent years, deep learning techniques have shown great success in OpenQA by using dense representation for document retrieval and reading comprehension for answer extraction. However, despite the advancement in the English language OpenQA, other languages such as Arabic have received less attention and are often addressed using traditional methods. In this paper, we use deep learning methods for Arabic OpenQA. The model consists of document retrieval to retrieve passages relevant to a question from large-scale free text resources such as Wikipedia and an answer reader to extract the precise answer to the given question. The model implements dense passage retriever for the passage retrieval task and the AraELECTRA for the reading comprehension task. The result was compared to traditional Arabic OpenQA approaches and deep learning methods in the English OpenQA. The results show that the dense passage retriever outperforms the traditional Term Frequency-Inverse Document Frequency (TF-IDF) information retriever in terms of the top-20 passage retrieval accuracy and improves our end-to-end question answering system in two Arabic question-answering benchmark datasets.
NOT_RELEVANT;ProQuest;Sentence boundary detection of various forms of Tunisian Arabic;"Mekki Asma;Zribi Inès;Ellouze Mariem;Belguith Lamia Hadrich";2022;10.1007/s10579-021-09538-4;NOT_FOUND;Sentence boundary detection (SBD) is an essential step for a very large number of natural language processing applications such as parsing, information retrieval, automatic summarization, machine translation, etc. In this paper, we tackle the problem of SBD of dialectal Arabic, especially for the Tunisian dialect. We compare the efficiency of three learning algorithms: Deep Neuronal Networks (DNN), Support Vector Machines (SVM) and Conditional Random Fields (CRF) to detect the boundaries of sentences written in different types of dialect. The best model achieved an F-measure of 84.37% using CRF which is a popular formalism for structured prediction in NLP and it has been widely applied in text segmentation.
NOT_RELEVANT;ProQuest;From Word Embeddings to Pre-Trained Language Models: A State-of-the-Art Walkthrough;Mars, Mourad;2022;10.3390/app12178805;NOT_FOUND;With the recent advances in deep learning, different approaches to improving pre-trained language models (PLMs) have been proposed. PLMs have advanced state-of-the-art (SOTA) performance on various natural language processing (NLP) tasks such as machine translation, text classification, question answering, text summarization, information retrieval, recommendation systems, named entity recognition, etc. In this paper, we provide a comprehensive review of prior embedding models as well as current breakthroughs in the field of PLMs. Then, we analyse and contrast the various models and provide an analysis of the way they have been built (number of parameters, compression techniques, etc.). Finally, we discuss the major issues and future directions for each of the main points.
NOT_RELEVANT;ProQuest;Sentence-CROBI: A Simple Cross-Bi-Encoder-Based Neural Network Architecture for Paraphrase Identification;"Jesus-German Ortiz-Barajas;Bel-Enguix, Gemma;Gómez-Adorno, Helena";2022;10.3390/math10193578;NOT_FOUND;"Since the rise of Transformer networks and large language models, cross-encoders have become the dominant architecture for various Natural Language Processing tasks. When dealing with sentence pairs, they can exploit the relationships between those pairs. On the other hand, bi-encoders can obtain a vector given a single sentence and are used in tasks such as textual similarity or information retrieval due to their low computational cost; however, their performance is inferior to that of cross-encoders. In this paper, we present Sentence-CROBI, an architecture that combines cross-encoders and bi-encoders to obtain a global representation of sentence pairs. We evaluated the proposed architecture in the paraphrase identification task using the Microsoft Research Paraphrase Corpus, the Quora Question Pairs dataset, and the PAWS-Wiki dataset. Our model obtains competitive results compared with the state-of-the-art by using model ensembles and a simple model configuration. These results demonstrate that a simple architecture that combines sentence pair and single-sentence representations without using complex pre-training or fine-tuning algorithms is a viable alternative for sentence pair tasks."
NOT_RELEVANT;ProQuest;A Deep Learning Model for Classifying the Hate and Offensive Language in Social Media Text;Bhandari, Nidhi;2022;10.23755/rm.v42i0.705;NOT_FOUND;Recently, we had introduced a model for identifying and removal of toxic content from twitter, using an Information Retrieval (IR) model SOIR (Semantic query Optimization-based Information Retrieval). Based on lexical and semantic analysis, SOIR identifies the class labels of tweets. The result demonstrates the superiority of the SOIR model. This model is accurate but social media is a big data problem and a significant amount of time and memory is required. In this paper the deep learning technique is used to process large-scale social media text data. First uses Natural Language Processing (NLP) based feature extraction to create four different sets of training samples i.e. TF-IDF-based features, POS Tagged Features, a reduced feature vector of POS and the combined vector of TF-IDF and POS tagged features. The deep Convolutional Neural Networks (CNN) is used to train the model and to classify hate and offensive language. The dataset has been obtained from Kaggle. The performance in terms of training accuracy, validation accuracy, training loss and validation loss has been measured with the time complexity. In addition, the class-wise Precision, Recall, F1-score, and Mean accuracy have also been investigated. From experimental results, we found TF-IDF and POS-based combined features provide superior performance.
NOT_RELEVANT;ProQuest;Building a Technology Recommender System Using Web Crawling and Natural Language Processing Technology;"Nathalie Campos Macias;Düggelin, Wilhelm;Ruf, Yesim;Thomas, Hanne";2022;10.3390/a15080272;NOT_FOUND;"Finding, retrieving, and processing information on technology from the Internet can be a tedious task. This article investigates if technological concepts such as web crawling and natural language processing are suitable means for knowledge discovery from unstructured information and the development of a technology recommender system by developing a prototype of such a system. It also analyzes how well the resulting prototype performs in regard to effectivity and efficiency. The research strategy based on design science research consists of four stages: (1) Awareness generation; (2) suggestion of a solution considering the information retrieval process; (3) development of an artefact in the form of a Python computer program; and (4) evaluation of the prototype within the scope of a comparative experiment. The evaluation yields that the prototype is highly efficient in retrieving basic and rather random extractive text summaries from websites that include the desired search terms. However, the effectivity, measured by the quality of results is unsatisfactory due to the aforementioned random arrangement of extracted sentences within the resulting summaries. It is found that natural language processing and web crawling are indeed suitable technologies for such a program whilst the use of additional technology/concepts would add significant value for a potential user. Several areas for incremental improvement of the prototype are identified."
NOT_RELEVANT;ProQuest;Music Score Recognition Method Based on Deep Learning;Lin, Qin;2022;10.1155/2022/3022767;NOT_FOUND;"In recent years, the recommendation application of artificial intelligence and deep music has gradually become a research hotspot. As a complex machine learning algorithm, deep learning can extract features with value laws through training samples. The rise of deep learning network will promote the development of artificial intelligence and also provide a new idea for music score recognition. In this paper, the improved deep learning algorithm is applied to the research of music score recognition. Based on the traditional neural network, the attention weight value improved convolutional neural network (CNN) and high execution efficiency deep belief network (DBN) are introduced to realize the feature extraction and intelligent recognition of music score. Taking the feature vector set extracted by CNN-DBN as input set, a feature learning algorithm based on CNN&amp;DBN was established to extract music score. Experiments show that the proposed model in a variety of different types of polyphony music recognition showed more accurate recognition and good performance; the recognition rate of the improved algorithm applied to the soundtrack identification is as high as 98.4%, which is significantly better than those of other classic algorithms, proving that CNN&amp;DBN can achieve better effect in music information retrieval. It provides data support for constructing knowledge graph in music field and indicates that deep learning has great research value in music retrieval field."
NOT_RELEVANT;ProQuest;Automatic Classification of Basic Nursing Teaching Resources Based on the Fusion of Multiple Neural Networks;"Hou, Tingting;Zamira Madina";2022;10.1155/2022/7176111;NOT_FOUND;Automatic classification is one of the hot topics in the field of information retrieval and natural language processing, but it still faces many problems to be solved. The classic automated classification approach has a sluggish classification speed and poor processing accuracy for resources with a large quantity of data. Based on this, an automated classification approach based on the integration of various neural networks for fundamental nursing teaching materials was presented. The automatic classification method of teaching resources was designed by extracting the characteristics of teaching resources, establishing the model of multiple neural network integration, and designing the classification index of basic nursing teaching resources. The experimental findings suggest that this technique has higher chi-square test parameters and better outcomes for the automated classification of large instructional materials than the classic rough set automatic classification method.
NOT_RELEVANT;ProQuest;A Contemporary Review on Utilizing Semantic Web Technologies in Healthcare, Virtual Communities, and Ontology-Based Information Processing Systems;"Narayanasamy, Senthil Kumar;Srinivasan, Kathiravan;Yuh-Chung, Hu;Masilamani, Satish Kumar;Kuo-Yi, Huang";2022;10.3390/electronics11030453;NOT_FOUND;The semantic web is an emerging technology that helps to connect different users to create their content and also facilitates the way of representing information in a manner that can be made understandable for computers. As the world is heading towards the fourth industrial revolution, the implicit utilization of artificial-intelligence-enabled semantic web technologies paves the way for many real-time application developments. The fundamental building blocks for the overwhelming utilization of semantic web technologies are ontologies, and it allows sharing as well as reusing the concepts in a standardized way so that the data gathered from heterogeneous sources receive a common nomenclature, and it paves the way for disambiguating the duplicates very easily. In this context, the right utilization of ontology capabilities would further strengthen its presence in many web-based applications such as e-learning, virtual communities, social media sites, healthcare, agriculture, etc. In this paper, we have given the comprehensive review of using the semantic web in the domain of healthcare, some virtual communities, and other information retrieval projects. As the role of semantic web is becoming pervasive in many domains, the demand for the semantic web in healthcare, virtual communities, and information retrieval has been gaining huge momentum in recent years. To obtain the correct sense of the meaning of the words or terms given in the textual content, it is deemed necessary to apply the right ontology to fix the ambiguity and shun any deviations that persist on the concepts. In this review paper, we have highlighted all the necessary information for a good understanding of the semantic web and its ontological frameworks.
NOT_RELEVANT;ProQuest;A Novel Framework of an IOT-Blockchain-Based Intelligent System;Alabdali, Aliaa M;2022;10.1155/2022/4741923;NOT_FOUND;With the growing need of technology into varied fields, dependency is getting directly proportional to ease of user-friendly smart systems. The advent of artificial intelligence in these smart systems has made our lives easier. Several Internet of Things- (IoT-) based smart refrigerator systems are emerging which support self-monitoring of contents, but the systems lack to achieve the optimized run time and data security. Therefore, in this research, a novel design is implemented with the hardware level of integration of equipment with a more sophisticated software design. It was attempted to design a new smart refrigerator system, which has the capability of automatic self-checking and self-purchasing, by integrating smart mobile device applications and IoT technology with minimal human intervention carried through Blynk application on a mobile phone. The proposed system automatically makes periodic checks and then waits for the owner’s decision to either allow the system to repurchase these products via Ethernet or reject the purchase option. The paper also discussed the machine level integration with artificial intelligence by considering several features and implemented state-of-the-art machine learning classifiers to give automatic decisions. The blockchain technology is cohesively combined to store and propagate data for the sake of data security and privacy concerns. In combination with IoT devices, machine learning, and blockchain technology, the proposed model of the paper can provide a more comprehensive and valuable feedback-driven system. The experiments have been performed and evaluated using several information retrieval metrics using visualization tools. Therefore, our proposed intelligent system will save effort, time, and money which helps us to have an easier, faster, and healthier lifestyle.
NOT_RELEVANT;ProQuest;Deep Neural Network and Pseudo Relevance Feedback Based Query Expansion;"Shukla, Abhishek Kumar;Das, Sujoy";2022;10.32604/cmc.2022.022411;NOT_FOUND;The neural network has attracted researchers immensely in the last couple of years due to its wide applications in various areas such as Data mining, Natural language processing, Image processing, and Information retrieval etc. Word embedding has been applied by many researchers for Information retrieval tasks. In this paper word embedding-based skip-gram model has been developed for the query expansion task. Vocabulary terms are obtained from the top “k” initially retrieved documents using the Pseudo relevance feedback model and then they are trained using the skip-gram model to find the expansion terms for the user query. The performance of the model based on mean average precision is 0.3176. The proposed model compares with other existing models. An improvement of 6.61%, 6.93%, and 9.07% on MAP value is observed compare to the Original query, BM25 model, and query expansion with the Chi-Square model respectively. The proposed model also retrieves 84, 25, and 81 additional relevant documents compare to the original query, query expansion with Chi-Square model, and BM25 model respectively and thus improves the recall value also. The per query analysis reveals that the proposed model performs well in 30, 36, and 30 queries compare to the original query, query expansion with Chi-square model, and BM25 model respectively.
NOT_RELEVANT;ProQuest;LeDoCl : A Semantic Model for Legal Documents Classification using Ensemble Methods;"Priyadarshini, R;Anuratha, K;Rajendran, N;Jeyanthi, S;Sujeetha, S";2021;NOT_FOUND;NOT_FOUND;NLP is one of the components of Machine Learning. Topic Modeling is a sub component of information retrieval Information Retrieval is a broad domain research in Natural Language Processing (NLP). This downside has been broadly studied in the perspective of cluster algorithms like K-means and K-fold. that tends to converge to at least one of diverse native issue counting on the selection of format method. To overcome the instabilities and assumptions in existing systems such as Vector Space Model (VSM) and SVD, Semantic based topic modeling (SLDA) and ensemble model with generation and integration is proposed. In the case of topic modelling. instability is visible in two distinct aspects. First. when the topic descriptors are examined over multiple runs. During which there will be considerable change in the term rankings and few terms may appear or disappear completely as well. Next. there could be instability due to the extent to which topics have association with document. through several executions. In the proposed system. ensemble learning comprises of algorithms Kernel Support Vector Machine (KSVM) and Random Forest algorithm which overcomes the instability. The first issue of appearance and disappearance of words between multiple runs is overcome by Gibbs Sampling based Semantic LDA (GSLDA). The second issue of alignment of topics with document is aided by using ESLDA. This ensemble SLDA algorithm show increased accuracy in terms of retrieval and reduced time interval compared to conventional models. The accuracy increases up to 98% using ESLDA compared to SLDA (82%) and term frequency methods (78%).
NOT_RELEVANT;ProQuest;Key Research Issues and Related Technologies in Crowdsourcing Data Collection;"Li, Yunhui;Chang, Liang;Long, Li;Bao, Xuguang;Gu, Tianlong";2021;10.1155/2021/8745897;NOT_FOUND;Crowdsourcing provides a distributed method to solve the tasks that are difficult to complete using computers and require the wisdom of human beings. Due to its fast and inexpensive nature, crowdsourcing is widely used to collect metadata and data annotation in many fields, such as information retrieval, machine learning, recommendation system, and natural language processing. Crowdsourcing helps enable the collection of rich and large-scale data, which promotes the development of researches driven by data. In recent years, a large amount of effort has been spent on crowdsourcing in data collection, to address the challenges, including quality control, cost control, efficiency, and privacy protection. In this paper, we introduce the concept and workflow of crowdsourcing data collection. Furthermore, we review the key research topics and related technologies in its workflow, including task design, task-worker matching, response aggregation, incentive mechanism, and privacy protection. Then, the limitations of the existing work are discussed, and the future development directions are identified.
MAYBE_RELEVANT;ProQuest;Conversation-Based Information Delivery Method for Facility Management;Kuan-Lin, Chen;2021;10.3390/s21144771;NOT_FOUND;Facility management platforms are widely used in the facility maintenance phase of the building life cycle. However, a large amount of complex building information affects facility managers’ efficiency and user experience in retrieving specific information on the facility management platform. Therefore, this research aims to develop a conversation-based method to improve the efficiency and user experience of facility management information delivery. The proposed method contains four major modules: decision mechanism, equipment dataset, intent analysis, and knowledge base. A chatbot prototype was developed based on the proposed method. The prototype was then validated through a feasibility test and field test at the Shulin Arts Comprehensive Administration Building in Taiwan. The results showed that the proposed method changes the traditional information delivery between users and the facility management platform. By integrating natural language processing (NLP), building information modelling (BIM), and ontological techniques, the proposed method can increase the efficiency of FM information retrieval.
NOT_RELEVANT;ProQuest;Deep Learning applications for COVID-19;"Shorten Connor;Khoshgoftaar, Taghi M;Furht Borko";2021;10.1186/s40537-020-00392-9;NOT_FOUND;This survey explores how Deep Learning has battled the COVID-19 pandemic and provides directions for future research on COVID-19. We cover Deep Learning applications in Natural Language Processing, Computer Vision, Life Sciences, and Epidemiology. We describe how each of these applications vary with the availability of big data and how learning tasks are constructed. We begin by evaluating the current state of Deep Learning and conclude with key limitations of Deep Learning for COVID-19 applications. These limitations include Interpretability, Generalization Metrics, Learning from Limited Labeled Data, and Data Privacy. Natural Language Processing applications include mining COVID-19 research for Information Retrieval and Question Answering, as well as Misinformation Detection, and Public Sentiment Analysis. Computer Vision applications cover Medical Image Analysis, Ambient Intelligence, and Vision-based Robotics. Within Life Sciences, our survey looks at how Deep Learning can be applied to Precision Diagnostics, Protein Structure Prediction, and Drug Repurposing. Deep Learning has additionally been utilized in Spread Forecasting for Epidemiology. Our literature review has found many examples of Deep Learning systems to fight COVID-19. We hope that this survey will help accelerate the use of Deep Learning for COVID-19 research.
NOT_RELEVANT;ProQuest;Convolutional neural network model based on text similarity for customer service;"Du, Wenshuang;Ge, Jiawei;Liu, Xuchen;Ai, Junjie";2020;10.1088/1742-6596/1550/3/032045;NOT_FOUND;Customer service is a good way for companies to communicate with customers, high-quality customer service can improve customer satisfaction and dependence on the enterprise. Text matching is a core problem in natural language understanding, and it can be applied to a large number of natural language processing tasks, such as information retrieval, question answering systems, repetition questions, dialogue systems, machine translation. These natural language processing tasks can be approximately abstracted into text matching problems. This paper combines text matching and convolutional neural networks to build an intelligent customer service model, and finally achieves ideal results in F1 value, recall rate and accuracy rate.
NOT_RELEVANT;ProQuest;A method of computing conceptual semantic similarity based on part-whole relationship;"Cao, Xinyu;Zhao, Jing;Zhang, Fan;Wu, Gang;Zhao, Chao;Wang, Haitao";2020;10.1088/1742-6596/1544/1/012046;NOT_FOUND;Conceptual semantic similarity calculation is an important content in research fields such as natural language processing, machine translation and is widely used in information retrieval, knowledge acquisition, text classification, and clustering. It’s very important to development of natural language processing by improving the accuracy of semantic similarity calculation. This paper proposes a method for calculating the similarity of concepts based on the part-whole relationship between concepts. This method is based on the partial overall relationship diagram and uses the hierarchical features and positions of the concepts in the diagram to calculate the conceptual similarity.
NOT_RELEVANT;ProQuest;Editorial Artificial Intelligence and Innovation Management;"Tanev, Stoyan;Sandstrom, Gregory";2019;NOT_FOUND;NOT_FOUND;Keywords: Artificial intelligence, AI, innovation, Austria, SME, AI innovation management, Big Data, Advanced Analytics, enterprise platform, AI value chain, AI maturity, front-end of innovation, environmental scanning, information processing, opportunity, innovation search field, information retrieval, decision-making, latent semantic indexing, design science, design thinking, 3S Process, business education, Harvard Case Method, data access, orchestration, governance, information mobility, connected health, data management, patient- centered. The results suggest potential new focus topics of further research such as, for example, AI-related business model development, proper management of expectations in AI-related innovation processes, and further insights into the constraints emerging from the historic aspects of data, along with required metadata expertise. According to them, organisational level solutions seem to hold wide-ranging potential for addressing many of the current data access challenges.
NOT_RELEVANT;ProQuest;Analyzing Documents with TF-IDF;Lavin, Matthew J;2019;10.46430/phen0082;NOT_FOUND;This lesson focuses on a foundational natural language processing and information retrieval method called Term Frequency - Inverse Document Frequency (tf-idf). This lesson explores the foundations of tf-idf, and will also introduce you to some of the questions and concepts of computationally oriented text analysis.
NOT_RELEVANT;ProQuest;Identification of New Parameters for Ontology Based Semantic Similarity Measures;"Jain, Shivani;Seeja, K R;Jindal, Rajni";2019;10.4108/eai.19-12-2018.156439;NOT_FOUND;A major challenge among various applications of computational linguistics, natural language processing and information retrieval is to measure semantic similarity accurately. In this research paper, various ontology-based approaches i.e. compute semantic similarity between words have been studied and listed their benefits and shortcomings on the various identified parameters. Earlier, correlation with human judgment was the single criteria for the judgment of good similarity measures. In this paper, more parameters for semantic similarity measures have been identified and a relative analysis of similarity measures is performed on the identified parameters. These identified parameters can be further utilized for formulating the new semantic similarity-measures in the latest research area of text mining, web mining and information retrieval. We have identified various parameters like features-set, applicability on various ontologies such as single ontology, cross ontology or fuzzy ontology, Ontology type, dataset applied and relationship mapping for the various measures. Through detailed analysis we have found that feature based and hybrid approaches has higher accuracy as compare to edge and content based methods and works in different type of ontologies. Recent research drawing interest to find new feature set in this area like fuzzy distance, graph generation and text snippets etc. Max Accuracy was achieved in single ontology 0.87 and 0.83 over cross ontologies. WordNet and MeSH are maximally utilized Global Ontologies.
NOT_RELEVANT;ProQuest;A deep learning method for named entity recognition in bidding document;"Ji, Yunfei;Tong, Chao;Liang, Jun;Yang, Xi;Zhao, Zheng;Wang, Xu";2019;10.1088/1742-6596/1168/3/032076;NOT_FOUND;Named entity recognition is a fundamental task of Natural Language Processing and belongs to the category of sequence labeling problems. In the text, the named entity is the main carrier of information, which is used to express the main content of the text. Accurately identifying these contents is essential for implementing various natural language processing techniques such as information extraction, information retrieval, machine translation, and question and answer system. Named entities in business documents contain a lot of important information that can bring significant business value to the business. In this paper, we propose the method of combining Bi-directional long-short-term memory network and conditional random field, combining n-gram features and character features, and introducing attention mechanism to identify the tenderee, bidding agent and project number three entities in the bidding documents. Compared with the LSTM, BiLSTM can obtain the context information better and extract more features. The CRF uses the features obtained by BiLSTM to decode and obtain the final labeling result. In the comparative experiment of the collected data sets of 20,000 bidding documents, the BiLSTM-CRF model proposed in this paper can produce better labeling effect than other models and meet our expectations.
NOT_RELEVANT;ProQuest;A Text Abstraction Summary Model Based on BERT Word Embedding and Reinforcement Learning;"Wang, Qicai;Liu, Peiyu;Zhu, Zhenfang;Yin, Hongxia;Zhang, Qiuyue;Zhang, Lindong";2019;10.3390/app9214701;NOT_FOUND;As a core task of natural language processing and information retrieval, automatic text summarization is widely applied in many fields. There are two existing methods for text summarization task at present: abstractive and extractive. On this basis we propose a novel hybrid model of extractive-abstractive to combine BERT (Bidirectional Encoder Representations from Transformers) word embedding with reinforcement learning. Firstly, we convert the human-written abstractive summaries to the ground truth labels. Secondly, we use BERT word embedding as text representation and pre-train two sub-models respectively. Finally, the extraction network and the abstraction network are bridged by reinforcement learning. To verify the performance of the model, we compare it with the current popular automatic text summary model on the CNN/Daily Mail dataset, and use the ROUGE (Recall-Oriented Understudy for Gisting Evaluation) metrics as the evaluation method. Extensive experimental results show that the accuracy of the model is improved obviously.
NOT_RELEVANT;ProQuest;COVER: a linguistic resource combining common sense and lexicographic information;"Mensa, Enrico;Radicioni, Daniele P;Lieto, Antonio";2018;10.1007/s10579-018-9417-z;NOT_FOUND;Lexical resources are fundamental to tackle many tasks that are central to present and prospective research in Text Mining, Information Retrieval, and connected to Natural Language Processing. In this article we introduce COVER , a novel lexical resource, along with COVERAGE , the algorithm devised to build it. In order to describe concepts, COVER proposes a compact vectorial representation that combines the lexicographic precision characterizing BabelNet and the rich common-sense knowledge featuring ConceptNet. We propose COVER as a reliable and mature resource, that has been employed in as diverse tasks as conceptual categorization, keywords extraction, and conceptual similarity. The experimental assessment is performed on the last task: we report and discuss the obtained results, pointing out future improvements. We conclude that COVER can be directly exploited to build applications, and coupled with existing resources, as well.
NOT_RELEVANT;ProQuest;The automatic processing of the texts in natural language. Some bibliometric indicators of the current state of this research area;"Barakhnin, V B;Duisenbayeva, A N;O Yu Kozhemyakina;Yergaliyev, Y N;Muhamedyev, R I";2018;10.1088/1742-6596/1117/1/012001;NOT_FOUND;This work reviews the bibliometric indicators of a rapidly developing field of research as automatic text processing (Natural language processing). The differential indicators of speed and acceleration were used to evaluate the development dynamics of NLP domains. The evaluation was based on the data from the Science direct bibliometric database. The evaluation of the Russian research segment was conducted according to e-library data. The calculations for the following subdomains of NLP were performed: Grammar Checking, Information Extraction, Text Categorization, Dialog Systems, Speech Recognition, Machine Translation, Information Retrieval, Question Answering, Opinion Mining, Smart advisors and others. The areas with high growth rates (Grammar Checking, Information Extraction, Machine Translation and Question Answering) and the areas that have lost the previously existing dynamics of growth of the publication activity (Information Retrieval, Opinion Mining, Text Categorization) have been identified.
NOT_RELEVANT;ProQuest;A Method to Improve Alignment Performance of Uygur and Chinese Words through Corpus Filtering;"Li, Qinqin;Yan, Xiaodong";2018;10.1088/1742-6596/1069/1/012106;NOT_FOUND;Word alignment is the most important part of the statistical machine translation system. The translation model and the ordinal model in statistical machine translation are constructed on the basis of word alignment results, and the errors in the word alignment stage will continue to these models. In the model, even larger mistakes may be caused in these models due to word alignment errors. The research of word alignment technology provides basic construction for corpus construction, speech recognition, bilingual dictionary compilation and information retrieval in the field of natural language processing. However, the research on word alignment technology in Chinese-Uighur is relatively late. We mainly study the alignment of Chinese and Uighur words based on the sentence level and apply the bilingual corpus filtering method based on the degree of alignment perplexity to the alignment of Chinese-Uighur words. The experimental results show that the method is feasible and has achieved the expected results in the primary stage, and the method provides a certain basis for the follow-up research of word alignment techniques.
NOT_RELEVANT;ProQuest;Arabic root extraction using a hybrid technique;"Khafajeh, Hayel;Yousef, Nidal;Abdeldeen, Mahmoud";2018;10.19101/IJACR 2017.733023;NOT_FOUND;Root extraction is one of the main text operations conducted by converting the conflation into its root. This process aims to overcome the morphological richness problem of the Arabic language. Root extraction gives a valuable support to many natural language processing applications such as information retrieval, machine translation, and text-summarizing applications. In this research, a hybrid technique to extract Arabic word roots has been developed. The proposed technique depends on optimization function, which is the enhancing process performed by playing a set of non-morphological rules to enhance the n-gram technique. The proposed technique is tested using a dataset containing more than 6000 distinguished words belonging to 141 different roots. The results show a marked improvement after using the hybrid method, the proposed technique extracts correctly about 99% of tripartite strong roots and about 86% of tripartite vowels roots.
NOT_RELEVANT;ProQuest;Scientific Articles Exploration System Model based in Immersive Virtual Reality and Natural Language Processing Techniques;"Alfaro, Luis;Linares, Ricardo;Herrera, Jose";2018;10.14569/IJACSA.2018.090736;NOT_FOUND;After having carried out a historical review and identifying the state of the art in relation to the interfaces for the exploration of scientific articles, the authors propose a model based in an immersive virtual environment, natural user interfaces and natural language processing, which provides an excellent experience for the user and allows for better use of some of its capabilities, for example, intuition and cognition in 3-dimensional environments. In this work, the Oculus Rift and Leap Motion Hardware devices are used. This work aims to contribute to the proposal of a tool which would facilitate and optimize the arduous task of reviewing literature in scientific databases. The case study is the exploration and information retrieval of scientific articles using ALICIA (Scientific database of Peru). Finally, conclusions and recommendations for future work are laid out and discussed.
NOT_RELEVANT;ProQuest;A Novel System for Generating Simple Sentences from Complex and Compound Sentences;"Das, Bidyut;Majumder, Mukta;Phadikar, Santanu";2018;10.5815/ijmecs.2018.01.06;NOT_FOUND;"In the field of natural language processing, simple sentence has a great importance; especially for multiple choice question generation, automatic text summarization, opinion mining, machine translation and information retrieval etc. Most of these tasks use simple sentences and include a sentence simplification module as pre-processing or post-processing task. But dedicated tasks for sentence simplification are hardly found. Here we have proposed a novel system for generating simple sentences from complex and compound sentences. Our proposed system is an initiative for simplifying sentence by converting complex and compound sentences into simple ones. Along with this the system classifies the simple sentences of an input corpus from other types of sentences. To generate simple sentences from complex and compound sentences we have proposed a novel algorithm which takes the dependency parsing of the input text and produce simple sentences as output. The experimental result demonstrates that the proposed technique is a promising one."
NOT_RELEVANT;ProQuest;Text mining for improved exposure assessment;"Larsson, Kristin;Baker, Simon;Silins, Ilona;Guo, Yufan;Stenius, Ulla;Korhonen, Anna;Berglund, Marika";2017;10.1371/journal.pone.0173132;NOT_FOUND;Chemical exposure assessments are based on information collected via different methods, such as biomonitoring, personal monitoring, environmental monitoring and questionnaires. The vast amount of chemical-specific exposure information available from web-based databases, such as PubMed, is undoubtedly a great asset to the scientific community. However, manual retrieval of relevant published information is an extremely time consuming task and overviewing the data is nearly impossible. Here, we present the development of an automatic classifier for chemical exposure information. First, nearly 3700 abstracts were manually annotated by an expert in exposure sciences according to a taxonomy exclusively created for exposure information. Natural Language Processing (NLP) techniques were used to extract semantic and syntactic features relevant to chemical exposure text. Using these features, we trained a supervised machine learning algorithm to automatically classify PubMed abstracts according to the exposure taxonomy. The resulting classifier demonstrates good performance in the intrinsic evaluation. We also show that the classifier improves information retrieval of chemical exposure data compared to keyword-based PubMed searches. Case studies demonstrate that the classifier can be used to assist researchers by facilitating information retrieval and classification, enabling data gap recognition and overviewing available scientific literature using chemical-specific publication profiles. Finally, we identify challenges to be addressed in future development of the system.
NOT_RELEVANT;ProQuest;Visualising Arabic Sentiments and Association Rules in Financial Text;"AL-Rubaiee, Hamed;Qiu, Renxi;Li, Dayou";2017;10.14569/IJACSA.2017.080201;NOT_FOUND;Text mining methods involve various techniques, such as text categorization, summarisation, information retrieval, document clustering, topic detection, and concept extraction. In addition, because of the difficulties involved in text mining, visualisation techniques can play a paramount role in the analysis and pre-processing of textual data. This paper will present two novel frameworks for the classification and extraction of the association rules and the visualisation of financial Arabic text in order to realize both the general structure and the sentiment within an accumulated corpus. However, mining unstructured data with natural language processing (NLP) and machine learning techniques can be arduous, especially where the Arabic language is concerned, because of limited research in this area. The results show that our frameworks can readily classify Arabic tweets. Furthermore, they can handle many antecedent text association rules for the positive class and the negative class.
NOT_RELEVANT;ProQuest;Exact Expected Average Precision of the Random Baseline for System Evaluation;Bestgen, Yves;2015;10.1515/pralin-2015-0007;NOT_FOUND;Average precision (AP) is one of the most widely used metrics in information retrieval and natural language processing research. It is usually thought that the expected AP of a system that ranks documents randomly is equal to the proportion of relevant documents in the collection. This paper shows that this value is only approximate, and provides a procedure for efficiently computing the exact value. An analysis of the difference between the approximate and the exact value shows that the discrepancy is large when the collection contains few documents, but becomes very small when it contains at least 600 documents.
MAYBE_RELEVANT;ProQuest;Hybrid Ontology for Semantic Information Retrieval Model Using Keyword Matching Indexing System;"Uthayan, K R;Anandha Mala, G S";2015;10.1155/2015/414910;NOT_FOUND;Ontology is the process of growth and elucidation of concepts of an information domain being common for a group of users. Establishing ontology into information retrieval is a normal method to develop searching effects of relevant information users require. Keywords matching process with historical or information domain is significant in recent calculations for assisting the best match for specific input queries. This research presents a better querying mechanism for information retrieval which integrates the ontology queries with keyword search. The ontology-based query is changed into a primary order to predicate logic uncertainty which is used for routing the query to the appropriate servers. Matching algorithms characterize warm area of researches in computer science and artificial intelligence. In text matching, it is more dependable to study semantics model and query for conditions of semantic matching. This research develops the semantic matching results between input queries and information in ontology field. The contributed algorithm is a hybrid method that is based on matching extracted instances from the queries and information field. The queries and information domain is focused on semantic matching, to discover the best match and to progress the executive process. In conclusion, the hybrid ontology in semantic web is sufficient to retrieve the documents when compared to standard ontology.
NOT_RELEVANT;ProQuest;Learning Document Semantic Representation with Hybrid Deep Belief Network;"Yan, Yan;Xu-Cheng, Yin;Li, Sujian;Yang, Mingyuan;Hong-Wei, Hao";2015;10.1155/2015/650527;NOT_FOUND;"High-level abstraction, for example, semantic representation, is vital for document classification and retrieval. However, how to learn document semantic representation is still a topic open for discussion in information retrieval and natural language processing. In this paper, we propose a new Hybrid Deep Belief Network (HDBN) which uses Deep Boltzmann Machine (DBM) on the lower layers together with Deep Belief Network (DBN) on the upper layers. The advantage of DBM is that it employs undirected connection when training weight parameters which can be used to sample the states of nodes on each layer more successfully and it is also an effective way to remove noise from the different document representation type; the DBN can enhance extract abstract of the document in depth, making the model learn sufficient semantic representation. At the same time, we explore different input strategies for semantic distributed representation. Experimental results show that our model using the word embedding instead of single word has better performance."
NOT_RELEVANT;ProQuest;Word Sense Disambiguation Using an Evolutionary Approach;Menai, Mohamed El Bachir;2014;NOT_FOUND;NOT_FOUND;  Word sense disambiguation is a combinatorial problem consisting in the computational assignment of a meaning to a word according to a particular context in which it occurs. Many natural language processing applications, such as machine translation, information retrieval, and information extraction, require this task which occurs at the semantic level. Evolutionary computation approaches can be effective to solve this problem, since they have been successfully used for many NP-hard optimization problems. In this article, researchers have investigated the main existing methods for the word sense disambiguation problem, propose a genetic algorithm to solve it, and apply it to Modern Standard Arabic. They evaluated its performance on a large corpus and compared it against those of some rival algorithms. The genetic algorithm exhibited more precise prediction results.
NOT_RELEVANT;ProQuest;Ontology Based Information Retrieval Using Vector Space Model;"Kolhe, Ankita Khemraj;Bharambe, Prashant;Zope, Nayana S";2013;NOT_FOUND;NOT_FOUND;Information retrieval (IR) is the science of searching for documents, for information within documents and for metadata about documents, as well as that of searching relational databases and the World Wide Web. In this paper, after a brief review on ranking models, a new ontology based approach for ranking HTML/TXT documents is proposed and evaluated in various circumstances. Our approach is applying the vector space model method. Increasing growth of information volume in the internet causes an increasing need to develop new semi) automatic methods for retrieval of documents and ranking them according to their relevance to the user query. This combination reserves the precision of ranking without losing the speed. Our approach exploits natural language processing techniques for extracting phrases and stemming words. The annotated documents and the expanded query will be processed to compute the relevance degree exploiting statistical methods. The outstanding features of our approach are (1) combining HTML, TXT, PDF documents, (2) finding frequency of each and every word, (3) removing stop keywords, (4) applying porter stemming algorithm, to remove the suffix of every word and (5) allowing input variable document using vector dimensions. A ranking system called Information Retrieval using Vector Space Model (IRVSM) is developed to implement and test the proposed model.
NOT_RELEVANT;ProQuest;Labeling Semantically Motivated Clusters of Verbal Relations;"Ferraro, Gabriela;Wanner, Leo";2012;NOT_FOUND;NOT_FOUND;Document clustering is a popular research eld in Natural Language Processing, Data Mining and Information Retrieval. The problem of lexical unit (LU) clustering has been less addressed, and even less so the problem of labeling LU clusters. However, in our application that deals with the distillation of relational tuples from patent claims as input to block diagram or a concept map drawing programs, this problem is central. The assessment of various document cluster labeling techniques lets us assume that despite some signicant dierences that need to be taken into account some of these techniques may also be applied to verbal relation cluster labeling we are concerned with. To conrm this assumption, we carry out a number of experiments and evaluate their outcome against baselines and gold standard labeled clusters.
NOT_RELEVANT;ProQuest;FEDERATED CONFERENCE ON COMPUTER SCIENCE AND INFORMATION SYSTEMS - FedCSIS 2011: 19 - 21 September, 2011: Szczecin, POLAND;Anonymous;2011;NOT_FOUND;NOT_FOUND;EVENTS OF FedCSIS 2011 * AAIA 2011 - 6th International Symposium Advances in Artificial Intelligence and Applications - http://aaia.fedcsis.org * AIMA 2011 - International Workshop on Artificial Intelligence in Medical Applications - http://aima.fedcsis.org * ASIR 2011 - 1st International Workshop on Advances in Semantic Information Retrieval - http://asir.fedcsis.org * WCO 2011 - Workshop on Computational Optimization - http://wco.fedcsis.org * ABICT 2011 - International Workshop on Advances in Business ICT - http://abict.fedcsis.org * CANA 2011 - Computer Aspects of Numerical Algorithms - http://cana.fedcsis.org * IHS 2011 - The 1st International Workshop on Interoperable Healthcare Systems - Challenges, Technologies, and Trends - http://ihs.fedcsis.org * ISSS 2011 - International Symposium on Services Science - http://isss.fedcsis.org * JAWS 2011 - Joint Agent-oriented Workshops in Synergy - http://jaws.fedcsis.org * ABC:
NOT_RELEVANT;ProQuest;MedEval — A Swedish medical test collection with doctors and patients user groups;Karin Friberg Heppin;2011;10.1186/2041-1480-2-S3-S4;NOT_FOUND;Background Test collections for information retrieval are scarce. Domain specific test collections even more so, and medical test collections in the Swedish language non-existent prior to the making of the MedEval test collection. Most research in information retrieval has been performed in the English language, thus most test collections contain English documents. However, English is morphologically poor compared to many other European languages and a number of interesting and important aspects have not been investigated. Building a medical test collection in Swedish opens new research opportunities. Methods This article describes the making of and potential uses of MedEval, a Swedish medical test collection with assessments, not only for topical relevance, but also for target reader group: Doctors or Patients. A user of the test collection may choose if she wishes to search in the Doctors or the Patients scenario where the topical relevance assessments have been adjusted with consideration to user group, or to search in a scenario which regards only topical relevance. In addition to having three user groups, MedEval, in its present form, has two indexes, one where the terms are lemmatized and one where the terms are lemmatized and the compounds split and the constituents indexed together with the whole compound. Results Differences discovered between the documents written for medical professionals and documents written for laypersons are presented. These differences may be utilized in further studies of retrieval of documents aimed at certain groups of readers. Differences between the groups of documents are, for example, that professional documents have a higher ratio of compounds, have a greater average word length and contain more multi-word expressions. An experiment is described where the user scenarios have been utilized, searching with expert terms and lay terms, separately and in combination in the different scenarios. The tendency discovered is that the medical expert gets best results using expert terms and the lay person best results using lay terms, but also quite good results using expert terms or lay and expert terms in combination. Conclusions The many features of MedEval gives a variety of research possibilities, such as comparing the effectiveness of search terms when it comes to retrieving documents aimed at the different user groups or to study the effect of compound decomposition in retrieval of documents. As Swedish, the language of MedEval, is a morphologically more complex language than English, it is possible to study additional aspects of the effect of natural language processing in information retrieval, for example utilizing different inflectional word forms in the retrieval of expert vs lay documents. MedEval is the first Swedish test collection of the medical domain. Availability The Department of Swedish at the University of Gothenburg is in the process of making the MedEval test collection available to academic researchers.
NOT_RELEVANT;ProQuest;Biomedical Informatics Techniques for Processing and Analyzing Web Blogs of Military Service Members;"Konovalov, Sergiy;Brandt, Cynthia";2010;10.2196/jmir.1538;NOT_FOUND;Introduction: Web logs (“blogs”) have become a popular mechanism for people to express their daily thoughts, feelings, and emotions. Many of these expressions contain health care-related themes, both physical and mental, similar to information discussed during a clinical interview or medical consultation. Thus, some of the information contained in blogs might be important for health care research, especially in mental health where stress-related conditions may be difficult and expensive to diagnose and where early recognition is often key to successful treatment. In the field of biomedical informatics, techniques such as information retrieval (IR) and natural language processing (NLP) are often used to unlock information contained in free-text notes. These methods might assist the clinical research community to better understand feelings and emotions post deployment and the burden of symptoms of stress among US military service members. Methods: In total, 90 military blog posts describing deployment situations and 60 control posts of Operation Enduring Freedom/Operation Iraqi Freedom (OEF/OIF) were collected. After “stop” word exclusion and stemming, a “bag-of-words” representation and term weighting was performed, and the most relevant words were manually selected out of the high-weight words. A pilot ontology was created using Collaborative Protégé, a knowledge management application. The word lists and the ontology were then used within General Architecture for Text Engineering (GATE), an NLP framework, to create an automated pipeline for recognition and analysis of blogs related to combat exposure. An independent expert opinion was used to create a reference standard and evaluate the results of the GATE pipeline. Results: The 2 dimensions of combat exposure descriptors identified were: words dealing with physical exposure and the soldiers’ emotional reactions to it. GATE pipeline was able to retrieve blog texts describing combat exposure with precision 0.9, recall 0.75, and F-score 0.82. Discussion: Natural language processing and automated information retrieval might potentially provide valuable tools for retrieving and analyzing military blog posts and uncovering military service members’ emotions and experiences of combat exposure.
NOT_RELEVANT;ProQuest;Measuring Semantic Similarity between Words Using Web Documents;"Takale, Sheetal A;Nandgaonkar, Sushma S";2010;10.14569/IJACSA.2010.010414;NOT_FOUND;Semantic similarity measures play an important role in the extraction of semantic relations. Semantic similarity measures are widely used in Natural Language Processing (NLP) and Information Retrieval (IR). The work proposed here uses web-based metrics to compute the semantic similarity between words or terms and also compares with the state-of-the-art. For a computer to decide the semantic similarity, it should understand the semantics of the words. Computer being a syntactic machine, it can not understand the semantics. So always an attempt is made to represent the semantics as syntax. There are various methods proposed to find the semantic similarity between words. Some of these methods have used the precompiled databases like WordNet, and Brown Corpus. Some are based on Web Search Engine. The approach presented here is altogether different from these methods. It makes use of snippets returned by the Wikipedia or any encyclopedia such as Britannica Encyclopedia. The snippets are preprocessed for stop word removal and stemming. For suffix removal an algorithm by M. F. Porter is referred. Luhn’s Idea is used for extraction of significant words from the preprocessed snippets. Similarity measures proposed here are based on the five different association measures in Information retrieval, namely simple matching, Dice, Jaccard, Overlap, Cosine coefficient. Performance of these methods is evaluated using Miller and Charle’s benchmark dataset. It gives higher correlation value of 0.80 than some of the existing methods.
NOT_RELEVANT;ProQuest;Automatic construction of English/Chinese parallel corpora;"Yang, Christopher C;Li, Kar Wing";2003;10.1002/asi.10261;NOT_FOUND;As the demand for global information increases significantly, multilingual corpora has become a valuable linguistic resource for applications to cross-lingual information retrieval and natural language processing. There are many domain-specific parallel or comparable corpora that are employed in machine translation and cross-lingual information retrieval. However, the Asian/Indo-European corpus, especially English/Chinese corpus, is relatively sparse. The objective of the present research is to construct English/Chinese parallel corpus automatically from the World Wide Web. In this paper, an alignment method is presented which is based on dynamic programming to identify the one-to-one Chinese and English title pairs. The method includes alignment at title level, word level and character level. The longest common subsequence (LCS) is applied to find the most reliable Chinese translation of an English word. Experiments have been conducted to investigate the performance of the proposed method using the daily press release articles by the Hong Kong SAR government as the test bed. The precision of the result is 0.998 while the recall is 0.806.
MAYBE_RELEVANT;ProQuest;NLPIR: A theoretical framework for applying natural language processing to information retrieval;"Zhou, Lina;Zhang, Dongsong";2003;10.1002/asi.10193;NOT_FOUND;The role of information retrieval (IR) in support of decision making and knowledge management has become increasingly significant. Confronted by various problems in traditional keyword-based IR, many researchers have been investigating the potential of natural language processing (NLP) technologies. Despite widespread application of NLP in IR and high expectations that NLP can address the problems of traditional IR, research and development of an NLP component for an IR systems still lacks support and guidance from a cohesive framework. In this paper, a theoretical framework called NLPIR is proposed that aims at integrating NLP into IR and at generalizing broad application of NLP in IR. Some existing NLP techniques are described to validate the framework, which not only can be applied to current research, but is also envisioned to support future research and development in IR that involve NLP.
NOT_RELEVANT;ProQuest;Table of Contents 2 -- No Title;NOT_FOUND;1986;NOT_FOUND;NOT_FOUND;"Meeting the Needs of the Information Age Frances L. Grant and Robert G. Nfai 12 Interdependence of PRECIS Role Operators: A Quantitative Analysis of their Associations Afn oranja Afahapatra and Strbal Cha dra Biswas 30 An Empirical Examination of Lotka's Law bfirartda Lee Pao 26 Evidence of Complex Citer Motivations Terrence A. Brooks 34 Towards Expert Systems for the Selection of Search Keys Raya Fide! 37 In Memorium: Phillip Morse Chi g-Chin Clre 45 BOOK REVIEWS 47 Instructions for Contributors 51 ;F; . y 1986, No. 2 Editorial Staternent Donald H. Kraft 55 Designing Menu Selection Systems Be Sh eider ta 57 Boolean rueries and Term Dependencies in Probabilistic Retrieval Models V. Brrrce Croft 71 A Comparison of Single Book Renewals by Subject and Patron Status for Similar Rates of Renewal and Return Rcgi ald P. Coatlv 78 EUROPEAN RESEARCH LETTER Social Science Information Research David Ellis 86 BOOK REVIEWS 89 ERRATUM 95 May 198B, No. 3 The Synthesis of Specialty Narratives from Co-Citation Clusters Henry Small 97 Cocited Author Mapping as a Valid Representation of Intellectual Structure Katherine lf . Online Searching Introduction and Overview- Lois F. Lupin and Donald T. Max kirrs 179 Database History: From Dinosaurs to Compact Discs M. Lynne Nenfeld and Martha Cornog 183 Natural Language Processing in Information Retrieval Tamas E. Doszkocs 191 Views on End-User Searching Marydee Ojala 197 Transparent Information Systems Through Gateways, Front Ends, Intermediaries, and Interfaces Martha E. i'illianrs 204 Information System Evolution in the Near Future Christopher For 215 Online Information Retrieval Reading List Donald T. Nawkins 220 Calibrating Databases Barnch Fischhojjand Donald MacGregor 222 Observations of End-User Online Searching Behavior Over Eleven Years Winijred Sewell and Sandra Teitelbanm 234 September 1986, No. 5 A Critical Analysis of Vector Space Model for Information Retrieval Vijay V. Raghauan and S. K. M. Wong 279 A System for Processing Bilingual Arabic/Englisb Text F. A. Mnsa 28g Organization of Programming Knowledge of Novices and Experts Susan Wiedenbeck 294 Information Processes."
NOT_RELEVANT;ScienceDirect;Characteristics of Malay translated hadith corpus;Siti Syakirah Sazali and Nurazzah Abdul Rahman and Zainab Abu Bakar;2022;10.1016/j.jksuci.2020.07.011;https://doi.org/10.1016/j.jksuci.2020.07.011;Annotated corpus can greatly assist in the natural language processing field. For example, computers can understand more of the document context, and indexing and clustering in information retrieval can be done precisely with less or no ambiguity of words. However, there are only a few annotated corpora in Malay language, which are not publicly shared. In this paper, we delve into analysing and annotating Malay translated hadith documents in terms of tagging and entities. There are three phases, which are manual filtering and cleaning, analysing the corpus and creating the benchmark. As the result, an analysis and benchmark of Malay translated hadith corpus were produced in term of part-of-speech and named entities tags that follows the Zipf’s law distribution.
NOT_RELEVANT;ScienceDirect;Accounting fraud detection using contextual language learning;Indranil Bhattacharya and Ana Mickovic;2024;10.1016/j.accinf.2024.100682;https://doi.org/10.1016/j.accinf.2024.100682;Accounting fraud is a widespread problem that causes significant damage in the economic market. Detection and investigation of fraudulent firms require a large amount of time, money, and effort for corporate monitors and regulators. In this study, we explore how textual contents from financial reports help in detecting accounting fraud. Pre-trained contextual language learning models, such as BERT, have significantly advanced natural language processing in recent years. We fine-tune the BERT model on Management Discussion and Analysis (MD&A) sections of annual 10-K reports from the Securities and Exchange Commission (SEC) database. Our final model outperforms the textual benchmark model and the quantitative benchmark model from the previous literature by 15% and 12%, respectively. Further, our model identifies five times more fraudulent firm-year observations than the textual benchmark by investigating the same number of firms, and three times more than the quantitative benchmark. Optimizing this investigation process, where more fraudulent observations are detected in the same size of the investigation sample, would be of great economic significance for regulators, investors, financial analysts, and auditors.
NOT_RELEVANT;ScienceDirect;Spatial intelligence and contextual relevance in AI-driven health information retrieval;Niko Yiannakoulias;2024;10.1016/j.apgeog.2024.103392;https://doi.org/10.1016/j.apgeog.2024.103392;The evolution of large language models (LLMs) has already significantly influenced online health information retrieval. As these models gain more widespread use, it is important to understand their ability to contextualize responses based on spatial and geographic information. This study investigates whether LLMs can vary responses based on geographic and spatial context. Using a structured set of prompts submitted to ChatGPT, responses were analyzed to discern patterns based on prompt question and geographic identifiers included in queries. The analysis used word frequency analysis and bidirectional encoder representations from transformers (BERT) embeddings to evaluate the variation in responses concerning geographic specificity. The results provide some evidence that LLMs can generate geographically tailored responses when the query specifies such a need, thereby supporting localized information retrieval. Moreover, prompt responses exhibit an association between spatial distance and word frequency/sentence embedding differences between texts. This result suggests a nuanced representation of spatial information, which could impact user experience by providing more relevant health information based on the user's location. This study lays the groundwork for further exploration into the spatial intelligence of LLMs and their impact on the accessibility of health information online.
NOT_RELEVANT;ScienceDirect;Leveraging artificial intelligence and mutual authentication to optimize content caching in edge data centers;Mbarek Marwan and Feda AlShahwan and Yassine Afoudi and Abdelkarim {Ait Temghart} and Mohamed Lazaar;2023;10.1016/j.jksuci.2023.101742;https://doi.org/10.1016/j.jksuci.2023.101742;Available online Edge data centers are designed to meet the stringent QoE requirements of delay-sensitive and computationally intensive services in Content Delivery Network (CDN) and 5G networks. The primary purpose of this paper was to formulate and solve the problem of optimizing many control variables jointly: (i) what contents to store by taking into consideration edge capacity, and (ii) what contents to recommend to each Internet of Everything (IoE) item, based on identity and access management (IAM). In reactive caching policy, we proposed a new Two-Factor Authentication (2FA) scheme founded upon the Elliptic Curve Cryptography (ECC) and one-way hash function for access control. More interestingly, we use Non-negative Matrix Factorization (NMF), Fuzzy C-Means (FCM), Random Forest (RF) and Pearson Correlation (PC) to improve the accuracy and latency of traditional data filtering models. The intelligent recommendation engine we propose is designed to be implemented by cloud for caching and prefetching contents at the edge. The experimental results validate the theoretical guarantees of the proposed solution and its ability to achieve significant performance gains compared to common baseline models.
MAYBE_RELEVANT;ScienceDirect;Information Retrieval meets Large Language Models: A strategic report from Chinese IR community;Qingyao Ai and Ting Bai and Zhao Cao and Yi Chang and Jiawei Chen and Zhumin Chen and Zhiyong Cheng and Shoubin Dong and Zhicheng Dou and Fuli Feng and Shen Gao and Jiafeng Guo and Xiangnan He and Yanyan Lan and Chenliang Li and Yiqun Liu and Ziyu Lyu and Weizhi Ma and Jun Ma and Zhaochun Ren and Pengjie Ren and Zhiqiang Wang and Mingwen Wang and Ji-Rong Wen and Le Wu and Xin Xin and Jun Xu and Dawei Yin and Peng Zhang and Fan Zhang and Weinan Zhang and Min Zhang and Xiaofei Zhu;2023;10.1016/j.aiopen.2023.08.001;https://doi.org/10.1016/j.aiopen.2023.08.001;The research field of Information Retrieval (IR) has evolved significantly, expanding beyond traditional search to meet diverse user information needs. Recently, Large Language Models (LLMs) have demonstrated exceptional capabilities in text understanding, generation, and knowledge inference, opening up exciting avenues for IR research. LLMs not only facilitate generative retrieval but also offer improved solutions for user understanding, model evaluation, and user-system interactions. More importantly, the synergistic relationship among IR models, LLMs, and humans forms a new technical paradigm that is more powerful for information seeking. IR models provide real-time and relevant information, LLMs contribute internal knowledge, and humans play a central role of demanders and evaluators to the reliability of information services. Nevertheless, significant challenges exist, including computational costs, credibility concerns, domain-specific limitations, and ethical considerations. To thoroughly discuss the transformative impact of LLMs on IR research, the Chinese IR community conducted a strategic workshop in April 2023, yielding valuable insights. This paper provides a summary of the workshop’s outcomes, including the rethinking of IR’s core values, the mutual enhancement of LLMs and IR, the proposal of a novel IR technical paradigm, and open challenges.
NOT_RELEVANT;ScienceDirect;A platform-based Natural Language processing-driven strategy for digitalising regulatory compliance processes for the built environment;Ruben Kruiper and Bimal Kumar and Richard Watson and Farhad Sadeghineko and Alasdair Gray and Ioannis Konstas;2024;10.1016/j.aei.2024.102653;https://doi.org/10.1016/j.aei.2024.102653;The digitalisation of the regulatory compliance process has been an active area of research for several decades. However, more recently the level of activities in this area has increased considerably. In the UK, the tragic incident of Grenfell fire in 2017 has been a major catalyst for this as a result of the Hackitt report’s recommendations pointing a lot of the blame on the broken regulatory regime in the country. The Hackitt report emphasises the need to overhaul the building regulations, but the approach to do so remains an open research question. Existing work in this space tends to overlook the processing of actual regulatory documents, or limits their scope to solving a relatively small subtask. This paper presents a new comprehensive platform approach to the digitalisation of the regulatory compliance processing. We present i-ReC (intelligent Regulatory Compliance), a platform approach to digitalisation of regulatory compliance that takes into consideration the enormous diversity of all the stakeholders’ activities. A historical perspective on research in this area is first presented to put things in perspective which identifies the challenges in such an endeavour and identifies the gaps in state-of-the-art. After enumerating all the challenges in implementing a platform-based approach to digitalising the regulatory compliance process, the implementation of some parts of the platform is described. Our research demonstrates that the identification and extraction of all relevant requirements from the corpus of several hundred regulatory documents is a key part of the whole process which underlies the entire process from authoring to eventually compliance checking of designs. Some of the issues that need addressing in this endeavour include ambiguous language, inconsistent use of terms, contradicting requirements and handling multi-word expressions. The implementation of these tools is driven by NLP, ML and Semantic Web technologies. A semantic search engine was developed and validated against other popular and comparable engines with a corpus of 420 (out of about 800) documents used in the UK for compliance checking of building designs. In every search scenario, our search engine performed better on all objective criteria. Limitations of the approach are discussed which includes the challenges around licensing for all the documents in the corpus. Further work includes improving the performance of SPaR.txt (the tool created to identify multi-word expressions) as well as the information retrieval engine by increasing the dataset and providing the model with examples from more diverse formats of regulations. There is also a need to develop and align strategies to collect a comprehensive set of domain vocabularies to be combined in a Knowledge Graph.
NOT_RELEVANT;ScienceDirect;Next generation of electronic medical record search engines to support chart reviews: A systematic user study and future research direction;Cheng Ye and Daniel Fabbri;2024;10.1016/j.ject.2024.03.003;https://doi.org/10.1016/j.ject.2024.03.003;"Objective
Little research has been done on the user-centered document ranking approach, especially in a crowdsourcing chart review environment. As the starting point of designing and implementing the next generation of Electronic Medical Record (EMR) search engines, a systematic user study is needed to better understand the users' needs, challenges, and future research directions of EMR search engines.
Materials and methods
One primary observation during the user study is the need for a ranking method to better support the so-called ""early stopping"" reviewing strategy (i.e., reviewing only a subset of EMRs of one patient to make the final decision) during the clinical chart reviews. The authors proposed two novel user-centered ranking metrics: ""critical documents"" and ""negative guarantee ratio,"" to better measure the power of a ranking method in supporting the “early stopping” requirements during clinical chart reviews.
Results
The evaluation results show that i) traditional information retrieval metrics, such as the precision-at-K, have limitations in guiding the design and development of EMR search engines to better support clinical chart reviews; ii) there is not a global optimal ranking method that fits the needs of different chart reviews and different users; iii) a learning-to-rank approach cannot guarantee a stable and optimal ranking for different chart reviews and different users; and iv) A user-centered ranking metric, such as the negative guarantee ratio (NGR) metric is able to measure the “early-stopping” performance of ranking methods.
Conclusions
User-centered ranking metrics can better measure the power of ranking methods in supporting clinical chart reviews. Future research should explore more user-centered ranking metrics and evaluate their impact on real-world EMR search engines."
NOT_RELEVANT;ScienceDirect;Extraction of object-action and object-state associations from Knowledge Graphs;Alexandros Vassiliades and Theodore Patkos and Vasilis Efthymiou and Antonis Bikakis and Nick Bassiliades and Dimitris Plexousakis;2024;10.1016/j.websem.2024.100816;https://doi.org/10.1016/j.websem.2024.100816;"Infusing autonomous artificial systems with knowledge about the physical world they inhabit is a critical and long-held aim for the Artificial Intelligence community. Training systems with relevant data is a typical approach; however, finding the data required is not always possible, especially when much of this knowledge is commonsense. In this paper, we present a comparison of topology-based and semantics-based methods for extracting information about object-action and object-state association relations from knowledge graphs, such as ConceptNet, WordNet, ATOMIC, YAGO, WebChild and DBpedia. Moreover, we propose a novel method for extracting information about object-action and object-state associations from knowledge graphs. Our method is composed of a set of techniques for locating, enriching, evaluating, cleaning and exposing knowledge from such resources, relying on semantic similarity methods. Some important aspects of our method are the flexibility in deciding how to deal with the noise that exists in the data, and the capability to determine the importance of a path through training, rather than through manual annotation."
MAYBE_RELEVANT;ScienceDirect;A meta-engine for building domain-specific search engines;Mayank Kejriwal;2021;10.1016/j.simpa.2020.100052;https://doi.org/10.1016/j.simpa.2020.100052;In recent years, domain-specific search (DSS) has emerged as a growing and important area of applied research in artificial intelligence (AI) and information retrieval (IR). Over the last 6 years of research, our group has developed a ‘meta-engine’ called myDIG (my Domain-specific Insight Graphs) that provides a relatively easy and customizable workflow for building DSSs without advanced technical training in crawling, information retrieval or user-interfaces. The myDIG system has been applied to some important and difficult use cases (most notably, fighting human trafficking), in addition to being used in classrooms by graduate students for building complex DSSs from scratch.
NOT_RELEVANT;ScienceDirect;Looking through glass: Knowledge discovery from materials science literature using natural language processing;Vineeth Venugopal and Sourav Sahoo and Mohd Zaki and Manish Agarwal and Nitya Nand Gosvami and N. M. Anoop Krishnan;2021;10.1016/j.patter.2021.100290;https://doi.org/10.1016/j.patter.2021.100290;"Summary
Most of the knowledge in materials science literature is in the form of unstructured data such as text and images. Here, we present a framework employing natural language processing, which automates text and image comprehension and precision knowledge extraction from inorganic glasses’ literature. The abstracts are automatically categorized using latent Dirichlet allocation (LDA) to classify and search semantically linked publications. Similarly, a comprehensive summary of images and plots is presented using the caption cluster plot (CCP), providing direct access to images buried in the papers. Finally, we combine the LDA and CCP with chemical elements to present an elemental map, a topical and image-wise distribution of elements occurring in the literature. Overall, the framework presented here can be a generic and powerful tool to extract and disseminate material-specific information on composition–structure–processing–property dataspaces, allowing insights into fundamental problems relevant to the materials science community and accelerated materials discovery."
NOT_RELEVANT;ScienceDirect;BPR algorithm: New broken plural rules for an Arabic stemmer;Hamood Alshalabi and Sabrina Tiun and Nazlia Omar and Elham {abdulwahab Anaam} and Yazid Saif;2022;10.1016/j.eij.2022.02.006;https://doi.org/10.1016/j.eij.2022.02.006;"One of the most important phases in text processing is stemming, whose aim is to aggregate all variations in a word into one group to aid natural language processing. The morphological structure of the Arabic language is more challenging than that of the English language; thus, it requires superior stemming algorithms for Arabic stemmers to be effective. One of the challenges is the irregular broken plural, which has been a problematic issue in Arabic natural language processing that affects the performance of Arabic information retrieval and other Arabic language engineering applications. Several studies have attempted to develop solutions to irregular plural problems, but the challenge remains, especially in extracting correct Arabic root words. In this paper, the broken plural rule (BPR) algorithm introduces new solutions to solve the problem in which an existing root-based method cannot extract correct roots by using their proposed rules. The BPR algorithm introduces several rules (main rules and subrules) to extract the correct roots of the Arabic irregular broken plural words. To evaluate the effectiveness of the BPR algorithm, we extracted roots from an Arabic standard dataset and applied the BPR algorithm as an enhancement to a root-based Arabic stemmer, ISRI. The obtained results from both evaluations showed encouraging results: (i) Only a few numbers of incorrect roots were stemmed on the large-sized Arabic word dataset. (ii) The enhanced root-based Arabic stemmer, ISRI + BPR, exhibited the best performance compared with the original ISRI stemmer and a well-known Arabic stemmer, ARLS 2. Thus, the proposed BPR algorithm has solved some of the irregular broken plural problems that eventually increase the performance of a root-based Arabic stemmer."
NOT_RELEVANT;ScienceDirect;Towards an LLM based approach for medical e-consent;Mouncef Naji and Maroua Masmoudi and Hajer Baazaoui Zghal;2024;10.1016/j.procs.2024.09.187;https://doi.org/10.1016/j.procs.2024.09.187;The question of informed and voluntary consent emerges as a matter of significance in healthcare. Obtaining informed consent, encounters many obstacles coupled with systemic, clinician-related, and patient-related factors, demanding interventions at different levels. This paper introduces a novel approach to present personalized consent based on Large Language Models (LLMs). The personalization of information is displayed through the combination of the LLM with a knowledge graph. We focus in our approach on how the knowledge graph enhances and personalize content generation, allowing therefore the acquisition of informed consent. The paper focuses as well on aspects related to hyper-parameters of information retrieval that help giving better prompt to the LLM. Experiments have showcased intresting results in terms of personalization and information retrieval using metrics of Rouge, Faithfulness and Relevance.
NOT_RELEVANT;ScienceDirect;El rol de la inteligencia artificial en la publicación científica: perspectivas desde la farmacia hospitalaria;Vicente Gimeno-Ballester and Cristina Trigo-Vicente;2024;10.1016/j.farma.2024.06.002;https://doi.org/10.1016/j.farma.2024.06.002;"Resumen
El artículo explora el impacto de la inteligencia artificial en la escritura científica, con especial atención a su aplicación en la farmacia hospitalaria. Se analizan herramientas de inteligencia artificial que optimizan la búsqueda de información, el análisis de la literatura, la calidad de la escritura y la redacción de manuscritos. Chatbots como Consensus, junto con plataformas como Scite y SciSpace, facilitan la búsqueda precisa en bases de datos científicas, ofreciendo respuestas con evidencia y referencias. SciSpace permite la generación de tablas comparativas y la formulación de preguntas sobre estudios, mientras que ResearchRabbit mapea la literatura científica para identificar tendencias. DeepL y ProWritingAid mejoran la calidad de la escritura al corregir errores gramaticales, de estilo y plagio. A.R.I.A. optimiza la gestión de referencias, mientras que Jenny AI ayuda a superar el bloqueo del escritor. Librerías de Python como LangChain permiten realizar búsquedas semánticas avanzadas y la creación de agentes. A pesar de sus beneficios, la inteligencia artificial plantea preocupaciones éticas como sesgos, desinformación y plagio. Se destaca la importancia de un uso responsable y la revisión crítica por expertos. En la farmacia hospitalaria, la inteligencia artificial puede mejorar la eficiencia y la precisión en la investigación y la comunicación científica. Los farmacéuticos pueden utilizar estas herramientas para mantenerse actualizados, mejorar la calidad de sus publicaciones, optimizar la gestión de la información y facilitar la toma de decisiones clínicas. En conclusión, la inteligencia artificial es una herramienta poderosa para la farmacia hospitalaria, siempre que se utilice de manera responsable y ética.
The article examines the impact of artificial intelligence on scientific writing, with a particular focus on its application in hospital pharmacy. It analyzes artificial intelligence tools that enhance information retrieval, literature analysis, writing quality, and manuscript drafting. Chatbots like Consensus, along with platforms such as Scite and SciSpace, enable precise searches in scientific databases, providing evidence-based responses and references. SciSpace facilitates the generation of comparative tables and the formulation of queries regarding studies, while ResearchRabbit maps the scientific literature to identify trends. Tools like DeepL and ProWritingAid improve writing quality by correcting grammatical, stylistic, and plagiarism errors. A.R.I.A. enhances reference management, and Jenny AI assists in overcoming writer's block. Python libraries such as LangChain enable advanced semantic searches and the creation of agents. Despite their benefits, artificial intelligence raises ethical concerns including biases, misinformation, and plagiarism. The importance of responsible use and critical review by experts is emphasized. In hospital pharmacy, artificial intelligence can enhance efficiency and precision in research and scientific communication. Pharmacists can use these tools to stay updated, enhance the quality of their publications, optimize information management, and facilitate clinical decision-making. In conclusion, artificial intelligence is a powerful tool for hospital pharmacy, provided it is used responsibly and ethically."
NOT_RELEVANT;ScienceDirect;Comparing ChatGPT and clinical nurses’ performances on tracheostomy care: A cross-sectional study;Tongyao Wang and Juan Mu and Jialing Chen and Chia-Chin Lin;2024;10.1016/j.ijnsa.2024.100181;https://doi.org/10.1016/j.ijnsa.2024.100181;"Background
The release of ChatGPT for general use in 2023 by OpenAI has significantly expanded the possible applications of generative artificial intelligence in the healthcare sector, particularly in terms of information retrieval by patients, medical and nursing students, and healthcare personnel.
Objective
To compare the performance of ChatGPT-3.5 and ChatGPT-4.0 to clinical nurses on answering questions about tracheostomy care, as well as to determine whether using different prompts to pre-define the scope of the ChatGPT affects the accuracy of their responses.
Design
Cross-sectional study.
Setting
The data collected from the ChatGPT was collected using the ChatGPT-3.5 and 4.0 using access provided by the University of Hong Kong. The data from the clinical nurses working in mainland China was collected using the Qualtrics survey program.
Participants
No participants were needed for collecting the ChatGPT responses. A total of 272 clinical nurses, with 98.5 % of them working in tertiary care hospitals in mainland China, were recruited using a snowball sampling approach.
Method
We used 43 tracheostomy care-related questions in a multiple-choice format to evaluate the performance of ChatGPT-3.5, ChatGPT-4.0, and clinical nurses. ChatGPT-3.5 and GPT-4.0 were both queried three times with the same questions by different prompts: no prompt, patient-friendly prompt, and act-as-nurse prompt. All responses were independently graded by two qualified otorhinolaryngology nurses on a 3-point accuracy scale (correct, partially correct, and incorrect). The Chi-squared test and Fisher exact test with post-hoc Bonferroni adjustment were used to assess the differences in performance between the three groups, as well as the differences in accuracy between different prompts.
Results
ChatGPT-4.0 showed significantly higher accuracy, with 64.3 % of responses rated as ‘correct’, compared to 60.5 % in ChatGPT-3.5 and 36.7 % in clinical nurses (X 2 = 74.192, p < .001). Except for the ‘care for the tracheostomy stoma and surrounding skin’ domain (X2 = 6.227, p = .156), scores from ChatGPT-3.5 and -4.0 were significantly better than nurses’ on domains related to airway humidification, cuff management, tracheostomy tube care, suction techniques, and management of complications. Overall, ChatGPT-4.0 consistently performed well in all domains, achieving over 50 % accuracy in each domain. Alterations to the prompt had no impact on the performance of ChatGPT-3.5 or -4.0.
Conclusion
ChatGPT may serve as a complementary medical information tool for patients and physicians to improve knowledge in tracheostomy care.
Tweetable abstract
ChatGPT-4.0 can answer tracheostomy care questions better than most clinical nurses. There is no reason nurses should not be using it."
NOT_RELEVANT;ScienceDirect;Design of automated model for inspecting and evaluating handwritten answer scripts: A pedagogical approach with NLP and deep learning;A.Sheik Abdullah and S. Geetha and A.B. {Abdul Aziz} and Utkarsh Mishra;2024;10.1016/j.aej.2024.08.067;https://doi.org/10.1016/j.aej.2024.08.067;We address common challenges examiners face, such as accidental question skipping, marking omissions, and potential bias in assessment. These issues often arise due to the necessity of examining scripts in separate sessions, driven by the high volume of examination materials. In response, we propose the implementation of a self-regulating examiner, harnessing contemporary technology to reduce examiner workload and mitigate the possibility of errors. This automated approach aims to ensure fairness and accuracy in evaluating response scripts, offering a promising solution to the challenges encountered by examiners in the field Our study introduces an innovative approach that seamlessly integrates technologies, including Optical Character Recognition (OCR) for text ex- traction, Natural Language Processing (NLP) for keyword analysis, and ma- chine learning for grading. The results of our method are efficiently presented through a user-friendly web application, providing a streamlined and understandable means for examiners to evaluate response scripts.
NOT_RELEVANT;ScienceDirect;[Translated article] The role of artificial intelligence in scientific publishing: perspectives from hospital pharmacy;Vicente Gimeno-Ballester and Cristina Trigo-Vicente;2024;10.1016/j.farma.2024.07.009;https://doi.org/10.1016/j.farma.2024.07.009;"The article examines the impact of artificial intelligence on scientific writing, with a particular focus on its application in hospital pharmacy. It analyses artificial intelligence tools that enhance information retrieval, literature analysis, writing quality, and manuscript drafting. Chatbots like Consensus, along with platforms such as Scite and SciSpace, enable precise searches in scientific databases, providing evidence-based responses and references. SciSpace facilitates the generation of comparative tables and the formulation of queries regarding studies, while ResearchRabbit maps the scientific literature to identify trends. Tools like DeepL and ProWritingAid improve writing quality by correcting grammatical, stylistic, and plagiarism errors. A.R.I.A. enhances reference management, and Jenny AI assists in overcoming writer's block. Python libraries such as langchain enable advanced semantic searches and the creation of agents. Despite their benefits, artificial intelligence raises ethical concerns including biases, misinformation, and plagiarism. The importance of responsible use and critical review by experts is emphasised. In hospital pharmacy, artificial intelligence can enhance efficiency and precision in research and scientific communication. Pharmacists can use these tools to stay updated, enhance the quality of their publications, optimise information management, and facilitate clinical decision-making. In conclusion, artificial intelligence is a powerful tool for hospital pharmacy, provided it is used responsibly and ethically.
Resumen
El artículo explora el impacto de la Inteligencia artificial en la escritura científica, con especial atención a su aplicación en la farmacia hospitalaria. Se analizan herramientas de inteligencia artificial que optimizan la búsqueda de información, el análisis de la literatura, la calidad de la escritura y la redacción de manuscritos. Chatbots como Consensus, junto con plataformas como Scite y SciSpace, facilitan la búsqueda precisa en bases de datos científicas, ofreciendo respuestas con evidencia y referencias. SciSpace permite la generación de tablas comparativas y la formulación de preguntas sobre estudios, mientras que ResearchRabbit mapea la literatura científica para identificar tendencias. DeepL y ProWritingAid mejoran la calidad de la escritura al corregir errores gramaticales, de estilo y plagio. A.R.I.A. optimiza la gestión de referencias, mientras que Jenny AI ayuda a superar el bloqueo del escritor. Librerías de Python como langchain permiten realizar búsquedas semánticas avanzadas y la creación de agentes. A pesar de sus beneficios, la inteligencia artificial plantea preocupaciones éticas como sesgos, desinformación y plagio. Se destaca la importancia de un uso responsable y la revisión crítica por expertos. En la farmacia hospitalaria, la inteligencia artificial puede mejorar la eficiencia y la precisión en la investigación y la comunicación científica. Los farmacéuticos pueden utilizar estas herramientas para mantenerse actualizados, mejorar la calidad de sus publicaciones, optimizar la gestión de la información y facilitar la toma de decisiones clínicas. En conclusión, la inteligencia artificial es una herramienta poderosa para la farmacia hospitalaria, siempre que se utilice de manera responsable y ética."
NOT_RELEVANT;ScienceDirect;Automatic summarization of MEDLINE citations for evidence-based medical treatment: A topic-oriented evaluation;Marcelo Fiszman and Dina Demner-Fushman and Halil Kilicoglu and Thomas C. Rindflesch;2009;10.1016/j.jbi.2008.10.002;https://doi.org/10.1016/j.jbi.2008.10.002;"As the number of electronic biomedical textual resources increases, it becomes harder for physicians to find useful answers at the point of care. Information retrieval applications provide access to databases; however, little research has been done on using automatic summarization to help navigate the documents returned by these systems. After presenting a semantic abstraction automatic summarization system for MEDLINE citations, we concentrate on evaluating its ability to identify useful drug interventions for 53 diseases. The evaluation methodology uses existing sources of evidence-based medicine as surrogates for a physician-annotated reference standard. Mean average precision (MAP) and a clinical usefulness score developed for this study were computed as performance metrics. The automatic summarization system significantly outperformed the baseline in both metrics. The MAP gain was 0.17 (p<0.01) and the increase in the overall score of clinical usefulness was 0.39 (p<0.05)."
NOT_RELEVANT;ScienceDirect;Natural language processing for clinical notes in dentistry: A systematic review;Farhana Pethani and Adam G. Dunn;2023;10.1016/j.jbi.2023.104282;https://doi.org/10.1016/j.jbi.2023.104282;"Objective
To identify and synthesise research on applications of natural language processing (NLP) for information extraction and retrieval from clinical notes in dentistry.
Materials and methods
A predefined search strategy was applied in EMBASE, CINAHL and Medline. Studies eligible for inclusion were those that that described, evaluated, or applied NLP to clinical notes containing either human or simulated patient information. Quality of the study design and reporting was independently assessed based on a set of questions derived from relevant tools including CHecklist for critical Appraisal and data extraction for systematic Reviews of prediction Modelling Studies (CHARMS). A narrative synthesis was conducted to present the results.
Results
Of the 17 included studies, 10 developed and evaluated NLP methods and 7 described applications of NLP-based information retrieval methods in dental records. Studies were published between 2015 and 2021, most were missing key details needed for reproducibility, and there was no consistency in design or reporting. The 10 studies developing or evaluating NLP methods used document classification or entity extraction, and 4 compared NLP methods to non-NLP methods. The quality of reporting on NLP studies in dentistry has modestly improved over time.
Conclusions
Study design heterogeneity and incomplete reporting of studies currently limits our ability to synthesise NLP applications in dental records. Standardisation of reporting and improved connections between NLP methods and applied NLP in dentistry may improve how we can make use of clinical notes from dentistry in population health or decision support systems. Protocol Registration. PROSPERO CRD42021227823."
NOT_RELEVANT;ScienceDirect;Assessing the performance of ChatGPT's responses to questions related to epilepsy: A cross-sectional study on natural language processing and medical information retrieval;Hyun-Woo Kim and Dong-Hyeon Shin and Jiyoung Kim and Gha-Hyun Lee and Jae Wook Cho;2024;10.1016/j.seizure.2023.11.013;https://doi.org/10.1016/j.seizure.2023.11.013;"Background
Epilepsy is a neurological condition marked by frequent seizures and various cognitive and psychological effects. Reliable information is essential for effective treatment. Natural language processing models like ChatGPT are increasingly used in healthcare for information access and data analysis, making it crucial to assess their accuracy.
Objective
This study aimed to investigate the accuracy of ChatGPT in providing educational information related to epilepsy.
Methods
We compared the answers from ChatGPT-4 and ChatGPT-3.5 to 57 common epilepsy questions based on the Korean Epilepsy Society's ""Epilepsy Patient and Caregiver Guide."" Two epileptologists reviewed the responses, with a third serving as an arbiter in cases of disagreement.
Results
Out of 57 questions, 40 responses from ChatGPT-4 had ""sufficient educational value,"" 16 were ""correct but inadequate,"" and one was ""mixed with correct and incorrect"" information. No answers were entirely incorrect. GPT-4 generally outperformed GPT-3.5 and was often on par with or better than the official guide.
Conclusions
ChatGPT-4 shows promise as a tool for delivering reliable epilepsy-related information and could help alleviate the educational burden on healthcare professionals. Further research is needed to explore the benefits and limitations of using such models in medical contexts."
NOT_RELEVANT;ScienceDirect;Structuring clinical text with AI: Old versus new natural language processing techniques evaluated on eight common cardiovascular diseases;Xianghao Zhan and Marie Humbert-Droz and Pritam Mukherjee and Olivier Gevaert;2021;10.1016/j.patter.2021.100289;https://doi.org/10.1016/j.patter.2021.100289;"Summary
Free-text clinical notes in electronic health records are more difficult for data mining while the structured diagnostic codes can be missing or erroneous. To improve the quality of diagnostic codes, this work extracts diagnostic codes from free-text notes: five old and new word vectorization methods were used to vectorize Stanford progress notes and predict eight ICD-10 codes of common cardiovascular diseases with logistic regression. The models showed good performance, with TF-IDF as the best vectorization model showing the highest AUROC (0.9499–0.9915) and AUPRC (0.2956–0.8072). The models also showed transferability when tested on MIMIC-III data with AUROC from 0.7952 to 0.9790 and AUPRC from 0.2353 to 0.8084. Model interpretability was shown by the important words with clinical meanings matching each disease. This study shows the feasibility of accurately extracting structured diagnostic codes, imputing missing codes, and correcting erroneous codes from free-text clinical notes for information retrieval and downstream machine-learning applications."
NOT_RELEVANT;ScienceDirect;ProtAgents: protein discovery via large language model multi-agent collaborations combining physics and machine learning††Electronic supplementary information (ESI) available: The full records of different conversation experiments along with additional materials are provided as supplementary materials. See DOI: https://doi.org/10.1039/d4dd00013g;Alireza Ghafarollahi and Markus J. Buehler;2024;10.1039/d4dd00013g;https://doi.org/10.1039/d4dd00013g;Designing de novo proteins beyond those found in nature holds significant promise for advancements in both scientific and engineering applications. Current methodologies for protein design often rely on AI-based models, such as surrogate models that address end-to-end problems by linking protein structure to material properties or vice versa. However, these models frequently focus on specific material objectives or structural properties, limiting their flexibility when incorporating out-of-domain knowledge into the design process or comprehensive data analysis is required. In this study, we introduce ProtAgents, a platform for de novo protein design based on Large Language Models (LLMs), where multiple AI agents with distinct capabilities collaboratively address complex tasks within a dynamic environment. The versatility in agent development allows for expertise in diverse domains, including knowledge retrieval, protein structure analysis, physics-based simulations, and results analysis. The dynamic collaboration between agents, empowered by LLMs, provides a versatile approach to tackling protein design and analysis problems, as demonstrated through diverse examples in this study. The problems of interest encompass designing new proteins, analyzing protein structures and obtaining new first-principles data – natural vibrational frequencies – via physics simulations. The concerted effort of the system allows for powerful automated and synergistic design of de novo proteins with targeted mechanical properties. The flexibility in designing the agents, on one hand, and their capacity in autonomous collaboration through the dynamic LLM-based multi-agent environment on the other hand, unleashes great potentials of LLMs in addressing multi-objective materials problems and opens up new avenues for autonomous materials discovery and design.
NOT_RELEVANT;ScienceDirect;Towards an efficient Malayalam Named Entity Recognizer Analysis on the Challenges;Sreeja {P S} and Anitha S Pillai;2020;10.1016/j.procs.2020.04.275;https://doi.org/10.1016/j.procs.2020.04.275;Named Entity Recognition (NER) also known as entity extraction plays an important role in identifying and classifying the named entities into different categories like name of person, place, organization, things, quantity, monetary value etc. that appear in a document. NER has applications in various Natural Language Processing (NLP) tasks such as information retrieval, question answering system, Machine Translation, Sentiment Analysis. NER systems can be developed using rule based, machine learning or hybrid approach. Now Deep learning is being used to develop efficient NER as these models are capable of learning patterns easily and efficiently. Quite a large number of work has been done in English compared to the work done for Indian Languages. The focus of this paper is to highlight the challenges in building an efficient NER for one of the south Indian language namely Malayalam. The different issues that we need to address in order to develop an efficient NER for Malayalam is presented.
NOT_RELEVANT;ScienceDirect;A modified Vector Space Model for semantic information retrieval;Callistus Ireneous Nakpih;2024;10.1016/j.nlp.2024.100081;https://doi.org/10.1016/j.nlp.2024.100081;In this research, we present a modified Vector Space Model which focuses on the semantic relevance of words for retrieving documents. The modified VSM resolves the problem of the classical model performing only lexical matching of query terms to document terms for retrievals. This problem also restricts the classical model from retrieving documents that do not have exact match of query terms even if they are semantically relevant to the query. In the modified model, we introduced a Query Relevance Update technique, which pads the original query set with semantically relevant document terms for optimised semantic retrieval results. The modified model also includes a novel tf−p which replaces the tf−idf technique of the classical VSM, which is used to compute the Term Frequency weights. The replacement of the tf−idf resolves the problem of the classical model penalising terms that occur across documents with the assumption that they are stop words, which in practice, there are usually such words which carry relevant semantic information for documents’ retrieval. We also extended the cosine similarity function with a proportionality weight pqd, which moderates biases for high frequency of terms in longer documents. The pqd ensures that the frequency of query terms including the updated ones are accounted for in proportionality with documents size for the overall ranking of documents. The simulated results reveal that, the modified VSM does achieve semantic retrieval of documents beyond lexical matching of query and document terms.
NOT_RELEVANT;ScienceDirect;Development of a vascular surgery-specific artificial intelligence chat interface using retrieval-augmented generation: VASC.AI, a specialized vascular surgery chatbot;Tiam Feridooni and Arshia P. Javidan and Daniyal N. Mahmood and Zoya Gomes and Andrew Dueck and Mark Wheatcroft and David Szalay;2024;10.1016/j.jvsvi.2024.100137;https://doi.org/10.1016/j.jvsvi.2024.100137;"Background
Large language models (LLMs) exhibit considerable potential in processing educational content for vascular surgery. However, commercially available LLMs are not optimized for medical education and can generate inaccurate information or “hallucinations.” Retrieval-augmented generation (RAG) is an advanced architecture that integrates specialized vascular surgery data into LLMs. This approach customizes LLMs, potentially decreasing the generation of incorrect information and enhancing their educational usefulness.
Methods
This study evaluated the proficiency of baseline Chat Generative Pre-trained Transformer (GPT)-3.5, ChatGPT-4, and ChatGPT-4o models using 244 text-based multiple-choice questions from six VESAP-5 modules, covering aortoiliac disease, cerebrovascular disease, lower extremity disease, renal and mesenteric disease, vascular medicine, and venous disease. The questions were input directly into each model between November 2023 and May 2024. Incorrect responses were categorized as either logical errors or information errors. Additionally, a vascular surgery-specific LLM, VASC.AI, was developed using OpenAI's application programming interface combined with RAG. This model used a database of >200,000 clinical abstracts, guidelines, and landmark trials, vectorized into embeddings for information retrieval. VASC.AI's proficiency was assessed using the same Vascular Education and Self-Assessment Program questions and compared against the baseline models.
Results
ChatGPT-4o and ChatGPT-4 demonstrated improved performance over ChatGPT-3.5, with ChatGPT-4o achieving an average correct response rate of 77.7% ± 7.6%, ChatGPT-4 at 69.0% ± 4.9%, and ChatGPT-3.5 at 55.3% ± 4.3%. VASC.AI significantly outperformed all baseline models, achieving a correct response rate of 93.8% ± 2.4%. Detailed analysis showed that ChatGPT-3.5 had 34.5% ± 13.9% logical errors and 65.5% ± 13.9% information errors, ChatGPT-4 had 22.3% ± 12.7% logical errors and 76.5% ± 12.7% information errors, and ChatGPT-4o had 25.5% ± 9.7% logical errors and 74.7% ± 9.5% information errors. VASC.AI's incorrect responses were solely due to logical errors, demonstrating the efficacy of RAG in providing accurate, specialized information.
Conclusions
The integration of RAG into LLMs significantly improves their performance in specialized medical fields. VASC.AI, tailored with up-to-date vascular surgery-specific data, outperforms general purpose LLMs in answering complex vascular surgery questions. This approach has the potential to enhance medical education, patient care, and clinical decision-making, representing a significant advancement in the application of AI in vascular surgery."
NOT_RELEVANT;ScienceDirect;AI literacy for ethical use of chatbot: Will students accept AI ethics?;Yusuke Kajiwara and Kouhei Kawabata;2024;10.1016/j.caeai.2024.100251;https://doi.org/10.1016/j.caeai.2024.100251;In AI literacy education, there are few examples of education based on AI ethical principles, and limited knowledge exists regarding curriculum design that incorporates AI ethical principles and its effects. Therefore, in this study, we propose a curriculum that teaches the ethical use of large language models (LLM) such as ChatGPT and verify its impact on educational effectiveness and technology acceptance among students aged 12 to 24. The validation results show that the proposed curriculum particularly contributes to the understanding of LLM concepts and their ethical use in decision support. We also demonstrate that experience using ChatGPT influences the level of understanding of ethical usage. Additionally, students aged 12 to 18 may actively adopt ChatGPT responses in decision support, and careful consideration is needed when using LLMs in the 12- to 18-year-old age group. Using technology acceptance model, AI ethical principles were also examined to determine technology acceptance, and it was found that usefulness, justice and fairness, privacy, and data protection directly impact attitudes toward ChatGPT. It has also become clear that students feel uneasy about using their personal information for learning ChatGPT, even if they have consented to the use of their personal information. This result suggests that AI developers and providers need to handle personal information carefully to foster a positive AI attitude.
MAYBE_RELEVANT;ScienceDirect;Support Vector Machines for a new Hybrid Information Retrieval System;Hamid Khalifi and Abderrahim Elqadi and Youssef Ghanou;2018;10.1016/j.procs.2018.01.108;https://doi.org/10.1016/j.procs.2018.01.108;Information Retrieval systems are used to extract, from a large database, relevant information for users. When the type of data is text, the complex nature of the database makes the process of retrieving information more difficult. Generally, such processes reformulate queries according to associations among information items before the query session. In this latter, semantic relationships or other approaches such as machine learning techniques can be applied to select the appropriate results to return. This paper presents a formal model and a new search algorithm. The proposed algorithm is applied to find associations between information items, and then use them to structure search results. It incorporates a natural language preprocessing stage, a statistical representation of short documents and queries and a machine learning model to select relevant results. On a series of experiments through Yahoo dataset, the proposed hybrid information retrieval system returned significantly satisfying results.
NOT_RELEVANT;ScienceDirect;Classification and Analysis of Arabic Natural Language Inference Systems;Mabrouka Ben-Sghaier and Wided Bakari and Mahmoud Neji;2020;10.1016/j.procs.2020.08.057;https://doi.org/10.1016/j.procs.2020.08.057;In natural language, the same meaning can be expressed by different texts. The process of determining the inference relationship occurring between a text T and a hypothesis H is called Natural Language Inference (NLI). The NLI task aims to provide a generic framework that captures, in a unifying manner, the inference across Natural Language Processing applications such as question answering, summarization, information retrieval, and machine translation. Many tasks and datasets have been created to support the development and evaluation of the ability of the NLI task in different languages. For the Arabic language, interest in this field is gradually increasing. This paper aims to provide an overview of state-of-the-art NLI approaches for Arabic, the relevant knowledge resources and the used tools in order to support a better understanding of this growing field. Moreover, this paper points to classify the proposed approaches for Arabic NLI and compare existing NLI systems.
MAYBE_RELEVANT;ScienceDirect;Leveraging AI for Current Research Information Systems: Opportunities and Challenges;Simone Hartmann and Daniel Niederlechner;2024;10.1016/j.procs.2024.11.056;https://doi.org/10.1016/j.procs.2024.11.056;Integrating Artificial Intelligence (AI) into Current Research Information Systems (CRIS) offers significant opportunities to enhance research management. This paper explores AI's potential to automate data handling, improve analytical capabilities, and enhance user experiences within CRIS. Key areas of impact include data enrichment, advanced information retrieval, trend analysis, and predictive analytics. The paper also addresses the challenges and ethical considerations of AI integration, such as data privacy, security, and algorithmic bias. Insights from a Live Poll at the CRIS2024 conference reveal high familiarity with AI among participants, optimism about its potential, and recognition of implementation challenges. By overcoming these obstacles, AI can transform CRIS, making research management more efficient and effective. The paper concludes by advocating for collaboration and dialogue to guide the responsible integration of AI in CRIS, ensuring alignment with stakeholder interests.
NOT_RELEVANT;ScienceDirect;Extraction and normalization of IR indexing terms and phrases in a highly inflectional language;Panagiotis Gakis and Theodoros Kokkinos and Christos Tsalidis;2023;10.1163/15699846-02301001;https://doi.org/10.1163/15699846-02301001;Term-based indexing of documents is conventionally implemented by stemmers or their corpus-based improvements, both of which encode implicit linguistic information. Terms are directly derived from document content such that a unique indexing approach is available at indexing run-time. For highly inflectional languages where term variation is high, such techniques are more error-prone. The main focus of the current study is the extraction and normalization of single terms and phrases and the proposal of authenticated control of indexing. The proposed approach relies on the use of explicit linguistic knowledge, appropriately encoded in large language resources. Such control guarantees the highest possible expansion factor for indexing terms as well as indexing consistency. Moreover, it offers a framework where different and eventually contradicting indexing criteria can be practiced, conventional and Natural Language Processing (NLP)-based Information Retrieval (IR) applications can be served, while adaptations can be made for tuning to a specific domain or corpus.
NOT_RELEVANT;ScienceDirect;A Retrieval-augmented Generation application for Question-Answering in Nutrigenetics Domain;Domenico Benfenati and Giovanni Maria {De Filippis} and Antonio Maria Rinaldi and Cristiano Russo and Cristian Tommasino;2024;10.1016/j.procs.2024.09.467;https://doi.org/10.1016/j.procs.2024.09.467;The domain of nutrigenetics investigates the complex relationship between genetic variations and individual dietary responses, encompassing a wide array of disciplines, including genomics, nutrition science, bioinformatics, and personalized medicine. This field is marked by its intricate data landscape, necessitating innovative approaches to effectively manage and interpret the vast volumes of information involved. Given nutrigenetic data sheer volume and complexity, traditional AI models often struggle to maintain comprehensive and up-to-date knowledge. In this paper, we propose an implementation of the Retrieval-Augmented Generation (RAG) strategy to address the question-answering task in nutrigenetic domain. This framework enhances the accuracy and relevancy of outputs produced by an advanced Large Language Model, circumventing the exhaustive model fine-tuning process. As a result, our RAG approach not only alleviates the computational demand but also fortifies against data leakage concerns, particularly critical in the sensitive area of nutrigenetics. The implementation of RAG in the nutrigenetic domain not only addresses the existing challenges but also paves the way for more advanced and efficient exploration of nutrigenetic data. Our proposed workflow could advance the understanding of nutrigenetic interactions and personalized nutrition.
NOT_RELEVANT;ScienceDirect;Understanding state-of-the-art situation of transport planning strategies in earthquake-prone areas by using AI-supported literature review methodology;Ali Enes Dingil and Ondrej Pribyl;2024;10.1016/j.heliyon.2024.e33645;https://doi.org/10.1016/j.heliyon.2024.e33645;"Aim
This review aims to explore earthquake-based transport strategies in seismic areas, providing state-of-the-art insights into the components necessary to guide urban planners and policymakers in their decision-making processes.
Outputs
The review provides a variety of methodologies and approaches employed for the reinforcement planning and emergency demand management to analyze and evaluate the impact of seismic events on transportation systems, in turn to develop strategies for preparedness, mitigation, response, and recovery phases. The selection of the appropriate approach depends on factors such as the specific transport system, urbanization level and type, built environment, and critical components involved.
Originality and value
Besides providing a distinctive illustration of the integration of transportation and seismic literature as a valuable consolidated resource, this article introduces a novel methodology named ALARM for conducting state-of-the-art reviews on any topic, incorporating AI through the utilization of large language models (LLMs) built upon transformer deep neural networks, along with indexing data structures (in this study mainly OPEN-AI DAVINCI-003 model and vector-storing index). Hence, it is of paramount significance as the first instance of implementing LLMs within academic review standards. This paves the way for the potential integration of AI and human collaboration to become a standard practice under enhanced criteria for comprehending and analyzing specific information."
NOT_RELEVANT;ScienceDirect;Question-answering system for combustion kinetics;Laura Pascazio and Dan Tran and Simon D. Rihm and Jiaru Bai and Sebastian Mosbach and Jethro Akroyd and Markus Kraft;2024;10.1016/j.proci.2024.105428;https://doi.org/10.1016/j.proci.2024.105428;In this paper, we introduce for the first time a natural language question-answering (QA) system specifically designed for the field of combustion kinetics. This system marks a significant step towards achieving the PrIMe vision as outlined by Frenklach in 2007, offering a user-friendly interface that allows researchers and practitioners to easily access and query information about chemical mechanisms. This QA system is a key component of “The World Avatar” (TWA), a dynamic framework built upon semantic web technologies. TWA is characterized by its layered structure, which includes a knowledge graph (KG), software agents, and real-world data integration. These layers collectively create a comprehensive unified system for managing and analyzing complex chemical data from various domains. We detail the enhancements made to TWA’s ontologies (OntoSpecies, OntoKin, and OntoCompChem) to meet specific challenges in chemical kinetics and improve their representation accuracy. By focusing on data provenance and interoperability, our approach ensures transparent and reliable data management that adheres to the FAIR principles, which is vital for precise information retrieval and analysis. The role of software agents in populating these ontologies is highlighted, showcasing how they transform raw data into meaningful structured knowledge and generate new insights within the TWA ecosystem. Additionally, the semantic web technologies’ interoperability feature facilitates data integration and exchange across different platforms and tools, making the data machine-actionable. We instantiated in the KG data on four H2/O2 and five CH4/O2 reaction mechanisms taken from the literature and we then demonstrate the QA system’s capabilities in answering questions related to these reaction mechanisms as a proof of concept. Lastly, we discuss the future directions of the TWA framework, which include not only future extensions of the QA system but also the integration of external tool to automate tasks such as generation of kinetic mechanism, further expanding TWA’s functionality and application in the field of chemical kinetics.
NOT_RELEVANT;ScienceDirect;Natural Language Processing methods and systems for biomedical ontology learning;Kaihong Liu and William R. Hogan and Rebecca S. Crowley;2011;10.1016/j.jbi.2010.07.006;https://doi.org/10.1016/j.jbi.2010.07.006;While the biomedical informatics community widely acknowledges the utility of domain ontologies, there remain many barriers to their effective use. One important requirement of domain ontologies is that they must achieve a high degree of coverage of the domain concepts and concept relationships. However, the development of these ontologies is typically a manual, time-consuming, and often error-prone process. Limited resources result in missing concepts and relationships as well as difficulty in updating the ontology as knowledge changes. Methodologies developed in the fields of Natural Language Processing, information extraction, information retrieval and machine learning provide techniques for automating the enrichment of an ontology from free-text documents. In this article, we review existing methodologies and developed systems, and discuss how existing methods can benefit the development of biomedical ontologies.
NOT_RELEVANT;ScienceDirect;Implementation and evaluation of an additional GPT-4-based reviewer in PRISMA-based medical systematic literature reviews;Assaf Landschaft and Dario Antweiler and Sina Mackay and Sabine Kugler and Stefan Rüping and Stefan Wrobel and Timm Höres and Hector Allende-Cid;2024;10.1016/j.ijmedinf.2024.105531;https://doi.org/10.1016/j.ijmedinf.2024.105531;"Background
PRISMA-based literature reviews require meticulous scrutiny of extensive textual data by multiple reviewers, which is associated with considerable human effort.
Objective
To evaluate feasibility and reliability of using GPT-4 API as a complementary reviewer in systematic literature reviews based on the PRISMA framework.
Methodology
A systematic literature review on the role of natural language processing and Large Language Models (LLMs) in automatic patient-trial matching was conducted using human reviewers and an AI-based reviewer (GPT-4 API). A RAG methodology with LangChain integration was used to process full-text articles. Agreement levels between two human reviewers and GPT-4 API for abstract screening and between a single reviewer and GPT-4 API for full-text parameter extraction were evaluated.
Results
An almost perfect GPT–human reviewer agreement in the abstract screening process (Cohen’s kappa > 0.9) and a lower agreement in the full-text parameter extraction were observed.
Conclusion
As GPT-4 has performed on a par with human reviewers in abstract screening, we conclude that GPT-4 has an exciting potential of being used as a main screening tool for systematic literature reviews, replacing at least one of the human reviewers."
MAYBE_RELEVANT;ScienceDirect;Sustainable Digitalization of Business with Multi-Agent RAG and LLM;Muhammad Arslan and Saba Munawar and Christophe Cruz;2024;10.1016/j.procs.2024.09.337;https://doi.org/10.1016/j.procs.2024.09.337;Businesses heavily rely on data sourced from various channels like news articles, financial reports, and consumer reviews to drive their operations, enabling informed decision-making and identifying opportunities. However, traditional manual methods for data extraction are often time-consuming and resource-intensive, prompting the adoption of digital transformation initiatives to enhance efficiency. Yet, concerns persist regarding the sustainability of such initiatives and their alignment with the United Nations (UN)’s Sustainable Development Goals (SDGs). This research aims to explore the integration of Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG) as a sustainable solution for Information Extraction (IE) and processing. The research methodology involves reviewing existing solutions for business decision-making, noting that many systems require training new machine learning models, which are resource-intensive and have significant environmental impacts. Instead, we propose a sustainable business solution using pre-existing LLMs that can work with diverse datasets. We link domain-specific datasets to tailor LLMs to company needs and employ a Multi-Agent architecture to divide tasks such as information retrieval, enrichment, and classification among specialized agents. This approach optimizes the extraction process and improves overall efficiency. Through the utilization of these technologies, businesses can optimize resource utilization, improve decision-making processes, and contribute to sustainable development goals, thereby fostering environmental responsibility within the corporate sector.
NOT_RELEVANT;ScienceDirect;Musician-AI partnership mediated by emotionally-aware smart musical instruments;Luca Turchet and Domenico Stefani and Johan Pauwels;2024;10.1016/j.ijhcs.2024.103340;https://doi.org/10.1016/j.ijhcs.2024.103340;The integration of emotion recognition capabilities within musical instruments can spur the emergence of novel art formats and services for musicians. This paper proposes the concept of emotionally-aware smart musical instruments, a class of musical devices embedding an artificial intelligence agent able to recognize the emotion contained in the musical signal. This spurs the emergence of novel services for musicians. Two prototypes of emotionally-aware smart piano and smart electric guitar were created, which embedded a recognition method for happiness, sadness, relaxation, aggressiveness and combination thereof. A user study, conducted with eleven pianists and eleven electric guitarists, revealed the strengths and limitations of the developed technology. On average musicians appreciated the proposed concept, who found its value in various musical activities. Most of participants tended to justify the system with respect to erroneous or partially erroneous classifications of the emotions they expressed, reporting to understand the reasons why a given output was produced. Some participants even seemed to trust more the system than their own judgments. Conversely, other participants requested to improve the accuracy, reliability and explainability of the system in order to achieve a higher degree of partnership with it. Our results suggest that, while desirable, perfect prediction of the intended emotion is not an absolute requirement for music emotion recognition to be useful in the construction of smart musical instruments.
NOT_RELEVANT;ScienceDirect;Extracting Collocations from Bengali Text Corpus;Bidyut Das;2012;10.1016/j.protcy.2012.05.049;https://doi.org/10.1016/j.protcy.2012.05.049;Automatic collocation extraction is very important in various applications in the field of natural language processing such as machine translation, word sense disambiguation, information retrieval, and language modelling in speech processing, lexicography and many more. The success of extracting collocations depends on the technique of preprocessing. A systematic pre-processing technique is described in this paper. Then the pre-processed data is used to extract collocation by using two methods: Point-wise Mutual Information and Fuzzy Bi-gram Index. The paper mainly focuses on bi-gram extraction from a Bengali news corpus. Collocations of higher length i.e., n-grams (n>2) are then obtained when the extracted collocations of lower lengths are treated as individual words.
NOT_RELEVANT;ScienceDirect;Risk markers identification in EHR using natural language processing: hemorrhagic and ischemic stroke cases;Sofia Grechishcheva and Egor Efimov and Oleg Metsker;2019;10.1016/j.procs.2019.08.189;https://doi.org/10.1016/j.procs.2019.08.189;This article describes the study results in the development of the method of analysis of semi-structured data from electronic health records to improve the quality of data describing patients’ treatment processes. Improving the accuracy of information retrieval from electronic medical records was achieved by using developed problem-solving oriented library. Moreover, the latent-semantic analysis of the electronic health records of chronic patients with chronic heart failure, diabetes mellitus, hypertension was performed. The main tokens characterizing different groups of patients were revealed. The developed library and semantic analysis based on it can be used to accurately automatic extraction of information from semi-structured electronic medical records. Automated markup of medical texts on the Russian language is also possible for the development of artificial intelligence systems and new generation clinical decision support systems.
NOT_RELEVANT;ScienceDirect;TaSbeeb: A judicial decision support system based on deep learning framework;Huda A. Almuzaini and Aqil M. Azmi;2023;10.1016/j.jksuci.2023.101695;https://doi.org/10.1016/j.jksuci.2023.101695;Since the early 1980s, the legal domain has shown a growing interest in Artificial Intelligence approaches to tackle the increasing number of cases worldwide. TaSbeeb is a deep learning (DL)-based judicial decision support system (JDSS) designed for legal professionals in Saudi courts by retrieving judicial reasoning, Qur’anic verses, and hadiths from a knowledge base. The proposed system consists of three phases: annotation, classification, and information retrieval. To annotate judicial text, we developed Ann-Judicial, a semi-automatic method. To handle the imbalanced corpus for classification, we devised homogeneous and heterogeneous stacking DL models. For information retrieval, we proposed Jud_RoBERTa, a judicial language model. TaSbeeb achieved high accuracy and F-scores in both the classification and information retrieval blocks, showing good accuracy despite complexities in the judicial field and interference between cases. Specifically, the classification phase achieved an accuracy and F-score of 95.8%, while the information retrieval phase achieved an accuracy of 79.8% and F-score of 79.3%. The proposed JDSS has potential for extension to other courts and can be used in judicial inspection. TaSbeeb represents a significant stride towards a more efficient and accurate judicial decision-making process in the Arabic legal system, which has been hindered by a lack of research on Arabic JDSS.
NOT_RELEVANT;ScienceDirect;Enhancing human phenotype ontology term extraction through synthetic case reports and embedding-based retrieval: A novel approach for improved biomedical data annotation;Abdulkadir Albayrak and Yao Xiao and Piyush Mukherjee and Sarah S. Barnett and Cherisse A. Marcou and Steven N. Hart;2024;10.1016/j.jpi.2024.100409;https://doi.org/10.1016/j.jpi.2024.100409;With the increasing utilization of exome and genome sequencing in clinical and research genetics, accurate and automated extraction of human phenotype ontology (HPO) terms from clinical texts has become imperative. Traditional methods for HPO term extraction, such as PhenoTagger, often face limitations in coverage and precision. In this study, we propose a novel approach that leverages large language models (LLMs) to generate synthetic sentences with clinical context, which were semantically encoded into vector embeddings. These embeddings are linked to HPO terms, creating a robust knowledgebase that facilitates precise information retrieval. Our method circumvents the known issue of LLM hallucinations by storing and querying these embeddings within a true database, ensuring accurate context matching without the need for a predictive model. We evaluated the performance of three different embedding models, all of which demonstrated substantial improvements over PhenoTagger. Top recall (sensitivity), precision (positive-predictive value, PPV), and F1 are 0.64, 0.64, and 0.64, respectively, which were 31%, 10%, and 21% better than PhenoTagger. Furthermore, optimal performance was achieved when we combined the best performing embedding model with PhenoTagger (a.k.a. Fused model), resulting in recall (sensitivity), precision (PPV), and F1 values of 0.7, 0.7, and 0.7, respectively, which are 10%, 10%, and 10% better than the best embedding models. Our findings underscore the potential of this integrated approach to enhance the precision and reliability of HPO term extraction, offering a scalable and effective solution for biomedical data annotation.
NOT_RELEVANT;ScienceDirect;Enhanced automated text categorization via Aquila optimizer with deep learning for Arabic news articles;Muhammad Swaileh A. Alzaidi and Alya Alshammari and Abdulkhaleq QA Hassan and Shouki A. Ebad and Hanan Al Sultan and Mohammed A. Alliheedi and Ali Abdulaziz Aljubailan and Khadija Abdullah Alzahrani;2024;10.1016/j.asej.2024.103189;https://doi.org/10.1016/j.asej.2024.103189;Text Classification is the traditional Natural Language Processing (NLP) task. Text classification (also known as categorization) has become a cutting-edge research area in recent years. However, this task has received less attention in Arabic due to the need for more extensive resources for training Arabic text classifiers. In the area of text classification for Arabic news articles, deep learning (DL) methods, namely recurrent neural network (RNN) and convolutional neural network (CNN), were effectively used. This model is trained on labelled datasets around many news topics to automatically categorize articles into predetermined classes. These DL techniques can efficiently discern the subject matter by leveraging the contextual and semantic data embedded in the Arabic text, enabling accurate classification. This application of DL facilitates effective retrieval and organization of Arabic news articles, which supports tasks such as personalized content recommendations, information retrieval, and summarization. Therefore, this study presents an Enhanced Automated Text Categorization via Aquila Optimizer with Deep Learning for Arabic News Articles (TCAODL-ANA) technique. The TCAODL-ANA technique aims to detect and classify Arabic news articles into seven classes. The TCAODL-ANA technique follows pre-processing and the FastText word embedding process to accomplish this. In addition, the TCAODL-ANA technique utilizes an effective attention-based bidirectional gated recurrent unit (ABiGRU) method to identify various news articles. To enhance the detection results of the ABiGRU method, the AO model is employed for the hyperparameter selection process. A comprehensive simulation evaluation is performed to emphasize the improved performance of the TCAODL-ANA technique. The investigational validation portrayed the superior outcomes of the TCAODL-ANA technique over existing techniques.
NOT_RELEVANT;ScienceDirect;Assamese Word Sense Disambiguation using Cuckoo Search Algorithm;Arjun Gogoi and Nomi Baruah and Lakhya Jyoti Nath;2021;10.1016/j.procs.2021.05.110;https://doi.org/10.1016/j.procs.2021.05.110;Natural language processing is associated with human-computer interaction, where several challenges require natural language understanding. The Word sense disambiguation problem comprises the computational assignment of meaning to a word according to a specific context in which it occurs. There are numerous natural language processing applications, such as machine translation, information retrieval, and information extraction, which require this task which takes place at the semantic level. To solve this problem unsupervised computation proposals can be effective since they have been successfully used for many real-world optimization problems. In this paper, we propose to solve the word sense disambiguation problem using the cuckoo search algorithm in the Assamese language. We illustrate the performance of our algorithm by carrying out experiments on an Assamese corpus. And comparing them against an unsupervised genetic algorithm that is implemented in the Assamese language. Results of the experiment show that the cuckoo algorithm can achieve more precision, recall and F-measure, attaining 87.5, 84, and 85.71 percentages respectively.
NOT_RELEVANT;ScienceDirect;Implantable QR code subcutaneous microchip using photoacoustic and ultrasound microscopy for secure and convenient individual identification and authentication;Nan Wan and Pengcheng Zhang and Zuheng Liu and Zhe Li and Wei Niu and Xiuye Rui and Shibo Wang and Myeongsu Seong and Pengbo He and Siqi Liang and Jiasheng Zhou and Rui Yang and Sung-Liang Chen;2023;10.1016/j.pacs.2023.100504;https://doi.org/10.1016/j.pacs.2023.100504;Individual identification and authentication techniques are merged into many aspects of human life with various applications, including access control, payment or banking transfer, and healthcare. Yet conventional identification and authentication methods such as passwords, biometrics, tokens, and smart cards suffer from inconvenience and/or insecurity. Here, inspired by quick response (QR) code and implantable microdevices, implantable and minimally-invasive QR code subcutaneous microchips (QRC-SMs) are proposed to be an effective approach to carry useful and private information, thus enabling individual identification and authentication. Two types of QRC-SMs, QRC-SMs with “hole” and “flat” elements and QRC-SMs with “titanium-coated” and “non-coated” elements, are designed and fabricated to store personal information. Corresponding ultrasound microscopy and photoacoustic microscopy are used for imaging the QR code pattern underneath skin, and open-source artificial intelligence algorithm is applied for QR code detection and recognition. Ex vivo experiments under tissue and in vivo experiments with QRC-SMs implanted in live mice have been performed, demonstrating successful information retrieval from implanted QRC-SMs. QRC-SMs are hidden subcutaneously and invisible to the eyes. They cannot be forgotten, misplaced or lost, and can always be ready for timely medical identification, access control, and payment or banking transfer. Hence, QRC-SMs provide promising routes towards private, secure, and convenient individual identification and authentication.
NOT_RELEVANT;ScienceDirect;Study of Named Entity Recognition methods in biomedical field;Anna Śniegula and Aneta Poniszewska-Marańda and Łukasz Chomątek;2019;10.1016/j.procs.2019.09.466;https://doi.org/10.1016/j.procs.2019.09.466;Natural Language Processing (NLP) is very important in modern data processing taking into consideration different sources, forms and purpose of data as well as information in different areas our industry, administration, public and private life. Our studies concern Natural Language Processing techniques in biomedical field. The increasing volume of information stored in medical health record databases both in natural language and in structured forms is creating increasing challenges for information retrieval (IR) technologies. The paper presents the comparison study of chosen Named Entity Recognition techniques for biomedical field.
NOT_RELEVANT;ScienceDirect;Multi-granularity retrieval of mineral resource geological reports based on multi-feature association;Kai Ma and Junyuan Deng and Miao Tian and Liufeng Tao and Junjie Liu and Zhong Xie and Hua Huang and Qinjun Qiu;2024;10.1016/j.oregeorev.2024.105889;https://doi.org/10.1016/j.oregeorev.2024.105889;Massive geologic report contains all kinds of multimodal geologic data information (geologic text, geologic maps, geologic tables, etc.), which contain a lot of rich geologic basic knowledge and expert experience knowledge about rocks and minerals, stratigraphic structure, geologic age, geographic location, and so on. Accurate retrieval of specific information from massive geologic data has become an important need for geologic information retrieval. However, the majority of existing research primarily revolves around extracting and associating information at a single granularity to facilitate geological semantic retrieval, which ignores many potential semantic associations, leading to ambiguity and fuzziness in semantic retrieval. To solve these problems, this paper proposes a multi-granularity (document-chapter-paragraph) geological information retrieval framework for accurate semantic retrieval. The framework firstly extracts topic feature information, spatiotemporal feature information, figure and table feature information based on the multi-granularity of geological reports. Then, an improved apriori algorithm is used to mine and visualize the associations among the feature information to discover the semantic associations of the geological reports at multiple levels of granularity. Finally, experiments are designed to validate the application of the proposed multi-granularity information retrieval framework on the accurate retrieval of geological reports. The experimental results show that the proposed multi-granularity information retrieval framework in this paper can dig deeper into underlying geo-semantic information and realize accurate retrieval.
NOT_RELEVANT;ScienceDirect;Rules and fuzzy rules in text: concept, extraction and usage;D.H. Kraft and M.J. Martı́n-Bautista and J. Chen and D. Sánchez;2003;10.1016/j.ijar.2003.07.005;https://doi.org/10.1016/j.ijar.2003.07.005;Several concepts and techniques have been imported from other disciplines such as Machine Learning and Artificial Intelligence to the field of textual data. In this paper, we focus on the concept of rule and the management of uncertainty in text applications. The different structures considered for the construction of the rules, the extraction of the knowledge base and the applications and usage of these rules are detailed. We include a review of the most relevant works of the different types of rules based on their representation and their application to most of the common tasks of Information Retrieval such as categorization, indexing and classification.
NOT_RELEVANT;ScienceDirect;Multirole of the internet of medical things (IoMT) in biomedical systems for managing smart healthcare systems: An overview of current and future innovative trends;Darin Mansor Mathkor and Noof Mathkor and Zaid Bassfar and Farkad Bantun and Petr Slama and Faraz Ahmad and Shafiul Haque;2024;10.1016/j.jiph.2024.01.013;https://doi.org/10.1016/j.jiph.2024.01.013;Internet of Medical Things (IoMT) is an emerging subset of Internet of Things (IoT), often called as IoT in healthcare, refers to medical devices and applications with internet connectivity, is exponentially gaining researchers’ attention due to its wide-ranging applicability in biomedical systems for Smart Healthcare systems. IoMT facilitates remote health biomedical system and plays a crucial role within the healthcare industry to enhance precision, reliability, consistency and productivity of electronic devices used for various healthcare purposes. It comprises a conceptualized architecture for providing information retrieval strategies to extract the data from patient records using sensors for biomedical analysis and diagnostics against manifold diseases to provide cost-effective medical solutions, quick hospital treatments, and personalized healthcare. This article provides a comprehensive overview of IoMT with special emphasis on its current and future trends used in biomedical systems, such as deep learning, machine learning, blockchains, artificial intelligence, radio frequency identification, and industry 5.0.
NOT_RELEVANT;ScienceDirect;Fact-checking: relevance assessment of references in the Polish political domain;Albert Sawczyn and Jakub Binkowski and Denis Janiak and Łukasz Augustyniak and Tomasz Kajdanowicz;2021;10.1016/j.procs.2021.08.132;https://doi.org/10.1016/j.procs.2021.08.132;The prevalence of fake news could be observed in circumstances of emotion-causing events, like elections or pandemics. In fear of the potential impact, many fact-checking organisations were established. However, fact-checking requires a large amount of human labor, and hence there is a strong demand for complete automation of this process. Nevertheless, this milestone has not been achieved yet, even for English. The problem grows for the less popular languages that suffer from a scarcity of available resources. To address this problem for the Polish language domain, we propose a solution for automating one of the fact-checking stages - relevance assessment, which is crucial when searching for evidence. Leveraging recent advancements in natural language processing, we have acquired relevant data and developed classifiers of evidence relevance with respect to claims in Polish. Our approach can assess the evidence relevance with a performance at a level of a 0.778 F1-score.
NOT_RELEVANT;ScienceDirect;A survey on literature based discovery approaches in biomedical domain;Vishrawas Gopalakrishnan and Kishlay Jha and Wei Jin and Aidong Zhang;2019;10.1016/j.jbi.2019.103141;https://doi.org/10.1016/j.jbi.2019.103141;Literature Based Discovery (LBD) refers to the problem of inferring new and interesting knowledge by logically connecting independent fragments of information units through explicit or implicit means. This area of research, which incorporates techniques from Natural Language Processing (NLP), Information Retrieval and Artificial Intelligence, has significant potential to reduce discovery time in biomedical research fields. Formally introduced in 1986, LBD has grown to be a significant and a core task for text mining practitioners in the biomedical domain. Together with its inter-disciplinary nature, this has led researchers across domains to contribute in advancing this field of study. This survey attempts to consolidate and present the evolution of techniques in this area. We cover a variety of techniques and provide a detailed description of the problem setting, the intuition, the advantages and limitations of various influential papers. We also list the current bottlenecks in this field and provide a general direction of research activities for the future. In an effort to be comprehensive and for ease of reference for off-the-shelf users, we also list many publicly available tools for LBD. We hope this survey will act as a guide to both academic and industry (bio)-informaticians, introduce the various methodologies currently employed and also the challenges yet to be tackled.
NOT_RELEVANT;ScienceDirect;Mining Unstructured Turkish Economy News Articles;Esra Kahya Özyirmidokuz;2014;10.1016/S2212-5671(14)00809-0;https://doi.org/10.1016/S2212-5671(14)00809-0;Text mining is the analysis of unstructured data by combining techniques from knowledge discovery in databases, natural language processing, information retrieval, and machine learning. Text mining allows us to analyze web content dynamically to find meaningful patterns within large collections of textual data. There are too many economic news articles to read. Therefore, it is a necessary to summarize them. In this study, TM is used to analyze the vast amount of text produced in newspaper articles in Turkey. We mine unstructured economy news with natural language processing techniques including tokenization, transform cases, filtering stopwords and stemming. Similarity analysis is also used to determine similar documents. The word vector is extracted. Therefore, economy news is structured into numeric representations that summarize them. In addition, k-means clustering is used. Consequently, the clusters and similarities of the articles are obtained.
MAYBE_RELEVANT;ScienceDirect;Differences in information accessed in a pharmacologic knowledge base using a conversational agent vs traditional search methods;Anita M. Preininger and Bedda L. Rosario and Adam M. Buchold and Jeff Heiland and Nawshin Kutub and Bryan S. Bohanan and Brett South and Gretchen P. Jackson;2021;10.1016/j.ijmedinf.2021.104530;https://doi.org/10.1016/j.ijmedinf.2021.104530;"Introduction
Clinicians rely on pharmacologic knowledge bases to answer medication questions and avoid potential adverse drug events. In late 2018, an artificial intelligence-based conversational agent, Watson Assistant (WA), was made available to online subscribers to the pharmacologic knowledge base, Micromedex®. WA allows users to ask medication-related questions in natural language. This study evaluated search method-dependent differences in the frequency of information accessed by traditional methods (keyword search and heading navigation) vs conversational agent search.
Materials and methods
We compared the proportion of information types accessed through the conversational agent to the proportion of analogous information types accessed by traditional methods during the first 6 months of 2020.
Results
Addition of the conversational agent allowed early adopters to access 22 different information types contained in the ‘quick answers’ portion of the knowledge base. These information types were accessed 117,550 times with WA during the study period, compared to 33,649,651 times using traditional search methods. The distribution across information types differed by method employed (c2 test, P < .0001). Single drug/dosing, FDA/non-FDA uses, adverse effects, and drug administration emerged as 4 of the top 5 information types accessed by either method. Intravenous compatibility was accessed more frequently using the conversational agent (7.7% vs. 0.6% for traditional methods), whereas dose adjustments were accessed more frequently via traditional methods (4.8% vs. 1.4% for WA).
Conclusion
In a widely used pharmacologic knowledge base, information accessed through conversational agents versus traditional methods differed. User-centered studies are needed to understand these differences."
NOT_RELEVANT;ScienceDirect;A Personal Agents in Ubiquitous Environment: A Survey;Michael Yoseph Ricky and Robin Solala Gulo;2015;10.1016/j.procs.2015.07.514;https://doi.org/10.1016/j.procs.2015.07.514;A personal agents can be implements in various areas. The previous work has been conducted in website and mobile in corresponding to information retrieval, mobile computing, and artificial intelligence. There are different methods and framework are proposed in previous research to obtain and enhance agent's performance for better recommendations. This research aims to present comparison previous research based on personal agent in different areas for understanding of proposed framework design, architecture and its implementations. Personal agent can be applied to analyse and assisting in completing task especially for solving one purpose, and multi agents system can be applied at education, industrial, commercial, governmental, military, and entertainment applications for solving multi purposes.
NOT_RELEVANT;ScienceDirect;Word Embedding based Textual Semantic Similarity Measure in Bengali;MD. Asif Iqbal and Omar Sharif and Mohammed Moshiul Hoque and Iqbal H. Sarker;2021;10.1016/j.procs.2021.10.010;https://doi.org/10.1016/j.procs.2021.10.010;Textual semantic similarity is a crucial constituent in many NLP tasks such as information retrieval, machine translation, information retrieval and textual forgery detection. It is a complicated task for rule-based techniques to address semantic similarity measures in low-resource languages due to the complex morphological structure and scarcity of linguistic resources. This paper investigates several word embedding techniques (Word2Vec, GloVe, FastText) to estimate the semantic similarity of Bengali sentences. Due to the unavailability of the standard dataset, this work developed a Bengali dataset containing 187031 text documents with 400824 unique words. Moreover, this work considers three semantic distance measures to compute the similarity between the word vectors using Cosine similarity with no weight, term frequency weighting and Part-of-Speech weighting. The performance of the proposed approach is evaluated on the developed dataset containing 50 pairs of Bengali sentences. The evaluation result shows that FastText with continuous bag-of-words with 100 vector size achieved the highest Pearson’s correlation (ρ) score of 77.28%.
NOT_RELEVANT;ScienceDirect;Assessment of the European mobility research landscape to support policy shaping through artificial intelligence models;Damir Valput and Ulrike Schmalz and Pablo Hernández and Annika Paul;2023;10.1016/j.trpro.2023.11.448;https://doi.org/10.1016/j.trpro.2023.11.448;This paper presents an approach for assessing EU-funded mobility research initiatives that relies on natural language processing (NLP) techniques. The developed prototype acts as a digital assistant that helps to analyze the mobility research landscape and delivers a bird-eye view of its status, gaps, and bottlenecks. We present data-based models that exploit common NLP techniques used for topic modeling and information retrieval to automatize the analysis of the textual data of over 40,000 H2020 and PF7 research projects and to deliver a series of metrics that support insight discovery. Further, we present an open-access dashboard that visually inspects the model results. Based on the developed models, we provide high-level strategic recommendations for future mobility development. A particular use case focuses on digitalization in mobility.
MAYBE_RELEVANT;ScienceDirect;Exploring a learning-to-rank approach to enhance the Retrieval Augmented Generation (RAG)-based electronic medical records search engines;Cheng Ye;2024;10.1016/j.infoh.2024.07.001;https://doi.org/10.1016/j.infoh.2024.07.001;"Background
This study addresses the challenge of enhancing Retrieval Augmented Generation (RAG) search engines for electronic medical records (EMR) by learning users' distinct search semantics. The specific aim is to develop a learning-to-rank system that improves the accuracy and relevance of search results to support RAG-based search engines.
Methods
Given a prompt or search query, the system first asks the user to label a few randomly selected documents, which contain some keywords, as relevant to the prompt or not. The system then identifies relevant sentences and adjusts word similarities by updating a medical semantic embedding. New documents are ranked by the number of relevant sentences identified by the weighted embedding. Only the top-ranked documents and sentences are provided to a Large-Language-Model (LLM) to generate answers for further review.
Findings
To evaluate our approach, four medical researchers labeled documents based on their relevance to specific diseases. We measured the information retrieval performance of our approach and two baseline methods. Results show that our approach achieved at least a 0.60 Precision-at-10 (P @ 10) score with only ten positive labels, outperforming the baseline methods. In our pilot study, we demonstrate that the learned semantic preference can transfer to the analysis of unseen datasets, boosting the accuracy of an RAG model in extracting and explaining cancer progression diagnoses from 0.14 to 0.50.
Interpretation
This study demonstrates that a customized learning-to-rank method can enhance state-of-the-art natural language models, such as LLMs, by quickly adapting to users' semantics. This approach supports EMR document retrieval and helps RAG models generate clinically meaningful answers to specific questions, underscoring the potential of user-tailored learning-to-rank methods in clinical practice."
NOT_RELEVANT;ScienceDirect;Natural Language Processing with Optimal Deep Learning-Enabled Intelligent Image Captioning System;Radwa Marzouk and Eatedal Alabdulkreem and Mohamed K. Nour and Mesfer Al Duhayyim and Mahmoud Othman and Abu Sarwar Zamani and Ishfaq Yaseen and Abdelwahed Motwakel;2022;10.32604/cmc.2023.033091;https://doi.org/10.32604/cmc.2023.033091;The recent developments in Multimedia Internet of Things (MIoT) devices, empowered with Natural Language Processing (NLP) model, seem to be a promising future of smart devices. It plays an important role in industrial models such as speech understanding, emotion detection, home automation, and so on. If an image needs to be captioned, then the objects in that image, its actions and connections, and any silent feature that remains under-projected or missing from the images should be identified. The aim of the image captioning process is to generate a caption for image. In next step, the image should be provided with one of the most significant and detailed descriptions that is syntactically as well as semantically correct. In this scenario, computer vision model is used to identify the objects and NLP approaches are followed to describe the image. The current study develops a Natural Language Processing with Optimal Deep Learning Enabled Intelligent Image Captioning System (NLPODL-IICS). The aim of the presented NLPODL-IICS model is to produce a proper description for input image. To attain this, the proposed NLPODL-IICS follows two stages such as encoding and decoding processes. Initially, at the encoding side, the proposed NLPODL-IICS model makes use of Hunger Games Search (HGS) with Neural Search Architecture Network (NASNet) model. This model represents the input data appropriately by inserting it into a predefined length vector. Besides, during decoding phase, Chimp Optimization Algorithm (COA) with deeper Long Short Term Memory (LSTM) approach is followed to concatenate the description sentences produced by the method. The application of HGS and COA algorithms helps in accomplishing proper parameter tuning for NASNet and LSTM models respectively. The proposed NLPODL-IICS model was experimentally validated with the help of two benchmark datasets. A widespread comparative analysis confirmed the superior performance of NLPODL-IICS model over other models.
NOT_RELEVANT;ScienceDirect;Construction and evaluation of gold standards for patent classification—A case study on quantum computing;Steve Harris and Anthony Trippe and David Challis and Nigel Swycher;2020;10.1016/j.wpi.2020.101961;https://doi.org/10.1016/j.wpi.2020.101961;This article discusses options for evaluation of patent and/or patent family classification algorithms by means of “gold standards”. It covers the creation criteria, and desirable attributes of evaluation mechanisms, then proposes an example gold standard, and discusses the results of applying the evaluation mechanism against the proposed gold standard and an existing commercial implementation.
MAYBE_RELEVANT;ScienceDirect;Evaluating LLMs on document-based QA: Exact answer selection and numerical extraction using CogTale dataset;Zafaryab Rasool and Stefanus Kurniawan and Sherwin Balugo and Scott Barnett and Rajesh Vasa and Courtney Chesser and Benjamin M. Hampstead and Sylvie Belleville and Kon Mouzakis and Alex Bahar-Fuchs;2024;10.1016/j.nlp.2024.100083;https://doi.org/10.1016/j.nlp.2024.100083;Document-based Question-Answering (QA) tasks are crucial for precise information retrieval. While some existing work focus on evaluating large language model’s (LLMs) performance on retrieving and answering questions from documents, assessing the LLMs performance on QA types that require exact answer selection from predefined options and numerical extraction is yet to be fully assessed. In this paper, we specifically focus on this underexplored context and conduct empirical analysis of LLMs (GPT-4 and GPT-3.5) on question types, including single-choice, yes–no, multiple-choice, and number extraction questions from documents. We use the CogTale dataset for evaluation, which provide human expert-tagged responses, offering a robust benchmark for precision and factual grounding. We found that LLMs, particularly GPT-4, can precisely answer many single-choice and yes–no questions given relevant context, demonstrating their efficacy in information retrieval tasks. However, their performance diminishes when confronted with multiple-choice and number extraction formats, lowering the overall performance of the models on this task, indicating that these models may not yet be sufficiently reliable for the task. This limits the applications of LLMs on applications demanding precise information extraction and inference from documents, such as meta-analysis tasks. Our work offers a framework for ongoing dataset evaluation, ensuring that LLM applications for information retrieval and document analysis continue to meet evolving standards.
NOT_RELEVANT;ScienceDirect;AI-based decision support system for public procurement;Lucia Siciliani and Vincenzo Taccardi and Pierpaolo Basile and Marco {Di Ciano} and Pasquale Lops;2023;10.1016/j.is.2023.102284;https://doi.org/10.1016/j.is.2023.102284;Tenders are powerful means of investment of public funds and represent a strategic development resource. Thus, improving the efficiency of procuring entities and developing evaluation models turn out to be essential to facilitate e-procurement procedures. With this contribution, we introduce our research to create a supporting system for the decision-making and monitoring process during the entire course of investments and contracts. This system employs artificial intelligence techniques based on natural language processing, focused on providing instruments for extracting useful information from both structured and unstructured (i.e., text) data. Therefore, we developed a framework based on a web app that provides integrated tools such as a semantic search engine, a summariser, an open information extraction engine in the form of triples (subject–predicate–object) for tender documents, and dashboards for analysing tender data.
NOT_RELEVANT;ScienceDirect;A Survey of Knowledge Graph Construction Using Machine Learning;Zhigang Zhao and Xiong Luo and Maojian Chen and Ling Ma;2023;10.32604/cmes.2023.031513;https://doi.org/10.32604/cmes.2023.031513;Knowledge graph (KG) serves as a specialized semantic network that encapsulates intricate relationships among real-world entities within a structured framework. This framework facilitates a transformation in information retrieval, transitioning it from mere string matching to far more sophisticated entity matching. In this transformative process, the advancement of artificial intelligence and intelligent information services is invigorated. Meanwhile, the role of machine learning method in the construction of KG is important, and these techniques have already achieved initial success. This article embarks on a comprehensive journey through the last strides in the field of KG via machine learning. With a profound amalgamation of cutting-edge research in machine learning, this article undertakes a systematical exploration of KG construction methods in three distinct phases: entity learning, ontology learning, and knowledge reasoning. Especially, a meticulous dissection of machine learning-driven algorithms is conducted, spotlighting their contributions to critical facets such as entity extraction, relation extraction, entity linking, and link prediction. Moreover, this article also provides an analysis of the unresolved challenges and emerging trajectories that beckon within the expansive application of machine learning-fueled, large-scale KG construction.
NOT_RELEVANT;ScienceDirect;Vision, status, and research topics of Natural Language Processing;Xieling Chen and Haoran Xie and Xiaohui Tao;2022;10.1016/j.nlp.2022.100001;https://doi.org/10.1016/j.nlp.2022.100001;The field of Natural Language Processing (NLP) has evolved with, and as well as influenced, recent advances in Artificial Intelligence (AI) and computing technologies, opening up new applications and novel interactions with humans. Modern NLP involves machines’ interaction with human languages for the study of patterns and obtaining meaningful insights. NLP is increasingly receiving attention across academia and industry and demonstrates extraordinary opportunities and across AI applications (e.g., question answering, information retrieval, sentiment analysis, and recommender systems) and helps to deal with new tasks such as machine translation and reading comprehension, with real world performance improving all the time. This editorial first provides an overview of the field of NLP in terms of research grants, publication venues, and research topics. We then introduce the mission of Natural Language Processing Journal, a new NLP-focused Elsevier journal intended as a forum for researchers and practitioners to publish theoretical, practical, and methodological achievements related to trustworthy AI development and applications for analyzing, processing, and modeling human languages.
NOT_RELEVANT;ScienceDirect;Building Textual Fuzzy Interpretive Structural Modeling to Analyze Factors of Student Mobility Based on User Generated Content;Ronak Razavisousan and Karuna Pande Joshi;2022;10.1016/j.jjimei.2022.100093;https://doi.org/10.1016/j.jjimei.2022.100093;Many factors influence student mobility across regions and countries. The roles of these factors, along with their interrelationship and interaction, make student mobility a complex decision-making issue. Many textual data generated on social media can answer many open questions about factors affecting human behavior, particularly social mobility. We have developed a novel methodology, called Textual Fuzzy Interpretive Structural Modeling (TFISM), that automatically analyses large textual datasets to identify the internal and external relationships between management or decision-making problems. This computational social science methodology enhances Interpretive Structural Modeling (ISM) approaches to allow the input to be textual data. It is multi-disciplinary and integrates ISM with Artificial Intelligence, Text extraction, and information retrieval techniques. TFISM is a domain-free method, while we have validated this methodology on two different datasets from social media and academic articles. In this paper, we present the results of our study to identify the critical factors and most influential factors for global student mobility.
NOT_RELEVANT;ScienceDirect;The Process and Algorithm Analysis of Text Mining System Based on Artificial Intelligence;Xiaoliang Chai and Songxiao Xu and Shilin Li and Junyu Zhao;2023;10.1016/j.procs.2023.11.066;https://doi.org/10.1016/j.procs.2023.11.066;The rapid development of the Internet leads to the rapid growth of network information, we call it information explosion. The Internet is full of information, and it is difficult for users to find this information and useful knowledge of the ocean. The Web has become the world's largest information repository, and there is an urgent need for efficient access to the valuable knowledge of vast amounts of web information. The purpose of this paper is to study the process and algorithm analysis of text mining system based on artificial intelligence. This paper presents an algorithm of document feature acquisition based on genetic algorithm. Selecting suitable features is an important task in specific text classification and information retrieval. Finding appropriate feature vectors to represent the text will undoubtedly help with subsequent sorting and grouping. Based on the genetic algorithm of variable length chromosome, this paper improves the crossover, mutation and selection operations, and proposes an algorithm to obtain text feature vectors. This method has a wide range of applications and good results.
NOT_RELEVANT;ScienceDirect;text2graphAPI: A library to transform text documents into different graph representations;Andric Valdez-Valenzuela and Helena Gómez-Adorno;2024;10.1016/j.softx.2024.101888;https://doi.org/10.1016/j.softx.2024.101888;This paper introduces a new Python API called text2graphAPI. It is an easy-to-use library for transforming text documents into different graph representations, such as Word-Cooccurrence, Heterogeneous, and Integrated Syntactic Graphs. In addition, it contains a text pre-processing module that supports input text in different languages: English, Spanish, and French. These generated graph structures can be used to solve tasks in various areas, such as Authorship Analysis, Information Retrieval, and Topic Classification, to name a few.
MAYBE_RELEVANT;ScienceDirect;A Domain Specific Multi-Document Reading Comprehension Method for Artificial Intelligence Application;Chen Lei and Zhao Baojin and Dong Xinran and Cui Zaixing;2022;10.1016/j.procs.2022.10.019;https://doi.org/10.1016/j.procs.2022.10.019;With the development of artificial intelligence, information retrieval and information extraction and knowledge services from large-scale texts are currently one of the most urgent needs of people. Machine reading comprehension technology is one of the key technologies that can be applied to knowledge mining. At present, multi-document reading comprehension has received a lot of attention, and its application scenarios are also very extensive. The main goal of this article is to find the answer to the question from a large number of smartphone manuals based on the questions raised by the user about the operation of the smartphone. This paper designs a pipeline structure with three modules: retrieval, extraction, and sorting. At the same time, it designs auxiliary tasks for the extraction model to improve the extraction ability, and uses a new answer scoring method to select answers. The final experiment proves that our method can effectively improve the answer's quality.
NOT_RELEVANT;ScienceDirect;ChatGPT for digital pathology research;Mohamed Omar and Varun Ullanat and Massimo Loda and Luigi Marchionni and Renato Umeton;2024;10.1016/S2589-7500(24)00114-6;https://doi.org/10.1016/S2589-7500(24)00114-6;"Summary
The rapid evolution of generative artificial intelligence (AI) models including OpenAI's ChatGPT signals a promising era for medical research. In this Viewpoint, we explore the integration and challenges of large language models (LLMs) in digital pathology, a rapidly evolving domain demanding intricate contextual understanding. The restricted domain-specific efficiency of LLMs necessitates the advent of tailored AI tools, as illustrated by advancements seen in the last few years including FrugalGPT and BioBERT. Our initiative in digital pathology emphasises the potential of domain-specific AI tools, where a curated literature database coupled with a user-interactive web application facilitates precise, referenced information retrieval. Motivated by the success of this initiative, we discuss how domain-specific approaches substantially minimise the risk of inaccurate responses, enhancing the reliability and accuracy of information extraction. We also highlight the broader implications of such tools, particularly in streamlining access to scientific research and democratising access to computational pathology techniques for scientists with little coding experience. This Viewpoint calls for an enhanced integration of domain-specific text-generation AI tools in academic settings to facilitate continuous learning and adaptation to the dynamically evolving landscape of medical research."
NOT_RELEVANT;ScienceDirect;Information retrieval and artificial intelligence;Karen {Sparck Jones};1999;10.1016/S0004-3702(99)00075-2;https://doi.org/10.1016/S0004-3702(99)00075-2;This paper addresses the relations between information retrieval (IR) and AI. It examines document retrieval, summarising its essential features and illustrating the state of its art by presenting one probabilistic model in detail, with some test results showing its value. The paper then analyses this model and related successful approaches, concentrating on and justifying their use of weak, redundant representation and reasoning. It goes on to other information management tasks and considers how the concepts and methods developed for retrieval may be applied to these, concluding by arguing that such ways of dealing with information may also have wider relevance to AI.
NOT_RELEVANT;ScienceDirect;Characterising global antimicrobial resistance research explains why One Health solutions are slow in development: An application of AI-based gap analysis;Cai Chen and Shu-Le Li and Yao-Yang Xu and Jue Liu and David W. Graham and Yong-Guan Zhu;2024;10.1016/j.envint.2024.108680;https://doi.org/10.1016/j.envint.2024.108680;"The global health crisis posed by increasing antimicrobial resistance (AMR) implicitly requires solutions based a One Health approach, yet multisectoral, multidisciplinary research on AMR is rare and huge knowledge gaps exist to guide integrated action. This is partly because a comprehensive survey of past research activity has never performed due to the massive scale and diversity of published information. Here we compiled 254,738 articles on AMR using Artificial Intelligence (AI; i.e., Natural Language Processing, NLP) methods to create a database and information retrieval system for knowledge extraction on research perfomed over the last 20 years. Global maps were created that describe regional, methodological, and sectoral AMR research activities that confirm limited intersectoral research has been performed, which is key to guiding science-informed policy solutions to AMR, especially in low-income countries (LICs). Further, we show greater harmonisation in research methods across sectors and regions is urgently needed. For example, differences in analytical methods used among sectors in AMR research, such as employing culture-based versus genomic methods, results in poor communication between sectors and partially explains why One Health-based solutions are not ensuing. Therefore, our analysis suggest that performing culture-based and genomic AMR analysis in tandem in all sectors is crucial for data integration and holistic One Health solutions. Finally, increased investment in capacity development in LICs should be prioritised as they are places where the AMR burden is often greatest. Our open-access database and AI methodology can be used to further develop, disseminate, and create new tools and practices for AMR knowledge and information sharing."
NOT_RELEVANT;ScienceDirect;Design and Development of Diagnostic Chabot for supporting Primary Health Care Systems;Bushra Kidwai and Nadesh RK;2020;10.1016/j.procs.2020.03.184;https://doi.org/10.1016/j.procs.2020.03.184;Technology is increasingly becoming a massive part of today’s healthcare scenario. Technology has changed the way how patients communicate with doctors and not only that, but also how healthcare is administered. Artificial intelligence and Chabots are two groundbreaking technologies that have changed how patients and doctors perceive healthcare. To make healthcare system more interactive a diagnostic Chabot is designed and developed using latest algorithms in machine learning, decision tree algorithm to help the user to form a diagnosis of their condition based on their symptoms. The system will be fed with information pertaining to various diseases and using NLP, it will be able to understand the user query and give a suitable response. The system can be used for effective information retrieval in a similar manner like siri, alexa etc but the scope will be limited to disease diagnosis.
MAYBE_RELEVANT;ScienceDirect;A survey on textual entailment based question answering;Aarthi Paramasivam and S. Jaya Nirmala;2022;10.1016/j.jksuci.2021.11.017;https://doi.org/10.1016/j.jksuci.2021.11.017;Question answering, an information retrieval system that seeks knowledge, is one of the classic applications in Natural Language Processing. A question answering system comprises numerous sets of subtasks. Some of the subtasks are Passage Retrieval, Answer Ranking, Question Similarity, Question Generation, Question Classification, Answer Selection, and Answer Validation. Numerous approaches have been experimented on in the question answering system to achieve accurate results. One such approach for the question answering system is Textual Entailment. Textual Entailment is a framework that captures significant semantic inference. Textual Entailment of two text fragments can be defined as the task of deciding whether the meaning of one text fragment can be inferred from another text fragment. This survey discusses how and why Textual Entailment is applied to various subtasks in question answering.
NOT_RELEVANT;ScienceDirect;Preprocessing Arabic text on social media;Mohamed Osman Hegazi and Yasser Al-Dossari and Abdullah Al-Yahy and Abdulaziz Al-Sumari and Anwer Hilal;2021;10.1016/j.heliyon.2021.e06191;https://doi.org/10.1016/j.heliyon.2021.e06191;Currently, social media plays an important role in daily life and routine. Millions of people use social media for different purposes. Large amounts of data flow through online networks every second, and these data contain valuable information that can be extracted if the data are properly processed and analyzed. However, most of the processing results are affected by preprocessing difficulties. This paper presents an approach to extract information from social media Arabic text. It provides an integrated solution for the challenges in preprocessing Arabic text on social media in four stages: data collection, cleaning, enrichment, and availability. The preprocessed Arabic text is stored in structured database tables to provide a useful corpus to which, information extraction and data analysis algorithms can be applied. The experiment in this study reveals that the implementation of the proposed approach yields a useful and full-featured dataset and valuable information. The resultant dataset presented the Arabic text in three structured levels with more than 20 features. Additionally, the experiment provides valuable information and processed results such as topic classification and sentiment analysis.
NOT_RELEVANT;ScienceDirect;Attention is not all you need: the complicated case of ethically using large language models in healthcare and medicine;Stefan Harrer;2023;10.1016/j.ebiom.2023.104512;https://doi.org/10.1016/j.ebiom.2023.104512;"Summary
Large Language Models (LLMs) are a key component of generative artificial intelligence (AI) applications for creating new content including text, imagery, audio, code, and videos in response to textual instructions. Without human oversight, guidance and responsible design and operation, such generative AI applications will remain a party trick with substantial potential for creating and spreading misinformation or harmful and inaccurate content at unprecedented scale. However, if positioned and developed responsibly as companions to humans augmenting but not replacing their role in decision making, knowledge retrieval and other cognitive processes, they could evolve into highly efficient, trustworthy, assistive tools for information management. This perspective describes how such tools could transform data management workflows in healthcare and medicine, explains how the underlying technology works, provides an assessment of risks and limitations, and proposes an ethical, technical, and cultural framework for responsible design, development, and deployment. It seeks to incentivise users, developers, providers, and regulators of generative AI that utilises LLMs to collectively prepare for the transformational role this technology could play in evidence-based sectors."
NOT_RELEVANT;ScienceDirect;On the capacity of artificial intelligence techniques and statistical methods to deal with low-quality data in medical supply chain environments;Francisco Javier {Santos Arteaga} and Debora {Di Caprio} and Madjid Tavana and David Cucchiari and Josep M. Campistol and Federico Oppenheimer and Fritz Diekmann and Ignacio Revuelta;2024;10.1016/j.engappai.2024.108610;https://doi.org/10.1016/j.engappai.2024.108610;We illustrate the capacity of Artificial Intelligence (AI) and Machine Learning (ML) techniques to preserve consistent categorization abilities whenever the quality of the data decreases, displaying mistakes or mismatches across matrix entries, while standard statistical methods exhibit significant modifications in the value of the corresponding coefficients. We design algorithms of different complexity to generate a series of comparable profiles. These profiles are compared within environments that allow for an immediate identification of the generating algorithms and within increasingly complex settings involving almost identical profiles derived from different algorithms. AI and ML techniques outperform standard statistical methods when distinguishing the algorithms generating the profiles. Building on these results, we perform a retrospective analysis where AI and ML techniques are applied to two empirical scenarios defined by different data series of patients transplanted through the period 2006–2019. The first scenario contains the variables describing the evolution of patients inputted correctly. In the second, we modify the content of the vectors of characteristics defining the evolution of patients by exchanging the values of a subset of realizations from two categorical variables. AI and ML techniques are consistently accurate when categorizing patients correctly within both scenarios, a feature particularly relevant when the quality of the information sources composing the medical chain varies. This latter problem is exacerbated among hospitals located in developing countries, where the quality of the data gathered limits their identification and extrapolation capacities.
NOT_RELEVANT;ScienceDirect;University students’ self-reported reliance on ChatGPT for learning: A latent profile analysis;Ana Stojanov and Qian Liu and Joyce Hwee Ling Koh;2024;10.1016/j.caeai.2024.100243;https://doi.org/10.1016/j.caeai.2024.100243;Although ChatGPT, a state-of-the-art, large language model, seems to be a disruptive technology in higher education, it is unclear to what extent students rely on this tool for completing different tasks. To address this gap, we asked university students (N = 490) recruited via CloudResearch to rate the extent to which they rely on ChatGPT for completing 13 tasks identified in a previous pilot study. Five distinct profiles emerged: ‘Versatile low reliers’ (38.2%) were characterised by low overall self-reported reliance across the tasks, while ‘all-rounders’ (10.4%) had high overall self-reported reliance. The ‘knowledge seekers’ (16.5%) scored particularly high on tasks such as content acquisition, information retrieval and summarising of texts, while the ‘proactive learners’ (11.8%) on tasks such as obtaining feedback, planning and quizzing. Finally, the ‘assignment delegators’ (23.1%) relied on ChatGPT for drafting assignments, writing homework and having ChatGPT write their assignment for them. The findings provide a nuanced understanding of how students rely on ChatGPT for learning.
NOT_RELEVANT;ScienceDirect;AraCovTexFinder: Leveraging the transformer-based language model for Arabic COVID-19 text identification;Md. Rajib Hossain and Mohammed Moshiul Hoque and Nazmul Siddique and M. Ali Akber Dewan;2024;10.1016/j.engappai.2024.107987;https://doi.org/10.1016/j.engappai.2024.107987;In light of the pandemic, the identification and processing of COVID-19-related text have emerged as critical research areas within the field of Natural Language Processing (NLP). With a growing reliance on online portals and social media for information exchange and interaction, a surge in online textual content, comprising disinformation, misinformation, fake news, and rumors has led to the phenomenon of an infodemic on the World Wide Web. Arabic, spoken by over 420 million people worldwide, stands as a significant low-resource language, lacking efficient tools or applications for the detection of COVID-19-related text. Additionally, the identification of COVID-19 text is an essential prerequisite task for detecting fake and toxic content associated with COVID-19. This gap hampers crucial COVID information retrieval and processing necessary for policymakers and health authorities. Addressing this issue, this paper introduces an intelligent Arabic COVID-19 text identification system named ‘AraCovTexFinder,’ leveraging a fine-tuned fusion-based transformer model. Recognizing the challenges posed by a scarcity of related text corpora, substantial morphological variations in the language, and a deficiency of well-tuned hyperparameters, the proposed system aims to mitigate these hurdles. To support the proposed method, two corpora are developed: an Arabic embedding corpus (AraEC) and an Arabic COVID-19 text identification corpus (AraCoV). The study evaluates the performance of six transformer-based language models (mBERT, XML-RoBERTa, mDeBERTa-V3, mDistilBERT, BERT-Arabic, and AraBERT), 12 deep learning models (combining Word2Vec, GloVe, and FastText embedding with CNN, LSTM, VDCNN, and BiLSTM), and the newly introduced model AraCovTexFinder. Through extensive evaluation, AraCovTexFinder achieves a high accuracy of 98.89 ± 0.001%, outperforming other baseline models, including transformer-based language and deep learning models. This research highlights the importance of specialized tools in low-resource languages to combat the infodemic relating to COVID-19, which can assist policymakers and health authorities in making informed decisions.
NOT_RELEVANT;ScienceDirect;Knowledge graph mining for realty domain using dependency parsing and QAT models;Alexander Zamiralov and Timur Sohin and Nikolay Butakov;2021;10.1016/j.procs.2021.10.004;https://doi.org/10.1016/j.procs.2021.10.004;The real estate business has a lot of risks, and in order to minimize them, you need a lot of information from different sources. Systems based on natural language processing can help customers find this information more easily: question answering, information retrieval, etc. The existing method of question answering requires data aligned with possible questions, which are not easy to obtain, in contrast, the knowledge-graph provides structured information. In this paper, we propose semi-automated ontology generation for the realty domain and a subsequent method for information retrieval related to the knowledge-graph of this ontology. The first contribution is the method for relation extraction method based on dependency-parsing and semantic similarity evaluation, which allows us to form ontology for a particular domain. The second contribution is knowledge-graph completion method based on question answering over text neural network. Our experimental analysis shows the efficiency of the proposed approaches.
NOT_RELEVANT;ScienceDirect;Applications of natural language processing in software traceability: A systematic mapping study;Zaki Pauzi and Andrea Capiluppi;2023;10.1016/j.jss.2023.111616;https://doi.org/10.1016/j.jss.2023.111616;A key part of software evolution and maintenance is the continuous integration from collaborative efforts, often resulting in complex traceability challenges between software artifacts: features and modules remain scattered in the source code, and traceability links become harder to recover. In this paper, we perform a systematic mapping study dealing with recent research recovering these links through information retrieval, with a particular focus on natural language processing (NLP). Our search strategy gathered a total of 96 papers in focus of our study, covering a period from 2013 to 2021. We conducted trend analysis on NLP techniques and tools involved, and traceability efforts (applying NLP) across the software development life cycle (SDLC). Based on our study, we have identified the following key issues, barriers, and setbacks: syntax convention, configuration, translation, explainability, properties representation, tacit knowledge dependency, scalability, and data availability. Based on these, we consolidated the following open challenges: representation similarity across artifacts, the effectiveness of NLP for traceability, and achieving scalable, adaptive, and explainable models. To address these challenges, we recommend a holistic framework for NLP solutions to achieve effective traceability and efforts in achieving interoperability and explainability in NLP models for traceability.
MAYBE_RELEVANT;ScienceDirect;A Review on recent research in information retrieval;S. Ibrihich and A. Oussous and O. Ibrihich and M. Esghir;2022;10.1016/j.procs.2022.03.106;https://doi.org/10.1016/j.procs.2022.03.106;In this paper, we present a survey of modeling and simulation approaches to describe information retrieval basics. We investigate its methods, its challenges, its models, its components and its applications. Our contribution is twofold: on the one hand, reviewing the literature on discovery some search techniques that help to get pertinent results and reach an effective search, and on the other hand, discussing the different research perspectives for study and compare more techniques used in information retrieval. This paper will also shedding the light on some of the famous AI applications in the legal field.
NOT_RELEVANT;ScienceDirect;A New Approach for Calculating Semantic Similarity between Words Using WordNet and Set Theory;Hanane EZZIKOURI and Youness MADANI and Mohammed ERRITALI and Mohamed OUKESSOU;2019;10.1016/j.procs.2019.04.182;https://doi.org/10.1016/j.procs.2019.04.182;Calculating semantic similarity between words is a challenging task of a lot of domains such as Natural language processing (NLP), information retrieval and plagiarism detection. WordNet is a lexical dictionary conceptually organized, where each concept has several characteristics: Synsets and Glosses. Synset represent sets of synonyms of a given word and Glosses are a short description. In this paper, we propose a new approach for calculating semantic similarity between two concepts. The proposed method is based on set theory’s concepts and WordNet properties, by calculating the relatedness between the synsets’ and glosses’s of the two concepts.
NOT_RELEVANT;ScienceDirect;Feature/vector entity retrieval and disambiguation techniques to create a supervised and unsupervised semantic table interpretation approach;Roberto Avogadro and Fabio D’Adda and Marco Cremaschi;2024;10.1016/j.knosys.2024.112447;https://doi.org/10.1016/j.knosys.2024.112447;Recently, there has been an increasing interest in extracting and annotating tables on the Web. This activity allows the transformation of textual data into machine-readable formats to enable the execution of various artificial intelligence tasks, e.g., semantic search and dataset extension. Semantic Table Interpretation (STI) is the process of annotating elements in a table. The paper explores Semantic Table Interpretation, addressing the challenges of Entity Retrieval and Entity Disambiguation in the context of Knowledge Graphs (KGs). It introduces LamAPI, an Information Retrieval system with string/type-based filtering and s-elBat, an Entity Disambiguation technique that combines heuristic and ML-based approaches. By applying the acquired know-how in the field and extracting algorithms, techniques and components from our previous STI approaches and the state of the art, we have created a new platform capable of annotating any tabular data, ensuring a high level of quality.
NOT_RELEVANT;ScienceDirect;KurdSum: A new benchmark dataset for the Kurdish text summarization;Soran Badawi;2023;10.1016/j.nlp.2023.100043;https://doi.org/10.1016/j.nlp.2023.100043;Summarizing a text is the process of condensing its content while still maintaining its essential information. With the abundance of digital information available, summarization has become a significant task in various fields, including information retrieval, NLP (Natural Language Processing), and machine learning. This task has been extensively studied in languages such as English and Chinese, but research on Kurdish language summarization is lacking. Therefore, we present the first-ever Kurdish summarization news dataset, KurdSum, which includes over 40,000 texts. We collected news articles from Kurdish websites, preprocessed the data, and manually created a summary for each article. We further assessed the performance of our benchmark dataset on four extractive systems (LEXRANK, TEXTRANK, ORACLE, and LEAD0-3) and three abstractive methods (Pointer-Generator, Sequence-to-Sequence and transformer-abstractive). Our experiments showed that the Pointer-Generator approach yielded superior ROUGE (Recall-Oriented Understudy for Gisting Evaluation) scores compared to other techniques and ORACLE outperformed other extractive methods. Our findings offer a promising direction for the summarization of Kurdish text and can contribute to developing NLP tools for processing the Kurdish language. Likewise, the dataset can serve as a benchmark dataset for Kurdish language summarization and a valuable resource for researchers interested in developing Kurdish summarization models.
NOT_RELEVANT;ScienceDirect;Leveraging Knowledge Graphs and Deep Learning for automatic art analysis;Giovanna Castellano and Vincenzo Digeno and Giovanni Sansaro and Gennaro Vessio;2022;10.1016/j.knosys.2022.108859;https://doi.org/10.1016/j.knosys.2022.108859;The growing availability of large collections of digitized artworks has disclosed new opportunities to develop intelligent systems for the automatic analysis of fine arts. Among other benefits, these tools can foster a deeper understanding of fine arts, ultimately supporting the spread of culture. However, most of the systems proposed in the literature are only based on visual features of digitized artwork images, which are sometimes only integrated with some metadata and textual comments. A Knowledge Graph (KG) that integrates a rich body of information about artworks, artists, painting schools, etc., in a unified structured framework, can provide a valuable resource for more powerful information retrieval and knowledge discovery tools in the artistic domain. To this end, in this paper we present ArtGraph:11ArtGraph and associated code are publicly available on https://doi.org/10.5281/zenodo.6337958.. an artistic KG based on WikiArt and DBpedia. The graph already provides knowledge discovery capabilities without having to train a learning system. In addition, we propose a novel KG-enabled fine art classification method based on ArtGraph, which is used to perform artwork attribute prediction tasks. The method extracts embeddings from ArtGraph and injects them as “contextual” knowledge into a Deep Learning model. Compared to the state-of-the-art, the proposed model provides encouraging results, suggesting that the exploitation of KGs in combination with Deep Learning can pave the way for bridging the gap between the Humanities and Computer Science communities.
NOT_RELEVANT;ScienceDirect;A Natural Language Processing System using CWS Pipeline for Extraction of Linguistic Features;Sandeep Kumar and Arun Solanki;2023;10.1016/j.procs.2023.01.155;https://doi.org/10.1016/j.procs.2023.01.155;Understanding the rules of grammar and linguistic features is essential to understanding the context of a language, which helps to understand that language. Similarly, for Natural Language processing, the linguistic feature allows understanding of the language. This paper introduced how Coreference, Word-sense, and Semantic knowledge (CWS) of linguistic features work. It would improve the Natural Language Understanding (NLU) and Natural Language Processing (NLP) tasks of any NLP model and NLP applications (either existing or new). This paper proposed a CWS pipeline method to enhance the efficiency and performance of NLP applications like text summarization, information retrieval, question-answer, machine reading comprehension, etc. The proposed CWS pipeline model used a pre-trained CoNLL-2012 coreference dataset extracted from the famous Ontonotes-5.0 dataset for the English language. The model implementation is done in Python language. The performance evaluation is done using the standard CoNLL-2012 coreference dataset for the English language. The coreference marked output is evaluated against the manually tagged gold standard dataset. The proposed CWS pipeline model gives 78.98% of the average F1 score on the MUC metric, 1.78% higher than the previous models' top result. CWS pipeline model performs better than existing models.
NOT_RELEVANT;ScienceDirect;Contextual word disambiguates of Ge'ez language with homophonic using machine learning;Mequanent Degu Belete and Ayodeji Olalekan Salau and Girma Kassa Alitasb and Tigist Bezabh;2024;10.1016/j.amper.2024.100169;https://doi.org/10.1016/j.amper.2024.100169;According to natural language processing experts, there are numerous ambiguous words in languages. Without automated word meaning disambiguation for any language, the development of natural language processing technologies such as information extraction, information retrieval, machine translation, and others are still challenging task. Therfore, this paper presents the development of a word sense disambiguation model for duplicate alphabet words for the Ge'ez language using corpus-based methods. Because there is no wordNet or public dataset for the Ge'ez language, 1010 samples of ambiguous words were gathered. Afterwards, the words were preprocessed and the text was vectorized using bag of words, Term Frequency-Inverse Document Frequency, and word embeddings such as word2vec and fastText. The vectorized texts are then analysed using the supervised machine learning algorithms such Naive Bayes, decision trees, random forests, K-nearest neighbor, linear support vector machine, and logistic regression. Bag of words paired with random forests outperformed all other combinations, with an accuracy of 99.52%. However, when Deep learning algorithms such as Deep neural network and Long Short-Term memory were used for the same dataset, a 100% accuracy was achieved.
NOT_RELEVANT;ScienceDirect;An intelligent use of stemmer and morphology analysis for Arabic information retrieval;Ali Alnaied and Mosa Elbendak and Abdullah Bulbul;2020;10.1016/j.eij.2020.02.004;https://doi.org/10.1016/j.eij.2020.02.004;Arabic Information Retrieval has gained significant attention due to an increasing usage of Arabic text on the web and social media networks. This paper discusses a new approach for Arabic stem, called Arabic Morphology Information Retrieval (AMIR), to generate/extract stems by applying a set of rules regarding the relationship among Arabic letters to find the root/stem of the respective words used as indexing terms for the text search in Arabic retrieval systems. To demonstrate the usefulness of the proposed algorithm, we highlight the benefits of the proposed rules for different Arabic information retrieval systems. Finally, we have evaluated AMIR system by comparing its performance with LUCENE, FARASA, and no-stemmer counterpart system in terms of mean average precisions. The results obtained demonstrate that AMIR has achieved a mean average precision of 0.34% while LUCENE, FARASA and no stemmer giving 0.27%, 0.28% and 0.21, respectively. This demonstrates that AMIR is able to improve Arabic stemmer and increases retrieval as well as being strong against any type of stem.
NOT_RELEVANT;ScienceDirect;Extending latent semantic analysis to manage its syntactic blindness;Raja Muhammad Suleman and Ioannis Korkontzelos;2021;10.1016/j.eswa.2020.114130;https://doi.org/10.1016/j.eswa.2020.114130;Natural Language Processing (NLP) is the sub-field of Artificial Intelligence that represents and analyses human language automatically. NLP has been employed in many applications, such as information retrieval, information processing and automated answer ranking. Semantic analysis focuses on understanding the meaning of text. Among other proposed approaches, Latent Semantic Analysis (LSA) is a widely used corpus-based approach that evaluates similarity of text based on the semantic relations among words. LSA has been applied successfully in diverse language systems for calculating the semantic similarity of texts. LSA ignores the structure of sentences, i.e., it suffers from a syntactic blindness problem. LSA fails to distinguish between sentences that contain semantically similar words but have opposite meanings. Disregarding sentence structure, LSA cannot differentiate between a sentence and a list of keywords. If the list and the sentence contain similar words, comparing them using LSA would lead to a high similarity score. In this paper, we propose xLSA, an extension of LSA that focuses on the syntactic structure of sentences to overcome the syntactic blindness problem of the original LSA approach. xLSA was tested on sentence pairs that contain similar words but have significantly different meaning. Our results showed that xLSA alleviates the syntactic blindness problem, providing more realistic semantic similarity scores.
NOT_RELEVANT;ScienceDirect;Evaluating Transformers and Linguistic Features integration for Author Profiling tasks in Spanish;José Antonio García-Díaz and Ghassan Beydoun and Rafel Valencia-García;2024;10.1016/j.datak.2024.102307;https://doi.org/10.1016/j.datak.2024.102307;Author profiling consists of extracting their demographic and psychographic information by examining their writings. This information can then be used to improve the reader experience and to detect bots or propagators of hoaxes and/or hate speech. Therefore, author profiling can be applied to build more robust and efficient Knowledge-Based Systems for tasks such as content moderation, user profiling, and information retrieval. Author profiling is typically performed automatically as a document classification task. Recently, language models based on transformers have also proven to be quite effective in this task. However, the size and heterogeneity of novel language models, makes it necessary to evaluate them in context. The contributions we make in this paper are four-fold: First, we evaluate which language models are best suited to perform author profiling in Spanish. These experiments include basic, distilled, and multilingual models. Second, we evaluate how feature integration can improve performance for this task. We evaluate two distinct strategies: knowledge integration and ensemble learning. Third, we evaluate the ability of linguistic features to improve the interpretability of the results. Fourth, we evaluate the performance of each language model in terms of memory, training, and inference times. Our results indicate that the use of lightweight models can indeed achieve similar performance to heavy models and that multilingual models are actually less effective than models trained with one language. Finally, we confirm that the best models and strategies for integrating features ultimately depend on the context of the task.
NOT_RELEVANT;ScienceDirect;Integrating social media and deep learning for real-time urban waterlogging monitoring;Muhammad Waseem Boota and Shan-e-hyder Soomro and Muhammad Irshad Ahmad and Sheheryar Khan and Haoming Xia and Yaochen Qin and Chaode Yan and Jikun Xu and Ayesha Yousaf and Muhammad Azeem Boota and Bilal Ahmed;2024;10.1016/j.ejrh.2024.102070;https://doi.org/10.1016/j.ejrh.2024.102070;"Study region
Swat district, Khyber Pakhtunkhwa (KPK) Province, Pakistan.
Study focus
With the rise of social-media data, there is an increasing need to promptly and precisely identify content related to disasters, such as urban waterlogging. Social-media data, being cost-effective and abundant, can offer valuable insights into geographic phenomena by analyzing human behavioral patterns, making it a powerful resource for detailed waterlogging (WLG) analysis in urban settings.
Innovative insights
This research introduces a novel framework for precise information retrieval and real-time extraction of WLG points and fine-grained information in disaster-affected areas using the Facebook platform. First, topic modeling and transfer learning techniques were developed to examine the spatiotemporal dynamics of WLG locations. Second, water depth data from textual content and visual representations were extracted using various deep learning frameworks and integrated through decision-making processes. Third, a unique fine-grained location corpus tailored to urban flooding scenarios was created using the named entity recognition (NER) model. Finally, the BERT-BiLSTM-CRF model was employed to extract WLG points accurately. Using the Swat district as a case study, the extracted WLG points covered at least 79 % of the officially documented WLG points and were primarily located near roadways, especially in low-elevation areas. This framework provides a viable approach for enhancing situational awareness and conducting spatiotemporal analysis of urban floods and WLG disasters at the municipal level in real-time."
NOT_RELEVANT;ScienceDirect;Large language models illuminate a progressive pathway to artificial intelligent healthcare assistant;Mingze Yuan and Peng Bao and Jiajia Yuan and Yunhao Shen and Zifan Chen and Yi Xie and Jie Zhao and Quanzheng Li and Yang Chen and Li Zhang and Lin Shen and Bin Dong;2024;10.1016/j.medp.2024.100030;https://doi.org/10.1016/j.medp.2024.100030;"With the rapid development of artificial intelligence, large language models (LLMs) have shown promising capabilities in mimicking human-level language comprehension and reasoning. This has sparked significant interest in applying LLMs to enhance various aspects of healthcare, ranging from medical education to clinical decision support. However, medicine involves multifaceted data modalities and nuanced reasoning skills, presenting challenges for integrating LLMs. This review introduces the fundamental applications of general-purpose and specialized LLMs, demonstrating their utilities in knowledge retrieval, research support, clinical workflow automation, and diagnostic assistance. Recognizing the inherent multimodality of medicine, the review emphasizes the multimodal LLMs and discusses their ability to process diverse data types like medical imaging and electronic health records to augment diagnostic accuracy. To address LLMs’ limitations regarding personalization and complex clinical reasoning, the review further explores the emerging development of LLM-powered autonomous agents for healthcare. Moreover, it summarizes the evaluation methodologies for assessing LLMs’ reliability and safety in medical contexts. LLMs have transformative potential in medicine; however, there is a pivotal need for continuous optimizations and ethical oversight before these models can be effectively integrated into clinical practice."
NOT_RELEVANT;ScienceDirect;A self-supervised language model selection strategy for biomedical question answering;Negar Arabzadeh and Ebrahim Bagheri;2023;10.1016/j.jbi.2023.104486;https://doi.org/10.1016/j.jbi.2023.104486;Large neural-based Pre-trained Language Models (PLM) have recently gained much attention due to their noteworthy performance in many downstream Information Retrieval (IR) and Natural Language Processing (NLP) tasks. PLMs can be categorized as either general-purpose, which are trained on resources such as large-scale Web corpora, and domain-specific which are trained on in-domain or mixed-domain corpora. While domain-specific PLMs have shown promising performance on domain-specific tasks, they are significantly more computationally expensive compared to general-purpose PLMs as they have to be either retrained or trained from scratch. The objective of our work in this paper is to explore whether it would be possible to leverage general-purpose PLMs to show competitive performance to domain-specific PLMs without the need for expensive retraining of the PLMs for domain-specific tasks. By focusing specifically on the recent BioASQ Biomedical Question Answering task, we show how different general-purpose PLMs show synergistic behaviour in terms of performance, which can lead to overall notable performance improvement when used in tandem with each other. More concretely, given a set of general-purpose PLMs, we propose a self-supervised method for training a classifier that systematically selects the PLM that is most likely to answer the question correctly on a per-input basis. We show that through such a selection strategy, the performance of general-purpose PLMs can become competitive with domain-specific PLMs while remaining computationally light since there is no need to retrain the large language model itself. We run experiments on the BioASQ dataset, which is a large-scale biomedical question-answering benchmark. We show that utilizing our proposed selection strategy can show statistically significant performance improvements on general-purpose language models with an average of 16.7% when using only lighter models such as DistilBERT and DistilRoBERTa, as well as 14.2% improvement when using relatively larger models such as BERT and RoBERTa and so, their performance become competitive with domain-specific large language models such as PubMedBERT.
NOT_RELEVANT;ScienceDirect;A reproducible survey on word embeddings and ontology-based methods for word similarity: Linear combinations outperform the state of the art;Juan J. Lastra-Díaz and Josu Goikoetxea and Mohamed Ali {Hadj Taieb} and Ana García-Serrano and Mohamed {Ben Aouicha} and Eneko Agirre;2019;10.1016/j.engappai.2019.07.010;https://doi.org/10.1016/j.engappai.2019.07.010;Human similarity and relatedness judgements between concepts underlie most of cognitive capabilities, such as categorisation, memory, decision-making and reasoning. For this reason, the proposal of methods for the estimation of the degree of similarity and relatedness between words and concepts has been a very active line of research in the fields of artificial intelligence, information retrieval and natural language processing among others. Main approaches proposed in the literature can be categorised in two large families as follows: (1) Ontology-based semantic similarity Measures (OM) and (2) distributional measures whose most recent and successful methods are based on Word Embedding (WE) models. However, the lack of a deep analysis of both families of methods slows down the advance of this line of research and its applications. This work introduces the largest, reproducible and detailed experimental survey of OM measures and WE models reported in the literature which is based on the evaluation of both families of methods on a same software platform, with the aim of elucidating what is the state of the problem. We show that WE models which combine distributional and ontology-based information get the best results, and in addition, we show for the first time that a simple average of two best performing WE models with other ontology-based measures or WE models is able to improve the state of the art by a large margin. In addition, we provide a very detailed reproducibility protocol together with a collection of software tools and datasets as supplementary material to allow the exact replication of our results.
NOT_RELEVANT;ScienceDirect;Integrating large language models and generative artificial intelligence tools into information literacy instruction;Alexander J. Carroll and Joshua Borycz;2024;10.1016/j.acalib.2024.102899;https://doi.org/10.1016/j.acalib.2024.102899;Generative artificial intelligence (AI) and large language models (LLMs) have induced a mixture of excitement and panic among educators. However, there is a lack of consensus over how much experience science and engineering students have with using these tools for research-related tasks. Likewise, it is not yet known how educators and information professionals can leverage these tools to teach students strategies for information retrieval and knowledge synthesis. This study assesses the extent of students' use of AI tools in research-related tasks and if information literacy instruction could impact their perception of these tools. Responses to Likert-scale questions indicate that many students did not have extensive experience using LLMs for research-related purposes prior to the information literacy sessions. However, after participating in a didactic lecture and discussion with an engineering librarian that explored how to use these tools effectively and responsibly, many students reported viewing these tools as potentially useful for future assignments. Student responses to open-response questions suggest that librarian-led information literacy training can assist students in developing more sophisticated understandings of the limitations and use cases for artificial intelligence in inquiry-based coursework.
NOT_RELEVANT;ScienceDirect;AOPWIKI-EXPLORER: An interactive graph-based query engine leveraging large language models;Saurav Kumar and Deepika Deepika and Karin Slater and Vikas Kumar;2024;10.1016/j.comtox.2024.100308;https://doi.org/10.1016/j.comtox.2024.100308;Adverse Outcome Pathways (AOPs) provide a basis for non-animal testing, by outlining the cascade of molecular and cellular events initiated upon stressor exposure, leading to adverse effects. In recent years, the scientific community has shown interest in developing AOPs through crowdsourcing, with the results archived in the AOP-Wiki: a centralized repository coordinated by the OECD, hosting nearly 512 AOPs (April, 2023). However, the AOP-Wiki platform currently lacks a versatile querying system, which hinders developers' exploration of the AOP network and impedes its practical use in risk assessment. This work proposes to unleash the full potential of the AOP-Wiki archive by adapting its data into a Labelled Property Graph (LPG) schema. Additionally, the tool offers a visual network query interface for both database-specific and natural language queries, facilitating the retrieval and analysis of graph data. The multi-query interface allows non-technical users to construct flexible queries, thereby enhancing the potential for AOP exploration. By reducing the time and technical requirements, the present query engine enhances the practical utilization of the valuable data within AOP-Wiki. To evaluate the platform, a case study is presented with three levels of use-case scenarios (simple, moderate, and complex queries). AOPWIKI-EXPLORER is freely available on GitHub (https://github.com/Crispae/AOPWiki_Explorer) for wider community reach and further enhancement.
NOT_RELEVANT;ScienceDirect;A literature review on question answering techniques, paradigms and systems;Marco Antonio {Calijorne Soares} and Fernando Silva Parreiras;2020;10.1016/j.jksuci.2018.08.005;https://doi.org/10.1016/j.jksuci.2018.08.005;"Background
Question Answering (QA) systems enable users to retrieve exact answers for questions posed in natural language.
Objective
This study aims at identifying QA techniques, tools and systems, as well as the metrics and indicators used to measure these approaches for QA systems and also to determine how the relationship between Question Answering and natural language processing is built.
Method
The method adopted was a Systematic Literature Review of studies published from 2000 to 2017.
Results
130 out of 1842 papers have been identified as describing a QA approach developed and evaluated with different techniques.
Conclusion
Question Answering researchers have concentrated their efforts in natural language processing, knowledge base and information retrieval paradigms. Most of the researches focused on open domain. Regarding the metrics used to evaluate the approaches, Precision and Recall are the most addressed."
NOT_RELEVANT;ScienceDirect;Arabic Named Entity Recognition in Arabic Tweets Using BERT-based Models;Brahim Ait Benali and Soukaina Mihi and Nabil Laachfoubi and Addi Ait Mlouk;2022;10.1016/j.procs.2022.07.109;https://doi.org/10.1016/j.procs.2022.07.109;With the large amount of unstructured data being broadcasted every day, building powerful methods enabling information retrieval and extraction becomes necessary. Unfortunately, named entity recognition is a difficult classification task to classify data into predefined labels, which is further challenged by the Arabic language's particular characteristics and complex nature. This work trains six BERT-based models (Bidirectional Encoder Representations from Transformers) and uses a BiLSTM-CRF architecture for the NER task on dialectal Arabic. Our fine-tuning approach yields new state-of-the-art results on publicly available dialectal Arabic social media datasets.
NOT_RELEVANT;ScienceDirect;Information extraction from medical case reports using OpenAI InstructGPT;Veronica Sciannameo and Daniele Jahier Pagliari and Sara Urru and Piercesare Grimaldi and Honoria Ocagli and Sara Ahsani-Nasab and Rosanna Irene Comoretto and Dario Gregori and Paola Berchialla;2024;10.1016/j.cmpb.2024.108326;https://doi.org/10.1016/j.cmpb.2024.108326;"Background and objective
Researchers commonly use automated solutions such as Natural Language Processing (NLP) systems to extract clinical information from large volumes of unstructured data. However, clinical text's poor semantic structure and domain-specific vocabulary can make it challenging to develop a one-size-fits-all solution. Large Language Models (LLMs), such as OpenAI's Generative Pre-Trained Transformer 3 (GPT-3), offer a promising solution for capturing and standardizing unstructured clinical information. This study evaluated the performance of InstructGPT, a family of models derived from LLM GPT-3, to extract relevant patient information from medical case reports and discussed the advantages and disadvantages of LLMs versus dedicated NLP methods.
Methods
In this paper, 208 articles related to case reports of foreign body injuries in children were identified by searching PubMed, Scopus, and Web of Science. A reviewer manually extracted information on sex, age, the object that caused the injury, and the injured body part for each patient to build a gold standard to compare the performance of InstructGPT.
Results
InstructGPT achieved high accuracy in classifying the sex, age, object and body part involved in the injury, with 94%, 82%, 94% and 89%, respectively. When excluding articles for which InstructGPT could not retrieve any information, the accuracy for determining the child's sex and age improved to 97%, and the accuracy for identifying the injured body part improved to 93%. InstructGPT was also able to extract information from non-English language articles.
Conclusions
The study highlights that LLMs have the potential to eliminate the necessity for task-specific training (zero-shot extraction), allowing the retrieval of clinical information from unstructured natural language text, particularly from published scientific literature like case reports, by directly utilizing the PDF file of the article without any pre-processing and without requiring any technical expertise in NLP or Machine Learning. The diverse nature of the corpus, which includes articles written in languages other than English, some of which contain a wide range of clinical details while others lack information, adds to the strength of the study."
NOT_RELEVANT;ScienceDirect;Comparing Fine-Tuning, Zero and Few-Shot Strategies with Large Language Models in Hate Speech Detection in English;Ronghao Pan and José {Antonio García-Díaz} and Rafael Valencia-García;2024;10.32604/cmes.2024.049631;https://doi.org/10.32604/cmes.2024.049631;Large Language Models (LLMs) are increasingly demonstrating their ability to understand natural language and solve complex tasks, especially through text generation. One of the relevant capabilities is contextual learning, which involves the ability to receive instructions in natural language or task demonstrations to generate expected outputs for test instances without the need for additional training or gradient updates. In recent years, the popularity of social networking has provided a medium through which some users can engage in offensive and harmful online behavior. In this study, we investigate the ability of different LLMs, ranging from zero-shot and few-shot learning to fine-tuning. Our experiments show that LLMs can identify sexist and hateful online texts using zero-shot and few-shot approaches through information retrieval. Furthermore, it is found that the encoder-decoder model called Zephyr achieves the best results with the fine-tuning approach, scoring 86.811% on the Explainable Detection of Online Sexism (EDOS) test-set and 57.453% on the Multilingual Detection of Hate Speech Against Immigrants and Women in Twitter (HatEval) test-set. Finally, it is confirmed that the evaluated models perform well in hate text detection, as they beat the best result in the HatEval task leaderboard. The error analysis shows that contextual learning had difficulty distinguishing between types of hate speech and figurative language. However, the fine-tuned approach tends to produce many false positives.
MAYBE_RELEVANT;ScienceDirect;A comprehensive survey on answer generation methods using NLP;Prashant Upadhyay and Rishabh Agarwal and Sumeet Dhiman and Abhinav Sarkar and Saumya Chaturvedi;2024;10.1016/j.nlp.2024.100088;https://doi.org/10.1016/j.nlp.2024.100088;Recent advancements in question-answering systems have significantly enhanced the capability of computers to understand and respond to queries in natural language. This paper presents a comprehensive review of the evolution of question answering systems, with a focus on the developments over the last few years. We examine the foundational aspects of a question answering framework, including question analysis, answer extraction, and passage retrieval. Additionally, we delve into the challenges that question answering systems encounter, such as the intricacies of question processing, the necessity of contextual data sources, and the complexities involved in real-time question answering. Our study categorizes existing question answering systems based on the types of questions they address, the nature of the answers they produce, and the various approaches employed to generate these answers. We also explore the distinctions between opinion-based, extraction-based, retrieval-based, and generative answer generation. The classification provides insight into the strengths and limitations of each method, paving the way for future innovations in the field. This review aims to offer a clear understanding of the current state of question answering systems and to identify the scaling needed to meet the rising expectations and demands of users for coherent and accurate automated responses in natural language.
NOT_RELEVANT;ScienceDirect;Learning to match patients to clinical trials using large language models;Maciej Rybinski and Wojciech Kusa and Sarvnaz Karimi and Allan Hanbury;2024;10.1016/j.jbi.2024.104734;https://doi.org/10.1016/j.jbi.2024.104734;"Objective:
This study investigates the use of Large Language Models (LLMs) for matching patients to clinical trials (CTs) within an information retrieval pipeline. Our objective is to enhance the process of patient-trial matching by leveraging the semantic processing capabilities of LLMs, thereby improving the effectiveness of patient recruitment for clinical trials.
Methods:
We employed a multi-stage retrieval pipeline integrating various methodologies, including BM25 and Transformer-based rankers, along with LLM-based methods. Our primary datasets were the TREC Clinical Trials 2021–23 track collections. We compared LLM-based approaches, focusing on methods that leverage LLMs in query formulation, filtering, relevance ranking, and re-ranking of CTs.
Results:
Our results indicate that LLM-based systems, particularly those involving re-ranking with a fine-tuned LLM, outperform traditional methods in terms of nDCG and Precision measures. The study demonstrates that fine-tuning LLMs enhances their ability to find eligible trials. Moreover, our LLM-based approach is competitive with state-of-the-art systems in the TREC challenges. The study shows the effectiveness of LLMs in CT matching, highlighting their potential in handling complex semantic analysis and improving patient-trial matching. However, the use of LLMs increases the computational cost and reduces efficiency. We provide a detailed analysis of effectiveness-efficiency trade-offs.
Conclusion:
This research demonstrates the promising role of LLMs in enhancing the patient-to-clinical trial matching process, offering a significant advancement in the automation of patient recruitment. Future work should explore optimising the balance between computational cost and retrieval effectiveness in practical applications."
MAYBE_RELEVANT;ScienceDirect;Research on Knowledge Driven Intelligent Question Answering System for Electric Power Customer Service;Yuanpeng Tan and Huifang Xu and Yaguang Wu and Zhonghao Zhang and Yeteng An and Yongping Xiong and Fang Wang;2021;10.1016/j.procs.2021.04.072;https://doi.org/10.1016/j.procs.2021.04.072;The electric power customer service domain-specific knowledge graph aims to describe the concepts, entities, events and their relationships in the electric power customer service business field, with a structured manner, and provide a more effective data organization, management, and cognitive ability for the customer service business. This paper proposes a method for constructing a knowledge graph in the field of electric customer service, and constructs a knowledge graph with over 15,000 entities and 20,000 relationships. Based on the knowledge graph, an intelligent question answering application architecture is designed, which consists of multiple functional modules such as dialogue process configuration, natural language processing, and business process processing. It provides more efficient and open knowledge retrieval services for the electric customer service business, and improves the intelligence level of customer service question answering.
NOT_RELEVANT;ScienceDirect;Word-length algorithm for language identification of under-resourced languages;Ali Selamat and Nicholas Akosu;2016;10.1016/j.jksuci.2014.12.004;https://doi.org/10.1016/j.jksuci.2014.12.004;Language identification is widely used in machine learning, text mining, information retrieval, and speech processing. Available techniques for solving the problem of language identification do require large amount of training text that are not available for under-resourced languages which form the bulk of the World’s languages. The primary objective of this study is to propose a lexicon based algorithm which is able to perform language identification using minimal training data. Because language identification is often the first step in many natural language processing tasks, it is necessary to explore techniques that will perform language identification in the shortest possible time. Hence, the second objective of this research is to study the effect of the proposed algorithm on the run-time performance of language identification. Precision, recall, and F1 measures were used to determine the effectiveness of the proposed word length algorithm using datasets drawn from the Universal Declaration of Human Rights Act in 15 languages. The experimental results show good accuracy on language identification at the document level and at the sentence level based on the available dataset. The improved algorithm also showed significant improvement in run time performance compared with the spelling checker approach.
NOT_RELEVANT;ScienceDirect;Deep Learning for Internet of Things Data Analytics;Tausifa Jan Saleem and Mohammad Ahsan Chishti;2019;10.1016/j.procs.2019.12.120;https://doi.org/10.1016/j.procs.2019.12.120;The recent technological innovations and brisk amalgamation of domains such as sensing and actuating technologies, embedded systems, wireless communication, and data analytics are accelerating the growth of Internet of Things (IoT). The massive number of sensors deployed in IoT generate humongous volumes of data for a broad range of applications such as smart home, smart healthcare, smart manufacturing, smart transportation, smart grid, smart agriculture etc. Analyzing such data in order to facilitate enhanced decision making, increase productivity and accuracy, ameliorate revenue is a critical process that makes IoT a precious idea for businesses and a standard of life improving paradigm. Although deriving concealed information and inferences out of IoT data is promising to improve the standard of our lives, it is a complicated task that cannot be accomplished by conventional paradigms. Deep Learning would play a vital role in creating smarter IoT as it has shown remarkable results in different fields including image recognition, information retrieval, speech recognition, natural language processing, indoor localization, physiological and psychological state detection etc. and these form the foundation services for IoT applications. In this regard, investigating the potential of Deep Learning for IoT data analytics becomes indispensable. Motivated to address this concern, this paper explores the flair of Deep Learning for analyzing data generated from IoT environments. A detailed discussion on various Deep Learning architectures, their role in IoT data analytics and potential use cases is also presented. Finally, open research challenges and future research directions are discussed in order to promote future research in this domain.
NOT_RELEVANT;ScienceDirect;MoreThanSentiments: A text analysis package;Jinhang Jiang and Karthik Srinivasan;2023;10.1016/j.simpa.2022.100456;https://doi.org/10.1016/j.simpa.2022.100456;Text mining on a large corpus of data has gained utility and popularity over recent years owing to advancements in information retrieval and machine learning methods. However, popular text mining software packages mainly focus on either sentiment analysis or semantic meaning extraction, requiring pretraining on a large corpus of text data. In comparison, MoreThanSentiments provides computation of newer text attribution measures, including boiler score, specificity, redundancy, and hard info, which have been proposed in accounting analytics literature. Our software package, available in Python, is flexible in terms of parameter setting and is adaptable to different applications. Through this package, we seek to simplify the process of deploying nontrivial information extraction techniques published in domain-specific text analysis research into domain-agnostic analytics applications.
MAYBE_RELEVANT;ScienceDirect;The power and potentials of Flexible Query Answering Systems: A critical and comprehensive analysis;Troels Andreasen and Gloria Bordogna and Guy De Tré and Janusz Kacprzyk and Henrik Legind Larsen and Sławomir Zadrożny;2024;10.1016/j.datak.2023.102246;https://doi.org/10.1016/j.datak.2023.102246;The popularity of chatbots, such as ChatGPT, has brought research attention to question answering systems, capable to generate natural language answers to user’s natural language queries. However, also in other kinds of systems, flexibility of querying, including but also going beyond the use of natural language, is an important feature. With this consideration in mind the paper presents a critical and comprehensive analysis of recent developments, trends and challenges of Flexible Query Answering Systems (FQASs). Flexible query answering is a multidisciplinary research field that is not limited to question answering in natural language, but comprises other query forms and interaction modalities, which aim to provide powerful means and techniques for better reflecting human preferences and intentions to retrieve relevant information. It adopts methods at the crossroad of several disciplines among which Information Retrieval (IR), databases, knowledge based systems, knowledge and data engineering, Natural Language Processing (NLP) and the semantic web may be mentioned. The analysis principles are inspired by the Preferred Reporting Items for Systematic Reviews and Meta-Analysis (PRISMA) framework, characterized by a top-down process, starting with relevant keywords for the topic of interest to retrieve relevant articles from meta-sources And complementing these articles with other relevant articles from seed sources Identified by a bottom-up process. to mine the retrieved publication data a network analysis is performed Which allows to present in a synthetic way intrinsic topics of the selected publications. issues dealt with are related to query answering methods Both model-based and data-driven (the latter based on either machine learning or deep learning) And to their needs for explainability and fairness to deal with big data Notably by taking into account data veracity. conclusions point out trends and challenges to help better shaping the future of the FQAS field.
NOT_RELEVANT;ScienceDirect;Fumeus: A family of Python tools for text mining with smoke terms;David M. Goldberg and Richard J. Gruss and Alan S. Abrahams;2022;10.1016/j.simpa.2022.100270;https://doi.org/10.1016/j.simpa.2022.100270;Synthesizing meaningful insights from voluminous textual datasets is complex and challenging. The task is especially difficult for sparse target classes. Recent works have proposed “smoke terms,” or machine-learned words and phrases prevalent in a target class. Smoke terms may be utilized to rank or sort text, or they may serve as features in follow-on machine learning models. This paper introduces Fumeus, a family of Python-based smoke term analysis tools. We provide functionality to generate new smoke terms and to use existing smoke term dictionaries to rank or sort datasets. These analyses have numerous academic, regulatory, and industry applications.
NOT_RELEVANT;ScienceDirect;Psychiatric document retrieval using a discourse-aware model;Liang-Chih Yu and Chung-Hsien Wu and Fong-Lin Jang;2009;10.1016/j.artint.2008.12.004;https://doi.org/10.1016/j.artint.2008.12.004;With the increased incidence of depression-related disorders, many psychiatric websites have been developed to provide huge amounts of educational documents along with rich self-help information. Psychiatric document retrieval aims to assist individuals to locate documents relevant to their depressive problems efficiently and effectively. By referring to relevant documents, individuals can understand how to alleviate their depression-related symptoms according to recommendations from health professionals. This work proposes the use of high-level discourse information extracted from queries and documents to improve the precision of retrieval results. The discourse information adopted herein includes negative life events, depressive symptoms and semantic relations between symptoms, which are beneficial for better understanding of users' queries. Experimental results show that the discourse-aware retrieval model achieves higher precision than the word-based retrieval models, namely the vector space model (VSM) and Okapi model, adopting word-level information alone.
NOT_RELEVANT;ScienceDirect;Word Sense Disambiguation: A comprehensive knowledge exploitation framework;Yinglin Wang and Ming Wang and Hamido Fujita;2020;10.1016/j.knosys.2019.105030;https://doi.org/10.1016/j.knosys.2019.105030;Word Sense Disambiguation (WSD) has been a basic and on-going issue since its introduction in natural language processing (NLP) community. Its application lies in many different areas including sentiment analysis, Information Retrieval (IR), machine translation and knowledge graph construction. Solutions to WSD are mostly categorized into supervised and knowledge-based approaches. In this paper, a knowledge-based method is proposed, modeling the problem with semantic space and semantic path hidden behind a given sentence. The approach relies on the well-known Knowledge Base (KB) named WordNet and models the semantic space and semantic path by Latent Semantic Analysis (LSA) and PageRank respectively. Experiments has proven the method’s effectiveness, achieving state-of-the-art performance in several WSD datasets.
NOT_RELEVANT;ScienceDirect;Graph-based Methods for Significant Concept Selection;Gasmi Karim and Torjmen-Khemakhem Mouna and Tamine Lynda and Ben Jemaa Maher;2015;10.1016/j.procs.2015.08.170;https://doi.org/10.1016/j.procs.2015.08.170;It is well known in information retrieval area that one important issue is the gap between the query and document vocabularies. Concept-based representation of both the document and the query is one of the most effective approaches that lowers the effect of text mismatch and allows the selection of relevant documents that deal with the shared semantics hidden behind both. However, identifying the best representative concepts from texts is still challenging. In this paper, we propose a graph-based method to select the most significant concepts to be integrated into a conceptual indexing system. More specifically, we build the graph whose nodes represented concepts and weighted edges represent semantic distances. The importance of concepts are computed using centrality algorithms that levrage between structural and contextual importance. We experimentally evaluated our method of concept selection using the standard ImageClef2009 medical data set. Results showed that our approach significantly improves the retrieval effectiveness in comparison to state-of-the-art retrieval models.
NOT_RELEVANT;ScienceDirect;BERT based clinical knowledge extraction for biomedical knowledge graph construction and analysis;Ayoub Harnoune and Maryem Rhanoui and Mounia Mikram and Siham Yousfi and Zineb Elkaimbillah and Bouchra {El Asri};2021;10.1016/j.cmpbup.2021.100042;https://doi.org/10.1016/j.cmpbup.2021.100042;Background: Knowledge is evolving over time, often as a result of new discoveries or changes in the adopted methods of reasoning. Also, new facts or evidence may become available, leading to new understandings of complex phenomena. This is particularly true in the biomedical field, where scientists and physicians are constantly striving to find new methods of diagnosis, treatment and eventually cure. Knowledge Graphs (KGs) offer a real way of organizing and retrieving the massive and growing amount of biomedical knowledge. Objective: We propose an end-to-end approach for knowledge extraction and analysis from biomedical clinical notes using the Bidirectional Encoder Representations from Transformers (BERT) model and Conditional Random Field (CRF) layer. Methods: The approach is based on knowledge graphs, which can effectively process abstract biomedical concepts such as relationships and interactions between medical entities. Besides offering an intuitive way to visualize these concepts, KGs can solve more complex knowledge retrieval problems by simplifying them into simpler representations or by transforming the problems into representations from different perspectives. We created a biomedical Knowledge Graph using using Natural Language Processing models for named entity recognition and relation extraction. The generated biomedical knowledge graphs (KGs) are then used for question answering. Results: The proposed framework can successfully extract relevant structured information with high accuracy (90.7% for Named-entity recognition (NER), 88% for relation extraction (RE)), according to experimental findings based on real-world 505 patient biomedical unstructured clinical notes. Conclusions:In this paper, we propose a novel end-to-end system for the construction of a biomedical knowledge graph from clinical textual using a variation of BERT models.
NOT_RELEVANT;ScienceDirect;Building lexicon-based sentiment analysis model for low-resource languages;Idi Mohammed and Rajesh Prasad;2023;10.1016/j.mex.2023.102460;https://doi.org/10.1016/j.mex.2023.102460;"Natural Language Processing (NLP) has transformed machine translation, sentiment analysis, information retrieval, and conversation systems. NLP applications rely on complete linguistic resources, which might be difficult for low-resource languages. NLP solutions for every language require a language-specific dataset. Dataset in a language is essential for NLP solution creation. Over 7000 languages are spoken worldwide. Only around 20 languages have text corpora for NLP applications. English has the most datasets, then Chinese and Spanish. Japanese has several Western European language datasets. For an accurate NLP system, most Asian and African languages lack training datasets. To address this challenge, we propose a methodology for building a lexicon-based sentiment analysis model for languages with limited resources. The Hausa language was used as training and evaluation language. The methodology combines lexicon creation; augmentation, annotation, and fine-tuning model, and has been tested on a corpus of Hausa tweets achieving an accuracy of 98 %. The results suggest that our proposed model is a promising tool for sentiment analysis in a variety of applications, such as social media monitoring, customer service, and market research. Our methodology can be used for any low-resource language. The outline of the work done in this paper can be shown as follows:•We propose a methodology for building a lexicon-based sentiment analysis model for languages with limited resources, using the Hausa language as a case study.•The methodology combines lexicon creation, augmentation, annotation, and fine-tuning model, and achieves an accuracy of 98 % on a corpus of Hausa tweets.•The results suggest that the proposed model is a promising tool for sentiment analysis in a variety of applications for low-resource languages."
NOT_RELEVANT;ScienceDirect;A novel root based Arabic stemmer;Mohammed N. Al-Kabi and Saif A. Kazakzeh and Belal M. {Abu Ata} and Saif A. Al-Rababah and Izzat M. Alsmadi;2015;10.1016/j.jksuci.2014.04.001;https://doi.org/10.1016/j.jksuci.2014.04.001;Stemming algorithms are used in information retrieval systems, indexers, text mining, text classifiers etc., to extract stems or roots of different words, so that words derived from the same stem or root are grouped together. Many stemming algorithms were built in different natural languages. Khoja stemmer is one of the known and widely used Arabic stemmers. In this paper, we introduced a new light and heavy Arabic stemmer. This new stemmer is presented in this study and compared with two well-known Arabic stemmers. Results showed that accuracy of our stemmer is slightly better than the accuracy yielded by each one of those two well-known Arabic stemmers used for evaluation and comparison. Evaluation tests on our novel stemmer yield 75.03% accuracy, while the other two Arabic stemmers yield slightly lower accuracy.
NOT_RELEVANT;ScienceDirect;Clinical trial search: Using biomedical language understanding models for re-ranking;Maciej Rybinski and Jerry Xu and Sarvnaz Karimi;2020;10.1016/j.jbi.2020.103530;https://doi.org/10.1016/j.jbi.2020.103530;Bidirectional Encoder Representations from Transformers (BERT) have achieved state-of-the-art effectiveness in some of the biomedical information processing applications. We investigate the effectiveness of these techniques for clinical trial search systems. In precision medicine, matching patients to relevant experimental evidence or prospective treatments is a complex task which requires both clinical and biological knowledge. To assist in this complex decision making, we investigate the effectiveness of different ranking models based on the BERT models under the same retrieval platform to ensure fair comparisons. An evaluation on the TREC Precision Medicine benchmarks indicates that our approach using the BERT model pre-trained on scientific abstracts and clinical notes achieves state-of-the-art results, on par with highly specialised, manually optimised heuristic models. We also report the best results to date on the TREC Precision Medicine 2017 ad hoc retrieval task for clinical trial search.
NOT_RELEVANT;ScienceDirect;Semantic Quality Assurance of Heterogeneous Unstructured Repair Reports;Michael Abramovici and Philip Gebus and Jens Christian Göbel and Philipp Savarino;2018;10.1016/j.procir.2018.03.334;https://doi.org/10.1016/j.procir.2018.03.334;Service technicians spend a considerable amount of their working hours in order to search for information regarding a current work order. In case of an IPS² malfunction for example, they can either search for potential failure causes within previously documented repair reports that describe a similar malfunction or manually inspect the IPS². On the one hand the former IT-related information procurement is caused by a large amount of irrelevant search results rendered by current state of the art information retrieval approaches implemented in maintenance-related IT systems. On the other hand many repair reports suffer from missing or ambiguous information and a holistically low data quality, which makes it difficult for the service technicians to derive task-related information from a particular repair report, although this report basically describes the same malfunction. These current difficulties will be amplified once service partners get access to maintenance-related information documented by other service partners (companies) within the IPS² network as the amount of available data will increase drastically and new problems will raise concerning the heterogeneity of the data. The paper on hand presents a semantic quality assurance concept for heterogeneous unstructured repair reports that addresses the low data quality problem by utilizing natural language processing and machine learning methods to automatically analyze the service technician’s inputs during the repair report creation process and by notifying him of potential losses in data quality. The concept’s feasibility has been shown by performing a case study with a prototype that utilizes the developed methods.
NOT_RELEVANT;ScienceDirect;A comparison of word embeddings for the biomedical natural language processing;Yanshan Wang and Sijia Liu and Naveed Afzal and Majid Rastegar-Mojarad and Liwei Wang and Feichen Shen and Paul Kingsbury and Hongfang Liu;2018;10.1016/j.jbi.2018.09.008;https://doi.org/10.1016/j.jbi.2018.09.008;"Background
Word embeddings have been prevalently used in biomedical Natural Language Processing (NLP) applications due to the ability of the vector representations being able to capture useful semantic properties and linguistic relationships between words. Different textual resources (e.g., Wikipedia and biomedical literature corpus) have been utilized in biomedical NLP to train word embeddings and these word embeddings have been commonly leveraged as feature input to downstream machine learning models. However, there has been little work on evaluating the word embeddings trained from different textual resources.
Methods
In this study, we empirically evaluated word embeddings trained from four different corpora, namely clinical notes, biomedical publications, Wikipedia, and news. For the former two resources, we trained word embeddings using unstructured electronic health record (EHR) data available at Mayo Clinic and articles (MedLit) from PubMed Central, respectively. For the latter two resources, we used publicly available pre-trained word embeddings, GloVe and Google News. The evaluation was done qualitatively and quantitatively. For the qualitative evaluation, we randomly selected medical terms from three categories (i.e., disorder, symptom, and drug), and manually inspected the five most similar words computed by embeddings for each term. We also analyzed the word embeddings through a 2-dimensional visualization plot of 377 medical terms. For the quantitative evaluation, we conducted both intrinsic and extrinsic evaluation. For the intrinsic evaluation, we evaluated the word embeddings’ ability to capture medical semantics by measruing the semantic similarity between medical terms using four published datasets: Pedersen’s dataset, Hliaoutakis’s dataset, MayoSRS, and UMNSRS. For the extrinsic evaluation, we applied word embeddings to multiple downstream biomedical NLP applications, including clinical information extraction (IE), biomedical information retrieval (IR), and relation extraction (RE), with data from shared tasks.
Results
The qualitative evaluation shows that the word embeddings trained from EHR and MedLit can find more similar medical terms than those trained from GloVe and Google News. The intrinsic quantitative evaluation verifies that the semantic similarity captured by the word embeddings trained from EHR is closer to human experts’ judgments on all four tested datasets. The extrinsic quantitative evaluation shows that the word embeddings trained on EHR achieved the best F1 score of 0.900 for the clinical IE task; no word embeddings improved the performance for the biomedical IR task; and the word embeddings trained on Google News had the best overall F1 score of 0.790 for the RE task.
Conclusion
Based on the evaluation results, we can draw the following conclusions. First, the word embeddings trained from EHR and MedLit can capture the semantics of medical terms better, and find semantically relevant medical terms closer to human experts’ judgments than those trained from GloVe and Google News. Second, there does not exist a consistent global ranking of word embeddings for all downstream biomedical NLP applications. However, adding word embeddings as extra features will improve results on most downstream tasks. Finally, the word embeddings trained from the biomedical domain corpora do not necessarily have better performance than those trained from the general domain corpora for any downstream biomedical NLP task."
NOT_RELEVANT;ScienceDirect;Search like an expert: Reducing expertise disparity using a hybrid neural index for COVID-19 queries;Vincent Nguyen and Maciej Rybinski and Sarvnaz Karimi and Zhenchang Xing;2022;10.1016/j.jbi.2022.104005;https://doi.org/10.1016/j.jbi.2022.104005;"Consumers from non-medical backgrounds often look for information regarding a specific medical information need; however, they are limited by their lack of medical knowledge and may not be able to find reputable resources. As a case study, we investigate reducing this knowledge barrier to allow consumers to achieve search effectiveness comparable to that of an expert, or a medical professional, for COVID-19 related questions. We introduce and evaluate a hybrid index model that allows a consumer to formulate queries using consumer language to find relevant answers to COVID-19 questions. Our aim is to reduce performance degradation between medical professional queries and those of a consumer. We use a universal sentence embedding model to project consumer queries into the same semantic space as professional queries. We then incorporate sentence embeddings into a search framework alongside an inverted index. Documents from this index are retrieved using a novel scoring function that considers sentence embeddings and BM25 scoring. We find that our framework alleviates the expertise disparity, which we validate using an additional set of crowdsourced—consumer—queries even in an unsupervised setting. We also propose an extension of our method, where the sentence encoder is optimised in a supervised setup. Our framework allows for a consumer to search using consumer queries to match the search performance with that of a professional."
NOT_RELEVANT;ScienceDirect;Effective matching of patients to clinical trials using entity extraction and neural re-ranking;Wojciech Kusa and Óscar E. Mendoza and Petr Knoth and Gabriella Pasi and Allan Hanbury;2023;10.1016/j.jbi.2023.104444;https://doi.org/10.1016/j.jbi.2023.104444;"Introduction:
Clinical trials (CTs) often fail due to inadequate patient recruitment. Finding eligible patients involves comparing the patient’s information with the CT eligibility criteria. Automated patient matching offers the promise of improving the process, yet the main difficulties of CT retrieval lie in the semantic complexity of matching unstructured patient descriptions with semi-structured, multi-field CT documents and in capturing the meaning of negation coming from the eligibility criteria.
Objectives:
This paper tackles the challenges of CT retrieval by presenting an approach that addresses the patient-to-trials paradigm. Our approach involves two key components in a pipeline-based model: (i) a data enrichment technique for enhancing both queries and documents during the first retrieval stage, and (ii) a novel re-ranking schema that uses a Transformer network in a setup adapted to this task by leveraging the structure of the CT documents.
Methods:
We use named entity recognition and negation detection in both patient description and the eligibility section of CTs. We further classify patient descriptions and CT eligibility criteria into current, past, and family medical conditions. This extracted information is used to boost the importance of disease and drug mentions in both query and index for lexical retrieval. Furthermore, we propose a two-step training schema for the Transformer network used to re-rank the results from the lexical retrieval. The first step focuses on matching patient information with the descriptive sections of trials, while the second step aims to determine eligibility by matching patient information with the criteria section.
Results
Our findings indicate that the inclusion criteria section of the CT has a great influence on the relevance score in lexical models, and that the enrichment techniques for queries and documents improve the retrieval of relevant trials. The re-ranking strategy, based on our training schema, consistently enhances CT retrieval and shows improved performance by 15% in terms of precision at retrieving eligible trials.
Conclusion
The results of our experiments suggest the benefit of making use of extracted entities. Moreover, our proposed re-ranking schema shows promising effectiveness compared to larger neural models, even with limited training data. These findings offer valuable insights for improving methods for retrieval of clinical documents."
NOT_RELEVANT;ScienceDirect;Improving search over Electronic Health Records using UMLS-based query expansion through random walks;David Martinez and Arantxa Otegi and Aitor Soroa and Eneko Agirre;2014;10.1016/j.jbi.2014.04.013;https://doi.org/10.1016/j.jbi.2014.04.013;"Objective
Most of the information in Electronic Health Records (EHRs) is represented in free textual form. Practitioners searching EHRs need to phrase their queries carefully, as the record might use synonyms or other related words. In this paper we show that an automatic query expansion method based on the Unified Medicine Language System (UMLS) Metathesaurus improves the results of a robust baseline when searching EHRs.
Materials and methods
The method uses a graph representation of the lexical units, concepts and relations in the UMLS Metathesaurus. It is based on random walks over the graph, which start on the query terms. Random walks are a well-studied discipline in both Web and Knowledge Base datasets.
Results
Our experiments over the TREC Medical Record track show improvements in both the 2011 and 2012 datasets over a strong baseline.
Discussion
Our analysis shows that the success of our method is due to the automatic expansion of the query with extra terms, even when they are not directly related in the UMLS Metathesaurus. The terms added in the expansion go beyond simple synonyms, and also add other kinds of topically related terms.
Conclusions
Expansion of queries using related terms in the UMLS Metathesaurus beyond synonymy is an effective way to overcome the gap between query and document vocabularies when searching for patient cohorts."
NOT_RELEVANT;ScienceDirect;Cimind: A phonetic-based tool for multilingual named entity recognition in biomedical texts;Chloé Cabot and Stéfan Darmoni and Lina F. Soualmia;2019;10.1016/j.jbi.2019.103176;https://doi.org/10.1016/j.jbi.2019.103176;"Background
Extracting concepts from biomedical texts is a key to support many advanced applications such as biomedical information retrieval. However, in clinical notes Named Entity Recognition (NER) has to deal with various types of errors such as spelling errors, grammatical errors, truncated sentences, and non-standard abbreviations. Moreover, in numerous countries, NER is challenged by the availability of many resources originally developed and only suitable for English texts. This paper presents the Cimind system, a multilingual system dedicated to named entity recognition in medical texts based on a phonetic similarity measure.
Methods
Cimind performs entity recognition by combining phonetic recognition using the DM phonetic algorithm to deal with spelling errors and string similarity measures. Three main steps are processed to identify terms in a controlled vocabulary: normalization, candidate selection by phonetic similarity and candidate ranking.
Results
Cimind was evaluated in the 2016 and 2017 editions of the CLEF eHealth challenge in the CépiDC/CDC tasks. In 2017, it obtained on each corpus the following results: English dataset: 83.9% P, 78.3% R, 81.0% F1; French raw dataset: 85.7% P, 68.9% R, 76.4% F1; French aligned dataset: 83.5% P, 77.5% R, 80.4% F1. It ranked first in French and fourth in English in officials runs."
NOT_RELEVANT;ScienceDirect;Retrieval methodology for similar NPP LCO cases based on domain specific NLP;No Kyu Seong and Jae Hee Lee and Jong Beom Lee and Poong Hyun Seong;2023;10.1016/j.net.2022.09.028;https://doi.org/10.1016/j.net.2022.09.028;"Nuclear power plants (NPPs) have technical specifications (Tech Specs) to ensure that the equipment and key operating parameters necessary for the safe operation of the power plant are maintained within limiting conditions for operation (LCO) determined by a safety analysis. The LCO of Tech Specs that identify the lowest functional capability of equipment required for safe operation for a facility must be complied for the safe operation of NPP. There have been previous studies to aid in compliance with LCO relevant to rule-based expert systems; however, there is an obvious limit to expert systems for implementing the rules for many situations related to LCO. Therefore, in this study, we present a retrieval methodology for similar LCO cases in determining whether LCO is met or not met. To reflect the natural language processing of NPP features, a domain dictionary was built, and the optimal term frequency-inverse document frequency variant was selected. The retrieval performance was improved by adding a Boolean retrieval model based on terms related to the LCO in addition to the vector space model. The developed domain dictionary and retrieval methodology are expected to be exceedingly useful in determining whether LCO is met."
NOT_RELEVANT;ScienceDirect;Why is a document relevant? Understanding the relevance scores in cross-lingual document retrieval;Erik Novak and Luka Bizjak and Dunja Mladenić and Marko Grobelnik;2022;10.1016/j.knosys.2022.108545;https://doi.org/10.1016/j.knosys.2022.108545;Modern cross-lingual document retrieval models are capable of finding documents relevant to the query. However, they do not have the capabilities for explaining why the document is relevant. This paper proposes a novel learning-to-rank model named LM-EMD that uses the multilingual BERT language model and Earth Mover’s Distance (EMD) to measure the document’s relevancy to the input query and provide interpretable insights into why a document is relevant. The model uses the query and document token’s contextual embeddings generated with multilingual BERT to measure their distances in the embedding space, which are then used by EMD to calculate the document’s relevance score and identify which document tokens contribute the most to its relevancy. We evaluate the model on five language pairs of varying degrees of similarity and analyze its performance. We find that the model (1) performs similar as the best performing comparing model on high-resource languages, (2) is less effective on low-resource languages, and (3) provides insight into why a document is relevant to the query.
NOT_RELEVANT;ScienceDirect;A Hybrid of Sentence-Level Approach and Fragment-Level Approach of Parallel Text Extraction from Comparable Text;Yin-Lai Yeong and Tien-Ping Tan and Keng Hoon Gan;2019;10.1016/j.procs.2019.11.139;https://doi.org/10.1016/j.procs.2019.11.139;Parallel texts are essential resources in linguistics, natural language processing, and multilingual information retrieval. Many studies attempt to extract parallel text from existing resources, particularly from comparable texts. The approaches to extract parallel text from comparable text can be divided into sentence-level approach and fragment-level approach. In this paper, an approach that combines sentence-level approach and fragment-level approach is proposed. The study was evaluated using statistical machine translation (SMT) and neural machine translation (NMT). The experiment results show a very significant improvement in the BLEU scores of SMT and NMT. The BLEU scores for SMT for the test in computer science domain and news domain increase from 17.45 and 41.45 to 18.56 and 48.65 respectively. On the other hand, the BLEU scores for NMT in the computer science domain and news domain increase from 14.42 and 19.39 to 21.17 and 41.75 respectively.
NOT_RELEVANT;ScienceDirect;Automatic AMR Generation for Simple Sentences Using Dependency Parser;N. Pelja Paul and P. Revathy and G.M. Sini and R. Binu;2016;10.1016/j.protcy.2016.05.119;https://doi.org/10.1016/j.protcy.2016.05.119;Information retrieval is an important task in the field of natural language processing. Retrieving information requires a basic representation at the sentence level. Representation for sentences with same meaning should be same so that we can claim that the representation seems to be good enough to use in various natural language tasks. AMR is such a semantic representation aimed at large-scale human annotation inorder to built a giant semantic bank. In this paper, we present an automatic AMR tool for simple sentences with the help of dependency parser.
NOT_RELEVANT;ScienceDirect;Morphological evaluation and sentiment analysis of Punjabi text using deep learning classification;Jaspreet Singh and Gurvinder Singh and Rajinder Singh and Prithvipal Singh;2021;10.1016/j.jksuci.2018.04.003;https://doi.org/10.1016/j.jksuci.2018.04.003;Morphological processing of Indian languages is one of the most escalating fields in the era of Natural Language Processing (NLP) since the last decade. The evaluation of Asian languages is a highly relevant field in the times of text mining and information retrieval. The morphological evaluation of a text can be employed for extraction and classification of knowledge. This paper amalgamates morphological evaluation and sentiment prediction of Punjabi language text. The textual data for Punjabi language is concerned with farmer suicide cases reported for Punjab state of India. The pre-processing phase of this study involves morphological evaluation and normalization of Punjabi words to their respective canonical forms. The next phase carries out training and testing of deep neural network model on refined Punjabi tokens obtained from the earlier phase. The proposed model classifies Punjabi tokens into four negatively oriented classes tailored for farmer suicide cases. The average accuracies of sentiment prediction obtained after 10-fold cross validation are 93.85%, 88.53%, 83.3%, and 95.45% for the four respective classes. The proposed framework yields satisfactory results on 275 Punjabi text documents with the overall accuracy of 90.29% for sentiment classification.
NOT_RELEVANT;ScienceDirect;Comparative Study of Arabic Stemming Algorithms for Topic Identification;Marwa Naili and Anja Habacha Chaibi and Henda Hajjami {Ben Ghezala};2019;10.1016/j.procs.2019.09.238;https://doi.org/10.1016/j.procs.2019.09.238;Stemming process is one of the important pre-processing steps in different natural language process tasks such as text mining and information retrieval. Yet, stemming process can be considered as a difficult step to realize according to the used language. In fact, due to the complex morphology of Arabic language, stemming results can be influenced. Thus, several algorithms have been proposed in order to overcome stemming problems. In this paper, we investigate different stemming algorithms by presenting a comparative study in the field of Arabic topic identification.
MAYBE_RELEVANT;ScienceDirect;A survey on question answering systems with classification;Amit Mishra and Sanjay Kumar Jain;2016;10.1016/j.jksuci.2014.10.007;https://doi.org/10.1016/j.jksuci.2014.10.007;Question answering systems (QASs) generate answers of questions asked in natural languages. Early QASs were developed for restricted domains and have limited capabilities. Current QASs focus on types of questions generally asked by users, characteristics of data sources consulted, and forms of correct answers generated. Research in the area of QASs began in 1960s and since then, a large number of QASs have been developed. To identify the future scope of research in this area, the need of a comprehensive survey on QASs arises naturally. This paper surveys QASs and classifies them based on different criteria. We identify the current status of the research in the each category of QASs, and suggest future scope of the research.
NOT_RELEVANT;ScienceDirect;A Knowledge-Based Deep Learning Approach for Automatic Fake News Detection using BERT on Twitter;Vinita Nair and Dr. Jyoti Pareek and Sanskruti Bhatt;2024;10.1016/j.procs.2024.04.178;https://doi.org/10.1016/j.procs.2024.04.178;Fake news generation and propagation is a major challenge of the digital age, resulting in various social impacts namely bandwagon, validity, echo chamber effects, deceiving the public with spams, misinformation, malicious content and many more. The widespread proliferation of fake news not only fosters misinformation but also undermines the credibility of news sources. The veracity of the information is a major concern at all the stages of generation, publication, and propagation. To comprehend the critical need for addressing this pervasive problem, this research paper presents a framework for automatic detection of fake news using a knowledge-based approach. An automatic fact checking mechanism is applied using concepts of Information Retrieval (IR), Natural Language Processing (NLP) and Graph theory. The knowledge base is generated using Twitter dataset, which basically contains four attributes: Subject-Predicate-Object (SPO) triplet, SPO sentiment polarity, SPO occurrence, and topic modeling. These attributes serve as pivotal indicators for the development of a knowledge base, subsequently employed to detect prevalent patterns and traits linked to deceptive or false information. We have employed Named Entity Recognition (NER) model to extract SPO triples and Latent Dirichlet Allocation (LDA) for topic modeling, thereby contributing to knowledge base generation. To evaluate the efficacy and efficiency of our proposed model, we utilize deep learning algorithms like RNN, GRU, LSTM, GPT-3 and BERT Transformer providing an acceptable level of accuracy. This research paper delivers valuable insights into addressing the proliferation of fake news on Twitter, employing data-driven approaches and advanced deep learning algorithms.
NOT_RELEVANT;ScienceDirect;QSST: A Quranic Semantic Search Tool based on word embedding;Ensaf Hussein Mohamed and Eyad Mohamed Shokry;2022;10.1016/j.jksuci.2020.01.004;https://doi.org/10.1016/j.jksuci.2020.01.004;"Retrieving information from the Quran is an important field for Quran scholars and Arabic researchers. There are two types of Quran searching techniques: semantic or concept-based and keyword-based. Concept-based search is a challenging task, especially in a complex corpus such as Quran. This paper presents a concept-based searching tool (QSST) for the Holy Quran. It consists of four phases. In the first phase, the Quran dataset is built by manually annotating Quran verses based on the ontology of Mushaf Al-Tajweed. The second phase is word Embedding, this phase generates features’ vectors for words by training a Continuous Bag of Words (CBOW) architecture on large Quranic and Classic Arabic corpus. The third phase includes calculating the features’ vectors of both input query and Quranic topics. Finally, retrieving the most relevant verses by computing the cosine similarity between both topic and query vectors. The performance of the proposed QSST is measured by comparing results against Mushaf Al-Tajweed. Then, precision, recall, and F-score are computed and their percentages were 76.91%, 72.23% 69.28% respectively. In addition, the results are evaluated by three Islamic experts and the average precision was 91.95%. Finally, QSST results are compared with the recent existing tools; QSST outperformed them."
NOT_RELEVANT;ScienceDirect;A question answering system based on mineral exploration ontology generation: A deep learning methodology;Qinjun Qiu and Miao Tian and Kai Ma and Yong Jian Tan and Liufeng Tao and Zhong Xie;2023;10.1016/j.oregeorev.2023.105294;https://doi.org/10.1016/j.oregeorev.2023.105294;Mineral exploration reports and documents are a rich data source that contains a large amount of geological environments in which mineral deposits form. Among them, it is difficult to extract the required answers from the large amount of geological data. Despite the availability of search engines and digital databases that can be used to store geological data, users are unable to retrieve the information needed for a specific field in a timely manner. As a result, users usually have to contend with the burden of browsing and filtering information, which can be a time-consuming process. To address this issue, we propose a robust end-to-end approach that can improve the efficiency and effectiveness of retrieving queries related to mineral exploration terms. First, we present an automated workflow for constructing automatic question-and-answer datasets based on the names and definitions in the mineral exploration ontology. The Bidirectional Encoder Representation from Transformers (BERT) model is trained to test the answers generated from the user input question. Finally, a prototype chatbot system based on the WeChat platform and constructed experiments for evaluation is presented. Our proposed method has powerful feature representation and learning capabilities and thus has the potential to be adopted by other specialized fields (especially where a large number of mineral exploration ontologies already exist).
NOT_RELEVANT;ScienceDirect;Semantic Search based on the Online Integration of NLP Techniques;Katsuya Masuda and Takuya Matsuzaki and Jun’ichi Tsujii;2011;10.1016/j.sbspro.2011.10.609;https://doi.org/10.1016/j.sbspro.2011.10.609;This paper introduces a framework for semantic information retrieval based on the integration of various natural language processing (NLP) techniques, each of which annotates a base text with different kinds of information extracted from the text. Instead of running the NLP modules on the fly for individual search requests, the NLP modules are applied to the text in advance and the results are indexed in a way that enables flexible and efficient integration of them. The query language is based on a variant of the region algebra, in which we can specify a sub- structure in the annotated text that may involve different kinds of annotations. Given a query, the retrieval engine searches for the sub-structure by aggregating the different kinds of annotations through a search algorithm for the extended region algebra. We demonstrate the effectiveness and flexibility of the proposed framework through experiments with TREC Genomics Track data.
NOT_RELEVANT;ScienceDirect;An ontology-based similarity measure for biomedical data – Application to radiology reports;Thusitha Mabotuwana and Michael C. Lee and Eric V. Cohen-Solal;2013;10.1016/j.jbi.2013.06.013;https://doi.org/10.1016/j.jbi.2013.06.013;"Background
Determining similarity between two individual concepts or two sets of concepts extracted from a free text document is important for various aspects of biomedicine, for instance, to find prior clinical reports for a patient that are relevant to the current clinical context. Using simple concept matching techniques, such as lexicon based comparisons, is typically not sufficient to determine an accurate measure of similarity.
Methods
In this study, we tested an enhancement to the standard document vector cosine similarity model in which ontological parent–child (is-a) relationships are exploited. For a given concept, we define a semantic vector consisting of all parent concepts and their corresponding weights as determined by the shortest distance between the concept and parent after accounting for all possible paths. Similarity between the two concepts is then determined by taking the cosine angle between the two corresponding vectors. To test the improvement over the non-semantic document vector cosine similarity model, we measured the similarity between groups of reports arising from similar clinical contexts, including anatomy and imaging procedure. We further applied the similarity metrics within a k-nearest-neighbor (k-NN) algorithm to classify reports based on their anatomical and procedure based groups. 2150 production CT radiology reports (952 abdomen reports and 1128 neuro reports) were used in testing with SNOMED CT, restricted to Body structure, Clinical finding and Procedure branches, as the reference ontology.
Results
The semantic algorithm preferentially increased the intra-class similarity over the inter-class similarity, with a 0.07 and 0.08 mean increase in the neuro–neuro and abdomen–abdomen pairs versus a 0.04 mean increase in the neuro–abdomen pairs. Using leave-one-out cross-validation in which each document was iteratively used as a test sample while excluding it from the training data, the k-NN based classification accuracy was shown in all cases to be consistently higher with the semantics based measure compared with the non-semantic case. Moreover, the accuracy remained steady even as k value was increased – for the two anatomy related classes accuracy for k=41 was 93.1% with semantics compared to 86.7% without semantics. Similarly, for the eight imaging procedures related classes, accuracy (for k=41) with semantics was 63.8% compared to 60.2% without semantics. At the same k, accuracy improved significantly to 82.8% and 77.4% respectively when procedures were logically grouped together into four classes (such as ignoring contrast information in the imaging procedure description). Similar results were seen at other k-values.
Conclusions
The addition of semantic context into the document vector space model improves the ability of the cosine similarity to differentiate between radiology reports of different anatomical and image procedure-based classes. This effect can be leveraged for document classification tasks, which suggests its potential applicability for biomedical information retrieval."
MAYBE_RELEVANT;ScienceDirect;NLP-based smart decision making for business and academics;Pradnya Sawant and Kavita Sonawane;2024;10.1016/j.nlp.2024.100090;https://doi.org/10.1016/j.nlp.2024.100090;Natural Language Processing (NLP) systems enable machines to understand, interpret, and generate human-like language, bridging the gap between human communication and computer understanding. Natural Language Interface to Databases (NLIDB) and Natural Language Interface to Visualization (NLIV) systems are designed to enable non-technical users to retrieve and visualize data through natural language queries. However, these systems often face challenges in handling complex correlation and analytical questions, limiting their effectiveness for comprehensive data analysis. Additionally, current Business Intelligence (BI) tools also struggle with understanding the context and semantics of complex questions, further hindering their usability for strategic decision-making. Also, when building these models for generating the queries from natural language, the system handles only the semantic parsing issues as each column header is being changed manually to their normal names by all existing models which is time-consuming, tedious, and subjective. Recent studies reflect the need for attention to context, semantics, and especially ambiguities in dealing with natural language questions. To address this problem, the proposed architecture focuses on understanding the context, correlation-based semantic analysis, and removal of ambiguities using a novel approach. An Enhanced Longest Common Subsequence (ELCS) is suggested where existing LCS is modified with a memorization component for mapping the natural language question tokens with ambiguous table column headers. This can speed up the overall process as human intervention is not required to manually change the column headers. The same is evidenced by carrying out thorough experimentation and comparative study in terms of precision, recall, and F1 score. By synthesizing the latest advancements and addressing challenges, this paper has proved how NLP can significantly enhance the accuracy and efficiency of information retrieval and visualization, broadening the inclusivity and usability of NLIDB, NLIV, and BI systems.
MAYBE_RELEVANT;ScienceDirect;A New Digital Conceptual Model Oriented Corporate Memory Constructing: Taking Data Mining Models as a Case;Choukri Djellali;2013;10.1016/j.procs.2013.06.136;https://doi.org/10.1016/j.procs.2013.06.136;The integration of knowledge can be considered as a guideline for managing problems that occur in the task of knowledge management, and more particularly, in the collaborative decision-making. Integration is necessary because it allows communication between different sources. Most of the proposed approaches provide limited support for all activities of the engineering process, in particular, the phase of integration. We propose a new approach to treat the integration of the corporate knowledge. This model exploits indexation techniques and natural language processing to increase productivity of knowledge engineering task during the integration of conceptual model. Our integration system offers several advantages, these include speed search due to the structure and integrity of indexing.
NOT_RELEVANT;ScienceDirect;Hybrid PoS-tagging: A cooperation of evolutionary and statistical approaches;Rana Forsati and Mehrnoush Shamsfard;2014;10.1016/j.apm.2013.11.047;https://doi.org/10.1016/j.apm.2013.11.047;The assigning of syntactic categories to words in a sentence, which is referred to as part-of-speech (PoS) tagging problem, plays an essential role in many natural language processing and information retrieval applications. Despite the vast scope of methods, PoS-tagging brings an array of challenges that require novel solutions. To address these challenges in a principled way, one solution would be to formulate the tagging problem as an optimization problem with well-specified objectives and then apply the evolutionary methods to solve the optimization problem. This paper discusses the relative advantages of different evolutionary approaches to handle Part-of-Speech tagging problem and aims at presenting novel language-independent evolutionary algorithms to solve the PoS tagging problem. We show that by exploiting statistical measures to evaluate the solutions in tagging process, the proposed algorithms are able to generate more accurate solution in a reasonable amount of time. The experiments we have conducted on few well known corpus reveal that the proposed algorithms achieve better average accuracy in comparison to other evolutionary-based and classical Part-of-Speech tagging methods.
NOT_RELEVANT;ScienceDirect;A Framework for Managing Requirements of Software Product Lines;Maximiliano Arias and Agustina Buccella and Alejandra Cechich;2018;10.1016/j.entcs.2018.06.002;https://doi.org/10.1016/j.entcs.2018.06.002;An emerging problem in the Software Product Line Engineering (SPLE) is the need for integral management of planned reuse. In SPLE there are two instances where managing requirements gains relevance. The first one arises during the construction of SPLs based on legacy software or previously developed SPLs. The second one appears when instantiating products from the SPLplatform, where instantiating variability meets the custom requirements of each product. The objective of this paper is to define a framework that allows management of requirements using Natural Language Processing and Information Retrieval techniques, to structure, clean, index and find reusable functionalities according to those requirements. This framework is built in a way that allows the combination of such techniques to evaluate the best combinations for finding the correct functionalities in each SPL domain.
NOT_RELEVANT;ScienceDirect;AQA-WebCorp: Web-based Factual Questions for Arabic;Wided Bakari and Patrice Bellot and Mahmoud Neji;2016;10.1016/j.procs.2016.08.140;https://doi.org/10.1016/j.procs.2016.08.140;"Working with corpus construction becomes an interesting alternative to different applications of natural language processing, such as, question-answering, machine translation, information retrieval, etc. Similarly, with the heterogeneous data and the user demands for the accurate information, many studies have accentuated the need of the Web to highlight the corpus construction. As well as, Arabic doesn’t have an equivalent number of linguistic corpuses as compared to other languages like English. In this paper, we focus on building our corpus of Arab questions-texts. We present a method for recovering text passages. This method is based on a real automatic interrogation of Google, in order to generate passages of texts and answer the factual questions. The first part of this paper describes the formal details about this method; the second part presents some experiments and results that validate our method."
NOT_RELEVANT;ScienceDirect;Fast and flexible packed string matching;Simone Faro and M. Oğuzhan Külekci;2014;10.1016/j.jda.2014.07.003;https://doi.org/10.1016/j.jda.2014.07.003;Searching for all occurrences of a pattern in a text is a fundamental problem in computer science with applications in many other fields, like natural language processing, information retrieval and computational biology. In the last two decades a general trend has appeared trying to exploit the power of the word RAM model to speed-up the performances of classical string matching algorithms. In this model an algorithm operates on words of length w, grouping blocks of characters, and arithmetic and logic operations on the words take one unit of time. In this paper we use specialized word-size packed string matching instructions, based on the Intel streaming SIMD extensions (SSE) technology, to design a very fast string matching algorithm. We evaluate our solution in terms of efficiency, stability and flexibility, where we propose to use the deviation in running time of an algorithm on distinct equal length patterns as a measure of stability. From our experimental results it turns out that, despite their quadratic worst case time complexity, the new presented algorithm becomes the clear winner on the average in many cases, when compared against the most recent and effective algorithms known in literature.
NOT_RELEVANT;ScienceDirect;Automatically finding relevant citations for clinical guideline development;Duy Duc An Bui and Siddhartha Jonnalagadda and Guilherme {Del Fiol};2015;10.1016/j.jbi.2015.09.003;https://doi.org/10.1016/j.jbi.2015.09.003;"Objective
Literature database search is a crucial step in the development of clinical practice guidelines and systematic reviews. In the age of information technology, the process of literature search is still conducted manually, therefore it is costly, slow and subject to human errors. In this research, we sought to improve the traditional search approach using innovative query expansion and citation ranking approaches.
Methods
We developed a citation retrieval system composed of query expansion and citation ranking methods. The methods are unsupervised and easily integrated over the PubMed search engine. To validate the system, we developed a gold standard consisting of citations that were systematically searched and screened to support the development of cardiovascular clinical practice guidelines. The expansion and ranking methods were evaluated separately and compared with baseline approaches.
Results
Compared with the baseline PubMed expansion, the query expansion algorithm improved recall (80.2% vs. 51.5%) with small loss on precision (0.4% vs. 0.6%). The algorithm could find all citations used to support a larger number of guideline recommendations than the baseline approach (64.5% vs. 37.2%, p<0.001). In addition, the citation ranking approach performed better than PubMed’s “most recent” ranking (average precision +6.5%, recall@k +21.1%, p<0.001), PubMed’s rank by “relevance” (average precision +6.1%, recall@k +14.8%, p<0.001), and the machine learning classifier that identifies scientifically sound studies from MEDLINE citations (average precision +4.9%, recall@k +4.2%, p<0.001).
Conclusions
Our unsupervised query expansion and ranking techniques are more flexible and effective than PubMed’s default search engine behavior and the machine learning classifier. Automated citation finding is promising to augment the traditional literature search."
NOT_RELEVANT;ScienceDirect;INTEX: an FST toolbox;Max Silberztein;2000;10.1016/S0304-3975(99)00015-8;https://doi.org/10.1016/S0304-3975(99)00015-8;"INTEX is an integrated Natural Language Processing toolbox based on finite state transducers (FSTs). It parses texts of several million words, and includes large-coverage dictionaries and grammars. Texts, Dictionaries and Grammars are represented internally by FSTs. The user may add his/her own dictionaries and grammars; these tools are applied to texts in order to locate lexical and syntactic patterns, remove ambiguities, and tag simple words as well as complex utterances. INTEX builds lemmatized concordances and indices of texts with respect to all types of finite state patterns; it is used as a lexical parser to produce the input of a syntactic parser, but can also be viewed as an information retrieval system."
NOT_RELEVANT;ScienceDirect;Opportunities and challenges of text mining in materials research;Olga Kononova and Tanjin He and Haoyan Huo and Amalie Trewartha and Elsa A. Olivetti and Gerbrand Ceder;2021;10.1016/j.isci.2021.102155;https://doi.org/10.1016/j.isci.2021.102155;"Summary
Research publications are the major repository of scientific knowledge. However, their unstructured and highly heterogenous format creates a significant obstacle to large-scale analysis of the information contained within. Recent progress in natural language processing (NLP) has provided a variety of tools for high-quality information extraction from unstructured text. These tools are primarily trained on non-technical text and struggle to produce accurate results when applied to scientific text, involving specific technical terminology. During the last years, significant efforts in information retrieval have been made for biomedical and biochemical publications. For materials science, text mining (TM) methodology is still at the dawn of its development. In this review, we survey the recent progress in creating and applying TM and NLP approaches to materials science field. This review is directed at the broad class of researchers aiming to learn the fundamentals of TM as applied to the materials science publications."
NOT_RELEVANT;ScienceDirect;Computing text semantic relatedness using the contents and links of a hypertext encyclopedia;Majid Yazdani and Andrei Popescu-Belis;2013;10.1016/j.artint.2012.06.004;https://doi.org/10.1016/j.artint.2012.06.004;We propose a method for computing semantic relatedness between words or texts by using knowledge from hypertext encyclopedias such as Wikipedia. A network of concepts is built by filtering the encyclopediaʼs articles, each concept corresponding to an article. Two types of weighted links between concepts are considered: one based on hyperlinks between the texts of the articles, and another one based on the lexical similarity between them. We propose and implement an efficient random walk algorithm that computes the distance between nodes, and then between sets of nodes, using the visiting probability from one (set of) node(s) to another. Moreover, to make the algorithm tractable, we propose and validate empirically two truncation methods, and then use an embedding space to learn an approximation of visiting probability. To evaluate the proposed distance, we apply our method to four important tasks in natural language processing: word similarity, document similarity, document clustering and classification, and ranking in information retrieval. The performance of the method is state-of-the-art or close to it for each task, thus demonstrating the generality of the knowledge resource. Moreover, using both hyperlinks and lexical similarity links improves the scores with respect to a method using only one of them, because hyperlinks bring additional real-world knowledge not captured by lexical similarity.
NOT_RELEVANT;ScienceDirect;Decision support environment for medical product safety surveillance;Taxiarchis Botsis and Christopher Jankosky and Deepa Arya and Kory Kreimeyer and Matthew Foster and Abhishek Pandey and Wei Wang and Guangfan Zhang and Richard Forshee and Ravi Goud and David Menschik and Mark Walderhaug and Emily Jane Woo and John Scott;2016;10.1016/j.jbi.2016.07.023;https://doi.org/10.1016/j.jbi.2016.07.023;We have developed a Decision Support Environment (DSE) for medical experts at the US Food and Drug Administration (FDA). The DSE contains two integrated systems: The Event-based Text-mining of Health Electronic Records (ETHER) and the Pattern-based and Advanced Network Analyzer for Clinical Evaluation and Assessment (PANACEA). These systems assist medical experts in reviewing reports submitted to the Vaccine Adverse Event Reporting System (VAERS) and the FDA Adverse Event Reporting System (FAERS). In this manuscript, we describe the DSE architecture and key functionalities, and examine its potential contributions to the signal management process by focusing on four use cases: the identification of missing cases from a case series, the identification of duplicate case reports, retrieving cases for a case series analysis, and community detection for signal identification and characterization.
NOT_RELEVANT;ScienceDirect;Chinese named entity recognition with multi-network fusion of multi-scale lexical information;Yan Guo and Hong-Chen Liu and Fu-Jiang Liu and Wei-Hua Lin and Quan-Sen Shao and Jun-Shun Su;2024;10.1016/j.jnlest.2024.100287;https://doi.org/10.1016/j.jnlest.2024.100287;Named entity recognition (NER) is an important part in knowledge extraction and one of the main tasks in constructing knowledge graphs. In today's Chinese named entity recognition (CNER) task, the BERT-BiLSTM-CRF model is widely used and often yields notable results. However, recognizing each entity with high accuracy remains challenging. Many entities do not appear as single words but as part of complex phrases, making it difficult to achieve accurate recognition using word embedding information alone because the intricate lexical structure often impacts the performance. To address this issue, we propose an improved Bidirectional Encoder Representations from Transformers (BERT) character word conditional random field (CRF) (BCWC) model. It incorporates a pre-trained word embedding model using the skip-gram with negative sampling (SGNS) method, alongside traditional BERT embeddings. By comparing datasets with different word segmentation tools, we obtain enhanced word embedding features for segmented data. These features are then processed using the multi-scale convolution and iterated dilated convolutional neural networks (IDCNNs) with varying expansion rates to capture features at multiple scales and extract diverse contextual information. Additionally, a multi-attention mechanism is employed to fuse word and character embeddings. Finally, CRFs are applied to learn sequence constraints and optimize entity label annotations. A series of experiments are conducted on three public datasets, demonstrating that the proposed method outperforms the recent advanced baselines. BCWC is capable to address the challenge of recognizing complex entities by combining character-level and word-level embedding information, thereby improving the accuracy of CNER. Such a model is potential to the applications of more precise knowledge extraction such as knowledge graph construction and information retrieval, particularly in domain-specific natural language processing tasks that require high entity recognition precision.
NOT_RELEVANT;ScienceDirect;A Lexical Distance Study of Arabic Dialects;Kathrein Abu Kwaik and Motaz Saad and Stergios Chatzikyriakidis and Simon Dobnik;2018;10.1016/j.procs.2018.10.456;https://doi.org/10.1016/j.procs.2018.10.456;Diglossia is a very common phenomenon in Arabic-speaking communities, where the spoken language is different from both Classical Arabic (CA) and Modern Standard Arabic (MSA). The spoken language is characterised as a number of dialects used in everyday communication as well as informal writing. In this paper, we highlight the lexical relation between the MSA and Dialectal Arabic (DA) in more than one Arabic region. We conduct a computational cross dialectal lexical distance study to measure the similarities and differences between dialects and the MSA. We exploit several methods from Natural Language Processing (NLP) and Information Retrieval (IR) like Vector Space Model (VSM), Latent Semantic Indexing (LSI) and Hellinger Distance (HD), and apply them on different Arabic dialectal corpora. We measure the overlap among all the dialects and compute the frequencies of the most frequent words in every dialect. The results are informative and indicate that Levantine dialects are very similar to each other and furthermore, that Palestinian appears to be the closest to MSA.
MAYBE_RELEVANT;ScienceDirect;Question Answering Systems: Survey and Trends;Abdelghani Bouziane and Djelloul Bouchiha and Noureddine Doumi and Mimoun Malki;2015;10.1016/j.procs.2015.12.005;https://doi.org/10.1016/j.procs.2015.12.005;The need to query information content available in various formats including structured and unstructured data (text in natural language, semi-structured Web documents, structured RDF data in the semantic Web, etc.) has become increasingly important. Thus, Question Answering Systems (QAS) are essential to satisfy this need. QAS aim at satisfying users who are looking to answer a specific question in natural language. In this paper we survey various QAS. We give also statistics and analysis. This can clear the way and help researchers to choose the appropriate solution to their issue. They can see the insufficiency, so that they can propose new systems for complex queries. They can also adapt or reuse QAS techniques for specific research issues.
NOT_RELEVANT;ScienceDirect;Enhancing a Rule-based Event Coder with Semantic Vectors;Jinhong K. Guo and David {Van Brackle} and Martin O. Hofmann;2014;10.1016/j.procs.2014.09.074;https://doi.org/10.1016/j.procs.2014.09.074;Rule based systems have achieved success in applications such as information retrieval and Natural Language Processing. However, due to the rigidity of pattern matching, these systems typically require a large number of rules to adequately cover the variations of expression in unstructured text. Consequently, knowledge engineering for a new domain and knowledge maintenance for a fielded system are labor intensive and expensive. In this paper, we present our research on enhancing a rule-based event coding system by relaxing the rigidity of pattern matching with a technique that formulates and matches patterns of the semantics of words instead of literal words. Our technique pairs literal words with semantic vectors that accumulate word meaning from the context of use of the word found in dictionaries, ontologies, and domain corpora. Our method improves the speed, accuracy, and coverage of the event coding algorithm without additional knowledge engineering effort. Operating on semantics instead of syntax, the improved system eases the workload of human analysts who screen input text for critical events. Our algorithms are based on high-dimensional distributed representations, and their effectiveness and versatility derive from the unintuitive properties of such representations---from the mathematical properties of high-dimensional spaces. Our current implementation encodes words, phrases, and rule patterns as semantic vectors using WordNet, We have started experimental evaluation using a large newswire dataset.
NOT_RELEVANT;ScienceDirect;Analysis of eligibility criteria representation in industry-standard clinical trial protocols;Sanmitra Bhattacharya and Michael N. Cantor;2013;10.1016/j.jbi.2013.06.001;https://doi.org/10.1016/j.jbi.2013.06.001;Previous research on standardization of eligibility criteria and its feasibility has traditionally been conducted on clinical trial protocols from ClinicalTrials.gov (CT). The portability and use of such standardization for full-text industry-standard protocols has not been studied in-depth. Towards this end, in this study we first compare the representation characteristics and textual complexity of a set of Pfizer’s internal full-text protocols to their corresponding entries in CT. Next, we identify clusters of similar criteria sentences from both full-text and CT protocols and outline methods for standardized representation of eligibility criteria. We also study the distribution of eligibility criteria in full-text and CT protocols with respect to pre-defined semantic classes used for eligibility criteria classification. We find that in comparison to full-text protocols, CT protocols are not only more condensed but also convey less information. We also find no correlation between the variations in word-counts of the ClinicalTrials.gov and full-text protocols. While we identify 65 and 103 clusters of inclusion and exclusion criteria from full text protocols, our methods found only 36 and 63 corresponding clusters from CT protocols. For both the full-text and CT protocols we are able to identify ‘templates’ for standardized representations with full-text standardization being more challenging of the two. In our exploration of the semantic class distributions we find that the majority of the inclusion criteria from both full-text and CT protocols belong to the semantic class “Diagnostic and Lab Results” while “Disease, Sign or Symptom” forms the majority for exclusion criteria. Overall, we show that developing a template set of eligibility criteria for clinical trials, specifically in their full-text form, is feasible and could lead to more efficient clinical trial protocol design.
NOT_RELEVANT;ScienceDirect;The interaction of domain knowledge and linguistic structure in natural language processing: interpreting hypernymic propositions in biomedical text;Thomas C Rindflesch and Marcelo Fiszman;2003;10.1016/j.jbi.2003.11.003;https://doi.org/10.1016/j.jbi.2003.11.003;"Interpretation of semantic propositions in free-text documents such as MEDLINE citations would provide valuable support for biomedical applications, and several approaches to semantic interpretation are being pursued in the biomedical informatics community. In this paper, we describe a methodology for interpreting linguistic structures that encode hypernymic propositions, in which a more specific concept is in a taxonomic relationship with a more general concept. In order to effectively process these constructions, we exploit underspecified syntactic analysis and structured domain knowledge from the Unified Medical Language System (UMLS). After introducing the syntactic processing on which our system depends, we focus on the UMLS knowledge that supports interpretation of hypernymic propositions. We first use semantic groups from the Semantic Network to ensure that the two concepts involved are compatible; hierarchical information in the Metathesaurus then determines which concept is more general and which more specific. A preliminary evaluation of a sample based on the semantic group Chemicals and Drugs provides 83% precision. An error analysis was conducted and potential solutions to the problems encountered are presented. The research discussed here serves as a paradigm for investigating the interaction between domain knowledge and linguistic structure in natural language processing, and could also make a contribution to research on automatic processing of discourse structure. Additional implications of the system we present include its integration in advanced semantic interpretation processors for biomedical text and its use for information extraction in specific domains. The approach has the potential to support a range of applications, including information retrieval and ontology engineering."
NOT_RELEVANT;ScienceDirect;A rule-based stemmer for Arabic Gulf dialect;Belal Abuata and Asma Al-Omari;2015;10.1016/j.jksuci.2014.04.003;https://doi.org/10.1016/j.jksuci.2014.04.003;Arabic dialects arewidely used from many years ago instead of Modern Standard Arabic language in many fields. The presence of dialects in any language is a big challenge. Dialects add a new set of variational dimensions in some fields like natural language processing, information retrieval and even in Arabic chatting between different Arab nationals. Spoken dialects have no standard morphological, phonological and lexical like Modern Standard Arabic. Hence, the objective of this paper is to describe a procedure or algorithm by which a stem for the Arabian Gulf dialect can be defined. The algorithm is rule based. Special rules are created to remove the suffixes and prefixes of the dialect words. Also, the algorithm applies rules related to the word size and the relation between adjacent letters. The algorithm was tested for a number of words and given a good correct stem ratio. The algorithm is also compared with two Modern Standard Arabic algorithms. The results showed that Modern Standard Arabic stemmers performed poorly with Arabic Gulf dialect and our algorithm performed poorly when applied for Modern Standard Arabic words.
NOT_RELEVANT;ScienceDirect;A New Enhanced Arabic Light Stemmer for IR in Medical Documents;Ra’ed M. Al-Khatib and Taha Zerrouki and Mohammed M. Abu Shquier and Amar Balla and Asef Al-Khateeb;2021;10.32604/cmc.2021.016155;https://doi.org/10.32604/cmc.2021.016155;This paper introduces a new enhanced Arabic stemming algorithm for solving the information retrieval problem, especially in medical documents. Our proposed algorithm is a light stemming algorithm for extracting stems and roots from the input data. One of the main challenges facing the light stemming algorithm is cutting off the input word, to extract the initial segments. When initiating the light stemmer with strong initial segments, the final extracting stems and roots will be more accurate. Therefore, a new enhanced segmentation based on deploying the Direct Acyclic Graph (DAG) model is utilized. In addition to extracting the powerful initial segments, the main two procedures (i.e., stems and roots extraction), should be also reinforced with more efficient operators to improve the final outputs. To validate the proposed enhanced stemmer, four data sets are used. The achieved stems and roots resulted from our proposed light stemmer are compared with the results obtained from five other well-known Arabic light stemmers using the same data sets. This evaluation process proved that the proposed enhanced stemmer outperformed other comparative stemmers.
NOT_RELEVANT;ScienceDirect;A web framework for information aggregation and management of multilingual hate speech;Rigas Kotsakis and Lazaros Vrysis and Nikolaos Vryzas and Theodora Saridou and Maria Matsiola and Andreas Veglis and Charalampos Dimoulas;2023;10.1016/j.heliyon.2023.e16084;https://doi.org/10.1016/j.heliyon.2023.e16084;Social media platforms have led to the creation of a vast amount of information produced by users and published publicly, facilitating participation in the public sphere, but also giving the opportunity for certain users to publish hateful content. This content mainly involves offensive/discriminative speech towards social groups or individuals (based on racial, religious, gender or other characteristics) and could possibly lead into subsequent hate actions/crimes due to persistent escalation. Content management and moderation in big data volumes can no longer be supported manually. In the current research, a web framework is presented and evaluated for the collection, analysis, and aggregation of multilingual textual content from various online sources. The framework is designed to address the needs of human users, journalists, academics, and the public to collect and analyze content from social media and the web in Spanish, Italian, Greek, and English, without prior training or a background in Computer Science. The backend functionality provides content collection and monitoring, semantic analysis including hate speech detection and sentiment analysis using machine learning models and rule-based algorithms, storing, querying, and retrieving such content along with the relevant metadata in a database. This functionality is assessed through a graphic user interface that is accessed using a web browser. An evaluation procedure was held through online questionnaires, including journalists and students, proving the feasibility of the use of the proposed framework by non-experts for the defined use-case scenarios.
NOT_RELEVANT;ScienceDirect;A Conceptual Model of Trademark Retrieval based on Conceptual Similarity;Fatahiyah Mohd Anuar and Rossitza Setchi and Yu-Kun Lai;2013;10.1016/j.procs.2013.09.123;https://doi.org/10.1016/j.procs.2013.09.123;"The rapid expansion of e-commerce at the beginning of 21st century has had a significant impact on intellectual property management. A particular area of concern is the misuse of trademarks and trademark protection. Trademarks are proprietary words and images with high reputational value; they are important assets, often used as a marketing tool, which require infringement protection. One of the issues considered during infringement litigation is the visual, conceptual and phonetic similarity of different trademarks. In particular, the conceptual similarity of trademarks is an area never previously studied in information retrieval. This paper focuses on this important aspect by proposing a conceptual model of the comparison process, aimed at retrieving conceptually similar trademarks. The proposed model employs natural language processing and semantic technology to compute the conceptual similarity between trademarks."
NOT_RELEVANT;ScienceDirect;Text Normalization in Social Media: Progress, Problems and Applications for a Pre-Processing System of Casual English;Eleanor Clark and Kenji Araki;2011;10.1016/j.sbspro.2011.10.577;https://doi.org/10.1016/j.sbspro.2011.10.577;The rapid expansion in user-generated content on the Web of the 2000s, characterized by social media, has led to Web content featuring somewhat less standardized language than the Web of the 1990s. User creativity and individuality of language creates problems on two levels. The first is that social media text is often unsuitable as data for Natural Language Processing tasks such as Machine Translation, Information Retrieval and Opinion Mining, due to the irregularity of the language featured. The second is that non-native speakers of English, older Internet users and non-members of the “in-group” often find such texts difficult to understand. This paper discusses problems involved in automatically normalizing social media English, various applications for its use, and our progress thus far in a rule-based approach to the issue. Particularly, we evaluate the performance of two leading open source spell checkers on data taken from the microblogging service Twitter, and measure the extent to which their accuracy is improved by pre-processing with our system. We also present our database rules and classification system, results of evaluation experiments, and plans for expansion of the project.
NOT_RELEVANT;ScienceDirect;Ontology based Semantic Annotation of Urdu Language Web Documents;Quratulain Rajput;2014;10.1016/j.procs.2014.08.148;https://doi.org/10.1016/j.procs.2014.08.148;Proliferation of multilingual text on the Internet has increased the demand for efficient information retrieval independent of language. Among variety of languages, the Urdu language is one of the most commonly spoken and written language in South Asia. However, due to unstructured format the access of relevant information is still a big challenge. The semantic web technologies enable the advancement in information retrieval systems by assigning semantics to information. This paper presents a semantic annotation framework that can annotate documents written in Urdu language. The framework uses domain specific ontology and context keywords instead of NLP (Natural Language processing) techniques. The experiment has been conducted to evaluate the presented annotation framework. The set of corpora used in the experiment belong to the online classified ads posted on the online Urdu newspapers. The purpose of this research is to find the challenges involved in semantic annotation of Urdu language web documents.
NOT_RELEVANT;ScienceDirect;A passage retrieval method based on probabilistic information retrieval model and UMLS concepts in biomedical question answering;Mourad Sarrouti and Said {Ouatik El Alaoui};2017;10.1016/j.jbi.2017.03.001;https://doi.org/10.1016/j.jbi.2017.03.001;"Background and Objective
Passage retrieval, the identification of top-ranked passages that may contain the answer for a given biomedical question, is a crucial component for any biomedical question answering (QA) system. Passage retrieval in open-domain QA is a longstanding challenge widely studied over the last decades. However, it still requires further efforts in biomedical QA. In this paper, we present a new biomedical passage retrieval method based on Stanford CoreNLP sentence/passage length, probabilistic information retrieval (IR) model and UMLS concepts.
Methods
In the proposed method, we first use our document retrieval system based on PubMed search engine and UMLS similarity to retrieve relevant documents to a given biomedical question. We then take the abstracts from the retrieved documents and use Stanford CoreNLP for sentence splitter to make a set of sentences, i.e., candidate passages. Using stemmed words and UMLS concepts as features for the BM25 model, we finally compute the similarity scores between the biomedical question and each of the candidate passages and keep the N top-ranked ones.
Results
Experimental evaluations performed on large standard datasets, provided by the BioASQ challenge, show that the proposed method achieves good performances compared with the current state-of-the-art methods. The proposed method significantly outperforms the current state-of-the-art methods by an average of 6.84% in terms of mean average precision (MAP).
Conclusion
We have proposed an efficient passage retrieval method which can be used to retrieve relevant passages in biomedical QA systems with high mean average precision."
NOT_RELEVANT;ScienceDirect;Analyzing Distillation Process of Hidden Terms in Web Documents for IR;M. Pradeepa and C. Deisy;2012;10.1016/j.proeng.2012.06.372;https://doi.org/10.1016/j.proeng.2012.06.372;The previous work in web based applications such as mining web content, pattern recognition and similarity measures between the web documents. This paper is about, analyzing web documents in an enhanced way and delve the distillation web document will be the next pace in hypertext mining. The sparse document is a very little data on the web, which may face problems like different words with almost identical or similar meanings and sparseness. Natural language processing (NLP) and information retrieval (IR) are the main obstacles of the above problem. The mining of hidden terms discovers the search queries from large external datasets (universal datasets). It helps to handle unseen data in a better way. The goal of this web document mining consists of an efficient information finding, filtering information based on user query, and discovers more topic focused keywords based on the rich source of global information datasets. The proposed method we use the Distillation model, it is the integration of probabilistic generative model, Gibbs sampling algorithm and deployment method. This model can be applied for different natural languages and data domains for achieving the goal.
NOT_RELEVANT;ScienceDirect;Empirical distributional semantics: Methods and biomedical applications;Trevor Cohen and Dominic Widdows;2009;10.1016/j.jbi.2009.02.002;https://doi.org/10.1016/j.jbi.2009.02.002;Over the past 15 years, a range of methods have been developed that are able to learn human-like estimates of the semantic relatedness between terms from the way in which these terms are distributed in a corpus of unannotated natural language text. These methods have also been evaluated in a number of applications in the cognitive science, computational linguistics and the information retrieval literatures. In this paper, we review the available methodologies for derivation of semantic relatedness from free text, as well as their evaluation in a variety of biomedical and other applications. Recent methodological developments, and their applicability to several existing applications are also discussed.
NOT_RELEVANT;ScienceDirect;Reviewer assignment algorithms for peer review automation: A survey;Xiquan Zhao and Yangsen Zhang;2022;10.1016/j.ipm.2022.103028;https://doi.org/10.1016/j.ipm.2022.103028;Assigning paper to suitable reviewers is of great significance to ensure the accuracy and fairness of peer review results. In the past three decades, many researchers have made a wealth of achievements on the reviewer assignment problem (RAP). In this survey, we provide a comprehensive review of the primary research achievements on reviewer assignment algorithm from 1992 to 2022. Specially, this survey first discusses the background and necessity of automatic reviewer assignment, and then systematically summarize the existing research work from three aspects, i.e., construction of candidate reviewer database, computation of matching degree between reviewers and papers, and reviewer assignment optimization algorithm, with objective comments on the advantages and disadvantages of the current algorithms. Afterwards, the evaluation metrics and datasets of reviewer assignment algorithm are summarized. To conclude, we prospect the potential research directions of RAP. Since there are few comprehensive survey papers on reviewer assignment algorithm in the past ten years, this survey can serve as a valuable reference for the related researchers and peer review organizers.
NOT_RELEVANT;ScienceDirect;Knowledge Repository of Ontology Learning Tools from Text;Agnieszka Konys;2019;10.1016/j.procs.2019.09.332;https://doi.org/10.1016/j.procs.2019.09.332;Ontologies are one of the fundamental elements of the Semantic Web, and they have gained a lot of popularity and recognition because they are viewed as the answer to the need for interoperable semantics in modern information systems. The intermingling of techniques in areas such as natural language processing, information retrieval, machine learning, data mining, and knowledge representation provide a lot of possibilities for development of ontology learning approaches. A rise in focus on the ability to cope with the scale of Web data required for ontology learning forces the potential growth of cross-language research, emphasizing the automatic or semi-automatic generation of the tools dedicated to text mining and information extraction. This paper presents the integration of ontology learning tools from text in the knowledge repository to incorporate the applied techniques and outputs of an ontology learning algorithm into the one complex multifunctional solution. The proposed knowledge repository covers various applicability of existing techniques of learning ontologies from text, and offers competency question-based reasoning mechanism for individuals to specify their profiles of ontology learning tools. The validation stage is also provided in the form of applied reasoning.
NOT_RELEVANT;ScienceDirect;Automatically extracting cancer disease characteristics from pathology reports into a Disease Knowledge Representation Model;Anni Coden and Guergana Savova and Igor Sominsky and Michael Tanenblatt and James Masanz and Karin Schuler and James Cooper and Wei Guan and Piet C. {de Groen};2009;10.1016/j.jbi.2008.12.005;https://doi.org/10.1016/j.jbi.2008.12.005;We introduce an extensible and modifiable knowledge representation model to represent cancer disease characteristics in a comparable and consistent fashion. We describe a system, MedTAS/P which automatically instantiates the knowledge representation model from free-text pathology reports. MedTAS/P is based on an open-source framework and its components use natural language processing principles, machine learning and rules to discover and populate elements of the model. To validate the model and measure the accuracy of MedTAS/P, we developed a gold-standard corpus of manually annotated colon cancer pathology reports. MedTAS/P achieves F1-scores of 0.97–1.0 for instantiating classes in the knowledge representation model such as histologies or anatomical sites, and F1-scores of 0.82–0.93 for primary tumors or lymph nodes, which require the extractions of relations. An F1-score of 0.65 is reported for metastatic tumors, a lower score predominantly due to a very small number of instances in the training and test sets.
NOT_RELEVANT;ScienceDirect;A Rich Arabic WordNet Resource for Al-Hadith Al-Shareef;Manar Alkhatib and Azza Abdel Monem and Khaled Shaalan;2017;10.1016/j.procs.2017.10.098;https://doi.org/10.1016/j.procs.2017.10.098;Most Arabic computational linguistics researchers have focused on Modern Standard Arabic. Linguistic resources and tools for Classical Arabic, especially Al-Hadith Al-Shareef (i.e. the Prophet Muhammad’s saying), remain relatively unexplored, despite its importance as a reference, in addition to its use in the holy Qur’an, used by all Muslims worldwide. Computational linguistics research tools for Al-Hadith Al-Shareef would be useful for both Islamic scholars and learners. Arabic WordNet is a remarkable language resource on its own, allowing a user to determine the relationships among words. It has proven its importance in many of language processing tasks needing an understanding of the meaning of language, including Information Retrieval, Word Sense Disambiguation, Machine Translation, Question Answering, Text Classification, and Text Summarization. In this paper, we propose an approach for developing a WordNet linguistic resource for Al-Hadith Al-Shareef that serves its purposes for various Arabic natural language processing tasks. In particular, we establish semantic connections between words in order to achieve a good understanding of the meanings of the Al-Hadith words. Our approach employs Classical Arabic dictionaries and Al-Hadith ontology. Al-Hadith WordNet has demonstrated its capability in a text classification task that we developed for evaluation proposes. The classifier has been applied on around 8500 synsets that include 6126 nominal, 1990 verbal, 310 adjectival, and 71 adverbial expressions.
NOT_RELEVANT;ScienceDirect;MeSH-based disambiguation method using an intrinsic information content measure of semantic similarity;Imen Gabsi and Hager Kammoun and Sarra brahmi and Ikram Amous;2017;10.1016/j.procs.2017.08.169;https://doi.org/10.1016/j.procs.2017.08.169;Word Sense Disambiguation represents a crucial task in many natural language processing applications such as information extraction, information retrieval and text summarization. It intends to identify the correct sense of an ambiguous term (target word) according to its context. In this paper, we present a biomedical knowledge-based disambiguation method for determining the adequate domain or sub-domain (sense) of an ambiguous biomedical term. This method uses the MeSH thesaurus as a knowledge resource and an intrinsic information content-based semantic similarity to measure the likeness between their different senses of the ambiguous term MeSH and its context. We define this latter as the set of unambiguous MeSH terms characterizing a document. We aim on one hand to minimize the time and computational complexity and on the other hand to increase the accuracy of the disambiguation by using such context. To evaluate this method, we involved it into a document indexing and retrieval system. The results of experiments, carried out on a sub-set of the OHSUMED collection, in terms of Mean Average Precision show that our method performs well.
NOT_RELEVANT;ScienceDirect;Improving biomedical named entity recognition through transfer learning and asymmetric tri-training;Medha Bhattacharya and Swati Bhat and Sirshasree Tripathy and Anvita Bansal and Monika Choudhary;2023;10.1016/j.procs.2023.01.244;https://doi.org/10.1016/j.procs.2023.01.244;Today, electronic health records have turned into prime sources of information for physicians looking after their patients. EHRs and computerized patient data resources have expedited the accelerated discovery of formerly obscure biomedical and clinical information. Because of the lengthy, error-prone, non-scalable, and expensive manual abstraction process, natural language processing (NLP) procedures are being wielded more and more in biomedical and clinical fields. One of the building blocks of all NLP systems, Named Entity Recognition (NER) is considered a sub-activity of information retrieval. To extract biomedical knowledge from electronic health records, a prerequisite is the efficient recognition of biomedical entities. Deep-learning techniques have gained more and more consideration recently for the above-mentioned task. Notwithstanding, these methods are based on high-caliber, high-cost, labeled data. In this work, a biomedical-named entity recognition model based on transfer learning and asymmetric tri-training is proposed to diminish the limited annotated data problem in the biomedical-named entity recognition domain. The proposed model showed a significant improvement of more than 9% over the baseline BiLSTM-CRF model in the exact F1 scores on four different datasets considered in this work.
NOT_RELEVANT;ScienceDirect;Prediction of Likes and Retweets Using Text Information Retrieval;Ishita Daga and Anchal Gupta and Raj Vardhan and Partha Mukherjee;2020;10.1016/j.procs.2020.02.273;https://doi.org/10.1016/j.procs.2020.02.273;Twitter is one of the major social media platforms today to study human behaviours by analysing their interactions. To ensure popularity of the tweet, the focus should be on the content of the tweet that results in numerous followings of that message with sufficient number of likes and retweets. The high quality of tweets, increases the online reputation of the users who post it. If a user can get the prediction of likes and retweets on his text before posting it on the internet, it would improve the popularity of the tweet from information sharing perspective. In this paper we employed different machine learning classifiers like SVM, Naïve Bayes, Logistic Regression, Random Forest, and Neural Network, on top of two different text processing approaches used in NLP (natural language processing), namely bag-of-words (TFIDF) and word embeddings (Doc2Vec), to check how many likes and retweets can a tweet generate. The results obtained indicate that all the models performed 10-15% better with the bag-of-word technique.
NOT_RELEVANT;ScienceDirect;Keynote II; Marie-Francine;2018;10.1016/j.procs.2018.10.121;https://doi.org/10.1016/j.procs.2018.10.121;About the Speaker Prof. Marie-Francine Moens heads the Language Intelligence and Information Retrieval research group at the Department of Computer Science of KU Leuven, Belgium. Her research expertise includes: content recognition in text, information extraction, discourse understanding, text mining, knowledge acquisition, machine reading of text, processing of noisy text such as user-generated content and speech transcripts, information retrieval, search models, and machine learning for natural language processing. She is author of more than 300 international peer-reviewed publications and of several books. She is involved in the organization or program committee (as program chair, area chair or reviewer) of major conferences on computational linguistics, information retrieval and machine learning. In 2011 and 2012 she was appointed as chair of the European Chapter of the Association for Computational Linguistics (EACL) and was a member of the executive board of the Association for Computational Linguistics (ACL). From 2010 until 2014 she was a member of the Research Council of KU Leuven and is currently a member of the Council of the Industrial Research Fund of KU Leuven. She is the scientific manager of the EU COST action iV&L (The European Network on Integrating Vision and Language). She was appointed as Scottish Informatics and Computer Science Alliance (SICSA) Distinguished Visiting Fellow in 2014.
MAYBE_RELEVANT;ScienceDirect;Advances and challenges in conversational recommender systems: A survey;Chongming Gao and Wenqiang Lei and Xiangnan He and Maarten {de Rijke} and Tat-Seng Chua;2021;10.1016/j.aiopen.2021.06.002;https://doi.org/10.1016/j.aiopen.2021.06.002;Recommender systems exploit interaction history to estimate user preference, having been heavily used in a wide range of industry applications. However, static recommendation models are difficult to answer two important questions well due to inherent shortcomings: (a) What exactly does a user like? (b) Why does a user like an item? The shortcomings are due to the way that static models learn user preference, i.e., without explicit instructions and active feedback from users. The recent rise of conversational recommender systems (CRSs) changes this situation fundamentally. In a CRS, users and the system can dynamically communicate through natural language interactions, which provide unprecedented opportunities to explicitly obtain the exact preference of users. Considerable efforts, spread across disparate settings and applications, have been put into developing CRSs. Existing models, technologies, and evaluation methods for CRSs are far from mature. In this paper, we provide a systematic review of the techniques used in current CRSs. We summarize the key challenges of developing CRSs in five directions: (1) Question-based user preference elicitation. (2) Multi-turn conversational recommendation strategies. (3) Dialogue understanding and generation. (4) Exploitation-exploration trade-offs. (5) Evaluation and user simulation. These research directions involve multiple research fields like information retrieval (IR), natural language processing (NLP), and human-computer interaction (HCI). Based on these research directions, we discuss some future challenges and opportunities. We provide a road map for researchers from multiple communities to get started in this area. We hope this survey can help to identify and address challenges in CRSs and inspire future research.
NOT_RELEVANT;ScienceDirect;Unsupervised grammar induction and similarity retrieval in medical language processing using the Deterministic Dynamic Associative Memory (DDAM) model;Stefan V. Pantazi;2010;10.1016/j.jbi.2010.07.003;https://doi.org/10.1016/j.jbi.2010.07.003;This paper is an overview of unsupervised grammar induction and similarity retrieval, two fundamental information processing functions of importance to medical language processing applications and to the construction of intelligent medical information systems. Existing literature with a focus on text segmentation tasks is reviewed. The review includes a comparison of existing approaches and reveals the longstanding interest in these traditionally distinct topics despite the significant computational challenges that characterizes them. Further, a unifying approach to unsupervised representation and processing of sequential data, the Deterministic Dynamic Associative Memory (DDAM) model, is introduced and described theoretically from both structural and functional perspectives. The theoretical descriptions of the model are complemented by a selection and discussion of interesting experimental results in the tasks of unsupervised grammar induction and similarity retrieval with applications to medical language processing. Notwithstanding the challenges associated with the evaluation of unsupervised information-processing models, it is concluded that the DDAM model demonstrates interesting properties that encourage further investigations in both theoretical and applied contexts.
NOT_RELEVANT;ScienceDirect;Domain-specific language models and lexicons for tagging;Anni R. Coden and Serguei V. Pakhomov and Rie K. Ando and Patrick H. Duffy and Christopher G. Chute;2005;10.1016/j.jbi.2005.02.009;https://doi.org/10.1016/j.jbi.2005.02.009;Accurate and reliable part-of-speech tagging is useful for many Natural Language Processing (NLP) tasks that form the foundation of NLP-based approaches to information retrieval and data mining. In general, large annotated corpora are necessary to achieve desired part-of-speech tagger accuracy. We show that a large annotated general-English corpus is not sufficient for building a part-of-speech tagger model adequate for tagging documents from the medical domain. However, adding a quite small domain-specific corpus to a large general-English one boosts performance to over 92% accuracy from 87% in our studies. We also suggest a number of characteristics to quantify the similarities between a training corpus and the test data. These results give guidance for creating an appropriate corpus for building a part-of-speech tagger model that gives satisfactory accuracy results on a new domain at a relatively small cost.
NOT_RELEVANT;ScienceDirect;PAVAL: A location-aware virtual personal assistant for retrieving geolocated points of interest and location-based services;Lorenzo Massai and Paolo Nesi and Gianni Pantaleo;2019;10.1016/j.engappai.2018.09.013;https://doi.org/10.1016/j.engappai.2018.09.013;Today most of the users on the move require contextualized local and georeferenced information. Several solutions aim to meet these trends, thus assisting users and satisfying their needs and preferences, such as virtual assistants and Location-Aware Recommender Systems (LARS), both in commercial and research literature. However, general purpose virtual assistants usually have to manage large domains, dealing with big amounts of data and online resources, losingfocus on more specific requirements and local information. On the other hand, traditional recommender systems are based on filtering techniques and contextual knowledge, and they usually do not rely on Natural Language Processing (NLP) features on users’ queries, which are useful to understand and contextualize users’ necessities on the spot. Therefore, comprehending the actual users’ information needs and other key information that can be included in the user query, such as geographical references, is a challenging task which is not yet fully accomplished by current state-of-the-art solutions. In this paper, we propose Paval (Location-Aware Virtual Personal Assistant 2 2The name Paval is chosen as a permutation of the initials of “Location-aware virtual personal assistant”.), a semantic assisting engine for suggesting local points of interest (POIs) and services by analyzing users’ natural language queries, in order to estimate the information need and potential geographic references expressed by the users. The system exploits NLP and semantic techniques providing as output recommendations on local geolocated POIs and services which best match the users’ requests, retrieved by querying our semantic Km4City Knowledge Base. The proposed system is validated against the most popular virtual assistants, such as Google Assistant, Apple Siri and Microsoft Cortana, focusing the assessment on the request of geolocated POIs and services, showing very promising capabilities in successfully estimating the users’ information needs and multiple geographic references.
NOT_RELEVANT;ScienceDirect;Necessary conditions for convergence of CNNs and initialization of convolution kernels;Huibin Zhang and Liping Feng and Xiaohua Zhang and Yuchi Yang and Jing Li;2022;10.1016/j.dsp.2022.103397;https://doi.org/10.1016/j.dsp.2022.103397;Despite the great success of deep learning in many fields such as computer vision, natural language processing, and information retrieval, there are relatively few studies on the convergence of deep convolutional neural networks (CNNs), and there is a lack of theoretical studies on the necessary conditions for the convergence of CNNs. The initialization of the convolution kernel of CNNs is an important factor in whether the network can converge. However, the existing initialization methods do not analyze the influence of their methods on the convergence performance of CNNs and did not analyze the conditions of their application, and thus the performance is not the best in different network models. In this work, the computational process of both forward and backward propagation of CNNs is considered as a mapping in linear normed space, and thus a necessary condition for CNNs stable converge is proposed. According to this necessary condition of convergence, we first derive initialization formulas for plain networks applicable to any activation function, and derive the initialization method of plain networks whose activation function is ReLU and PReLU. Secondly, the necessary conditions for convergence of CNNs proposed in this work can explain the mathematical reasons why the BN contribute to the training of CNNs, and this problem has always been an active research topic. Finally, we find that the learning rate and the initialization of the convolutional kernel jointly affect the convergence performance of the network. Based on the plain networks convolution kernel initialization method, we derive the convolution kernel initialization method of network with BN layer related to the learning rate. In order to verify the effectiveness of the proposed initialization method, we test it on the CIFAR-10 and CIFAR-100 datasets, and performed image classification with four network models (VGG-19, ResNet-110, DenseNet-100 and WideResNet28-10), and the experimental results showed that the initialization method proposed in this work improved the accuracy of image classification.
NOT_RELEVANT;ScienceDirect;Development of Sindhi text corpus;Mazhar Ali Dootio and Asim Imdad Wagan;2021;10.1016/j.jksuci.2019.02.002;https://doi.org/10.1016/j.jksuci.2019.02.002;Sindhi language is a rich language with plenty of literary and general texts. There are number of books, newspapers, magazines and internet material available to develop Sindhi text corpus but yet proper and useful text corpus could not be developed and presented online for research, language features analysis, linguistics analysis and information retrieval systems. The lack of resources for research on computational linguistics and NLP applications for Sindhi language are challenging tasks at this stage. However, we have developed Sindhi text corpora in order to provide text resources to computational linguists, Natural Languages process (NLP) experts and researchers. Online books, newspapers, magazines, blogs and social websites are utilized to build Sindhi text corpus. Sindhi sentiment based text corpus is developed and analyzed with Document Term Matrix and TF-IDF models using 2-gram technique of n-gram model. The corpus may be useful for research on language variation analysis, sentiment analysis, aspect based sentiment analysis, semantic analysis, machine translation, information retrieval, Word2Vec, topic modeling and cluster analysis.
NOT_RELEVANT;ScienceDirect;Temporal expression extraction with extensive feature type selection and a posteriori label adjustment;Michele Filannino and Goran Nenadic;2015;10.1016/j.datak.2015.09.002;https://doi.org/10.1016/j.datak.2015.09.002;The automatic extraction of temporal information from written texts is pivotal for many Natural Language Processing applications such as question answering, text summarisation and information retrieval. It allows to filter information and infer temporal flows of events. This paper presents ManTIME, a general domain temporal expression identification and normalisation system, and systematically explores the impact of different features and training corpora on the performance. The identification phase combines the use of conditional random fields along with a post-processing pipeline, whereas the normalisation phase is carried out using NorMA, an open-source rule-based temporal normaliser. We investigate the performance variation with respect to different feature types. Specifically, we show that the use of WordNet-based features in the identification task negatively affects the overall performance, and that there is no statistically significant difference in the results based on gazetteers, shallow parsing and propositional noun phrases labels on top of the morpho-lexical features. We also show that the use of silver data (alone or in addition to the human-annotated ones) does not improve the performance. We evaluate six combinations of training data and post-processing pipeline with respect to the TempEval-3 benchmark test set. The best run achieved 0.95 (precision), 0.85 (recall) and 0.90 (Fβ=1) in the identification phase. Normalisation accuracies are 0.86 (for type attribute) and 0.77 (for value attribute). The proposed approach ranked 3rd in the TempEval-3 challenge (task A) as the best performing machine learning-based system among 21 participants.
NOT_RELEVANT;ScienceDirect;A Weighted Multi-Layer Analytics Based Model for Emoji Recommendation;Amira M. Idrees and Abdul Lateef Marzouq Al-Solami;2024;10.32604/cmc.2023.046457;https://doi.org/10.32604/cmc.2023.046457;The developed system for eye and face detection using Convolutional Neural Networks (CNN) models, followed by eye classification and voice-based assistance, has shown promising potential in enhancing accessibility for individuals with visual impairments. The modular approach implemented in this research allows for a seamless flow of information and assistance between the different components of the system. This research significantly contributes to the field of accessibility technology by integrating computer vision, natural language processing, and voice technologies. By leveraging these advancements, the developed system offers a practical and efficient solution for assisting blind individuals. The modular design ensures flexibility, scalability, and ease of integration with existing assistive technologies. However, it is important to acknowledge that further research and improvements are necessary to enhance the system’s accuracy and usability. Fine-tuning the CNN models and expanding the training dataset can improve eye and face detection as well as eye classification capabilities. Additionally, incorporating real-time responses through sophisticated natural language understanding techniques and expanding the knowledge base of ChatGPT can enhance the system’s ability to provide comprehensive and accurate responses. Overall, this research paves the way for the development of more advanced and robust systems for assisting visually impaired individuals. By leveraging cutting-edge technologies and integrating them into a modular framework, this research contributes to creating a more inclusive and accessible society for individuals with visual impairments. Future work can focus on refining the system, addressing its limitations, and conducting user studies to evaluate its effectiveness and impact in real-world scenarios.
NOT_RELEVANT;ScienceDirect;Textual entailment as an evaluation metric for abstractive text summarization;Swagat Shubham Bhuyan and Saranga Kingkor Mahanta and Partha Pakray and Benoit Favre;2023;10.1016/j.nlp.2023.100028;https://doi.org/10.1016/j.nlp.2023.100028;Automated text summarization systems require to be heedful of the reader and the communication goals since it may be the determining component of whether the original textual content is actually worth reading in full. The summary can also assist enhance document indexing for information retrieval, and it is generally much less biased than a human-written summary. A crucial part while building intelligent systems is evaluating them. Consequently, the choice of evaluation metric(s) is of utmost importance. Current standard evaluation metrics like BLEU and ROUGE, although fairly effective for evaluation of extractive text summarization systems, become futile when it comes to comparing semantic information between two texts, i.e in abstractive summarization. We propose textual entailment as a potential metric to evaluate abstractive summaries. The results show the contribution of text entailment as a strong automated evaluation model for such summaries. The textual entailment scores between the text and generated summaries, and between the reference and predicted summaries were calculated, and an overall summarizer score was generated to give a fair idea of how efficient the generated summaries are. We put forward some novel methods that use the entailment scores and the final summarizer scores for a reasonable evaluation of the same across various scenarios. A Final Entailment Metric Score (FEMS) was generated to get an insightful idea in order to compare both the generated summaries.
NOT_RELEVANT;ScienceDirect;Topic Segmentation for Textual Document Written in Arabic Language;Anja Habacha Chaibi and Marwa Naili and Samia Sammoud;2014;10.1016/j.procs.2014.08.124;https://doi.org/10.1016/j.procs.2014.08.124;Topic segmentation is important for many natural language processing applications such as information retrieval, text summarization. In our work, we are interested in the topic segmentation of textual document. We present a survey of related works particularly C99 and TextTiling. Then, we propose an adaptation of these topic segmenters for textual document written in Arabic language named as ArabC99 and ArabTextTiling. For experimental results, we construct an Arabic corpus based on newspapers of different Arab countries. Finally, we evaluate the performance of these new segmenters by comparing them together and to related works using the metrics WindowDiff and F-measure.
NOT_RELEVANT;ScienceDirect;A Sequence-to-Sequence based Approach For the double Transliteration of Tunisian Dialect;Jihene Younes and Emna Souissi and Hadhemi Achour and Ahmed Ferchichi;2018;10.1016/j.procs.2018.10.481;https://doi.org/10.1016/j.procs.2018.10.481;Transliteration consists of automatically transforming a grapheme’s transcription from one writing system to another, while preserving its pronunciation. It is usually used in the context of machine translation and cross language information retrieval, mainly to deal with the issue of named entities and technical terms. In the case of some Arabic dialects, which are used on the social web in both Latin and Arabic scripts and which are still low-resource languages, transliteration is of great benefit for the automatic generation of various linguistic resources (parallel corpora and lexica), useful for their automatic processing. In this work, we focus on the Tunisian dialect transliteration. We propose a deep learning based Sequence-to-Sequence approach to perform a word-level transliteration of the user generated Tunisian dialect on the social web, in both Latin to Arabic and Arabic to Latin senses.
NOT_RELEVANT;ScienceDirect;Degree centrality for semantic abstraction summarization of therapeutic studies;Han Zhang and Marcelo Fiszman and Dongwook Shin and Christopher M. Miller and Graciela Rosemblat and Thomas C. Rindflesch;2011;10.1016/j.jbi.2011.05.001;https://doi.org/10.1016/j.jbi.2011.05.001;Automatic summarization has been proposed to help manage the results of biomedical information retrieval systems. Semantic MEDLINE, for example, summarizes semantic predications representing assertions in MEDLINE citations. Results are presented as a graph which maintains links to the original citations. Graphs summarizing more than 500 citations are hard to read and navigate, however. We exploit graph theory for focusing these large graphs. The method is based on degree centrality, which measures connectedness in a graph. Four categories of clinical concepts related to treatment of disease were identified and presented as a summary of input text. A baseline was created using term frequency of occurrence. The system was evaluated on summaries for treatment of five diseases compared to a reference standard produced manually by two physicians. The results showed that recall for system results was 72%, precision was 73%, and F-score was 0.72. The system F-score was considerably higher than that for the baseline (0.47).
NOT_RELEVANT;ScienceDirect;Using the contextual language model BERT for multi-criteria classification of scientific articles;Ashwin Karthik Ambalavanan and Murthy V. Devarakonda;2020;10.1016/j.jbi.2020.103578;https://doi.org/10.1016/j.jbi.2020.103578;"Background
Finding specific scientific articles in a large collection is an important natural language processing challenge in the biomedical domain. Systematic reviews and interactive article search are the type of downstream applications that benefit from addressing this problem. The task often involves screening articles for a combination of selection criteria. While machine learning was previously used for this purpose, it is not known if different criteria should be modeled together or separately in an ensemble model. The performance impact of the modern contextual language models on the task is also not known.
Methods
We framed the problem as text classification and conducted experiments to compare ensemble architectures, where the selection criteria were mapped to the components of the ensemble. We proposed a novel cascade ensemble analogous to the step-wise screening process employed in developing the gold standard. We compared performance of the ensembles with a single integrated model, which we refer to as the individual task learner (ITL). We used SciBERT, a variant of BERT pre-trained on scientific articles, and conducted experiments using a manually annotated dataset of ~49 K MEDLINE abstracts, known as Clinical Hedges.
Results
The cascade ensemble had significantly higher precision (0.663 vs. 0.388 vs. 0.478 vs. 0.320) and F measure (0.753 vs. 0.553 vs. 0.628 vs. 0.477) than ITL and ensembles using Boolean logic and a feed-forward network. However, ITL had significantly higher recall than the other classifiers (0.965 vs. 0.872 vs. 0.917 vs. 0.944). In fixed high recall studies, ITL achieved 0.509 precision @ 0.970 recall and 0.381 precision @ 0.985 recall on a subset that was studied earlier, and 0.295 precision @ 0.985 recall on the full dataset, all of which were improvements over the previous studies.
Conclusion
Pre-trained neural contextual language models (e.g. SciBERT) performed well for screening scientific articles. Performance at high fixed recall makes the single integrated model (ITL) more suitable among the architectures considered here, for systematic reviews. However, high F measure of the cascade ensemble makes it a better approach for interactive search applications. The effectiveness of the cascade ensemble architecture suggests broader applicability beyond this task and the dataset, and the approach is analogous to query optimization in Information Retrieval and query optimization in databases."
NOT_RELEVANT;ScienceDirect;PICO element detection in medical text without metadata: Are first sentences enough?;Ke-Chun Huang and I-Jen Chiang and Furen Xiao and Chun-Chih Liao and Charles Chih-Ho Liu and Jau-Min Wong;2013;10.1016/j.jbi.2013.07.009;https://doi.org/10.1016/j.jbi.2013.07.009;Efficient identification of patient, intervention, comparison, and outcome (PICO) components in medical articles is helpful in evidence-based medicine. The purpose of this study is to clarify whether first sentences of these components are good enough to train naive Bayes classifiers for sentence-level PICO element detection. We extracted 19,854 structured abstracts of randomized controlled trials with any P/I/O label from PubMed for naive Bayes classifiers training. Performances of classifiers trained by first sentences of each section (CF) and those trained by all sentences (CA) were compared using all sentences by ten-fold cross-validation. The results measured by recall, precision, and F-measures show that there are no significant differences in performance between CF and CA for detection of O-element (F-measure=0.731±0.009 vs. 0.738±0.010, p=0.123). However, CA perform better for I-elements, in terms of recall (0.752±0.012 vs. 0.620±0.007, p<0.001) and F-measures (0.728±0.006 vs. 0.662±0.007, p<0.001). For P-elements, CF have higher precision (0.714±0.009 vs. 0.665±0.010, p<0.001), but lower recall (0.766±0.013 vs. 0.811±0.012, p<0.001). CF are not always better than CA in sentence-level PICO element detection. Their performance varies in detecting different elements.
NOT_RELEVANT;ScienceDirect;A multi-technique approach to bridge electronic case report form design and data standard adoption;Ching-Heng Lin and Nai-Yuan Wu and Der-Ming Liou;2015;10.1016/j.jbi.2014.08.013;https://doi.org/10.1016/j.jbi.2014.08.013;"Background and objective
The importance of data standards when integrating clinical research data has been recognized. The common data element (CDE) is a consensus-based data element for data harmonization and sharing between clinical researchers, it can support data standards adoption and mapping. However, the lack of a suitable methodology has become a barrier to data standard adoption. Our aim was to demonstrate an approach that allowed clinical researchers to design electronic case report forms (eCRFs) that complied with the data standard.
Methods
We used a multi-technique approach, including information retrieval, natural language processing and an ontology-based knowledgebase to facilitate data standard adoption using the eCRF design. The approach took research questions as query texts with the aim of retrieving and associating relevant CDEs with the research questions.
Results
The approach was implemented using a CDE-based eCRF builder, which was evaluated using CDE- related questions from CRFs used in the Parkinson Disease Biomarker Program, as well as CDE-unrelated questions from a technique support website. Our approach had a precision of 0.84, a recall of 0.80, a F-measure of 0.82 and an error of 0.31. Using the 303 testing CDE-related questions, our approach responded and provided suggested CDEs for 88.8% (269/303) of the study questions with a 90.3% accuracy (243/269). The reason for any missed and failed responses was also analyzed.
Conclusion
This study demonstrates an approach that helps to cross the barrier that inhibits data standard adoption in eCRF building and our evaluation reveals the approach has satisfactory performance. Our CDE-based form builder provides an alternative perspective regarding data standard compliant eCRF design."
NOT_RELEVANT;ScienceDirect;Architecture of an expert system for composite document analysis, representation, and retrieval;Edward A Fox and Robert K France;1987;10.1016/0888-613X(87)90012-0;https://doi.org/10.1016/0888-613X(87)90012-0;The CODER (COmposite Document Expert/extended/effective Retrieval) project is a multi-yeare effort to investigate how best to apply artificial intelligence methods to increase the effectiveness of information retrieval systems handling collections of composite documents. To ensure system adaptability and to allow controlled experimentation, CODER has been designed as a distributed expert system. The use of individually tailored specialist experts, coupled with standardized blackboard modules for communication and control and external knowledge bases for maintenance of factual world knowledge, allows for quick prototyping, incremental development, and flexibility under change. The system as a whole is being implemented under UNIX as a set of MU-Prolog and C modules communicating through pipes and TCP/IP sockets.
NOT_RELEVANT;ScienceDirect;POS Tagging for Arabic Text Using Bee Colony Algorithm;Ahmad Alhasan and Ahmad T. Al-Taani;2018;10.1016/j.procs.2018.10.471;https://doi.org/10.1016/j.procs.2018.10.471;Part-of-Speech (POS) Tagging is the process of automatically determining the proper grammatical tag or syntactic category of a word depending on a its context. POS Tagging is an essential step in most Natural Language Processing (NLP) applications such as text summarization, question answering, information extraction and information retrieval. In this study, we propose an efficient tagging approach for the Arabic language using Bee Colony Optimization algorithm. The problem is represented as a graph and a novel technique is proposed to assign scores to possible tags of a sentence, then the bees find the best solution path. The proposed approach is evaluated using KALIMAT corpus which consists of 18M words. Experimental results showed that the proposed approach achieved 98.2% of accuracy compared to 98%, 97.4% and 94.6% for Hybrid, Hidden Markov Model and Rule-Based methods respectively. Furthermore, the proposed approach determined all the tags presented in the corpus while the mentioned approaches can identify only three tags.
NOT_RELEVANT;ScienceDirect;A Part-Of-Speech term weighting scheme for biomedical information retrieval;Yanshan Wang and Stephen Wu and Dingcheng Li and Saeed Mehrabi and Hongfang Liu;2016;10.1016/j.jbi.2016.08.026;https://doi.org/10.1016/j.jbi.2016.08.026;"In the era of digitalization, information retrieval (IR), which retrieves and ranks documents from large collections according to users’ search queries, has been popularly applied in the biomedical domain. Building patient cohorts using electronic health records (EHRs) and searching literature for topics of interest are some IR use cases. Meanwhile, natural language processing (NLP), such as tokenization or Part-Of-Speech (POS) tagging, has been developed for processing clinical documents or biomedical literature. We hypothesize that NLP can be incorporated into IR to strengthen the conventional IR models. In this study, we propose two NLP-empowered IR models, POS-BoW and POS-MRF, which incorporate automatic POS-based term weighting schemes into bag-of-word (BoW) and Markov Random Field (MRF) IR models, respectively. In the proposed models, the POS-based term weights are iteratively calculated by utilizing a cyclic coordinate method where golden section line search algorithm is applied along each coordinate to optimize the objective function defined by mean average precision (MAP). In the empirical experiments, we used the data sets from the Medical Records track in Text REtrieval Conference (TREC) 2011 and 2012 and the Genomics track in TREC 2004. The evaluation on TREC 2011 and 2012 Medical Records tracks shows that, for the POS-BoW models, the mean improvement rates for IR evaluation metrics, MAP, bpref, and P@10, are 10.88%, 4.54%, and 3.82%, compared to the BoW models; and for the POS-MRF models, these rates are 13.59%, 8.20%, and 8.78%, compared to the MRF models. Additionally, we experimentally verify that the proposed weighting approach is superior to the simple heuristic and frequency based weighting approaches, and validate our POS category selection. Using the optimal weights calculated in this experiment, we tested the proposed models on the TREC 2004 Genomics track and obtained average of 8.63% and 10.04% improvement rates for POS-BoW and POS-MRF, respectively. These significant improvements verify the effectiveness of leveraging POS tagging for biomedical IR tasks."
NOT_RELEVANT;ScienceDirect;Arabic light-based stemmer using new rules;Hamood Alshalabi and Sabrina Tiun and Nazlia Omar and Fatima N. AL-Aswadi and Kamal {Ali Alezabi};2022;10.1016/j.jksuci.2021.08.017;https://doi.org/10.1016/j.jksuci.2021.08.017;Superior stemming algorithms aid significantly in many natural language processing (NLP) applications such as information retrieval. Arabic light-based stemmer is one of the most important stemming algorithms. However, partially due to the highly inflected and complexity of Arabic language morphological structure, most of the existing Arabic light-based stemmer algorithms eliminate a few numbers of suffixes and prefixes or both in the process of recognising the infix patterns to determine roots. The elimination of suffixes and prefixes leads to many inefficient results. Hence, this study aims to develop an improved light-based algorithm of the Arabic stemmer by proposing an appropriate suffixes and prefixes list, which is supported by rules according to word length (without using a morpheme or patterns on a stem). Our improved Dlight Arabic stemmer focuses on determining and removing the infix patterns under many rules on length-words and according to a specific order of the stages of the stemming to extract the double, triple and quadruple roots from long and short Arabic words. To evaluate our proposed light-based Arabic stemmer, we compared our stemmer against existing Arabic stemmers, namely Light10, Condlight and ARLST. The experimental results showed the proposed Develop Arabic Light-Based Stemmer (Dlight) obtained the best performance with 68% of F-measure, while the other three Arabic stemmers yield slightly lower F-measure. Finally, establishing an appropriate list of suffixes and prefixes with word length rules to stem Arabic words can improve the performance of a light-based Arabic stemmer.
NOT_RELEVANT;EBSCOhost;A case study of duplications detection for educational domain thorough ad hoc search and identification NLP-based method.;"Mikhaylov, S.N.; Chuikova, V.V.; Sokolova, Marina V.; Potapenko, A.M.";NOT_FOUND;10.1111/exsy.12200;NOT_FOUND;During the organization and planning of lecture courses for a discipline, its content may be overlapped and partially delivered in more than one course. Sometimes this action causes time loss through unnecessary repeating. This paper introduces an automated tool for duplications detections adapting methods of natural language processing used for Web search. The experiment for unstructured electronic document repositories clustering for thematic duplicate identification in different documents in the case of educational domain is presented. A prototype of this Web service-based software search engine is being designed and discussed. The experiment aimed to identify thematic duplicates of various courses within one of the teaching disciplines is also presented. [ABSTRACT FROM AUTHOR] Copyright of Expert Systems is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;A Novel classification framework for the Thirukkural for building an efficient search system.;"Ramalingam, Anita; Navaneethakrishnan, Subalalitha Chinnaudayar";NOT_FOUND;10.3233/JIFS-211667;NOT_FOUND;Thirukkural, a Tamil classic literature, which was written in 300 BCE is a didactic literature. Though Thirukkural comprises 1330 couplets which are organized into three sections and 133 chapters, in order to retrieve meaningful Thirukkural for a given query in search systems, a better organization of the Thirukkural is needed. This paper lays such a foundation by classifying the Thirukkural into ten new categories called superclasses that is helpful for building a better Information Retrieval (IR) system. The classifier is trained using Multinomial Naïve Bayes algorithm. Each superclass is further classified into two subcategories based on the didactic information. The proposed classification framework is evaluated using precision, recall and F-score metrics and achieved an overall F-score of 82.33% and a comparison analysis has been done with the Support Vector Machine, Logistic Regression and Random Forest algorithms. An IR system is built on top of the proposed system and the performance comparison has been done with the Google search and a locally built keyword search. The proposed classification framework has achieved a mean average precision score of 89%, whereas the Google search and keyword search have yielded 59% and 68% respectively. [ABSTRACT FROM AUTHOR] Copyright of Journal of Intelligent & Fuzzy Systems is the property of IOS Press and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;A novel method for providing relational databases with rich semantics and natural language processing.;"Hamaz, Kamal; Benchikha, Fouzia";NOT_FOUND;10.1108/JEIM-01-2015-0005;NOT_FOUND;Purpose With the development of systems and applications, the number of users interacting with databases has increased considerably. The relational database model is still considered as the most used model for data storage and manipulation. However, it does not offer any semantic support for the stored data which can facilitate data access for the users. Indeed, a large number of users are intimidated when retrieving data because they are non-technical or have little technical knowledge. To overcome this problem, researchers are continuously developing new techniques for Natural Language Interfaces to Databases (NLIDB). Nowadays, the usage of existing NLIDBs is not widespread due to their deficiencies in understanding natural language (NL) queries. In this sense, the purpose of this paper is to propose a novel method for an intelligent understanding of NL queries using semantically enriched database sources.Design/methodology/approach First a reverse engineering process is applied to extract relational database hidden semantics. In the second step, the extracted semantics are enriched further using a domain ontology. After this, all semantics are stored in the same relational database. The phase of processing NL queries uses the stored semantics to generate a semantic tree.Findings The evaluation part of the work shows the advantages of using a semantically enriched database source to understand NL queries. Additionally, enriching a relational database has given more flexibility to understand contextual and synonymous words that may be used in a NL query.Originality/value Existing NLIDBs are not yet a standard option for interfacing a relational database due to their lack for understanding NL queries. Indeed, the techniques used in the literature have their limits. This paper handles those limits by identifying the NL elements by their semantic nature in order to generate a semantic tree. This last is a key solution towards an intelligent understanding of NL queries to relational databases. [ABSTRACT FROM AUTHOR] Copyright of Journal of Enterprise Information Management is the property of Emerald Publishing Limited and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;A Review and Future Perspectives of Arabic Question Answering Systems.;"Ray, Santosh K.; Shaalan, Khaled";NOT_FOUND;10.1109/TKDE.2016.2607201;NOT_FOUND;Question Answering Systems (QASs) have emerged as a good alternative for information seekers to retrieve precise information over the Internet. A good amount of research has been done to improve the performance of QASs across several languages, including European and Asian languages. However, Arabic, a morphologically rich Semitic language spoken by over 422 million people, has not seen similar development in the field of question answering. This article reviews the developments taking place in Arabic QASs as well as the challenges faced by researchers in developing Arabic QASs. After conducting an extensive literature survey of a number of English and Arabic QASs, this article classifies them according to several criteria. The most commonly used architecture for the development of an Arabic QAS, known as pipeline architecture, has been presented. In order to encourage and support the new researchers and scholars in conducting research in Arabic QASs, a list of techniques, tools, and computational linguistic resources, required to implement the components of the presented pipelined architecture, are described in this article in a simple and persuasive manner. Finally, the gap analysis between the research in Arabic and English QASs has been performed and accordingly, some future directions for research in Arabic QASs have been proposed. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Knowledge & Data Engineering is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;A Review of Modern Fashion Recommender Systems.;"DELDJOO, YASHAR; NAZARY, FATEMEH; RAMISA, ARNAU; MCAULEY, JULIAN; PELLEGRINI, GIOVANNI; BELLOGIN, ALEJANDRO; DI NOIA, TOMMASO";NOT_FOUND;10.1145/3624733;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;EBSCOhost;Accounting fraud detection using contextual language learning.;"Bhattacharya, Indranil; Mickovic, Ana";NOT_FOUND;10.1016/j.accinf.2024.100682;NOT_FOUND;• Financial reporting text is an important source of information for fraud detection. • Application of BERT in accounting fraud detection setting. • Contextual learning improves accounting fraud detection relative to benchmarks. • Final model outperforms existing textual and quantitative benchmark models. • We provide practical insights for financial investigators. Accounting fraud is a widespread problem that causes significant damage in the economic market. Detection and investigation of fraudulent firms require a large amount of time, money, and effort for corporate monitors and regulators. In this study, we explore how textual contents from financial reports help in detecting accounting fraud. Pre-trained contextual language learning models, such as BERT, have significantly advanced natural language processing in recent years. We fine-tune the BERT model on Management Discussion and Analysis (MD&A) sections of annual 10-K reports from the Securities and Exchange Commission (SEC) database. Our final model outperforms the textual benchmark model and the quantitative benchmark model from the previous literature by 15% and 12%, respectively. Further, our model identifies five times more fraudulent firm-year observations than the textual benchmark by investigating the same number of firms, and three times more than the quantitative benchmark. Optimizing this investigation process, where more fraudulent observations are detected in the same size of the investigation sample, would be of great economic significance for regulators, investors, financial analysts, and auditors. [ABSTRACT FROM AUTHOR] Copyright of International Journal of Accounting Information Systems is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;An augmented semantic search tool for multilingual news analytics.;"Harikumar, Sandhya; Sathyajit, Rohit; Karumudi, Gnana Venkata Naga Sai Kalyan";NOT_FOUND;10.3233/JIFS-221184;NOT_FOUND;News feeds generate colossal amount of data consisting of important information hidden in the intricacies. State of the art methods are still at infancy in providing a very generic and publicly available solution to skim through the important information in the news from various sources and an ability to search using specific keywords in different languages. This paper focuses on designing a tool to extract semantic details from news articles published through various internet sources in various languages. The semantic information is stored within DBMS for ease of organizing and retrieving the data. Further, a querying facility to search through entire articles based on the keyword or date-based search is also proposed to view the crisp content. The news articles in English, and two Indian languages - Hindi and Malayalam are considered for experimentation. The proposed strategy consists of two main components namely, Generative model creation and Query engine. Generative model aims to extract important entities and keywords along with their relevance to the article and other similar articles using Latent Dirichlet Allocation(LDA) and Named Entity Recognition(NER). Query engine is to facilitate on the fly retrieval of semantic content from the database, based on user keyword. The search engine, along with database indexing, reduces the access time to the database thereby retrieving the information in less time. Experimental results show that the proposed method is effective in terms of quality of information and time consumed for information retrieval. [ABSTRACT FROM AUTHOR] Copyright of Journal of Intelligent & Fuzzy Systems is the property of IOS Press and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;An ensemble clustering approach for topic discovery using implicit text segmentation.;"Memon, Muhammad Qasim; Lu, Yu; Chen, Penghe; Memon, Aasma; Pathan, Muhammad Salman; Zardari, Zulfiqar Ali";NOT_FOUND;10.1177/0165551520911590;NOT_FOUND;Text segmentation (TS) is the process of dividing multi-topic text collections into cohesive segments using topic boundaries. Similarly, text clustering has been renowned as a major concern when it comes to multi-topic text collections, as they are distinguished by sub-topic structure and their contents are not associated with each other. Existing clustering approaches follow the TS method which relies on word frequencies and may not be suitable to cluster multi-topic text collections. In this work, we propose a new ensemble clustering approach (ECA) is a novel topic-modelling-based clustering approach, which induces the combination of TS and text clustering. We improvised a LDA-onto (LDA-ontology) is a TS-based model, which presents a deterioration of a document into segments (i.e. sub-documents), wherein each sub-document is associated with exactly one sub-topic. We deal with the problem of clustering when it comes to a document that is intrinsically related to various topics and its topical structure is missing. ECA is tested through well-known datasets in order to provide a comprehensive presentation and validation of clustering algorithms using LDA-onto. ECA exhibits the semantic relations of keywords in sub-documents and resultant clusters belong to original documents that they contain. Moreover, present research sheds the light on clustering performances and it indicates that there is no difference over performances (in terms of F -measure) when the number of topics changes. Our findings give above par results in order to analyse the problem of text clustering in a broader spectrum without applying dimension reduction techniques over high sparse data. Specifically, ECA provides an efficient and significant framework than the traditional and segment-based approach, such that achieved results are statistically significant with an average improvement of over 10.2%. For the most part, proposed framework can be evaluated in applications where meaningful data retrieval is useful, such as document summarization, text retrieval, novelty and topic detection. [ABSTRACT FROM AUTHOR] Copyright of Journal of Information Science is the property of Sage Publications, Ltd. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;An improved Urdu stemming algorithm for text mining based on multi-step hybrid approach.;"Jabbar, Abdul; Iqbal, Sajid; Akhunzada, Adnan; Abbas, Qaisar";NOT_FOUND;10.1080/0952813X.2018.1467495;NOT_FOUND;"Stemming is the basic operation in Natural language processing (NLP) to remove derivational and inflectional affixes without performing a morphological analysis. This practice is essential to extract the root or stem. In NLP domains, the stemmer is used to improve the process of information retrieval (IR), text classifications (TC), text mining (TM) and related applications. In particular, Urdu stemmers utilize only uni-gram words from the input text by ignoring bigrams, trigrams, and n-gram words. To improve the process and efficiency of stemming, bigrams and trigram words must be included. Despite this fact, there are a few developed methods for Urdu stemmers in the past studies. Therefore, in this paper, we proposed an improved Urdu stemmer, using hybrid approach divided into multi-step operation, to deal with unigram, bigram, and trigram features as well. To evaluate the proposed Urdu stemming method, we have used two corpora; word corpus and text corpus. Moreover, two different evaluation metrics have been applied to measure the performance of the proposed algorithm. The proposed algorithm achieved an accuracy of 92.97% and compression rate of 55%. These experimental results indicate that the proposed system can be used to increase the effectiveness and efficiency of the Urdu stemmer for better information retrieval and text mining applications. [ABSTRACT FROM AUTHOR] Copyright of Journal of Experimental & Theoretical Artificial Intelligence is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)"
NOT_RELEVANT;EBSCOhost;An open source and modular search engine for biomedical literature retrieval.;"Almeida, Hayda; Jean‐Louis, Ludovic; Meurs, Marie‐Jean";NOT_FOUND;10.1111/coin.12125;NOT_FOUND;Abstract: This work presents the bioMine system, a full‐text natural language search engine for biomedical literature. bioMine provides search capabilities based on the full‐text content of documents belonging to a database composed of scientific articles and allows users to submit their search queries using natural language. Beyond the text content of articles, the system engine also uses article metadata, empowering the search by considering extra information from picture and table captions. bioMine is publicly released as an open‐source system under the MIT license. [ABSTRACT FROM AUTHOR] Copyright of Computational Intelligence is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Applications of natural language processing in software traceability: A systematic mapping study.;"Pauzi, Zaki; Capiluppi, Andrea";NOT_FOUND;10.1016/j.jss.2023.111616;NOT_FOUND;A key part of software evolution and maintenance is the continuous integration from collaborative efforts, often resulting in complex traceability challenges between software artifacts: features and modules remain scattered in the source code, and traceability links become harder to recover. In this paper, we perform a systematic mapping study dealing with recent research recovering these links through information retrieval, with a particular focus on natural language processing (NLP). Our search strategy gathered a total of 96 papers in focus of our study, covering a period from 2013 to 2021. We conducted trend analysis on NLP techniques and tools involved, and traceability efforts (applying NLP) across the software development life cycle (SDLC). Based on our study, we have identified the following key issues, barriers, and setbacks: syntax convention, configuration, translation, explainability, properties representation, tacit knowledge dependency, scalability, and data availability. Based on these, we consolidated the following open challenges: representation similarity across artifacts, the effectiveness of NLP for traceability, and achieving scalable, adaptive, and explainable models. To address these challenges, we recommend a holistic framework for NLP solutions to achieve effective traceability and efforts in achieving interoperability and explainability in NLP models for traceability. [Display omitted] • Our search strategy across multiple library databases gathered a total of 96 papers. • Trend analysis was conducted throughout the years 2013 to 2021. • Our study highlighted open challenges from key issues, barriers, and setbacks. • We proposed two key recommendations to address these open challenges. [ABSTRACT FROM AUTHOR] Copyright of Journal of Systems & Software is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Automated arabic text classification with P- Stemmer, machine learning, and a tailored news article taxonomy.;"Kanan, Tarek; Fox, Edward A.";NOT_FOUND;10.1002/asi.23609;NOT_FOUND;Arabic news articles in electronic collections are difficult to study. Browsing by category is rarely supported. Although helpful machine-learning methods have been applied successfully to similar situations for English news articles, limited research has been completed to yield suitable solutions for Arabic news. In connection with a Qatar National Research Fund ( QNRF)-funded project to build digital library community and infrastructure in Qatar, we developed software for browsing a collection of about 237,000 Arabic news articles, which should be applicable to other Arabic news collections. We designed a simple taxonomy for Arabic news stories that is suitable for the needs of Qatar and other nations, is compatible with the subject codes of the International Press Telecommunications Council, and was enhanced with the aid of a librarian expert as well as five Arabic-speaking volunteers. We developed tailored stemming (i.e., a new Arabic light stemmer called P- Stemmer) and automatic classification methods (the best being binary Support Vector Machines classifiers) to work with the taxonomy. Using evaluation techniques commonly used in the information retrieval community, including 10-fold cross-validation and the Wilcoxon signed-rank test, we showed that our approach to stemming and classification is superior to state-of-the-art techniques. [ABSTRACT FROM AUTHOR] Copyright of Journal of the Association for Information Science & Technology is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Automatic smart contract comment generation via large language models and in-context learning.;"Zhao, Junjie; Chen, Xiang; Yang, Guang; Shen, Yiheng";NOT_FOUND;10.1016/j.infsof.2024.107405;NOT_FOUND;Designing effective automatic smart contract comment generation approaches can facilitate developers' comprehension, boosting smart contract development and improving vulnerability detection. The previous approaches can be divided into two categories: fine-tuning paradigm-based approaches and information retrieval-based approaches. However, for the fine-tuning paradigm-based approaches, the performance may be limited by the quality of the gathered dataset for the downstream task and they may have knowledge-forgetting issues, which can reduce the generality of the fine-tuned model. While for the information retrieval-based approaches, it is difficult for them to generate high-quality comments if similar code does not exist in the historical repository. Therefore we want to utilize the domain knowledge related to smart contract code comment generation in large language models (LLMs) to alleviate the disadvantages of these two types of approaches. In this study, we propose an approach SCCLLM based on LLMs and in-context learning. Specifically, in the demonstration selection phase, SCCLLM retrieves the top- k code snippets from the historical corpus by considering syntax, semantics, and lexical information. In the in-context learning phase, SCCLLM utilizes the retrieved code snippets as demonstrations for in-context learning, which can help to utilize the related knowledge for this task in the LLMs. In the LLMs inference phase, the input is the target smart contract code snippet, and the output is the corresponding comment generated by the LLMs. We select a large corpus from a smart contract community Etherscan.io as our experimental subject. Extensive experimental results show the effectiveness of SCCLLM when compared with baselines in automatic evaluation and human evaluation. We also show the rationality of our customized demonstration selection strategy in SCCLLM by ablation studies. Our study shows using LLMs and in-context learning is a promising direction for automatic smart contract comment generation, which calls for more follow-up studies. [ABSTRACT FROM AUTHOR] Copyright of Information & Software Technology is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;BIMASR: Framework for Voice-Based BIM Information Retrieval.;"Shin, Sangyun; Issa, Raja R. A.";NOT_FOUND;10.1061/(ASCE)CO.1943-7862.0002138;NOT_FOUND;Voice is the most convenient means for human beings to communicate with others, even if the objects of their communication are not other humans but machines or computers. Many industries, and even the architecture, engineering, construction, and operations (AECO) industry, have attempted to study and apply speech recognition systems in their operations to improve work efficiency and productivity. However, previous studies on speech recognition had two limitations: they used keywords requiring basic knowledge of building information modeling (BIM) commands for using them and in searching BIM data, they relied on the Industry Foundation Classes (IFC) format, which involves converting BIM data to IFC. Such methods did not conduce to direct retrieval in BIM software. In the latter case, data search was possible, but data manipulation was not. To improve on the limitations of previous studies, this study developed a building information modeling automatic speech recognition (BIMASR) framework that requires no knowledge of BIM commands, which allows for the input of natural language (NL)-based questions into BIM software using human voice to search and manipulate data. The framework consists of three modules: one for voice recognition, one for natural language processing (syntax and semantic analysis), and one for BIM data preprocessing and interworking with relational databases. The manipulation of BIM data with NL-based speech recognition converts the BIM operating environment from an expert-oriented into a user-oriented environment. This conversion allows for more BIM interaction and the popularization of BIM use and enhances the use of BIM in dynamic environments such as virtual reality, augmented reality, and holograms, where conventional input devices are typically absent. [ABSTRACT FROM AUTHOR] Copyright of Journal of Construction Engineering & Management is the property of American Society of Civil Engineers and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Biomedical named entity recognition and linking datasets: survey and our recent development.;"Huang, Ming-Siang; Lai, Po-Ting; Lin, Pei-Yen; You, Yu-Ting; Tsai, Richard Tzong-Han; Hsu, Wen-Lian";NOT_FOUND;10.1093/bib/bbaa054;NOT_FOUND;"Natural language processing (NLP) is widely applied in biological domains to retrieve information from publications. Systems to address numerous applications exist, such as biomedical named entity recognition (BNER), named entity normalization (NEN) and protein–protein interaction extraction (PPIE). High-quality datasets can assist the development of robust and reliable systems; however, due to the endless applications and evolving techniques, the annotations of benchmark datasets may become outdated and inappropriate. In this study, we first review commonlyused BNER datasets and their potential annotation problems such as inconsistency and low portability. Then, we introduce a revised version of the JNLPBA dataset that solves potential problems in the original and use state-of-the-art named entity recognition systems to evaluate its portability to different kinds of biomedical literature, including protein–protein interaction and biology events. Lastly, we introduce an ensembled biomedical entity dataset (EBED) by extending the revised JNLPBA dataset with PubMed Central full-text paragraphs, figure captions and patent abstracts. This EBED is a multi-task dataset that covers annotations including gene, disease and chemical entities. In total, it contains 85000 entity mentions, 25000 entity mentions with database identifiers and 5000 attribute tags. To demonstrate the usage of the EBED, we review the BNER track from the AI CUP Biomedical Paper Analysis challenge. Availability: The revised JNLPBA dataset is available at https://iasl-btm.iis.sinica.edu.tw/BNER/Content/Re vised_JNLPBA.zip. The EBED dataset is available at https://iasl-btm.iis.sinica.edu.tw/BNER/Content/AICUP _EBED_dataset.rar. Contact: Email: thtsai@g.ncu.edu.tw , Tel. 886-3-4227151 ext. 35203, Fax: 886-3-422-2681 Email: hsu@iis.sinica.edu.tw , Tel. 886-2-2788-3799 ext. 2211, Fax: 886-2-2782-4814 Supplementary information: Supplementary data are available at Briefings in Bioinformatics online. [ABSTRACT FROM AUTHOR] Copyright of Briefings in Bioinformatics is the property of Oxford University Press / USA and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)"
NOT_RELEVANT;EBSCOhost;Building siamese attention-augmented recurrent convolutional neural networks for document similarity scoring.;"Han, Sifei; Shi, Lingyun; Richie, Russell; Tsui, Fuchiang R.";NOT_FOUND;10.1016/j.ins.2022.10.032;NOT_FOUND;• We proposed a new architecture - the Siamese attention-augmented recurrent convolutional neural network (S-ARCNN). • We compared the performance of S-ARCNN with eight popular models for measuring document similarity. • Our model outperformed the state-of-the-art Transformer based model (Sentence BERT) by over 5% in F1. • Simply fitting an optimal decision threshold can significantly improve pre-trained BERT model for the new task. • S-ARCNN performed best in longer question pairs (length > = 50 words). Automatically measuring document similarity is imperative in natural language processing, with applications ranging from recommendation to duplicate document detection. State-of-the-art approach in document similarity commonly involves deep neural networks, yet there is little study on how different architectures may be combined. Thus, we introduce the Siamese Attention-augmented Recurrent Convolutional Neural Network (S-ARCNN) that combines multiple neural network architectures. In each subnetwork of S-ARCNN, a document passes through a bidirectional Long Short-Term Memory (bi-LSTM) layer, which sends representations to local and global document modules. A local document module uses convolution, pooling, and attention layers, whereas a global document module uses last states of the bi-LSTM. Both local and global features are concatenated to form a single document representation. Using the Quora Question Pairs dataset, we evaluated S-ARCNN, Siamese convolutional neural networks (S-CNNs), Siamese LSTM, and two BERT models. While S-CNNs (82.02% F1) outperformed S-ARCNN (79.83% F1) overall, S-ARCNN slightly outperformed S-CNN on duplicate question pairs with more than 50 words (39.96% vs. 39.42% accuracy). With the potential advantage of S-ARCNN for processing longer documents, S-ARCNN may help researchers identify collaborators with similar research interests, help editors find potential reviewers, or match resumes with job descriptions. [ABSTRACT FROM AUTHOR] Copyright of Information Sciences is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Deep supervised hashing network with integrated regularisation.;"Liao, Jianxin; Li, Baoran; Yang, Di; Wang, Jingyu; Qi, Qi; Wang, Jing";NOT_FOUND;10.1049/iet-ipr.2018.6644;NOT_FOUND;Hashing has been widely deployed to approximate nearest neighbour search for large‐scale multimedia retrieval tasks due to storage and retrieval efficiency. State‐of‐the‐art supervised hashing methods for image retrieval construct deep structures to simultaneously learn image representation and generate good hash codes, and the key step among them is simultaneously learned feature representation and binary hash code. Existing methods use similarity and regularity loss to train deep hashing systems, but these two functions usually work together but not cooperative, which may lead to inadequate performance of the whole system. In this study, a new method for training deep hashing system to learn compact binary codes is presented. The deep supervised hashing network with integrated regularisation (DSHIR) system develop the zero division restriction as a new part of the loss function, which settles the problem of cooperatively guiding the system generate similarity preserving binary codes. DSHIR system also modifies the similarity handling loss to better extract features from image data, which promotes the performance compared to existing end‐to‐end deep hashing systems. Experiments show that DSHIR yields about 10 per cent higher mean average precision on CIFAR‐10 dataset, and also promote on other evaluation indexes compared with state‐of‐the‐art systems. [ABSTRACT FROM AUTHOR] Copyright of IET Image Processing (Wiley-Blackwell) is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Detecting new Chinese words from massive domain texts with word embedding.;"Qian, Yu; Du, Yang; Deng, Xiongwen; Ma, Baojun; Ye, Qiongwei; Yuan, Hua";NOT_FOUND;10.1177/0165551518786676;NOT_FOUND;"Textual information retrieval (TIR) is based on the relationship between word units. Traditional word segmentation techniques attempt to discern the word units accurately from texts; however, they are unable to appropriately and efficiently identify all new words. Identification of new words, especially in languages such as Chinese, remains a challenge. In recent years, word embedding methods have used numerical word vectors to retain the semantic and correlated information between words in a corpus. In this article, we propose the word-embedding-based method (WEBM), a novel method that combines word embedding and frequent n-gram string mining for discovering new words from domain corpora. First, we mapped all word units in a domain corpus to a high-dimension word vector space. Second, we used a frequent n-gram word string mining method to identify a set of candidates for new words. We designed a pruning strategy based on the word vectors to quantify the possibility of a word string being a new word, thereby allowing the evaluation of candidates based on the similarity of word units in the same string. In a comparative study, our experimental results revealed that WEBM had a great advantage in detecting new words from massive Chinese corpora. [ABSTRACT FROM AUTHOR] Copyright of Journal of Information Science is the property of Sage Publications, Ltd. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)"
NOT_RELEVANT;EBSCOhost;Developing intuitive and explainable algorithms through inspiration from human physiology and computational biology.;"Turki, Houcemeddine; Taieb, Mohamed Ali Hadj; Aouicha, Mohamed Ben";NOT_FOUND;10.1093/bib/bbab081;NOT_FOUND;In this letter, we explain how intuitive and explainable methods inspired from human physiology and computational biology can serve to simplify and ameliorate the way we process and generate knowledge resources. [ABSTRACT FROM AUTHOR] Copyright of Briefings in Bioinformatics is the property of Oxford University Press / USA and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Editorial Artificial Intelligence and Innovation Management.;"Tanev, Stoyan; Sandstrom, Gregory";NOT_FOUND;10.22215/timreview/1286;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;EBSCOhost;Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs.;"Malkov, Yu A.; Yashunin, D. A.";NOT_FOUND;10.1109/TPAMI.2018.2889473;NOT_FOUND;We present a new approach for the approximate K-nearest neighbor search based on navigable small world graphs with controllable hierarchy (Hierarchical NSW, HNSW). The proposed solution is fully graph-based, without any need for additional search structures (typically used at the coarse search stage of the most proximity graph techniques). Hierarchical NSW incrementally builds a multi-layer structure consisting of a hierarchical set of proximity graphs (layers) for nested subsets of the stored elements. The maximum layer in which an element is present is selected randomly with an exponentially decaying probability distribution. This allows producing graphs similar to the previously studied Navigable Small World (NSW) structures while additionally having the links separated by their characteristic distance scales. Starting the search from the upper layer together with utilizing the scale separation boosts the performance compared to NSW and allows a logarithmic complexity scaling. Additional employment of a heuristic for selecting proximity graph neighbors significantly increases performance at high recall and in case of highly clustered data. Performance evaluation has demonstrated that the proposed general metric space search index is able to strongly outperform previous opensource state-of-the-art vector-only approaches. Similarity of the algorithm to the skip list structure allows straightforward balanced distributed implementation. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Pattern Analysis & Machine Intelligence is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Efficient feature extraction model for validation performance improvement of duplicate bug report detection in software bug triage systems.;"Soleimani Neysiani, Behzad; Babamir, Seyed Morteza; Aritsugi, Masayoshi";NOT_FOUND;10.1016/j.infsof.2020.106344;NOT_FOUND;There are many duplicate bug reports in the semi-structured software repository of various software bug triage systems. The duplicate bug report detection (DBRD) process is a significant problem in software triage systems. The DBRD problem has many issues, such as efficient feature extraction to calculate similarities between bug reports accurately, building a high-performance duplicate detector model, and handling continuous real-time queries. Feature extraction is a technique that converts unstructured data to structured data. The main objective of this study is to improve the validation performance of DBRD using a feature extraction model. This research focuses on feature extraction to build a new general model containing all types of features. Moreover, it introduces a new feature extractor method to describe a new viewpoint of similarity between texts. The proposed method introduces new textual features based on the aggregation of term frequency and inverse document frequency of text fields of bug reports in uni-gram and bi-gram forms. Further, a new hybrid measurement metric is proposed for detecting efficient features, whereby it is used to evaluate the efficiency of all features, including the proposed ones. The validation performance of DBRD was compared for the proposed features and state-of-the-art features. To show the effectiveness of our model, we applied it and other related studies to DBRD of the Android, Eclipse, Mozilla, and Open Office datasets and compared the results. The comparisons showed that our proposed model achieved (i) approximately 2% improvement for accuracy and precision and more than 4.5% and 5.9% improvement for recall and F1-measure , respectively, by applying the linear regression (LR) and decision tree (DT) classifiers and (ii) a performance of 91%−99% (average ~97%) for the four metrics, by applying the DT classifier as the best classifier. Our proposed features improved the validation performance of DBRD concerning runtime performance. The pre-processing methods (primarily stemming) could improve the validation performance of DBRD slightly (up to 0.3%), but rule-based machine learning algorithms are more useful for the DBRD problem. The results showed that our proposed model is more effective both for the datasets for which state-of-the-art approaches were effective (i.e., Mozilla Firefox) and those for which state-of-the-art approaches were less effective (i.e., Android). The results also showed that the combination of all types of features could improve the validation performance of DBRD even for the LR classifier with less validation performance, which can be implemented easily for software bug triage systems. Without using the longest common subsequence (LCS) feature, which is effective but time-consuming, our proposed features could cover the effectiveness of LCS with lower time-complexity and runtime overhead. In addition, a statistical analysis shows that the results are reliable and can be generalized to other datasets or similar classifiers. [ABSTRACT FROM AUTHOR] Copyright of Information & Software Technology is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Excavating the mother lode of human-generated text: A systematic review of research that uses the wikipedia corpus.;"Mehdi, Mohamad; Okoli, Chitu; Mesgari, Mostafa; Nielsen, Finn Årup; Lanamäki, Arto";NOT_FOUND;10.1016/j.ipm.2016.07.003;NOT_FOUND;Although primarily an encyclopedia, Wikipedia’s expansive content provides a knowledge base that has been continuously exploited by researchers in a wide variety of domains. This article systematically reviews the scholarly studies that have used Wikipedia as a data source, and investigates the means by which Wikipedia has been employed in three main computer science research areas: information retrieval, natural language processing, and ontology building. We report and discuss the research trends of the identified and examined studies. We further identify and classify a list of tools that can be used to extract data from Wikipedia, and compile a list of currently available data sets extracted from Wikipedia. [ABSTRACT FROM AUTHOR] Copyright of Information Processing & Management is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Exploiting named entity recognition for improving syntactic-based web service discovery.;"Lizarralde, Ignacio; Mateos, Cristian; Rodriguez, Juan Manuel; Zunino, Alejandro";NOT_FOUND;10.1177/0165551518793321;NOT_FOUND;Web Services have become essential to the software industry as they represent reusable, remotely accessible functionality and data. Since Web Services must be discovered before being consumed, many discovery approaches applying classic Information Retrieval techniques, which store and process textual service descriptions, have arisen. These efforts are affected by term mismatch: a description relevant to a query can be retrieved only if they share many words. We present an approach to improve Web Service discoverability that automatically augments Web Service descriptions and can be used on top of such existing syntactic-based approaches. We exploit Named Entity Recognition to identify entities in descriptions and expand them with information from public text corpora, for example, Wikidata, mitigating term mismatch since it exploits both synonyms and hypernyms. We evaluated our approach together with classical syntactic-based service discovery approaches using a real 1274-service dataset, achieving up to 15.06% better Recall scores, and up to 17% Precision-at-1, 8% Precision-at-2 and 4% Precision-at-3. [ABSTRACT FROM AUTHOR] Copyright of Journal of Information Science is the property of Sage Publications, Ltd. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Feedback evaluations to promote image captioning.;"He, Jun; Zhao, Yijia; Sun, Bo; Yu, Lejun";NOT_FOUND;10.1049/iet-ipr.2019.1317;NOT_FOUND;"Image captioning can be treated as a policy gradient problem. A retrieval model to obtain the discriminability score to distinguish between two images, given the caption for one of them, has been proposed previously; the discriminability score and one of the image captioning evaluation metrics were optimised using policy gradient. Based on this, two methods to evaluate the caption and caption‐generating process, referred to as feedback evaluations, are proposed in this study. The results of the evaluations were used to improve the model. First, an auxiliary retrieval loss (ARL) is introduced to evaluate the generated caption to improve the discriminability of the model. ARL has been utilised as a feedback evaluation method because it calculates similarity between the generated caption and convolutional neural network features. With ARL, a higher similarity and better discriminability were achieved. Second, an evaluation reward is introduced to evaluate the captioning process. With ER, the overall evaluation metrics can be improved. A policy gradient was used, and a captioning model could be trained by jointly adjusting the captioning process and captioning itself. The attention long short‐term memory network was trained with ARL and ER successively and it demonstrated state‐of‐the‐art performance on the COCO database. [ABSTRACT FROM AUTHOR] Copyright of IET Image Processing (Wiley-Blackwell) is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)"
NOT_RELEVANT;EBSCOhost;Incorporating Deep Median Networks for Arabic Document Retrieval Using Word Embeddings-Based Query Expansion.;"Farhan, Yasir Hadi; Tareq, Mustafa Abd; Shakir, Mohanaad; Shannaq, Boumedyen";NOT_FOUND;10.1633/JISTaP.2024.12.3.3;NOT_FOUND;The information retrieval (IR) process often encounters a challenge known as query-document vocabulary mismatch, where user queries do not align with document content, impacting search effectiveness. Automatic query expansion (AQE) techniques aim to mitigate this issue by augmenting user queries with related terms or synonyms. Word embedding, particularly Word2Vec, has gained prominence for AQE due to its ability to represent words as real-number vectors. However, AQE methods typically expand individual query terms, potentially leading to query drift if not carefully selected. To address this, researchers propose utilizing median vectors derived from deep median networks to capture query similarity comprehensively. Integrating median vectors into candidate term generation and combining them with the BM25 probabilistic model and two IR strategies (EQE1 and V2Q) yields promising results, outperforming baseline methods in experimental settings. [ABSTRACT FROM AUTHOR] Copyright of Journal of Information Science Theory & Practice (JIStaP) is the property of Korea Institute of Science & Technology Information and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;INEX Tweet Contextualization task: Evaluation, results and lesson learned.;"Bellot, Patrice; Moriceau, Véronique; Mothe, Josiane; SanJuan, Eric; Tannier, Xavier";NOT_FOUND;10.1016/j.ipm.2016.03.002;NOT_FOUND;Microblogging platforms such as Twitter are increasingly used for on-line client and market analysis. This motivated the proposal of a new track at CLEF INEX lab of Tweet Contextualization . The objective of this task was to help a user to understand a tweet by providing him with a short explanatory summary (500 words). This summary should be built automatically using resources like Wikipedia and generated by extracting relevant passages and aggregating them into a coherent summary. Running for four years, results show that the best systems combine NLP techniques with more traditional methods. More precisely the best performing systems combine passage retrieval, sentence segmentation and scoring, named entity recognition, text part-of-speech (POS) analysis, anaphora detection, diversity content measure as well as sentence reordering. This paper provides a full summary report on the four-year long task. While yearly overviews focused on system results, in this paper we provide a detailed report on the approaches proposed by the participants and which can be considered as the state of the art for this task. As an important result from the 4 years competition, we also describe the open access resources that have been built and collected. The evaluation measures for automatic summarization designed in DUC or MUC were not appropriate to evaluate tweet contextualization, we explain why and depict in detailed the LogSim measure used to evaluate informativeness of produced contexts or summaries. Finally, we also mention the lessons we learned and that it is worth considering when designing a task. [ABSTRACT FROM AUTHOR] Copyright of Information Processing & Management is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Information is essential for competitive and cost-effective public procurement.;"Gorgun, Mustafa Kaan; Kutlu, Mucahid; Tas, Bedri Kamil Onur";NOT_FOUND;10.1177/01655515221141042;NOT_FOUND;Public authorities promote transparent public procurement practices to increase competition and reduce public procurement costs. In this article, we focus on public procurement of the European Union (EU). We employ a multidisciplinary approach to analyse economic effects of information in public procurement. We quantify the information content of 2,390,630 EU public procurement notices published in 22 different languages using natural language processing techniques. Subsequently, we examine the impact of the information content on public procurement outcomes. We find that higher information levels have significant positive effects. Competition is considerably higher when notices contain more information. On average, contract prices would be 6%–8% lower if notices were to contain adequate information. EU governments could save up to € 80 billion if all public procurement notices were to have detailed information. Based on our comprehensive analysis, we believe that authorities should regulate the information content of notices to promote competition and cost-effectiveness in public procurement. [ABSTRACT FROM AUTHOR] Copyright of Journal of Information Science is the property of Sage Publications, Ltd. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Integrating large language models and generative artificial intelligence tools into information literacy instruction.;"Carroll, Alexander J.; Borycz, Joshua";NOT_FOUND;10.1016/j.acalib.2024.102899;NOT_FOUND;Generative artificial intelligence (AI) and large language models (LLMs) have induced a mixture of excitement and panic among educators. However, there is a lack of consensus over how much experience science and engineering students have with using these tools for research-related tasks. Likewise, it is not yet known how educators and information professionals can leverage these tools to teach students strategies for information retrieval and knowledge synthesis. This study assesses the extent of students' use of AI tools in research-related tasks and if information literacy instruction could impact their perception of these tools. Responses to Likert-scale questions indicate that many students did not have extensive experience using LLMs for research-related purposes prior to the information literacy sessions. However, after participating in a didactic lecture and discussion with an engineering librarian that explored how to use these tools effectively and responsibly, many students reported viewing these tools as potentially useful for future assignments. Student responses to open-response questions suggest that librarian-led information literacy training can assist students in developing more sophisticated understandings of the limitations and use cases for artificial intelligence in inquiry-based coursework. [ABSTRACT FROM AUTHOR] Copyright of Journal of Academic Librarianship is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
MAYBE_RELEVANT;EBSCOhost;Interactive query expansion for professional search applications.;"Russell-Rose, Tony; Gooch, Philip; Kruschwitz, Udo";NOT_FOUND;10.1177/02663821211034079;NOT_FOUND;Knowledge workers (such as healthcare information professionals, patent agents and recruitment professionals) undertake work tasks where search forms a core part of their duties. In these instances, the search task is often complex and time-consuming and requires specialist expert knowledge to formulate accurate search strategies. Interactive features such as query expansion can play a key role in supporting these tasks. However, generating query suggestions within a professional search context requires that consideration be given to the specialist, structured nature of the search strategies they employ. In this paper, we investigate a variety of query expansion methods applied to a collection of Boolean search strategies used in a variety of real-world professional search tasks. The results demonstrate the utility of context-free distributional language models and the value of using linguistic cues to optimise the balance between precision and recall. [ABSTRACT FROM AUTHOR] Copyright of Business Information Review is the property of Sage Publications Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
MAYBE_RELEVANT;EBSCOhost;Knowledge powered by artificial intelligence.;Pichman, Brian;NOT_FOUND;10.1177/18758789241299017;NOT_FOUND;Generative Artificial Intelligence (GenAI) has revolutionized knowledge management, offering unprecedented capabilities for creating, proofing, summarizing, and evaluating documentation. This paper explores how AI, particularly large language models (LLMs), and Retrieval Augmented Generation (RAG) systems, can streamline the development of knowledge articles while addressing ethical concerns such as data ownership and bias. We examine practical applications, including real-time collaboration, multilingual support, personalized information retrieval, and automated knowledge forecasting. Additionally, we explore AI’s role in bridging legacy systems, reducing biases, and enhancing decision-making. Ultimately, AI extends beyond generating content, shaping a more efficient, inclusive, and innovative approach to knowledge management. This article is based upon a presentation given at the 2024 NISO Plus Conference that was held in Baltimore, MD, USA, February 13–14, 2024. [ABSTRACT FROM AUTHOR] Copyright of Information Services & Use is the property of IOS Press and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Leveraging AI-based Decision Support for Opportunity Analysis.;"Groher, Wolfgang; Rademacher, Friedrich-Wilhelm; Csillaghy, André";NOT_FOUND;10.22215/timreview/1289;NOT_FOUND;"The dynamics and speed of change in corporate environments have increased. At the front-end of innovation, firms are challenged to evaluate growing amounts of information within shorter time frames in order to stay competitive. Either they spend significant time on structured data analysis, at the risk of delayed market launch, or they follow their intuition, at the risk of not meeting market trends. Both scenarios constitute a significant risk for a firm's continued existence.Motivated by this, a conceptual model is presented in this paper that aims at remediating these risks. Grounded on design science methodology, it concentrates on previous assessments of innovation search fields. These innovation search fields assist in environmental scanning and lay the foundation for deciding which opportunities to pursue. The model applies a novel AI-based approach, which draws on natural language processing and information retrieval. To provide decision support, the approach includes market-, technology-, and firm-related criteria. This allows us to replace intuitive decision-making by fact-based considerations. In addition, an often-iterative approach for environmental scanning is replaced by a more straightforward process. Early testing of the conceptual model has shown results of increased quality and speed of decision-making. Further testing and feedback is still required to enhance and calibrate the AI-functionality. Applied in business environments, the approach can contribute to remediate fuzziness in early front-end activities, thus helping direct innovation managers to ""do the right things"". [ABSTRACT FROM AUTHOR] Copyright of Technology Innovation Management Review is the property of Carleton University, Talent First Network, Technology Innovation Management Review and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)"
NOT_RELEVANT;EBSCOhost;Listwise approach based on the cross‐correntropy for learning to rank.;"Wu, Mintao; Zhu, Jihua; Wang, Jun; Pang, Shanmin; Li, Yaochen";NOT_FOUND;10.1049/el.2018.0815;NOT_FOUND;The problem of learning to rank is addressed and a novel listwise approach by taking document retrieval as an example is proposed. It first introduces the concept of cross‐correntropy into learning to rank and then proposes the listwise loss function based on the cross‐correntropy between the ranking list given by the label and the one predicted by training model. The use of the cross‐correntropy loss leads to the development of the listwise approach called ListCCE, which employs the gradient descent algorithm to train a neural network model. Experimental results tested on publicly available data sets show that the proposed approach performs better than some existing approaches. [ABSTRACT FROM AUTHOR] Copyright of Electronics Letters (Wiley-Blackwell) is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
MAYBE_RELEVANT;EBSCOhost;Machine learning and ontology-based novel semantic document indexing for information retrieval.;"Sharma, Anil; Kumar, Suresh";NOT_FOUND;10.1016/j.cie.2022.108940;NOT_FOUND;• Document key phrases to ontology concept mapping with limited or no related concepts in the ontology. • Analytic hierarchy process based application specific concept feature weights. • Document's concept term variations and synonyms mapped on domain ontology. • Average F-measure enhanced by 25% compared to the state-of-the-art. The goal of information retrieval (IR) systems is to find the contents most closely related to the user's information needs from a pool of information. However, conventional IR methods neglect semantic descriptions of document contents and index documents based on the words that they include. When users and indexing systems use different terms to express the same subject, a vocabulary gap emerges. To overcome this limitation and to enhance the effectiveness of the IR systems, this paper introduced a novel hybrid semantic document indexing employing machine learning and domain ontology. The presented technique uses a skip-gram with negative sampling-based machine learning model and a domain ontology to determine the concepts for annotating unstructured documents. The proposed work also introduced multiple feature based novel concept ranking algorithm where statistical, semantic, and scientific named entity features of the concept were used to assign relevance weight to the annotations. The fuzzy analytical hierarchy process was used to derive the parameters of these feature weights. The final step is to rank the concepts according to their relevance to the document. Five benchmark publicly accessible datasets from the computer science domain were used in a series of experiments to validate the results of presented method. Experiment findings showed that the proposed method performs better than state-of-the-art techniques on these datasets, by improving average accuracy by 29%, while an improvement of 25% was recorded in F-measure. The improvement in average accuracy demonstrates that the performance of the proposed approach is better than the state-of-the-art methods in extracting document concepts accurately even when the same concept is referred to by distinct terms in the document and domain ontologies. The proposed system's ability to find similar concepts when the documents possess no concept from domain ontology is demonstrated by the improvement in F-measure, which is attributed to high recall rates of the proposed indexing scheme while maintaining high accuracy. [ABSTRACT FROM AUTHOR] Copyright of Computers & Industrial Engineering is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Measuring the Semantic Uncertainty of News Events for Evolution Potential Estimation.;"XIANGFENG LUO; JUNYU XUAN; JIE LU; GUANGQUAN ZHANG";NOT_FOUND;10.1145/2903719;NOT_FOUND;The evolution potential estimation of news events can support the decision making of both corporations and governments. For example, a corporation could manage its public relations crisis in a timely manner if a negative news event about this corporation is known with large evolution potential in advance. However, existing state-of-the-art methods are mainly based on time series historical data, which are not suitable for the news events with limited historical data and bursty properties. In this article, we propose a purely content-based method to estimate the evolution potential of the news events. The proposed method considers a news event at a given time point as a system composed of different keywords, and the uncertainty of this system is defined and measured as the Semantic Uncertainty of this news event. At the same time, an uncertainty space is constructed with two extreme states: the most uncertain state and the most certain state. We believe that the Semantic Uncertainty has correlation with the content evolution of the news events, so it can be used to estimate the evolution potential of the news events. In order to verify the proposed method, we present detailed experimental setups and results measuring the correlation of the Semantic Uncertainty with the Content Change of news events using collected news events data. The results show that the correlation does exist and is stronger than the correlation of value from the time-series-based method with the Content Change. Therefore, we can use the Semantic Uncertainty to estimate the evolution potential of news events. [ABSTRACT FROM AUTHOR] Copyright of ACM Transactions on Information Systems is the property of Association for Computing Machinery and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Modeling and Learning Distributed Word Representation with Metadata for Question Retrieval.;"Zhou, Guangyou; Huang, Jimmy Xiangji";NOT_FOUND;10.1109/TKDE.2017.2665625;NOT_FOUND;Community question answering (cQA) has become an important issue due to the popularity of cQA archives on the Web. This paper focuses on addressing the lexical gap problem in question retrieval. Question retrieval in cQA archives aims to find the existing questions that are semantically equivalent or relevant to the queried questions. However, the lexical gap problem brings a new challenge for question retrieval in cQA. In this paper, we propose to model and learn distributed word representations with metadata of category information within cQA pages for question retrieval using two novel category powered models. One is a basic category powered model called MB-NET and the other one is an enhanced category powered model called ME-NET which can better learn the distributed word representations and alleviate the lexical gap problem. To deal with the variable size of word representation vectors, we employ the framework of fisher kernel to transform them into the fixed-length vectors. Experimental results on large-scale English and Chinese cQA data sets show that our proposed approaches can significantly outperform state-of-the-art retrieval models for question retrieval in cQA. Moreover, we further conduct our approaches on large-scale automatic evaluation experiments. The evaluation results show that promising and significant performance improvements can be achieved. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Knowledge & Data Engineering is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Opera Clustering: K-means on librettos datasets.;"Harim Jeong; Joo Hun Yoo";NOT_FOUND;10.7472/jksii.2022.23.2.45;NOT_FOUND;With the development of artificial intelligence analysis methods, especially machine learning, various fields are widely expanding their application ranges. However, in the case of classical music, there still remain some difficulties in applying machine learning techniques. Genre classification or music recommendation systems generated by deep learning algorithms are actively used in general music, but not in classical music. In this paper, we attempted to classify opera among classical music. To this end, an experiment was conducted to determine which criteria are most suitable among, composer, period of composition, and emotional atmosphere, which are the basic features of music. To generate emotional labels, we adopted zero-shot classification with four basic emotions, ‘happiness’, ‘sadness’, ‘anger’, and ‘fear.’ After embedding the opera libretto with the doc2vec processing model, the optimal number of clusters is computed based on the result of the elbow method. Decided four centroids are then adopted in k-means clustering to classify unsupervised libretto datasets. We were able to get optimized clustering based on the result of adjusted rand index scores. With these results, we compared them with notated variables of music. As a result, it was confirmed that the four clusterings calculated by machine after training were most similar to the grouping result by period. Additionally, we were able to verify that the emotional similarity between composer and period did not appear significantly. At the end of the study, by knowing the period is the right criteria, we hope that it makes easier for music listeners to find music that suits their tastes. [ABSTRACT FROM AUTHOR] Copyright of Journal of Internet Computing & Services is the property of Korean Society for Internet Information and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Predicting taxi demand hotspots using automated Internet Search Queries.;"Markou, Ioulia; Kaiser, Kevin; Pereira, Francisco C.";NOT_FOUND;10.1016/j.trc.2019.03.001;NOT_FOUND;Highlights • Popular events can cause distinct taxi demand hotspots. • Internet search queries are proven useful for the prediction of demand hotspots. • Queries expansion with two new terms return more representative results. • MedLDA performs better than an independent topic modelling and classification process. • Terms with time expressions seem to be good prediction indicators in topics. Abstract Disruptions due to special events are a well-known challenge in transport operations, since the transport system is typically designed for habitual demand. Part of the problem relates to the difficulty in collecting comprehensive and reliable information early enough to prepare mitigation measures. A tool that automatically scans the internet for events and predicts their impact would strongly support transport management in many cities in the world. This study addresses the challenges related to retrieving and analyzing web documents about real world events, and using them for demand explanation (if related to a past event) and prediction (if a future one). Transport demand is predicted with a supervised topic modeling algorithm by utilizing information about social events retrieved using various strategies, which made use of search aggregation, natural language processing, and query expansion. It was found that a two-step process produced the highest accuracy for transport demand prediction, where different (but related) queries are used to retrieve an initial set of documents, and then, based on these documents, a final query is constructed that obtains the set of predictive documents. These are then used to model the most discriminating topics related to the transport demand. A framework was proposed that sequentially handles all stages of data gathering, enrichment, and prediction with the intention of generating automated search queries. [ABSTRACT FROM AUTHOR] Copyright of Transportation Research Part C: Emerging Technologies is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Processing social media in real-time.;"Spina, Damiano; Zubiaga, Arkaitz; Sheth, Amit; Strohmaier, Markus";NOT_FOUND;10.1016/j.ipm.2018.06.006;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;EBSCOhost;Recognition of cursive video text using a deep learning framework.;"Mirza, Ali; Siddiqi, Imran";NOT_FOUND;10.1049/iet-ipr.2019.1070;NOT_FOUND;This study focuses on cursive text recognition appearing in videos, using a complete framework of deep neural networks. While mature video optical character recognition systems (V‐OCRs) are available for text in non‐cursive scripts, recognition of cursive scripts is marked by many challenges. These include complex and overlapping ligatures, context‐dependent shape variations and presence of a large number of dots and diacritics. The authors present an analytical technique for recognition of cursive caption text that relies on a combination of convolutional and recurrent neural networks trained in an end‐to‐end framework. Text lines extracted from video frames are preprocessed to segment the background and are fed to a convolutional neural network for feature extraction. The extracted feature sequences are fed to different variants of bi‐directional recurrent neural networks along with the ground truth transcription to learn sequence‐to‐sequence mapping. Finally, a connectionist temporal classification layer is employed to produce the final transcription. Experiments on a data set of more than 40,000 text lines from 11,192 video frames of various News channel videos reported an overall character recognition rate of 97.63%. The proposed work employs Urdu text as a case study but the findings can be generalised to other cursive scripts as well. [ABSTRACT FROM AUTHOR] Copyright of IET Image Processing (Wiley-Blackwell) is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Retrieval Contrastive Learning for Aspect-Level Sentiment Classification.;"Jian, Zhongquan; Li, Jiajian; Wu, Qingqiang; Yao, Junfeng";NOT_FOUND;10.1016/j.ipm.2023.103539;NOT_FOUND;"Aspect-Level Sentiment Classification (ALSC) aims to assign specific sentiments to a sentence toward different aspects, which is one of the crucial challenges in the field of Natural Language Processing (NLP). Despite numerous approaches being proposed and obtaining prominent results, the majority of them focus on leveraging the relationships between the aspect and opinion words in a single instance while ignoring correlations with other instances, which will make models inevitably become trapped in local optima due to the absence of a global viewpoint. Instance representation derived from a single instance, on the one hand, the contained information is insufficient due to the lack of descriptions from other perspectives; on the other hand, its stored knowledge is redundant since the inability to filter extraneous content. To obtain a polished instance representation, we developed a Retrieval Contrastive Learning (RCL) framework to subtly extract intrinsic knowledge across instances. RCL consists of two modules: (a) obtaining retrieval instances by sparse retriever and dense retriever, and (b) extracting and learning the knowledge of the retrieval instances by using Contrastive Learning (CL). To demonstrate the superiority of RCL, five ALSC models are employed to conduct comprehensive experiments on three widely-known benchmarks. Compared with the baselines, ALSC models achieve substantial improvements when trained with RCL. Especially, ABSA-DeBERTa with RCL obtains new state-of-the-art results, which outperform the advanced methods by 0.92%, 0.23%, and 0.47% in terms of Macro F1 gains on Laptops, Restaurants, and Twitter, respectively. • We proposed RCL to enable ALSC models to generate polished representations. • RCL has two modules: obtain retrieval instances and learn common features by CL. • The sparse and dense retrievers are used to obtain high-quality retrieval instances. • The ALSC model can be enhanced greatly by training with RCL. • ABSA-DeBERTa obtains new state-of-the-art results by being trained with RCL. [ABSTRACT FROM AUTHOR] Copyright of Information Processing & Management is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)"
NOT_RELEVANT;EBSCOhost;Retrieve–Revise–Refine: A novel framework for retrieval of concise entailing legal article set.;"Nguyen, Chau; Nguyen, Phuong; Nguyen, Le-Minh";NOT_FOUND;10.1016/j.ipm.2024.103949;NOT_FOUND;The retrieval of entailing legal article sets aims to identify a concise set of legal articles that holds an entailment relationship with a legal query or its negation. Unlike traditional information retrieval that focuses on relevance ranking, this task demands conciseness. However, prior research has inadequately addressed this need by employing traditional methods. To bridge this gap, we propose a three-stage Retrieve–Revise–Refine framework which explicitly addresses the need for conciseness by utilizing both small and large language models (LMs) in distinct yet complementary roles. Empirical evaluations on the COLIEE 2022 and 2023 datasets demonstrate that our framework significantly enhances performance, achieving absolute increases in the macro F2 score by 3.17% and 4.24% over previous state-of-the-art methods, respectively. Specifically, our Retrieve stage, employing various tailored fine-tuning strategies for small LMs, achieved a recall rate exceeding 0.90 in the top-5 results alone—ensuring comprehensive coverage of entailing articles. In the subsequent Revise stage, large LMs narrow this set, improving precision while sacrificing minimal coverage. The Refine stage further enhances precision by leveraging specialized insights from small LMs, resulting in a relative improvement of up to 19.15% in the number of concise article sets retrieved compared to previous methods. Our framework offers a promising direction for further research on specialized methods for retrieving concise sets of entailing legal articles, thereby more effectively meeting the task's demands. • Propose a novel three-stage Retrieve–Revise–Refine framework for concise legal article set retrieval. • Achieve 3.17% and 4.24% higher macro F2 scores on two evaluated datasets. • Retrieve 19.15% more concise sets of legal articles compared to previous methods. • Ablation studies show over 90% coverage within top-5 results in the Retrieval stage. [ABSTRACT FROM AUTHOR] Copyright of Information Processing & Management is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Reviewer assignment algorithms for peer review automation: A survey.;"Zhao, Xiquan; Zhang, Yangsen";NOT_FOUND;10.1016/j.ipm.2022.103028;NOT_FOUND;Assigning paper to suitable reviewers is of great significance to ensure the accuracy and fairness of peer review results. In the past three decades, many researchers have made a wealth of achievements on the reviewer assignment problem (RAP). In this survey, we provide a comprehensive review of the primary research achievements on reviewer assignment algorithm from 1992 to 2022. Specially, this survey first discusses the background and necessity of automatic reviewer assignment, and then systematically summarize the existing research work from three aspects, i.e., construction of candidate reviewer database, computation of matching degree between reviewers and papers, and reviewer assignment optimization algorithm, with objective comments on the advantages and disadvantages of the current algorithms. Afterwards, the evaluation metrics and datasets of reviewer assignment algorithm are summarized. To conclude, we prospect the potential research directions of RAP. Since there are few comprehensive survey papers on reviewer assignment algorithm in the past ten years, this survey can serve as a valuable reference for the related researchers and peer review organizers. [ABSTRACT FROM AUTHOR] Copyright of Information Processing & Management is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Swat: A system for detecting salient Wikipedia entities in texts.;"Ponza, Marco; Ferragina, Paolo; Piccinno, Francesco";NOT_FOUND;10.1111/coin.12216;NOT_FOUND;We study the problem of entity salience by proposing the design and implementation of Swat, a system that identifies the salient Wikipedia entities occurring in an input document. Swat consists of several modules that are able to detect and classify on‐the‐fly Wikipedia entities as salient or not, based on a large number of syntactic, semantic, and latent features properly extracted via a supervised process, which has been trained over millions of examples drawn from the New York Times corpus. The validation process is performed through a large experimental assessment, eventually showing that Swat improves known solutions over all publicly available datasets. We release Swat via an API that we describe and comment in the paper to ease its use in other software. [ABSTRACT FROM AUTHOR] Copyright of Computational Intelligence is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
MAYBE_RELEVANT;EBSCOhost;Systematic mapping study on question answering frameworks over linked data.;"Tasar, Ceren Ocal; Komesli, Murat; Unalir, Murat Osman";NOT_FOUND;10.1049/iet-sen.2018.5105;NOT_FOUND;Employing linked data technologies and semantic endpoints for question answering systems are expanding approaches among the researchers. Therefore, systems that combine syntactic and semantic analysis and enrich input questions by sentence‐level recognition are examined. A systematic mapping study is conducted to identify and analyse the studies from major databases, journals and proceedings of conferences or workshops published between 2010 and 2017. With a set of 14 research questions, inclusion and exclusion criteria are specified. 53 studies are selected as primary studies from an initial set of 845 papers. This study provides a mapping while focusing on the methods and identifying the gaps between required and existing approaches. Popular approaches which have gained the most attention among researchers are given as a conclusion. Moreover, a comparison between the authors' study and related work in the literature is given to point out the differences and the contributions of their study. As the result of the comparison, it is concluded that the study is a novel and original topic on question answering frameworks. [ABSTRACT FROM AUTHOR] Copyright of IET Software (Wiley-Blackwell) is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Text mining approaches for dealing with the rapidly expanding literature on COVID-19.;"Wang, Lucy Lu; Lo, Kyle";NOT_FOUND;10.1093/bib/bbaa296;NOT_FOUND;"More than 50 000 papers have been published about COVID-19 since the beginning of 2020 and several hundred new papers continue to be published every day. This incredible rate of scientific productivity leads to information overload, making it difficult for researchers, clinicians and public health officials to keep up with the latest findings. Automated text mining techniques for searching, reading and summarizing papers are helpful for addressing information overload. In this review, we describe the many resources that have been introduced to support text mining applications over the COVID-19 literature; specifically, we discuss the corpora, modeling resources, systems and shared tasks that have been introduced for COVID-19. We compile a list of 39 systems that provide functionality such as search, discovery, visualization and summarization over the COVID-19 literature. For each system, we provide a qualitative description and assessment of the system's performance, unique data or user interface features and modeling decisions. Many systems focus on search and discovery, though several systems provide novel features, such as the ability to summarize findings over multiple documents or linking between scientific articles and clinical trials. We also describe the public corpora, models and shared tasks that have been introduced to help reduce repeated effort among community members; some of these resources (especially shared tasks) can provide a basis for comparing the performance of different systems. Finally, we summarize promising results and open challenges for text mining the COVID-19 literature. [ABSTRACT FROM AUTHOR] Copyright of Briefings in Bioinformatics is the property of Oxford University Press / USA and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)"
NOT_RELEVANT;EBSCOhost;The “General Problem Solver” Does Not Exist: Mortimer Taube and the Art of AI Criticism.;Garvey, Shunryu Colin;NOT_FOUND;10.1109/MAHC.2021.3051686;NOT_FOUND;"This article reconfigures the history of artificial intelligence (AI) and its accompanying tradition of criticism by excavating the work of Mortimer Taube, a pioneer in information and library sciences, whose magnum opus, Computers and Common Sense: The Myth of Thinking Machines (1961), has been mostly forgotten. To convey the essence of his distinctive critique, the article focuses on Taube's attack on the general problem solver (GPS), the second major AI program. After examining his analysis of the social construction of this and other ""thinking machines,"" it concludes that, despite technical changes in AI, much of Taube's criticism remains relevant today. Moreover, his status as an ""information processing"" insider who criticized AI on behalf of the public good challenges the boundaries and focus of most critiques of AI from the past half-century. In sum, Taube's work offers an alternative model from which contemporary AI workers and critics can learn much. [ABSTRACT FROM AUTHOR] Copyright of IEEE Annals of the History of Computing is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)"
NOT_RELEVANT;EBSCOhost;Unlocking maintenance insights in industrial text through semantic search.;"Naqvi, Syed Meesam Raza; Ghufran, Mohammad; Varnier, Christophe; Nicod, Jean-Marc; Javed, Kamran; Zerhouni, Noureddine";NOT_FOUND;10.1016/j.compind.2024.104083;NOT_FOUND;Maintenance records in Computerized Maintenance Management Systems (CMMS) contain valuable human knowledge on maintenance activities. These records primarily consist of noisy and unstructured texts written by maintenance experts. The technical nature of the text, combined with a concise writing style and frequent use of abbreviations, makes it difficult to be processed through classical Natural Language Processing (NLP) pipelines. Due to these complexities, this text must be normalized before feeding to classical machine learning models. Developing these custom normalization pipelines requires manual labor and domain expertise and is a time-consuming process that demands constant updates. This leads to the under-utilization of this valuable source of information to generate insights to help with maintenance decision support. This study proposes a Technical Language Processing (TLP) pipeline for semantic search in industrial text using BERT (Bidirectional Encoder Representations), a transformer-based Large Language Model (LLM). The proposed pipeline can automatically process complex unstructured industrial text and does not require custom preprocessing. To adapt the BERT model for the target domain, three unsupervised domain fine-tuning techniques are compared to identify the best strategy for leveraging available tacit knowledge in industrial text. The proposed approach is validated on two industrial maintenance records from the mining and aviation domains. Semantic search results are analyzed from a quantitative and qualitative perspective. Analysis shows that TSDAE, a state-of-the-art unsupervised domain fine-tuning technique, can efficiently identify intricate patterns in the industrial text regardless of associated complexities. BERT model fine-tuned with TSDAE on industrial text achieved a precision of 0.94 and 0.97 for mining excavators and aviation maintenance records, respectively. • Industrial text stores crucial human knowledge about various assets. • The free-form nature of the industrial text makes it difficult to process. • Transformer-based models can leverage tacit knowledge in the industrial text. • LLMs can enable semantic search in complex industrial text with high precision. [ABSTRACT FROM AUTHOR] Copyright of Computers in Industry is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;User satisfaction with Arabic COVID-19 apps: Sentiment analysis of users' reviews using machine learning techniques.;"Ramzy, Mina; Ibrahim, Bahaa";NOT_FOUND;10.1016/j.ipm.2024.103644;NOT_FOUND;• We provide a benchmark dataset composed of 114,499 reviews from 18 Arabic COVID-19 Apps. • The ANN algorithm provides the best performance with 89 % accuracy and 89 % F1. • The proposed model can be generalized for Arab sentiment analysis to mobile apps for new COVID-19 strains that may appear in the future. • There are positive sentiments (71 %) among most users of Arabic COVID-19 apps, which reflects its positive role in infection control. Digital technologies such as mobile health (mHealth) apps with a variety of features can be essential tools for controlling pandemics. Therefore, many Arab countries have launched COVID-19 mHealth apps to reduce the spread of infection among their citizens. Recently, empirical studies have shown that user reviews include useful details to develop apps. However, Arab citizens' satisfaction with the COVID-19 mHealth apps has not been examined yet. Our study aims to provide Arabic sentiment analysis of users' reviews to explore their satisfaction with Arabic Covid-19 apps. To achieve this goal, we have provided a benchmark dataset composed of 114,499 reviews from 18 Arabic COVID-19 Apps. Six machine learning (ML) models were tested and compared (Support Vector Machine (SVM), K-Nearest Neighbor (KNN), Naive Bayes (NB), Logistic Regression (LR), Random Forest (RF), and Artificial Neural Network (ANN)) using a representative sample of 8220 reviews, which were annotated manually. Then, the best-performing algorithms were applied to the benchmark dataset to explore the polarity of Arab sentiment toward the apps. In a later step, we conducted a thematic analysis of both positive and negative reviews to determine which factors positively and negatively influence the effectiveness of apps. The findings show that the ANN algorithm provides the best performance with 89 % accuracy and 89 % F1. 71 % of user reviews include positive sentiments, while only 21 % include negative sentiments. Frequently crashes, update issues, and bugs were among the most prominent negative factors that affected the effectiveness of apps from the users' point of view. Finally, we presented a set of recommendations to address the negative factors and improve the effectiveness of Arabic COVID-19 apps. [ABSTRACT FROM AUTHOR] Copyright of Information Processing & Management is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
MAYBE_RELEVANT;EBSCOhost;Utilizing BERT for Information Retrieval: Survey, Applications, Resources, and Challenges.;"Wang, Jiajia; Huang, Jimmy Xiangji; Tu, Xinhui; Wang, Junmei; Huang, Angela Jennifer; Laskar, Md Tahmid Rahman; Bhuiyan, Amran";NOT_FOUND;10.1145/3648471;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;EBSCOhost;A compressed trie structure using divided keys.;"Oono, Masaki; Atlam, El-Sayed; Fuketa, Masao; Morita, Kazuhiro; Jun-ichi Aoe";NOT_FOUND;10.1504/IJCAT.2009.023615;NOT_FOUND;A link-trie structure is an efficient data structure for collocation information using a trie structure. The link-trie stores two basic words into the trie and defines link-information by a link-function. This paper presents how to apply the link-trie into a general set of keys and compress the storage capacity. The method divides a key into several sub-keys and defines link-information between these sub-keys. From simulation results for 100,000 keys, it turns out that the presented method compresses the storage capacity by 30% smaller than the normal trie. [ABSTRACT FROM AUTHOR] Copyright of International Journal of Computer Applications in Technology is the property of Inderscience Enterprises Ltd. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;A hybrid approach to Arabic named entity recognition.;"Shaalan, Khaled; Oudah, Mai";NOT_FOUND;10.1177/0165551513502417;NOT_FOUND;In this paper, we propose a hybrid named entity recognition (NER) approach that takes the advantages of rule-based and machine learning-based approaches in order to improve the overall system performance and overcome the knowledge elicitation bottleneck and the lack of resources for underdeveloped languages that require deep language processing, such as Arabic. The complexity of Arabic poses special challenges to researchers of Arabic NER, which is essential for both monolingual and multilingual applications. We used the hybrid approach to develop an Arabic NER system that is capable of recognizing 11 types of Arabic named entities: Person, Location, Organization, Date, Time, Price, Measurement, Percent, Phone Number, ISBN and File Name. Extensive experiments were conducted using decision trees, Support Vector Machines and logistic regression classifiers to evaluate the system performance. The empirical results indicate that the hybrid approach outperforms both the rule-based and the ML-based approaches when they are processed independently. More importantly, our system outperforms the state-of-the-art of Arabic NER in terms of accuracy when applied to ANERcorp standard dataset, with F-measures 0.94 for Person, 0.90 for Location and 0.88 for Organization. [ABSTRACT FROM PUBLISHER] Copyright of Journal of Information Science is the property of Sage Publications, Ltd. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;A hybrid approach to fuzzy name search incorporating language-based and text-based principles.;"Paul Wu Horng-Jyh; Na Jin-Cheon; Christopher Khoo Soo-Guan";NOT_FOUND;10.1177/0165551506068146;NOT_FOUND;Name Search is an important search function in various types of information retrieval systems, such as online library catalogs and electronic yellow pages. It is also difficult, because of the high degree of fuzziness required in matching name variants. Previous approaches to name search systems use ad hoc combinations of search heuristics. This paper first discusses two approaches to name modeling - the natural language processing (NLP) and information retrieval (IR) models - and proposes a hybrid approach. The approach demonstrates a critical combination of complementary NILP and IR features that produces more effective fuzzy name matching. Two principles, position-as-attribute and position-transition-likelihood, are introduced as the principles for integrating the advantageous aspects of both approaches. They have been implemented in an NLP- and JR-hybrid model system called Friendly Name Search (FNS) for real world applications in multilingual directory searches on the Singapore Yellow pages website. [ABSTRACT FROM AUTHOR] Copyright of Journal of Information Science is the property of Sage Publications, Ltd. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;A hybrid approach to managing job offers and candidates;"Kessler, Rémy; Béchet, Nicolas; Roche, Mathieu; Torres-Moreno, Juan-Manuel; El-Bèze, Marc";NOT_FOUND;10.1016/j.ipm.2012.03.002;NOT_FOUND;Abstract: The evolution of the job market has resulted in traditional methods of recruitment becoming insufficient. As it is now necessary to handle volumes of information (mostly in the form of free text) that are impossible to process manually, an analysis and assisted categorization are essential to address this issue. In this paper, we present a combination of the E-Gen and Cortex systems. E-Gen aims to perform analysis and categorization of job offers together with the responses given by the candidates. E-Gen system strategy is based on vectorial and probabilistic models to solve the problem of profiling applications according to a specific job offer. Cortex is a statistical automatic summarization system. In this work, E-Gen uses Cortex as a powerful filter to eliminate irrelevant information contained in candidate answers. Our main objective is to develop a system to assist a recruitment consultant and the results obtained by the proposed combination surpass those of E-Gen in standalone mode on this task. [Copyright &y& Elsevier] Copyright of Information Processing & Management is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;A knowledge acquisition methodology to ontology construction for information retrieval from medical documents.;"Valencia-García, Rafael; Fernández-Breis, Jesualdo Tomás; Ruiz-Martínez, Juana María; García-Sánchez, Francisco; Martínez-Béjar, Rodrigo";NOT_FOUND;10.1111/j.1468-0394.2008.00464.x;NOT_FOUND;Vast amounts of medical information reside within text documents, so that the automatic retrieval of such information would certainly be beneficial for clinical activities. The need for overcoming the bottleneck provoked by the manual construction of ontologies has generated several studies and research on obtaining semi-automatic methods to build ontologies. Most techniques for learning domain ontologies from free text have important limitations. Thus, they can extract concepts so that only taxonomies are generally produced although there are other types of semantic relations relevant in knowledge modelling. This paper presents a language-independent approach for extracting knowledge from medical natural language documents. The knowledge is represented by means of ontologies that can have multiple semantic relationships among concepts. [ABSTRACT FROM AUTHOR] Copyright of Expert Systems is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;A knowledge management application in enterprises.;El-Korany, Abeer;NOT_FOUND;10.1504/IJMED.2007.014989;NOT_FOUND;"An abstract of the article ""A knowledge management application in enterprises,"" by Abeer El-Korany is presented."
NOT_RELEVANT;EBSCOhost;A lemmatization method for Mongolian and its application to indexing for information retrieval;"Khaltar, Badam-Osor; Fujii, Atsushi";NOT_FOUND;10.1016/j.ipm.2009.01.008;NOT_FOUND;In Mongolian, two different alphabets are used, Cyrillic and Mongolian. In this paper, we focus solely on the Mongolian language using the Cyrillic alphabet, in which a content word can be inflected when concatenated with one or more suffixes. Identifying the original form of content words is crucial for natural language processing and information retrieval. We propose a lemmatization method for Mongolian. The advantage of our lemmatization method is that it does not rely on noun dictionaries, enabling us to lemmatize out-of-dictionary words. We also apply our method to indexing for information retrieval. We use newspaper articles and technical abstracts in experiments that show the effectiveness of our method. Our research is the first significant exploration of the effectiveness of lemmatization for information retrieval in Mongolian. [Copyright &y& Elsevier] Copyright of Information Processing & Management is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;An Automated Framework for Incorporating News into Stock Trading Strategies.;"Nuij, Wijnand; Milea, Viorel; Hogenboom, Frederik; Frasincar, Flavius; Kaymak, Uzay";NOT_FOUND;10.1109/TKDE.2013.133;NOT_FOUND;In this paper we present a framework for automatic exploitation of news in stock trading strategies. Events are extracted from news messages presented in free text without annotations. We test the introduced framework by deriving trading strategies based on technical indicators and impacts of the extracted events. The strategies take the form of rules that combine technical trading indicators with a news variable, and are revealed through the use of genetic programming. We find that the news variable is often included in the optimal trading rules, indicating the added value of news for predictive purposes and validating our proposed framework for automatically incorporating news in stock trading strategies. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Knowledge & Data Engineering is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;An automated online crisis dispatcher.;"Fitrianie, Siska; Rothkrantz, Leon J. M.";NOT_FOUND;10.1504/IJEM.2008.019910;NOT_FOUND;The article features the dialogue system that can play as a crisis hotline dispatcher, hence, helping human operators at the crisis centers disseminate precise information during crisis events. It notes that the dialogue system offers a natural user interaction through its ability to start a user-friendly dialogue that takes care of the content, context, and user's emotion. It also retrieves information regarding crisis situations from users while controlling the communication flow.
NOT_RELEVANT;EBSCOhost;An English Language Question Answering System for a Large Relational Database.;"Waltz, David L.; Montgomery, Christine A.";NOT_FOUND;10.1145/359545.359550;NOT_FOUND;"By typing requests in English, casual users will be able to obtain explicit answers from a large relational database of aircraft flight and maintenance data using a system called PLANES. The design and implementation of this system is described and illustrated with detailed examples of the operation of system components and examples of overall system operation. The language processing portion of the system uses a number of augmented transition networks, each of which matches phrases with a specific meaning, along with context registers (history keepers) and concept case frames; these are used for judging meaningfulness of questions, generating dialogue for clarifying partially understood questions, and resolving ellipsis and pronoun reference problems. Other system components construct a formal query for the relational database, and optimize the order of searching relations. Methods are discussed for handling vague or complex questions and for providing browsing ability. Also included are discussions of important issues in programming natural language systems for limited domains, and the relationship of this system to others. [ABSTRACT FROM AUTHOR] Copyright of Communications of the ACM is the property of Association for Computing Machinery and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)"
NOT_RELEVANT;EBSCOhost;An improvement key deletion method for double-array structure using single-nodes;"Oono, Masaki; Fuketa, Masao; Morita, Kazuhiro; Kashiji, Shinkaku; Aoe, Jun-ichi";NOT_FOUND;10.1016/S0306-4573(02)00090-0;NOT_FOUND;A trie is a well known method for various dictionaries, such as spelling check and morphological analysis. A double-array structure is an efficient data structure combining fast access of a matrix form with the compactness of a list form. The drawback of the double-array is that the space efficiency degrades by empty elements produced in key deletion. Morita presented a key deletion method eliminating empty elements. However, the space efficiency of this method is low for high frequent deletion and deletion takes much time because the cost depends on the number of the empty elements. This paper presents a fast and compact deletion method by using the property of nodes that have no brothers. From simulation results for 100,000 keys, the present method is about 330 times faster than Morita’s method and keeps high space efficiency. [Copyright &y& Elsevier] Copyright of Information Processing & Management is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Automatic identification of light stop words for Persian information retrieval systems.;"Sadeghi, Mohammad; Vegas, Jesús";NOT_FOUND;10.1177/0165551514530655;NOT_FOUND;Stop word identification is one of the most important tasks for many text processing applications such as information retrieval. Stop words occur too frequently in documents in a collection and do not contribute significantly to determining the context or information about the documents. These words are worthless as index terms and should be removed during indexing as well as before querying by an information retrieval system. In this paper, we propose an automatic aggregated methodology based on term frequency, normalized inverse document frequency and information model to extract the light stop words from Persian text. We define a ‘light stop word’ as a stop word that has few letters and is not a compound word. In the Persian language, a complete stop word list can be derived by combining the light stop words. The evaluation results, using a standard corpus, show a good percentage of coincidence between the Persian and English stop words and a significant improvement in the number of index terms. Specifically, the first 32 Persian light stop words have a great impact on the index size reduction and the set of stop words can reduce the number of index terms by about 27%. [ABSTRACT FROM PUBLISHER] Copyright of Journal of Information Science is the property of Sage Publications, Ltd. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Automatic Parsing for Content Analysis.;"Damerau, Frederick J.; Borrow, D. G.";NOT_FOUND;10.1145/362384.362495;NOT_FOUND;Although automatic syntactic and semantic analysis is not yet possible for all of an unrestricted natural language text, some applications, of which content analysis is one, do not have such a stringent coverage requirement. Preliminary studies show that the Harvard Syntactic Analyzer can produce correct and unambiguous identification of the subject and object of certain verbs for approximately half of the relevant occurrences. This provides a degree of coverage for content analysis variables which compares favorably to manual methods, in which only a sample of the total available text is normally processed. [ABSTRACT FROM AUTHOR] Copyright of Communications of the ACM is the property of Association for Computing Machinery and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
MAYBE_RELEVANT;EBSCOhost;Automatically structuring domain knowledge from text: An overview of current research;"Clark, Malcolm; Kim, Yunhyong; Kruschwitz, Udo; Song, Dawei; Albakour, Dyaa; Dignum, Stephen; Beresi, Ulises Cerviño; Fasli, Maria; De Roeck, Anne";NOT_FOUND;10.1016/j.ipm.2011.07.002;NOT_FOUND;This paper presents an overview of automatic methods for building domain knowledge structures (domain models) from text collections. Applications of domain models have a long history within knowledge engineering and artificial intelligence. In the last couple of decades they have surfaced noticeably as a useful tool within natural language processing, information retrieval and semantic web technology. Inspired by the ubiquitous propagation of domain model structures that are emerging in several research disciplines, we give an overview of the current research landscape and some techniques and approaches. We will also discuss trade-offs between different approaches and point to some recent trends. [Copyright &y& Elsevier] Copyright of Information Processing & Management is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Categorization of services for seeking information in biomedical literature: a typology for improvement of practice.;"Jung-jae Kim; Rebholz-Schuhmann, Dietrich";NOT_FOUND;10.1093/bib/bbn032;NOT_FOUND;"Biomedical researchers have to efficiently explore the scientific literature, keeping the focus on their research. This goal can only be achieved if the available means for accessing the literature meet the researchers' retrieval needs and if they understand how the tools filter the perpetually increasing number of documents. We have examined existing web-based services for information retrieval in order to give users guidance to improve their everyday practice of literature analysis. We propose two dimensions along which the services may be categorized: categories of input and output formats; and categories of behavioural usage. The categorization would be helpful for biologists to understand the differences in the input and output formats and the tasks they fulfil in information-retrieval activities. Also, they may inspire future bioinformaticians to further innovative development in this field. [ABSTRACT FROM AUTHOR] Copyright of Briefings in Bioinformatics is the property of Oxford University Press / USA and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)"
NOT_RELEVANT;EBSCOhost;Descriptive Document Clustering via Discriminant Learning in a Co-Embedded Space of Multilevel Similarities.;"Mu, Tingting; Goulermas, John Y.; Korkontzelos, Ioannis; Ananiadou, Sophia";NOT_FOUND;10.1002/asi.23374;NOT_FOUND;Descriptive document clustering aims at discovering clusters of semantically interrelated documents together with meaningful labels to summarize the content of each document cluster. In this work, we propose a novel descriptive clustering framework, referred to as CEDL. It relies on the formulation and generation of 2 types of heterogeneous objects, which correspond to documents and candidate phrases, using multilevel similarity information. CEDL is composed of 5 main processing stages. First, it simultaneously maps the documents and candidate phrases into a common co-embedded space that preserves higher-order, neighbor-based proximities between the combined sets of documents and phrases. Then, it discovers an approximate cluster structure of documents in the common space. The third stage extracts promising topic phrases by constructing a discriminant model where documents along with their cluster memberships are used as training instances. Subsequently, the final cluster labels are selected from the topic phrases using a ranking scheme using multiple scores based on the extracted co-embedding information and the discriminant output. The final stage polishes the initial clusters to reduce noise and accommodate the multitopic nature of documents. The effectiveness and competitiveness of CEDL is demonstrated qualitatively and quantitatively with experiments using document databases from different application fields. [ABSTRACT FROM AUTHOR] Copyright of Journal of the Association for Information Science & Technology is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Development of ontology from Indian agricultural e-governance data using IndoWordNet: a semantic web approach.;"Sinha, Bhaskar; Chandra, Somnath; Garg, Megha";NOT_FOUND;10.1108/JKM-10-2014-0441;NOT_FOUND;Purpose -- The purpose of this explorative research study is to focus on the implementation of semantic Web technology on agriculture domain of e-governance data. The study contributes to an understanding of problems and difficulties in implantations of unstructured and unformatted unique datasets of multilingual local language-based electronic dictionary (IndoWordnet). Design/methodology/approach -- An approach to an implementation in the perspective of conceptual logical concept to realization of agriculture-based terms and terminology extracted from linked multilingual IndoWordNet while maintaining the support and specification of the World Wide Web Consortium (W3C) standard of semantic Web technology to generate ontology and uniform unicode structured datasets. Findings -- The findings reveal the fact about partial support of extraction of terms, relations and concepts while linking to IndoWordNet, resulting in the form of SynSets, lexical relations of Words and relations between themselves. This helped in generation of ontology, hierarchical modeling and creation of structured metadata datasets. Research limitations/implications -- IndoWordNet has limitations, as it is not fully revised version due to diversified cultural base in India, and the new version is yet to be released in due time span. As mentioned in Section 5, implications of these ideas and experiments will have good impact in doing more exploration and better applications using such wordnet. Practical implications -- Language developer tools and frameworks have been used to get tagged annotated raw data processed and get intermediate results, which provides as a source for the generation of ontology and dynamic metadata. Social implications -- The results are expected to be applied for other e-governance applications. Better use of applications in social and government departments. Originality/value -- The authors have worked out experimental facts and raw information source datasets, revealing satisfactory results such as SynSets, sensecount, semantic and lexical relations, class concepts hierarchy and other related output, which helped in developing ontology of domain interest and, hence, creation of a dynamic metadata which can be globally used to facilitate various applications support. [ABSTRACT FROM AUTHOR] Copyright of Journal of Knowledge Management is the property of Emerald Publishing Limited and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Distance Learning for Similarity Estimation.;"Jie Yu; Amores, Jaume; Sebe, Nicu; Radeva, Petia; Qi Tian";NOT_FOUND;10.1109/TPAMI.2007.70714;NOT_FOUND;In this paper, we present a general guideline to find a better distance measure for similarity estimation based on statistical analysis of distribution models and distance functions. A new set of distance measures are derived from the harmonic distance, the geometric distance, and their generalized variants according to the Maximum Likelihood theory. These measures can provide a more accurate feature model than the classical euclidean and Manhattan distances. We also find that the feature elements are often from heterogeneous sources that may have different influence on similarity estimation. Therefore, the assumption of single isotropic distribution model is often inappropriate. To alleviate this problem, we use a boosted distance measure framework that finds multiple distance measures, which fit the distribution of selected feature elements best for accurate similarity estimation. The new distance measures for similarity estimation are tested on two applications: stereo matching and motion tracking in video sequences. The performance of boosted distance measure is further evaluated on several benchmark data sets from the UCI repository and two image retrieval applications. In all the experiments, robust results are obtained based on the proposed methods. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Pattern Analysis & Machine Intelligence is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Elicitation and use of relevance feedback information;"Vechtomova, Olga; Karamuftuoglu, Murat";NOT_FOUND;10.1016/j.ipm.2004.10.006;NOT_FOUND;Abstract: The paper presents two approaches to interactively refining user search formulations and their evaluation in the new High Accuracy Retrieval from Documents (HARD) track of TREC-12. The first method consists of asking the user to select a number of sentences that represent documents. The second method consists of showing to the user a list of noun phrases extracted from the initial document set. Both methods then expand the query based on the user feedback. The TREC results show that one of the methods is an effective means of interactive query expansion and yields significant performance improvements. The paper presents a comparison of the methods and detailed analysis of the evaluation results. [Copyright &y& Elsevier] Copyright of Information Processing & Management is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Extraction of complex index terms in non-English IR: A shallow parsing based approach;"Vilares, Jesús; Alonso, Miguel A.; Vilares, Manuel";NOT_FOUND;10.1016/j.ipm.2007.12.005;NOT_FOUND;The performance of information retrieval systems is limited by the linguistic variation present in natural language texts. Word-level natural language processing techniques have been shown to be useful in reducing this variation. In this article, we summarize our work on the extension of these techniques for dealing with phrase-level variation in European languages, taking Spanish as a case in point. We propose the use of syntactic dependencies as complex index terms in an attempt to solve the problems deriving from both syntactic and morpho-syntactic variation and, in this way, to obtain more precise index terms. Such dependencies are obtained through a shallow parser based on cascades of finite-state transducers in order to reduce as far as possible the overhead due to this parsing process. The use of different sources of syntactic information, queries or documents, has been also studied, as has the restriction of the dependencies applied to those obtained from noun phrases. Our approaches have been tested using the CLEF corpus, obtaining consistent improvements with regard to classical word-level non-linguistic techniques. Results show, on the one hand, that syntactic information extracted from documents is more useful than that from queries. On the other hand, it has been demonstrated that by restricting dependencies to those corresponding to noun phrases, important reductions of storage and management costs can be achieved, albeit at the expense of a slight reduction in performance. [Copyright &y& Elsevier] Copyright of Information Processing & Management is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Fast and compact updating algorithms of a double-array structure;"Morita, Kazuhiro; Atlam, El-Sayed; Fuketa, Masao; Tsuda, Kazuhiko; Aoe, Jun-ichi";NOT_FOUND;10.1016/S0020-0255(03)00189-0;NOT_FOUND;In many information retrieval applications, it is necessary to be able to adopt a trie search for looking at the input character by character. As a fast and compact data structure for a trie, a double-array is presented. However, the insertion time is not faster than other dynamic retrieval methods because the double-array is a semi-static retrieval method that cannot treat high frequent updating. Further, the space efficiency of the double-array degrades with the number of deletions because it keeps empty elements produced by deletion. This paper presents a fast insertion algorithm by linking empty elements to find inserting positions quickly and a compression algorithm by reallocating empty elements for each deletion. From the simulation results for 100 thousands keys, it turned out that the insertion time and the space efficiency are achieved. [Copyright &y& Elsevier] Copyright of Information Sciences is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Genetic-based approaches in ranking function discovery and optimization in information retrieval — A framework;"Fan, Weiguo; Pathak, Praveen; Zhou, Mi";NOT_FOUND;10.1016/j.dss.2009.04.005;NOT_FOUND;An Information Retrieval (IR) system consists of document collection, queries issued by users, and the matching/ranking functions used to rank documents in the predicted order of relevance for a given query. A variety of ranking functions have been used in the literature. But studies show that these functions do not perform consistently well across different contexts. In this paper we propose a two-stage integrated framework for discovering and optimizing ranking functions used in IR. The first stage, discovery process, is accomplished by intelligently leveraging the structural and statistical information available in HTML documents by using Genetic Programming techniques to yield novel ranking functions. In the second stage, the optimization process, document retrieval scores of various well-known ranking functions are combined using Genetic Algorithms. The overall discovery and optimization framework is tested on the well-known TREC collection of web documents for both the ad-hoc retrieval task and the routing task. Utilizing our framework we observe a significant increase in retrieval performance compared to some of the well-known stand alone ranking functions. [Copyright &y& Elsevier] Copyright of Decision Support Systems is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Identifying Features in Opinion Mining via Intrinsic and Extrinsic Domain Relevance.;"Hai, Zhen; Chang, Kuiyu; Kim, Jung-Jae; Yang, Christopher C.";NOT_FOUND;10.1109/TKDE.2013.26;NOT_FOUND;The vast majority of existing approaches to opinion feature extraction rely on mining patterns only from a single review corpus, ignoring the nontrivial disparities in word distributional characteristics of opinion features across different corpora. In this paper, we propose a novel method to identify opinion features from online reviews by exploiting the difference in opinion feature statistics across two corpora, one domain-specific corpus (i.e., the given review corpus) and one domain-independent corpus (i.e., the contrasting corpus). We capture this disparity via a measure called domain relevance (DR), which characterizes the relevance of a term to a text collection. We first extract a list of candidate opinion features from the domain review corpus by defining a set of syntactic dependence rules. For each extracted candidate feature, we then estimate its intrinsic-domain relevance (IDR) and extrinsic-domain relevance (EDR) scores on the domain-dependent and domain-independent corpora, respectively. Candidate features that are less generic (EDR score less than a threshold) and more domain-specific (IDR score greater than another threshold) are then confirmed as opinion features. We call this interval thresholding approach the intrinsic and extrinsic domain relevance (IEDR) criterion. Experimental results on two real-world review domains show the proposed IEDR approach to outperform several other well-established methods in identifying opinion features. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Knowledge & Data Engineering is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Improving automatic bug assignment using time‐metadata in term‐weighting.;"Shokripour, Ramin; Anvik, John; Kasirun, Zarinah M.; Zamani, Sima";NOT_FOUND;10.1049/iet-sen.2013.0150;NOT_FOUND;Assigning newly reported bugs to project developers is a time‐consuming and tedious task for triagers using the traditional manual bug triage process. Previous efforts for creating automatic bug assignment systems use machine learning and information‐retrieval techniques. These approaches commonly use tf‐idf, a statistical computation technique for weighting terms based on term frequency. However, tf‐idf does not consider the metadata, such as the time frame at which a term was used, when calculating the weight of the terms. This study proposes an alternate term‐weighting technique to improve the accuracy of automatic bug assignment approaches that use a term‐weighting technique. This technique includes the use of metadata in addition to the statistical computation to calculate the term weights. Moreover, it restricts the set of terms used to only nouns. It was found that when using only nouns and the proposed term‐weighting technique, the accuracy of an automatic bug assignment approach improves from 12 to 49% over tf‐idf for three open‐source projects. [ABSTRACT FROM AUTHOR] Copyright of IET Software (Wiley-Blackwell) is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Independent component analysis for near-synonym choice;"Yu, Liang-Chih; Chien, Wei-Nan";NOT_FOUND;10.1016/j.dss.2012.12.038;NOT_FOUND;Abstract: Despite their similar meanings, near-synonyms may have different usages in different contexts, and the development of algorithms that can verify whether near-synonyms do match their given contexts has been the focus of increasing concern. Such algorithms have many applications such as query expansion for information retrieval (IR), alternative word selection for writing support systems, and (near-)duplicate detection for text summarization. In this paper, we propose a framework that incorporates latent semantic analysis (LSA) and independent component analysis (ICA) to automatically select suitable near-synonyms according to the given context. LSA is used to discover useful latent features that do not frequently occur in the contexts of near-synonyms, and ICA is used to estimate a set of independent components by minimizing the dependence between features. An SVM classifier is then trained with the independent components for best near-synonym prediction. In experiments, we evaluate the proposed method on both Chinese and English sentences, and compare its performance to state-of-the-art supervised and unsupervised methods. Experimental results show that training on the independent components that contain useful contextual features with minimized term dependence can improve the classifiers'' ability to discriminate among near-synonyms, thus yielding better performance. [Copyright &y& Elsevier] Copyright of Decision Support Systems is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;"Information Retrieval: Searching in the 21st Century; Human Information Retrieval.";Larson, Ray R.;NOT_FOUND;10.1002/asi.21399;NOT_FOUND;"The article reviews two books ""Information Retrieval: Searching in the 21st Century,"" by Ayşe Göker and ""Human Information Retrieval,"" by Julian Warner."
NOT_RELEVANT;EBSCOhost;Integrating Discriminant and Descriptive Information for Dimension Reduction and Classification.;"Jie Yu; Qi Tian; Ting Rui; Huang, Thomas S.";NOT_FOUND;10.1109/TCSVT.2007.890861;NOT_FOUND;In this paper, a novel hybrid dimension reduction technique for classification is proposed based on the hybrid analysis of principal component analysis (PCA) and linear discriminant analysis (LDA). LDA is known for capturing the most discriminant features of the data in the projected space while PCA is known for preserving the most descriptive ones after projection. Our hybrid technique integrates discriminant and descriptive information and finds a richer set of alternatives beyond LDA and PCA in a 2-D parametric space, which fits a specific classification task and data distribution better. Theoretical study shows that our technique also alleviates the singularity problem of scatter matrix, which is caused by small training set, and increases the effective dimension of the projected subspace. In order to find the hybrid features adaptively and avoid exhaustive parameter searching, we further propose a boosted hybrid analysis method that incorporates a nonlinear boosting process to enhance a set of hybrid classifiers and combine them into a more accurate one. Compared with the other techniques that aim at combining PCA and LDA, our approaches are novel because our method finds alternatives to LDA and PCA in a 2-D parameter space and the boosting process provides enhancement and robust combination of the classifiers. Extensive experiments are conducted on benchmark and real image databases to compare our proposed methods with the state-of-the-art linear and nonlinear discriminant analysis techniques. The results show the superior performance of our hybrid analysis methods. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Circuits & Systems for Video Technology is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Introduction to the special issue on patent processing;"Fujii, Atsushi; Iwayama, Makoto; Kando, Noriko";NOT_FOUND;10.1016/j.ipm.2006.11.004;NOT_FOUND;Abstract: The processing of intellectual property documents, such as patents, has been important to the industry, business, and law communities. Recently, the importance of patent processing has also been recognized in academic research communities, particularly by information retrieval and natural language processing researchers. In addition, large test collections that include patents have recently become available, to enable the systematic evaluation of methodologies from a scientific point of view. In the light of these activities, this special issue is intended to collect advanced research papers on patent processing. As an introduction to the special issue on patent processing, this paper surveys the relevant literature and outlines the papers selected for the special issue. [Copyright &y& Elsevier] Copyright of Information Processing & Management is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Is 1 noun worth 2 adjectives? Measuring relative feature utility;Losee, Robert M.;NOT_FOUND;10.1016/j.ipm.2005.11.002;NOT_FOUND;Abstract: Are two adjectives worth the same as a single noun when documents are ordered based on decreasing topicality? We propose an easy to interpret single number Relative Feature Utility (RFU) measure of the relative worth of using specific linguistic or non-linguistic features or sets of features in computational systems that order or filter media, such as information retrieval and classification systems. This measure allows one to make easily interpreted claims about the relative utility of features such as parts-of-speech, term suffixes, phrases vs. single terms, annotations, hyperlinks, citations, index terms, and metadata when ordering natural language text or other media. Data is provided for the RFU for stemming characteristics, part-of-speech tags, and phrase lengths, as well as retrieval characteristics and procedures. Using this linear measure of the relative utility of features makes available a wide range of cost-benefit analyses and decision theoretic techniques, allowing the study of whether or not to use many different kinds of representational information or tagging systems, and for the design of indexing and metadata systems. Some characteristics of natural languages used in the spectrum from softer to harder sciences, as well as medical terminology, are studied. [Copyright &y& Elsevier] Copyright of Information Processing & Management is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Karen Spärck Jones.;"Robertson, Stephen; Tait, John";NOT_FOUND;10.1002/asi.20784;NOT_FOUND;An obituary is presented for computer science researcher Karen Spärck Jones.
NOT_RELEVANT;EBSCOhost;Language-modeling kernel based approach for information retrieval.;"Ying Xie; Raghavan, Vijay V.";NOT_FOUND;10.1002/asi.20711;NOT_FOUND;In this presentation, we propose a novel integrated information retrieval approach that provides a unified solution for two challenging problems in the field of information retrieval. The first problem is how to build an optimal vector space corresponding to users' different information needs when applying the vector space model. The second one is how to smoothly incorporate the advantages of machine learning techniques into the language modeling approach. To solve these problems, we designed the language-modeling kernel function, which has all the modeling powers provided by language modeling techniques. In addition, for each information need, this kernel function automatically determines an optimal vector space, for which a discriminative learning machine, such as the support vector machine, can be applied to find an optimal decision boundary between relevant and nonrelevant documents. Large-scale experiments on standard test-beds show that our approach makes significant improvements over other state-of-the-art information retrieval methods. [ABSTRACT FROM AUTHOR] Copyright of Journal of the American Society for Information Science & Technology is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Lexical and Syntactic knowledge for Information Retrieval;Ferrández, Antonio;NOT_FOUND;10.1016/j.ipm.2011.01.003;NOT_FOUND;Abstract: Traditional Information Retrieval (IR) models assume that the index terms of queries and documents are statistically independent of each other, which is intuitively wrong. This paper proposes the incorporation of the lexical and syntactic knowledge generated by a POS-tagger and a syntactic Chunker into traditional IR similarity measures for including this dependency information between terms. Our proposal is based on theories of discourse structure by means of the segmentation of documents and queries into sentences and entities. Therefore, we measure dependencies between entities instead of between terms. Moreover, we handle discourse references for each entity. It has been evaluated on Spanish and English corpora as well as on Question Answering tasks obtaining significant increases. [Copyright &y& Elsevier] Copyright of Information Processing & Management is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Mining knowledge from natural language texts using fuzzy associated concept mapping;"Wang, W.M.; Cheung, C.F.; Lee, W.B.; Kwok, S.K.";NOT_FOUND;10.1016/j.ipm.2008.05.002;NOT_FOUND;Abstract: Natural Language Processing (NLP) techniques have been successfully used to automatically extract information from unstructured text through a detailed analysis of their content, often to satisfy particular information needs. In this paper, an automatic concept map construction technique, Fuzzy Association Concept Mapping (FACM), is proposed for the conversion of abstracted short texts into concept maps. The approach consists of a linguistic module and a recommendation module. The linguistic module is a text mining method that does not require the use to have any prior knowledge about using NLP techniques. It incorporates rule-based reasoning (RBR) and case based reasoning (CBR) for anaphoric resolution. It aims at extracting the propositions in text so as to construct a concept map automatically. The recommendation module is arrived at by adopting fuzzy set theories. It is an interactive process which provides suggestions of propositions for further human refinement of the automatically generated concept maps. The suggested propositions are relationships among the concepts which are not explicitly found in the paragraphs. This technique helps to stimulate individual reflection and generate new knowledge. Evaluation was carried out by using the Science Citation Index (SCI) abstract database and CNET News as test data, which are well known databases and the quality of the text is assured. Experimental results show that the automatically generated concept maps conform to the outputs generated manually by domain experts, since the degree of difference between them is proportionally small. The method provides users with the ability to convert scientific and short texts into a structured format which can be easily processed by computer. Moreover, it provides knowledge workers with extra time to re-think their written text and to view their knowledge from another angle. [Copyright &y& Elsevier] Copyright of Information Processing & Management is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Natural language asymmetry and internet infrastructures.;di Sciullo, Anna Maria;NOT_FOUND;10.1504/IJEB.2005.007276;NOT_FOUND;"This article presents the summary of an article published in the ""International Journal of Electronic Business,"" which examines the main features of an information retrieval and extraction system based on natural language asymmetric relations. The article illustrates that asymmetric relations help in improving the performance of search engines. An information retrieval and extraction system based on the recovery of a subset of asymmetric relations is compared with current operating search engines based on keyword search and Boolean analysis. Finally, it has been shown that natural language asymmetries are important elements of internet infrastructures and ensure greater precision to internet communication."
NOT_RELEVANT;EBSCOhost;Natural language processing and query expansion in legal information retrieval: Challenges and a response.;"Maxwell, Tamsin; Schafer, Burkhard";NOT_FOUND;10.1080/13600860903570194;NOT_FOUND;As methods in legal information retrieval (IR) evolve to meet the demands of rapidly increasing stores of electronic information, there is the intuitive appeal of capturing detail in legal queries with natural language processing (NLP). One difficulty with this approach is that incorporation of word dependencies in IR has not been shown to consistently and reliably improve results over a unigram bag-of-words approach. We consider challenges faced when incorporating NLP in IR and briefly review three proposals in this vein, highlighting how these might have responded better to requirements in legal search. We then present our novel response based on split query expansion that accounts for the way lawyers seek to apply search results whilst meeting the challenges identified in a unique and flexible manner. [ABSTRACT FROM AUTHOR] Copyright of International Review of Law, Computers & Technology is the property of Routledge and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Predictive hierarchical human augmented map generation for itinerary perception.;"Kaur, B.; Bhattacharya, J.";NOT_FOUND;10.1049/el.2016.0397;NOT_FOUND;A new class of augmented map application is introduced which can provide detailed knowledge about any area, to a user. This brief particularly focuses on obtaining itinerary perception subject to different environmental conditions. This refers to extraction of traffic related information from an augmented map. The problem is modelled as a machine learning technique where the traffic distribution at different times (including same days, different days and different weather) are observed continuously using a service robot. This data is posed as a Gaussian process for post‐estimation. Our system consists of a vision sensor which will acquire the region of interest input, queried to a database of traffic density distributions, learned from the scenes at different points of time. The user interacting with the system will obtain an information pertaining to the region conditioned on environmental and timing events. [ABSTRACT FROM AUTHOR] Copyright of Electronics Letters (Wiley-Blackwell) is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Project-Based As-Needed Information Retrieval from Unstructured AEC Documents.;"Fan, Hongqin; Xue, Fan; Li, Heng";NOT_FOUND;10.1061/(ASCE)ME.1943-5479.0000341;NOT_FOUND;With the increasing complexity of architecture, engineering, and construction (AEC) projects and fast track execution of project works, written documents are becoming more and more important for project coordination, communication, and works control. Finding all the relevant information from unstructured construction documents is critical to various management tasks such as work planning, progress control, and claims. A framework is proposed in this research to retrieve project-wide as-needed information from AEC documents. Through this framework, improvement in the levels of precision and recall in the information retrieval process can be made effective through the use of a project-specific term dictionary and dependency grammar parsing information of textual documents. Their effectiveness is demonstrated through a series of experimental tests conducted on a real life building redevelopment project with different information retrieval and ranking strategies. The results and findings are presented in this paper along with discussion on the related issues on research and system development. [ABSTRACT FROM AUTHOR] Copyright of Journal of Management in Engineering is the property of American Society of Civil Engineers and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Roundtable: What's Next in Software Analytics.;"Hassan, Ahmed E.; Hindle, Abram; Runeson, Per; Shepperd, Martin; Devanbu, Prem; Kim, Sunghun";NOT_FOUND;10.1109/MS.2013.85;NOT_FOUND;"For this special issue, the guest editors asked a panel of six established experts in software analytics to highlight what they thought were the most important, or overlooked, aspect of this field. They all pleaded for a much broader view of analytics than seen in current practice: software analytics should go beyond developers (Ahmed Hassan) and numbers (Per Runeson). Analytics should also prove its relevance to practitioners (Abram Hindle, Martin Shepperd). There are now opportunities for ""natural"" software analytics based on statistical natural language processing (Prem Devanbu). Lastly, software analytics needs information analysts and field agents like Chloe O'Brian and Jack Bauer in the TV show 24 (Sung Kim). [ABSTRACT FROM PUBLISHER] Copyright of IEEE Software is the property of IEEE Computer Society and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)"
NOT_RELEVANT;EBSCOhost;RUBRIC: A System for Rule-Based Information Retrieval.;"McCune, Brian P.; Tong, Richard M.; Dean, Jeffrey S.; Shapiro, Daniel G.";NOT_FOUND;NOT_FOUND;NOT_FOUND;A research prototype software system for conceptual information retrieval has been developed. The goal of the system, called RUBRIC, is to provide more automated and relevant access to unformatted textual databases. The approach is to use production rules from artificial intelligence to define a hierarchy of retrieval subtopics, with fuzzy context expressions and specific word phrases at the bottom. RUBRIC allows the definition of detailed queries starting at a conceptual level, partial matching of a query and a document, selection of only the highest ranked documents for presentation to the user, and detailed explanation of how and why a particular document was selected. Initial experiments indicate that a RUBRIC rule set better matches human retrieval judgment than a standard Boolean keyword expression, given equal amounts of effort in defining each. The techniques presented may be useful in stand-alone retrieval systems, front-ends to existing information retrieval systems, or real-time document filtering and routing. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Software Engineering is the property of IEEE Computer Society and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Sistemas de recuperación de información adaptados al dominio biomédico.;"Marrero, Mónica; Sánchez-Cuadrado, Sonia; Urbano, Julián; Morato, Jorge; Moreiro, José-Antonio";NOT_FOUND;10.3145/epi.2010.may.04;NOT_FOUND;The terminology used in biomedicine has lexical characteristics that have required the elaboration of terminological resources and information retrieval systems with specific functionalities. The main characteristics are the high rates of synonymy and homonymy, due to phenomena such as the proliferation of polysemic acronyms and their interaction with common language. Information retrieval systems in the biomedical domain use techniques oriented to the treatment of these lexical peculiarities. In this paper we review some of these techniques, such as the application of Natural Language Processing (BioNLP), the incorporation of lexical-semantic resources, and the application of Named Entity Recognition (BioNER). Finally, we present the evaluation methods adopted to assess the suitability of these techniques for retrieving biomedical resources. (English) [ABSTRACT FROM AUTHOR] La terminología usada en biomedicina tiene rasgos léxicos que han requerido la elaboración de recursos terminológicos y sistemas de recuperación de información con funciones específicas. Las principales características son las elevadas tasas de sinonimia y homonimia, debidas a fenómenos como la proliferación de siglas polisémicas y su interacción con el lenguaje común. Los sistemas de recuperación de información en el dominio biomédico utilizan técnicas orientadas al tratamiento de estas peculiaridades léxicas. Se revisan algunas de estas técnicas, como la aplicación de Procesamiento del Lenguaje Natural (BioNLP), la incorporación de recursos léxico-semánticos, y la aplicación de Reconocimiento de Entidades (BioNER). Se presentan los métodos de evaluación adoptados para comprobar la adecuación de estas técnicas en la recuperación de recursos biomédicos. (Spanish) [ABSTRACT FROM AUTHOR] Copyright of El Profesional de la Información is the property of EPI SCP and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Spatially Aware Term Selection for Geotagging.;"Van Laere, Olivier; Quinn, Jonathan; Schockaert, Steven; Dhoedt, Bart";NOT_FOUND;10.1109/TKDE.2013.42;NOT_FOUND;The task of assigning geographic coordinates to textual resources plays an increasingly central role in geographic information retrieval. The ability to select those terms from a given collection that are most indicative of geographic location is of key importance in successfully addressing this task. However, this process of selecting spatially relevant terms is at present not well understood, and the majority of current systems are based on standard term selection techniques, such as $(\chi^2)$ or information gain, and thus fail to exploit the spatial nature of the domain. In this paper, we propose two classes of term selection techniques based on standard geostatistical methods. First, to implement the idea of spatial smoothing of term occurrences, we investigate the use of kernel density estimation (KDE) to model each term as a two-dimensional probability distribution over the surface of the Earth. The second class of term selection methods we consider is based on Ripley's K statistic, which measures the deviation of a point set from spatial homogeneity. We provide experimental results which compare these classes of methods against existing baseline techniques on the tasks of assigning coordinates to Flickr photos and to Wikipedia articles, revealing marked improvements in cases where only a relatively small number of terms can be selected. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Knowledge & Data Engineering is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Structuring Tweets for Improving Twitter Search.;"Luo, Zhunchen; Yu, Yang; Osborne, Miles; Wang, Ting";NOT_FOUND;10.1002/asi.23332;NOT_FOUND;Spam and wildly varying documents make searching in Twitter challenging. Most Twitter search systems generally treat a Tweet as a plain text when modeling relevance. However, a series of conventions allows users to Tweet in structural ways using a combination of different blocks of texts. These blocks include plain texts, hashtags, links, mentions, etc. Each block encodes a variety of communicative intent and the sequence of these blocks captures changing discourse. Previous work shows that exploiting the structural information can improve the structured documents (e.g., web pages) retrieval. In this study we utilize the structure of Tweets, induced by these blocks, for Twitter retrieval and Twitter opinion retrieval. For Twitter retrieval, a set of features, derived from the blocks of text and their combinations, is used into a learning-to-rank scenario. We show that structuring Tweets can achieve state-of-the-art performance. Our approach does not rely on social media features, but when we do add this additional information, performance improves significantly. For Twitter opinion retrieval, we explore the question of whether structural information derived from the body of Tweets and opinionatedness ratings of Tweets can improve performance. Experimental results show that retrieval using a novel unsupervised opinionatedness feature based on structuring Tweets achieves comparable performance with a supervised method using manually tagged Tweets. Topic-related specific structured Tweet sets are shown to help with query-dependent opinion retrieval. [ABSTRACT FROM AUTHOR] Copyright of Journal of the Association for Information Science & Technology is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Turning WordNet into an Information Retrieval Resource: Systematic Polysemy and Conversion to Hierarchical Codes.;Mihalcea, Rada;NOT_FOUND;10.1142/S0218001403002605;NOT_FOUND;This paper addresses the problem of transforming WordNet into a resource tailored to Information Retrieval (IR) applications. We address two of the major drawbacks pointed out in previous literature in relation to this semantic network. One is the fine granularity of senses defined in WordNet, which proves useless from an IR perspective. To solve this problem, we propose a set of methods that enable the automatic transformation of WordNet into a coarse grained dictionary. The other drawback is the encoding used in this resource, and the methods for accessing related words across the semantic net. Due to the high number of connections among concepts, the simple computation of a path in this net, or the generation of related concepts may become a computationally intensive process. This effect is highly undesirable in time sensitive applications such as IR applications. We propose a methodology for hierarchical encoding that enables increased efficiency in WordNet-based IR systems. [ABSTRACT FROM AUTHOR] Copyright of International Journal of Pattern Recognition & Artificial Intelligence is the property of World Scientific Publishing Company and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;EBSCOhost;Why are these similar? Investigating item similarity types in a large digital library.;"Gonzalez ‐ Agirre, Aitor; Rigau, German; Agirre, Eneko; Aletras, Nikolaos; Stevenson, Mark";NOT_FOUND;10.1002/asi.23482;NOT_FOUND;We introduce a new problem, identifying the type of relation that holds between a pair of similar items in a digital library. Being able to provide a reason why items are similar has applications in recommendation, personalization, and search. We investigate the problem within the context of Europeana, a large digital library containing items related to cultural heritage. A range of types of similarity in this collection were identified. A set of 1,500 pairs of items from the collection were annotated using crowdsourcing. A high intertagger agreement (average 71.5 Pearson correlation) was obtained and demonstrates that the task is well defined. We also present several approaches to automatically identifying the type of similarity. The best system applies linear regression and achieves a mean Pearson correlation of 71.3, close to human performance. The problem formulation and data set described here were used in a public evaluation exercise, the * SEM shared task on Semantic Textual Similarity. The task attracted the participation of 6 teams, who submitted 14 system runs. All annotations, evaluation scripts, and system runs are freely available. [ABSTRACT FROM AUTHOR] Copyright of Journal of the Association for Information Science & Technology is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)
NOT_RELEVANT;Taylor & Francis;Empirical evaluation of deep learning models for sentiment analysis;Ajeet Ram Pathak, Manjusha Pandey and Siddharth Rautaray;2019;10.1080/09720510.2019.1609554;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;Taylor & Francis;Artificial Intelligence and Music Discovery;Stephanie Bonjack and Nicole Trujillo;2024;10.1080/10588167.2023.2287924;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;Taylor & Francis;Exploiting salient semantic analysis for information retrieval;Jing Luo, Bo Meng, Changqin Quan and Xinhui Tu;2016;10.1080/17517575.2015.1080301;NOT_FOUND;NOT_FOUND
MAYBE_RELEVANT;Taylor & Francis;AQUA: A Closed-Domain Question Answering System;Maria Vargas-Vera and Miltiadis D. Lytras;2010;10.1080/10580530.2010.493825;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;Taylor & Francis;An evaluation of different statistical techniques of collocation extraction using a probability measure to word combinations;Raj Kishor Bisht, H. S. Dhami and Neeraj Tiwari;2006;10.1080/09296170600850064;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;Taylor & Francis;The effect of gamified project-based learning with AIGC in information literacy education;Wendan Huang, Taoli Wang and Yixin Tong;2024;10.1080/14703297.2024.2423012;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;Taylor & Francis;An Overview of Deep Generative Models;Jungang Xu, Hui Li and Shilong Zhou;2015;10.1080/02564602.2014.987328;NOT_FOUND;NOT_FOUND
MAYBE_RELEVANT;Taylor & Francis;Conversational Voice Assistants and a Case Study of Long-Term Users: A Human Information Behaviours Perspective;Indra Mckie, Bhuva Narayan and Baki Kocaballi;2022;10.1080/24750158.2022.2104738;NOT_FOUND;NOT_FOUND
MAYBE_RELEVANT;Taylor & Francis;Natural language processing and query expansion in legal information retrieval: Challenges and a response;Tamsin Maxwell and Burkhard Schafer;2010;10.1080/13600860903570194;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;Taylor & Francis;Geo-referencing with semi-automatic gazetteer expansion using lexico-syntactical patterns and co-reference analysis;Julio Godoy, John Atkinson and Andrea Rodriguez;2011;10.1080/13658816.2010.513981;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;ACM Digital Library;Utilizing BERT for Information Retrieval: Survey, Applications, Resources, and Challenges;Wang, Jiajia and Huang, Jimmy Xiangji and Tu, Xinhui and Wang, Junmei and Huang, Angela Jennifer and Laskar, Md Tahmid Rahman and Bhuiyan, Amran;2024;10.1145/3648471;NOT_FOUND;Recent years have witnessed a substantial increase in the use of deep learning to solve various natural language processing (NLP) problems. Early deep learning models were constrained by their sequential or unidirectional nature, such that they struggled to capture the contextual relationships across text inputs. The introduction of bidirectional encoder representations from transformers (BERT) leads to a robust encoder for the transformer model that can understand the broader context and deliver state-of-the-art performance across various NLP tasks. This has inspired researchers and practitioners to apply BERT to practical problems, such as information retrieval (IR). A survey that focuses on a comprehensive analysis of prevalent approaches that apply pretrained transformer encoders like BERT to IR can thus be useful for academia and the industry. In light of this, we revisit a variety of BERT-based methods in this survey, cover a wide range of techniques of IR, and group them into six high-level categories: (i) handling long documents, (ii) integrating semantic information, (iii) balancing effectiveness and efficiency, (iv) predicting the weights of terms, (v) query expansion, and (vi) document expansion. We also provide links to resources, including datasets and toolkits, for BERT-based IR systems. Additionally, we highlight the advantages of employing encoder-based BERT models in contrast to recent large language models like ChatGPT, which are decoder-based and demand extensive computational resources. Finally, we summarize the comprehensive outcomes of the survey and suggest directions for future research in the area.
NOT_RELEVANT;ACM Digital Library;Mathematical Information Retrieval: A Review;Dadure, Pankaj and Pakray, Partha and Bandyopadhyay, Sivaji;2024;10.1145/3699953;NOT_FOUND;Mathematical formulas are commonly used to demonstrate theories and basic fundamentals in the Science, Technology, Engineering, and Mathematics (STEM) domain. The burgeoning research in the STEM domain results in the mass production of scientific documents that contain both textual and mathematical terms. In scientific information, the definition of mathematical formulas is expressed through context and symbolic structure that adheres to strong domain-specific notions. Whereas the retrieval of textual information is well-researched, and numerous text-based search engines are present. However, textual information retrieval systems are inadequate for searching scientific information containing mathematical formulas, including simple symbols to complicated mathematical structures. The retrieval of mathematical information is in its infancy, and it requires the inclusion of new technologies and tools to promote the retrieval of scientific information and the management of digital libraries. This article provides a comprehensive study of mathematical information retrieval and highlights their challenges and future opportunities.
MAYBE_RELEVANT;ACM Digital Library;Towards Seamless User Query to REST API Conversion;Xu, Han;2024;10.1145/3627673.3680275;NOT_FOUND;Integrating Large Language Models (LLMs) with external tools and APIs is essential for fields such as information retrieval and knowledge management. While LLMs have made significant strides, their effective integration with external APIs-essential for real-world applications-remains challenging. This paper introduces RESTful-Llama, a novel method designed to empower open-source LLMs to accurately convert natural language instructions into well-formed RESTful API calls. Moreover, RESTful-Llama utilizes DOC-Prompt, a newly proposed technique for generating fine-tuning datasets from publicly available API documentation. Initial experiments demonstrate that RESTful-Llama significantly enhances the accuracy of generated REST API requests.
NOT_RELEVANT;ACM Digital Library;JayBot -- Aiding University Students and Admission with an LLM-based Chatbot;Odede, Julius and Frommholz, Ingo;2024;10.1145/3627508.3638293;NOT_FOUND;"This demo paper presents JayBot, an LLM-based chatbot system aimed at enhancing the user experience of prospective and current students, faculty, and staff at a UK university. The objective of JayBot is to provide information to users on general enquiries regarding course modules, duration, fees, entry requirements, lecturers, internship, career paths, course employability and other related aspects. Leveraging the use cases of generative artificial intelligence (AI), the chatbot application was built using OpenAI’s advanced large language model (GPT-3.5 turbo); to tackle issues such as hallucination as well as focus and timeliness of results, an embedding transformer model has been combined with a vector database and vector search. Prompt engineering techniques were employed to enhance the chatbot’s response abilities. Preliminary user studies indicate JayBot’s effectiveness and efficiency. The demo will showcase JayBot in a university admission use case and discuss further application scenarios."
MAYBE_RELEVANT;ACM Digital Library;Bias and Unfairness in Information Retrieval Systems: New Challenges in the LLM Era;Dai, Sunhao and Xu, Chen and Xu, Shicheng and Pang, Liang and Dong, Zhenhua and Xu, Jun;2024;10.1145/3637528.3671458;NOT_FOUND;With the rapid advancements of large language models (LLMs), information retrieval (IR) systems, such as search engines and recommender systems, have undergone a significant paradigm shift. This evolution, while heralding new opportunities, introduces emerging challenges, particularly in terms of biases and unfairness, which may threaten the information ecosystem. In this paper, we present a comprehensive survey of existing works on emerging and pressing bias and unfairness issues in IR systems when the integration of LLMs. We first unify bias and unfairness issues as distribution mismatch problems, providing a groundwork for categorizing various mitigation strategies through distribution alignment. Subsequently, we systematically delve into the specific bias and unfairness issues arising from three critical stages of LLMs integration into IR systems: data collection, model development, and result evaluation. In doing so, we meticulously review and analyze recent literature, focusing on the definitions, characteristics, and corresponding mitigation strategies associated with these issues. Finally, we identify and highlight some open problems and challenges for future work, aiming to inspire researchers and stakeholders in the IR field and beyond to better understand and mitigate bias and unfairness issues of IR in this LLM era. We also consistently maintain a GitHub repository for the relevant papers and resources in this rising direction at https://github.com/KID-22/LLM-IR-Bias-Fairness-Survey.
NOT_RELEVANT;ACM Digital Library;Enhancing Arabic Information Retrieval for Question Answering;Alghamdi, Muath and Abushawarib, Mohammed and Ellouh, Mahmoud and Ghaleb, Mustafa and Felemban, Muhamad;2024;10.1145/3644713.3644763;NOT_FOUND;"In the modern landscape of Natural Language Processing (NLP), intelligent chatbots like ChatGPT 3.5 and Google’s Bard have shown remarkable competence in generic question-answering (QA) tasks. However, their performance falters when navigating domain-specific QA, particularly in the Arabic language, which is celebrated for its complex morphology and syntax. This paper presents a comprehensive approach to address these issues. The aim of this research is to build a chatbot tailored for a university community. We first create an extensive Arabic Q&amp;A dataset by extracting data from academic documents, employing state-of-the-art Optical Character Recognition (OCR) tools. Then, we evaluate multiple text similarity measures like Pooled FastText Word embedding, BM25 ranking functions, and various semantic sentence embedding models. A thorough performance assessment reveals that the domain-specific model excels at both sentence-level similarity and context-relevance tasks. The developed web application chatbot, leveraging LangChain library and Retrieval Augmented Generation (RAG) methods, outperforms existing chatbots in domain-specific, Arabic language QA scenarios."
NOT_RELEVANT;ACM Digital Library;Tutorial on Landing Generative AI in Industrial Social and E-commerce Recsys;Xu, Da and Zhang, Danqing and Zheng, Lingling and Yang, Bo and Yang, Guangyu and Xu, Shuyuan and Liang, Cindy;2024;10.1145/3627673.3679099;NOT_FOUND;Over the past two years, GAI has evolved rapidly, influencing various fields including social and e-commerce Recsys. Despite exciting advances, landing these innovations in real-world Recsys remains challenging due to the sophistication of modern industrial product and systems. Our tutorial begins with a brief overview of building industrial Recsys and GAI fundamentals, followed by the ongoing efforts and opportunities to enhance personalized recommendations with foundation models.We then explore the integration of curation capabilities into Recsys, such as repurposing raw content, incorporating external knowledge, and generating personalized insights/explanations to foster transparency and trust. Next, the tutorial illustrates how AI agents can transform Recsys through interactive reasoning and action loops, shifting away from traditional passive feedback models. Finally, we shed insights on real-world solutions for human-AI alignment and responsible GAI practices.A critical component of the tutorial is detailing the AI, Infrastructure, LLMOps, and Product roadmap (including the evaluation and responsible AI practices) derived from the production solutions in LinkedIn, Amazon, TikTok, and Microsoft. While GAI in Recsys is still in its early stages, this tutorial provides valuable insights and practical solutions for the Recsys and GAI communities.
NOT_RELEVANT;ACM Digital Library;Deep API Sequence Generation via Golden Solution Samples and API Seeds;Huang, Yuekai and Wang, Junjie and Wang, Song and Wei, Moshi and Shi, Lin and Liu, Zhe and Wang, Qing;2024;10.1145/3695995;NOT_FOUND;Automatic API recommendation can accelerate developers’ programming, and has been studied for years. There are two orthogonal lines of approaches for this task, i.e., information retrieval-based (IR-based) approaches and sequence to sequence (seq2seq) model based approaches. Although these approaches were reported to have remarkable performance, our observation finds two major drawbacks, i.e., IR-based approaches lack the consideration of relations among the recommended APIs, and seq2seq models do not model the API’s semantic meaning. To alleviate the above two problems, we propose APIGens, which is a retrieval-enhanced large language model (LLM) based API recommendation approach to recommend an API sequence for a natural language query. The approach first retrieves similar programming questions in history based on the input natural language query, and then scores the results based on API documents via a scorer model. Finally, these results are used as samples for few-shot learning of LLM. To reduce the risk of encountering local optima, we also extract API seeds from the retrieved results to increase the search scope during the LLM generation process. The results show that our approach can achieve 48.41% ROUGE@10 on API sequence recommendation and the 82.61% MAP on API set recommendation, largely outperforming the state-of-the-art baselines.
MAYBE_RELEVANT;ACM Digital Library;TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision;Zhou, Ruiwen and Yang, Yingxuan and Wen, Muning and Wen, Ying and Wang, Wenhao and Xi, Chunling and Xu, Guoqiang and Yu, Yong and Zhang, Weinan;2024;10.1145/3626772.3657788;NOT_FOUND;Several large language model (LLM) agents have been constructed for diverse purposes such as web navigation and online shopping, leveraging the broad knowledge and text comprehension capabilities of LLMs. Many of these works rely on in-context examples to achieve generalization without requiring fine-tuning. However, few have addressed the challenge of selecting and effectively utilizing these examples. Recent approaches have introduced trajectory-level retrieval with task meta-data and the use of trajectories as in-context examples to enhance overall performance in some sequential decision making tasks like computer control. Nevertheless, these methods face issues like plausible examples retrieved without task-specific state transition dynamics and long input with plenty of irrelevant context due to using complete trajectories. In this paper, we propose a novel framework (TRAD) to tackle these problems. TRAD first employs Thought Retrieval for step-level demonstration selection through thought matching, enhancing the quality of demonstrations and reducing irrelevant input noise. Then, Aligned Decision is introduced to complement retrieved demonstration steps with their preceding or subsequent steps, providing tolerance for imperfect thought and offering a balance between more context and less noise. Extensive experiments on ALFWorld and Mind2Web benchmarks demonstrate that TRAD not only surpasses state-of-the-art models but also effectively reduces noise and promotes generalization. Furthermore, TRAD has been deployed in real-world scenarios of a global business insurance company and yields an improved success rate of robotic process automation. Our codes are available at: https://github.com/skyriver-2000/TRAD-Official.
NOT_RELEVANT;ACM Digital Library;Emotional Intelligence Attention Unsupervised Learning Using Lexicon Analysis for Irony-based Advertising;Ahmed, Usman and Lin, Jerry Chun-Wei and Srivastava, Gautam;2024;10.1145/3580496;NOT_FOUND;Social media platforms have made increasing use of irony in recent years. Users can express their ironic thoughts with audio, video, and images attached to text content. When you use irony, you are making fun of a situation or trying to make a point. It can also express frustration or highlight the absurdity of a situation. The use of irony in social media is likely to continue to increase, no matter the reason. By using syntactic information in conjunction with semantic exploration, we show that attention networks can be enhanced. Using learned embedding, unsupervised learning encodes word order into a joint space. By evaluating the entropy of an example class and adding instances, the active learning method uses the shared representation as a query to retrieve semantically similar sentences from a knowledge base. In this way, the algorithm can identify the instance with the maximum uncertainty and extract the most informative example from the training set. An ironic network trained for each labelled record is used to train a classifier (model). The partial training model and the original labelled data generate pseudo-labels for the unlabeled data. To correctly predict the label of a dataset, a classifier (attention network) updates the pseudo-labels for the remaining datasets. After the experimental evaluation of the 1,021 annotated texts, the proposed model performed better than the baseline models, achieving an F1 score of 0.63 on ironic tasks and 0.59 on non-ironic tasks. We also found that the proposed model generalized well to new instances of datasets.
MAYBE_RELEVANT;ACM Digital Library;Some Useful Things to Know When Combining IR and NLP: The Easy, the Hard and the Ugly;Alonso, Omar and Church, Kenneth;2024;10.1145/3616855.3636452;NOT_FOUND;"Deep nets such as GPT are at the core of the current advances in many systems and applications. Things are moving fast; techniques become obsolete quickly (within weeks). How can we take advantage of new discoveries and incorporate them into our existing work? Are new developments radical improvements, or incremental repetitions of established concepts, or combinations of both?In this tutorial, we aim to bring interested researchers and practitioners up to speed on the recent and ongoing techniques around ML and Deep learning in the context of IR and NLP. Additionally, our goal is to clarify terminology, emphasize fundamentals, and outline problems and new research opportunities."
MAYBE_RELEVANT;ACM Digital Library;Information Retrieval Meets Large Language Models;Liu, Zheng and Zhou, Yujia and Zhu, Yutao and Lian, Jianxun and Li, Chaozhuo and Dou, Zhicheng and Lian, Defu and Nie, Jian-Yun;2024;10.1145/3589335.3641299;NOT_FOUND;The advent of large language models (LLMs) presents both opportunities and challenges for the information retrieval (IR) community. On one hand, LLMs will revolutionize how people access information, meanwhile the retrieval techniques can play a crucial role in addressing many inherent limitations of LLMs. On the other hand, there are open problems regarding the collaboration of retrieval and generation, the potential risks of misinformation, and the concerns about cost-effectiveness. To seize the critical moment for development, it calls for the joint effort from academia and industry on many key issues, including identification of new research problems, proposal of new techniques, and creation of new evaluation protocols. It has been one year since the launch of ChatGPT in November last year, and the entire community is currently undergoing a profound transformation in techniques. Therefore, this workshop will be a timely venue to exchange ideas and forge collaborations. The organizers, committee members, and invited speakers are composed of a diverse group of researchers coming from leading institutions in the world. This event will be made up of multiple sessions, including invited talks, paper presentations, hands-on tutorials, and panel discussions. All the materials collected for this workshop will be archived and shared publicly, which will present a long-term value to the community.
NOT_RELEVANT;ACM Digital Library;Conversational Bibliographic Search;Nilles, Markus;2023;10.1145/3539618.3591789;NOT_FOUND;In almost every area of research, it is necessary to find experts and publications on a topic. However, finding experts and publications is a difficult task not only for computers, but also for humans. For example, searching for experts, a user often enters a topic into a search engine, which then checks which people have published on that topic. A problem arises when a user does not make their query specific enough which can happen intentionally, e.g. when the user is doing a navigational search, or unintentionally, e.g., when the user lacks knowledge. As a result, the quality of the search results may not be very high and the best results may not be found. Current and widely used search engines for bibliographic metadata, such as dblp[2], ResearchGate, Google Scholar or Semantic Scholar allow only keyword-based searches. Kreutz et al.[1] presented SchenQL, a query language for bibliographic metadata that allows users to formulate their queries more easily and precisely than SQL. However, it requires training to understand the language and is not as easy for non-experts to use e.g. Google Scholar.To address the limitations of insufficient attention to the user's search intent and lack of search support, we aim to develop a conversational retrieval system in the domain of bibliographic metadata. This conversational search system assists users in achieving their search intent through a natural language dialog. It should be possible not only to find experts, but also to search for bibliographic metadata with the help of the system without prior knowledge.In this work, we aim to answer the research question: How beneficial is a conversational information retrieval system for searching bibliographic data? To address the research question, our contribution is threefold. First, we present an architecture for such a conversational information retrieval system for bibliographic metadata. Second, we will implement all the components of this system and evaluate our system by comparing it to existing bibliographic data search engines in terms of effectiveness, efficiency, and user satisfaction. Third, we will create and publish a dataset consisting of user queries that we will use to train our system.The architecture we propose consists of five main components: i) user intent classification, ii) a keyword extractor, iii) a search module, iv) a conversational module and v) the conversation history.i) The task of user intent classification is to determine the goal the user wants to achieve with their search query. The user intent classification consists of a set of corresponding user intent classifiers, each of which is responsible for one intent. If no classifier can match the user's query to their intent, or multiple classifiers conclude that the query matches their intents, the system asks the user to specify the query accordingly. ii) If the intent is correctly determined, a keyword extractor extracts the actual search term from the query. iii) After the intent and the search term have been determined, the actual search takes place in the search module. The user's query could be reformulated into a SchenQL query (Kreutz et al.[1] and sent to the database. iv) In the conversational module, the results are converted into a natural language response and the user is given suggestions for further queries related to the previous ones. v) The conversation history stores the user's queries and the system's responses, both to consider the entire session when determining intent and to improve the system's components. For example, new question formulations could improve the accuracy of the user intent classifiers as well as reveal what new intents a user of such a conversational search system might have that have not yet been implemented.In first experiments, we defined four user intents and already evaluated the user intent classification. The four user intents are: (1) searching for persons/authors/experts on a topic, (2) searching for publications by author name, (3) searching for publications on a topic and (4) searching for similar topics of a topic. Our classifiers achieved an accuracy of 0.998 in correctly determining user intent.In the future, we not only want to evaluate the individual components of a conversational information retrieval system for bibliographic data, but also want to work out the advantages and disadvantages of the conversational information retrieval system for bibliographic data, in comparison to already existing systems that do not support the user in their search process via natural language conversations.
NOT_RELEVANT;ACM Digital Library;Biomedical Information Retrieval with Positive-Unlabeled Learning and Knowledge Graphs;Wang, Yuqi and Chen, Qiuyi and Zhang, Haiyang and Wang, Wei and Wang, Qiufeng and Pan, Yushan and Xie, Liangru and Huang, Kaizhu and Nguyen, Anh;2024;10.1145/3702647;NOT_FOUND;The rapid growth of biomedical publications has presented significant challenges in the field of information retrieval. Most existing work focuses on document retrieval given explicit queries. However, in real applications such as curated biomedical database maintenance, explicit queries are missing. In this paper, we propose a two-step model for biomedical information retrieval in the case that only a small set of example documents is available without explicit queries. Initially, we extract keywords from the observed documents using large pre-trained language models and biomedical knowledge graphs. These keywords are then enriched with domain-specific entities. Information retrieval techniques can subsequently use the collected entities to rank the documents. Following this, we introduce an iterative Positive-Unlabeled learning method to classify all unlabeled documents. Experiments conducted on the PubMed dataset demonstrate that the proposed technique outperforms the state-of-the-art positive-unlabeled learning methods. The results underscore the effectiveness of integrating large language models and biomedical knowledge graphs in improving zero-shot information retrieval performance in the biomedical domain.
NOT_RELEVANT;ACM Digital Library;REFinD: Relation Extraction Financial Dataset;Kaur, Simerjot and Smiley, Charese and Gupta, Akshat and Sain, Joy and Wang, Dongsheng and Siddagangappa, Suchetha and Aguda, Toyin and Shah, Sameena;2023;10.1145/3539618.3591911;NOT_FOUND;A number of datasets for Relation Extraction (RE) have been created to aide downstream tasks such as information retrieval, semantic search, question answering and textual entailment. However, these datasets fail to capture financial-domain specific challenges since most of these datasets are compiled using general knowledge sources such as Wikipedia, web-based text and news articles, hindering real-life progress and adoption within the financial world. To address this limitation, we propose REFinD, the first large-scale annotated dataset of relations, with ~29K instances and 22 relations amongst 8 types of entity pairs, generated entirely over financial documents. We also provide an empirical evaluation with various state-of-the-art models as benchmarks for the RE task and highlight the challenges posed by our dataset. We observed that various state-of-the-art deep learning models struggle with numeric inference, relational and directional ambiguity. To encourage further research in this direction, REFinD is available at https://www.jpmorgan.com/technology/artificial-intelligence/initiatives/refind-dataset/problem-motivation-outcome.
NOT_RELEVANT;ACM Digital Library;Enhancements to Threat, Vulnerability, and Mitigation Knowledge for Cyber Analytics, Hunting, and Simulations;Hemberg, Erik and Turner, Matthew J. and Rutar, Nick and O’reilly, Una-May;2024;10.1145/3615668;NOT_FOUND;Cross-linked threat, vulnerability, and defensive mitigation knowledge is critical in defending against diverse and dynamic cyber threats. Cyber analysts consult it by deductively or inductively creating a chain of reasoning to identify a threat starting from indicators they observe or vice versa. Cyber hunters use it abductively to reason when hypothesizing specific threats. Threat modelers use it to explore threat postures. We aggregate five public sources of threat knowledge and three public sources of knowledge that describe cyber defensive mitigations, analytics, and engagements and which share some unidirectional links between them. We unify the sources into a graph, and in the graph, we make all unidirectional cross-source links bidirectional. This enhancement of the knowledge makes the questions that analysts and automated systems formulate easier to answer. We demonstrate this in the context of various cyber analytic and hunting tasks as well as modeling and simulations. Because the number of linked entries is very sparse, to further increase the analytic utility of the data, we use natural language processing and supervised machine learning to identify new links. These two contributions demonstrably increase the value of the knowledge sources for cyber security activities.
NOT_RELEVANT;ACM Digital Library;CRUISE-Screening: Living Literature Reviews Toolbox;Kusa, Wojciech and Knoth, Petr and Hanbury, Allan;2023;10.1145/3583780.3614736;NOT_FOUND;Keeping up with research and finding related work is still a time-consuming task for academics. Researchers sift through thousands of studies to identify a few relevant ones. Automation techniques can help by increasing the efficiency and effectiveness of this task. To this end, we developed CRUISE-Screening, a web-based application for conducting living literature reviews -- a type of literature review that is continuously updated to reflect the latest research in a particular field. CRUISE-Screening is connected to several search engines via an API, which allows for updating the search results periodically. Moreover, it can facilitate the process of screening for relevant publications by using text classification and question answering models. CRUISE-Screening can be used both by researchers conducting literature reviews and by those working on automating the citation screening process to validate their algorithms. The application is open-source, and a demo is available under this URL: https://citation-screening.ec.tuwien.ac.at.
MAYBE_RELEVANT;ACM Digital Library;NLP And IR Applications For Financial Reporting And Non-Financial Disclosure. Framework Implementation And Roadmap For Feasible Integration With The Accounting Process;Faccia, Alessio and Petratos, Pythagoras;2023;10.1145/3582768.3582796;NOT_FOUND;Corporations produce financial and non-financial reports containing structured and unstructured data. In general, all organisations report information of some kind. Natural Language Processing (NLP) and Information Retrieval (IR) were fields developed from approximately the 1950s and have presented important applications, especially in the last three decades. Nevertheless, applications in accounting and finance have not developed accordingly, and a comprehensive framework is missing in the existing literature. This paper examines how NLP and IR can facilitate reporting and disclosure, both Financial and Non-Financial. The paper provides a brief literature review on NLP/IR applications in accounting and finance. It better informs and expands on the discussion of NLP/IR applications in academic research, professional organisations (i.e., IFRS), and industry. It explores some innovative applications of NLP/IR in unstructured data and its use in reporting and disclosure and FinTech applications. The main contribution is the definition of a complete framework that consistently analyses the possible NLP/IR applications in the accounting processes. We find that there can be many more applications of NLP/IR in accounting and finance and suggest future directions for research.
NOT_RELEVANT;ACM Digital Library;Neural Retrievers are Biased Towards LLM-Generated Content;Dai, Sunhao and Zhou, Yuqi and Pang, Liang and Liu, Weihao and Hu, Xiaolin and Liu, Yong and Zhang, Xiao and Wang, Gang and Xu, Jun;2024;10.1145/3637528.3671882;NOT_FOUND;Recently, the emergence of large language models (LLMs) has revolutionized the paradigm of information retrieval (IR) applications, especially in web search, by generating vast amounts of human-like texts on the Internet. As a result, IR systems in the LLM era are facing a new challenge: the indexed documents are now not only written by human beings but also automatically generated by the LLMs. How these LLM-generated documents influence the IR systems is a pressing and still unexplored question. In this work, we conduct a quantitative evaluation of IR models in scenarios where both human-written and LLM-generated texts are involved. Surprisingly, our findings indicate that neural retrieval models tend to rank LLM-generated documents higher. We refer to this category of biases in neural retrievers towards the LLM-generated content as the source bias. Moreover, we discover that this bias is not confined to the first-stage neural retrievers, but extends to the second-stage neural re-rankers. Then, in-depth analyses from the perspective of text compression indicate that LLM-generated texts exhibit more focused semantics with less noise, making it easier for neural retrieval models to semantic match. To mitigate the source bias, we also propose a plug-and-play debiased constraint for the optimization objective, and experimental results show its effectiveness. Finally, we discuss the potential severe concerns stemming from the observed source bias and hope our findings can serve as a critical wake-up call to the IR community and beyond. To facilitate future explorations of IR in the LLM era, the constructed two new benchmarks are available at https://github.com/KID-22/Source-Bias.
MAYBE_RELEVANT;ACM Digital Library;Psychology-informed Information Access Systems Workshop;Schedl, Markus and Moscati, Marta and Sguerra, Bruno and Hennequin, Romain and Lex, Elisabeth;2024;10.1145/3616855.3635722;NOT_FOUND;The Psychology-informed Information Access Systems (PsyIAS) workshop bridges the fields of machine learning and psychology, aiming to connect the research communities of information retrieval, recommender systems, natural language processing, as well as cognitive and behavioral psychology. It serves as a forum for multidisciplinary discussions about the use of psychological constructs, theories, and empirical findings for modeling and predicting user preferences, intents, and behaviors. PsyIAS particularly focuses on research that incorporates such psychology-inspired models into the search, retrieval, and recommendation processes, creates corresponding algorithms and systems, or looks into the role of cognitive processes underlying human information access. More information can be found at https://sites.google.com/view/psyias.
NOT_RELEVANT;ACM Digital Library;Boon: A Neural Search Engine for Cross-Modal Information Retrieval;Gong, Yan and Cosma, Georgina;2023;10.1145/3606040.3617440;NOT_FOUND;Visual-Semantic Embedding (VSE) networks can help search engines understand the meaning behind visual content and associate it with relevant textual information, leading to accurate search results. VSE networks can be used in cross-modal search engines to embed image and textual descriptions in a shared space, enabling image-to-text and text-to-image retrieval tasks. However, the full potential of VSE networks for search engines has yet to be fully explored. This paper presents Boon, a novel cross-modal search engine that combines two state-of-the-art networks: the GPT-3.5-turbo large language model, and the VSE network VITR (VIsion Transformers with Relation-focused learning) to enhance the engine's capabilities in extracting and reasoning with regional relationships in images. VITR employs encoders from CLIP that were trained with 400 million image-description pairs and it was fine-turned on the RefCOCOg dataset. Boon's neural-based components serve as its main functionalities: 1) a 'cross-modal search engine' that enables end-users to perform image-to-text and text-to-image retrieval. 2) a 'multi-lingual conversational AI' component that enables the end-user to converse about one or more images selected by the end-user. Such a feature makes the search engine accessible to a wide audience, including those with visual impairments. 3) Boon is multi-lingual and can take queries and handle conversations about images in multiple languages. Boon was implemented using the Django and PyTorch frameworks. The interface and capabilities of the Boon search engine are demonstrated using the RefCOCOg dataset, and the engine's ability to search for multimedia through the web is facilitated by Google's API.
NOT_RELEVANT;ACM Digital Library;Question answering algorithm based on deep learning;Nie, Liangyu and Ye, Jiacheng;2022;10.1145/3501409.3501541;NOT_FOUND;With the rapid development of Internet technology, people are no longer limited to simply obtaining information from the Internet and then manually screening, but prefer to use high-tech to retrieve information quickly and accurately. Therefore, this paper designs an intelligent question answering system to solve and apply the above problems. At present, intelligent question answering system has become a hot direction in the field of natural language processing. The purpose of this paper is to build a deep learning-based question answering algorithm based on deep learning technology and lay a foundation for its future application.
NOT_RELEVANT;ACM Digital Library;Towards Better Evidence Extraction Methods for Fact-Checking Systems;Azevedo, Pedro and Rocha, Gil and Esteves, Diego and Cardoso, Henrique Lopes;2022;10.1145/3486622.3493930;NOT_FOUND;Given current levels of misinformation spread, never before have fact-checking frameworks been so critical. Unfortunately, the performance of Automated Fact-checking systems is still poor due to the complexity of the task. In this paper, we present an ablation study of a framework submitted to the FEVER 1.0 challenge. Based on our findings, we explore how triple-based information retrieval, coreference resolution, and recent language model representations can impact the performance of each subtask. We show the importance of recall and precision in the retrieval of documents and sentences that can be provided to justify the veracity of a given claim. We reach state-of-the-art results in the Document Retrieval task and we show promising results when using coreference resolution to improve the Sentence Retrieval task.
NOT_RELEVANT;ACM Digital Library;A new conceptual framework for enhancing legal information retrieval at the Brazilian Superior Court of Justice;Gomes, Thiago and Ladeira, Marcelo;2020;10.1145/3415958.3433087;NOT_FOUND;Effective retrieval of jurisprudence (case-law) is imperative to achieve consistency and predictability for any legal system. In this work, we propose and proceed to an empirical evaluation of a framework for jurisprudence retrieval of the Brazilian Superior Court of Justice in order to ease the task of retrieval of other decisions with the same legal opinion. The experimental results shown that our approach based on text similarity performs better than the legacy system of the Court based on Boolean queries. The building of complex Boolean queries is very specialized and we aim to offer a tool able to use free text as queries without any operator. With the legacy system as baseline, we compare the TF-IDF traditional retrieval model, the BM25 probabilistic model and the Word2Vec model. Our results indicate that the Word2Vec Skip-Gram model, trained on a specialized legal corpus and BM25 yield similar performance and surpasses the legacy system. Combining BM25 model with embedding models improved the performance up to 19%.
NOT_RELEVANT;ACM Digital Library;A Review of Modern Fashion Recommender Systems;Deldjoo, Yashar and Nazary, Fatemeh and Ramisa, Arnau and McAuley, Julian and Pellegrini, Giovanni and Bellogin, Alejandro and Noia, Tommaso Di;2023;10.1145/3624733;NOT_FOUND;The textile and apparel industries have grown tremendously over the past few years. Customers no longer have to visit many stores, stand in long queues, or try on garments in dressing rooms, as millions of products are now available in online catalogs. However, given the plethora of options available, an effective recommendation system is necessary to properly sort, order, and communicate relevant product material or information to users. Effective fashion recommender systems (RSs) can have a noticeable impact on billions of customers’ shopping experiences and increase sales and revenues on the provider side.The goal of this survey is to provide a review of RSs that operate in the specific vertical domain of garment and fashion products. We have identified the most pressing challenges in fashion RS research and created a taxonomy that categorizes the literature according to the objective they are trying to accomplish (e.g., item or outfit recommendation, size recommendation, and explainability, among others) and type of side information (users, items, context). We have also identified the most important evaluation goals and perspectives (outfit generation, outfit recommendation, pairing recommendation, and fill-in-the-blank outfit compatibility prediction) and the most commonly used datasets and evaluation metrics.
MAYBE_RELEVANT;ACM Digital Library;Keyword Augmented Retrieval: Novel framework for Information Retrieval integrated with speech interface;Purwar, Anupam and Sundar, Rahul;2024;10.1145/3639856.3639916;NOT_FOUND;"Retrieving answers in a quick and low cost manner without hallucinations from a combination of structured and unstructured data using Language models is a major hurdle. This is what prevents employment of Language models in knowledge retrieval automation. This becomes accentuated when one wants to integrate a speech interface on top of a text based knowledge retrieval system. Besides, for commercial search and chat-bot applications, complete reliance on commercial large language models (LLMs) like GPT 3.5 etc. can be very costly. In the present study, the authors have addressed the aforementioned problem by first developing a keyword based search framework which augments discovery of the context from the document to be provided to the LLM. The keywords in turn are generated by a relatively smaller LLM and cached for comparison with keywords generated by the same smaller LLM against the query raised. This significantly reduces time and cost to find the context within documents. Once the context is set, a larger LLM uses that to provide answers based on a prompt tailored for Q&amp;A. This research work demonstrates that use of keywords in context identification reduces the overall inference time and cost of information retrieval. Given this reduction in inference time and cost with the keyword augmented retrieval framework, a speech based interface for user input and response readout was integrated. This allowed a seamless interaction with the language model."
MAYBE_RELEVANT;ACM Digital Library;False Positive Intent Detection Framework for Chatbot Annotation;Lim, Lecia Kai Heng and Agarwal, Samarth and Zhang, Xuejie and Lu, John Jianan;2023;10.1145/3582768.3582798;NOT_FOUND;For chatbots answering thousands of user queries daily, it requires huge annotation efforts or explicit signals from users to identify incorrect chatbot predictions. Identification of such False Positives is key to improving chatbot accuracy and is a challenging problem due to the high cost and limited explicit signals from users. In this paper, we present a framework for automatically detecting False Positive intents in an employee chatbot through implicit feedback by capturing specific user behavior using techniques such as detection of repeated queries and leveraging on active learning sampling strategies to find cases where the chatbot might have provided an incorrect response. Using this approach within the bank, annotators can prioritize their efforts and detect False Positive intent approximately three times better than manual screening of random chatbot dialogues. This framework can be reused across different chatbot applications.
MAYBE_RELEVANT;ACM Digital Library;The 3rd International Workshop on Interactive and Scalable Information Retrieval Methods for eCommerce (ISIR-eCom 2024);Dave, Vachik S. and Pang, Linsey and Cui, Xiquan and Luo, Chen and Zamani, Hamed and Wu, Lingfei and Karypis, George;2024;10.1145/3616855.3635724;NOT_FOUND;Over the past few years, consumer behavior has shifted from traditional in-store shopping to online shopping. For example, eCommerce sales have grown from around 5% of total US sales in 2012 to around 15.4% in year 2023. This rapid growth of eCommerce has created new challenges and vital new requirements for intelligent information retrieval systems. Which lead to the primary motivations of this workshop:(1) Since the pandemic hit, eCommerce became an important part of people's routine and they started using online shop- ping for smallest grocery items to big electronics as well as cars. With such a large assortment of products and millions of users, achieving higher scalability without losing accuracy is a leading concern for information retrieval systems for eCommerce.(2) The diverse buyers make the relevance of the results highly subjective, because relevance varies for different buyers. The most suitable and intuitive solution to this problem is to make the system interactive and provide correct relevance for different users. Hence, interactive information retrieval systems are becoming necessity in eCommerce.(3) To handle sudden change in buyers' behavior, industries adopted existing sub-optimal information retrieval techniques for various eCommerce tasks. Parallelly, they also started exploring/researching for better solutions and in dire need of help from research community.This workshop will provide a forum to discuss and learn the latest trends for interactive and scalable information retrieval approaches for eCommerce. It will provide academic and industrial researchers a platform to present their latest works, share research ideas, present and discuss various challenges, and identify the areas where further research is needed. It will foster the development of a strong research community focused on solving eCommerce-related information retrieval problems that provide superior eCommerce experience to all users.
MAYBE_RELEVANT;ACM Digital Library;Unleashing the Power of Large Language Models for Legal Applications;Zhang, Dell and Petrova, Alina and Trautmann, Dietrich and Schilder, Frank;2023;10.1145/3583780.3615993;NOT_FOUND;The use of Large Language Models (LLMs) is revolutionizing the legal industry. In this technical talk, we would like to explore the various use cases of LLMs in legal tasks, discuss the best practices, investigate the available resources, examine the ethical concerns, and suggest promising research directions.
NOT_RELEVANT;ACM Digital Library;A Fair and Comprehensive Comparison of Multimodal Tweet Sentiment Analysis Methods;"Cheema, Gullal S. and Hakimov, Sherzod and M\""{u}ller-Budack, Eric and Ewerth, Ralph";2021;10.1145/3463945.3469058;NOT_FOUND;Opinion and sentiment analysis is a vital task to characterize subjective information in social media posts. In this paper, we present a comprehensive experimental evaluation and comparison with six state-of-the-art methods, from which we have re-implemented one of them. In addition, we investigate different textual and visual feature embeddings that cover different aspects of the content, as well as the recently introduced multimodal CLIP embeddings. Experimental results are presented for two different publicly available benchmark datasets of tweets and corresponding images. In contrast to the evaluation methodology of previous work, we introduce a reproducible and fair evaluation scheme to make results comparable. Finally, we conduct an error analysis to outline the limitations of the methods and possibilities for the future work.
NOT_RELEVANT;ACM Digital Library;The 3rd International Workshop on Mining and Learning in the Legal Domain;Makrehchi, Masoud and Zhang, Dell and Petrova, Alina and Armour, John;2023;10.1145/3583780.3615308;NOT_FOUND;The increasing accessibility of legal corpora and databases create opportunities to develop data-driven techniques and advanced tools that can facilitate a variety of tasks in the legal domain, such as legal search and research, legal document review and summary, legal contract drafting, and legal outcome prediction. Compared with other application domains, the legal domain is characterized by the huge scale of natural language text data, the high complexity of specialist knowledge, and the critical importance of ethical considerations. The MLLD workshop aims to bring together researchers and practitioners to share the latest research findings and innovative approaches in employing data mining, machine learning, information retrieval, and knowledge management techniques to transform the legal sector. Building upon the previous successes, the third edition of the MLLD workshop will emphasize the exploration of new research opportunities brought about by recent rapid advances in Large Language Models and Generative AI. We encourage submissions that intersect computer science and law, from both academia and industry, embodying the interdisciplinary spirit of CIKM.
NOT_RELEVANT;ACM Digital Library;A Compare-Aggregate Model with Latent Clustering for Answer Selection;Yoon, Seunghyun and Dernoncourt, Franck and Kim, Doo Soon and Bui, Trung and Jung, Kyomin;2019;10.1145/3357384.3358148;NOT_FOUND;In this paper, we propose a novel method for a sentence-level answer-selection task that is a fundamental problem in natural language processing. First, we explore the effect of additional information by adopting a pretrained language model to compute the vector representation of the input text and by applying transfer learning from a large-scale corpus. Second, we enhance the compare-aggregate model by proposing a novel latent clustering method to compute additional information within the target corpus and by changing the objective function from listwise to pointwise. To evaluate the performance of the proposed approaches, experiments are performed with the WikiQA and TREC-QA datasets. The empirical results demonstrate the superiority of our proposed approach, which achieve state-of-the-art performance for both datasets.
NOT_RELEVANT;ACM Digital Library;A Joint Model for Text and Image Semantic Feature Extraction;Cao, Jiarun and Wang, Chongwen and Gao, Liming;2018;10.1145/3302425.3302437;NOT_FOUND;Most of the current information retrieval are based on keyword information appearing in the text or statistical information according to the number of vocabulary words. It is also possible to add additional semantic information by using synonyms, polysemous words, etc. to increase the accuracy of similarity and screening. However, in the current network, in addition to generate a large number of new words every day, pictures, audio, video and other information will appear too. Therefore, the manual features are difficult to express on this kind of newly appearing data, and the low-dimensional feature abstraction is very difficult to represent the overall semantics of text and images. In this paper, we propose a semantic feature extraction algorithm based on deep network, which applies the local attention mechanism to the feature generation model of pictures and texts. The retrieval of text and image information is converted into the similarity calculation of the vector, which improves the retrieval speed and ensures the semantic relevance of the result. Through the compilation of many years of news text and image data to complete the training and testing of text and image feature extraction models, the results show that the depth feature model has great advantages in semantic expression and feature extraction. On the other hand, add the similarity calculation to the training processing also improve the retrieval accuracy.
MAYBE_RELEVANT;ACM Digital Library;Offline versus Online Representation Learning of Documents Using External Knowledge;Tamine, Lynda and Soulier, Laure and Nguyen, Gia-Hung and Souf, Nathalie;2019;10.1145/3349527;NOT_FOUND;"An intensive recent research work investigated the combined use of hand-curated knowledge resources and corpus-driven resources to learn effective text representations. The overall learning process could be run by online revising the learning objective or by offline refining an original learned representation. The differentiated impact of each of the learning approaches on the quality of the learned representations has not been studied so far in the literature. This article focuses on the design of comparable offline vs. online knowledge-enhanced document representation learning models and the comparison of their effectiveness using a set of standard IR and NLP downstream tasks. The results of quantitative and qualitative analyses show that (1) offline vs. online learning approaches have dissimilar result trends regarding the task as well as the dataset distribution counts with regard to domain application; (2) while considering external knowledge resources is undoubtedly beneficial, the way used to express relational constraints could affect semantic inference effectiveness. The findings of this work present opportunities for the design of future representation learning models, but also for providing insights about the evaluation of such models."
NOT_RELEVANT;ACM Digital Library;Venue Topic Model–enhanced Joint Graph Modelling for Citation Recommendation in Scholarly Big Data;Wang, Wei and Gong, Zhiguo and Ren, Jing and Xia, Feng and Lv, Zhihan and Wei, Wei;2020;10.1145/3404995;NOT_FOUND;Natural language processing technologies, such as topic models, have been proven to be effective for scholarly recommendation tasks with the ability to deal with content information. Recently, venue recommendation is becoming an increasingly important research task due to the unprecedented number of publication venues. However, traditional methods focus on either the author’s local network or author-venue similarity, where the multiple relationships between scholars and venues are overlooked, especially the venue–venue interaction. To solve this problem, we propose an author topic model–enhanced joint graph modeling approach that consists of venue topic modeling, venue-specific topic influence modeling, and scholar preference modeling. We first model the venue topic with Latent Dirichlet Allocation. Then, we model the venue-specific topic influence in an asymmetric and low-dimensional way by considering the topic similarity between venues, the top-influence of venues, and the top-susceptibility of venues. The top-influence characterizes venues’ capacity of exerting topic influence on other venues. The top-susceptibility captures venues’ propensity of being topically influenced by other venues. Extensive experiments on two real-world datasets show that our proposed joint graph modeling approach outperforms the state-of-the-art methods.
NOT_RELEVANT;ACM Digital Library;A Method to Detect Inconsistent Annotations in a Medical Document using UMLS;Mahato, Devasish and Dudhal, Disha and Revagade, Dhanashree and Bhargava, Yesoda;2019;10.1145/3368567.3368577;NOT_FOUND;Information retrieval from clinical documents relies heavily on annotated corpus. Any inconsistency in annotations, in form of heterogeneous annotations for similar concepts, could be detrimental to the quality of information retrieved. In extreme cases, this may lead to incorrect deductions about patient's medical history and result in erroneous decisions. In the present work, a complete end-to-end system that identifies inconsistencies in a clinically annotated document is presented and analysed. Unified Medical Language System(UMLS) is used to identify medical concepts in the clinical document. The output is presented in a clustered format wherein, each cluster identifies a unique medical concept and contains its semantic synonyms. For each semantic synonym, inconsistent annotations and the sentences in which they occur in the document are listed. The work could be useful for annotation experts who need automated tools to verify their work.
NOT_RELEVANT;ACM Digital Library;Reranking for Efficient Transformer-based Answer Selection;Matsubara, Yoshitomo and Vu, Thuy and Moschitti, Alessandro;2020;10.1145/3397271.3401266;NOT_FOUND;IR-based Question Answering (QA) systems typically use a sentence selector to extract the answer from retrieved documents. Recent studies have shown that powerful neural models based on the Transformer can provide an accurate solution to Answer Sentence Selection (AS2). Unfortunately, their computation cost prevents their use in real-world applications. In this paper, we show that standard and efficient neural rerankers can be used to reduce the amount of sentence candidates fed to Transformer models without hurting Accuracy, thus improving efficiency up to four times. This is an important finding as the internal representation of shallower neural models is dramatically different from the one used by a Transformer model, e.g., word vs. contextual embeddings.
NOT_RELEVANT;ACM Digital Library;Deep Bayesian Data Mining;Chien, Jen-Tzung;2020;10.1145/3336191.3371870;NOT_FOUND;"This tutorial addresses the fundamentals and advances in deep Bayesian mining and learning for natural language with ubiquitous applications ranging from speech recognition to document summarization, text classification, text segmentation, information extraction, image caption generation, sentence generation, dialogue control, sentiment classification, recommendation system, question answering and machine translation, to name a few. Traditionally, ""deep learning"" is taken to be a learning process where the inference or optimization is based on the real-valued deterministic model. The ""semantic structure"" in words, sentences, entities, actions and documents drawn from a large vocabulary may not be well expressed or correctly optimized in mathematical logic or computer programs. The ""distribution function"" in discrete or continuous latent variable model for natural language may not be properly decomposed or estimated. This tutorial addresses the fundamentals of statistical models and neural networks, and focus on a series of advanced Bayesian models and deep models including hierarchical Dirichlet process, Chinese restaurant process, hierarchical Pitman-Yor process, Indian buffet process, recurrent neural network (RNN), long short-term memory, sequence-to-sequence model, variational auto-encoder (VAE), generative adversarial network (GAN), attention mechanism, memory-augmented neural network, skip neural network, temporal difference VAE, stochastic neural network, stochastic temporal convolutional network, predictive state neural network, and policy neural network. Enhancing the prior/posterior representation is addressed. We present how these models are connected and why they work for a variety of applications on symbolic and complex patterns in natural language. The variational inference and sampling method are formulated to tackle the optimization for complicated models. The word and sentence embeddings, clustering and co-clustering are merged with linguistic and semantic constraints. A series of case studies, tasks and applications are presented to tackle different issues in deep Bayesian mining, searching, learning and understanding. At last, we will point out a number of directions and outlooks for future studies. This tutorial serves the objectives to introduce novices to major topics within deep Bayesian learning, motivate and explain a topic of emerging importance for data mining and natural language understanding, and present a novel synthesis combining distinct lines of machine learning work."
NOT_RELEVANT;ACM Digital Library;Cerebro: Novelty Detection in Product Reviews;Sinha, Ankita and Subrahmaniam, Vignesh;2020;10.1145/3380688.3380701;NOT_FOUND;The recent boom in e-commerce has created active electronic communities where consumers share their thoughts about the product and the company. These reviews play a very important part in building customer opinion about the said item. For a popular product or service, there might be thousands of reviews, making it difficult for the customer to make an informed decision about the product. In this paper, we present a way to surface only those reviews that contain information relevant to the user. To address this problem, we try to surface out the reviews that are outliers to the general cluster of reviews during a particular time period.We are leveraging anomaly detection algorithms to achieve this.
NOT_RELEVANT;ACM Digital Library;Inferring Search Queries from Web Documents via a Graph-Augmented Sequence to Attention Network;Han, Fred.X and Niu, Di and Lai, Kunfeng and Guo, Weidong and He, Yancheng and Xu, Yu;2019;10.1145/3308558.3313746;NOT_FOUND;We study the problem of search query inference from web documents, where a short, comprehensive natural language query is inferred from a long article. Search query generation or inference is of great value to search engines and recommenders in terms of locating potential target users and ranking content. Despite being closely related to other NLP tasks like abstract generation and keyword extraction, we point out that search query inference is, in fact, a new problem, in that the generated natural language query, which consists of a few words, is expected to be comprehensive enough to lead to the click-through of the corresponding document. Therefore, query generation requires an accurate inference of query words, as well as a deeper level of understanding on document semantic structures. Toward this end, we propose a novel generative model called the Graph-augmented Sequence to Attention (G-S2A) network. Adopting an Encoder-Decoder architecture, G-S2A incorporates a sentence-level Graph Convolutional Network (GCN), a keyword-level GCN, as well as a hierarchical recurrent neural network (RNN) into the encoder to generate structural document representations. An attentional Transformer decoder is then applied to combine different types of encoded features to generate a target query. On a query-document dataset from a real-world search engine, our model outperforms several neural generative models on a wide range of metrics.
NOT_RELEVANT;ACM Digital Library;Probabilistic Word Embeddings in Neural IR: A Promising Model That Does Not Work as Expected (For Now);Purpura, Alberto and Maggipinto, Marco and Silvello, Gianmaria and Susto, Gian Antonio;2019;10.1145/3341981.3344217;NOT_FOUND;"In this paper, we discuss how a promising word vector representation based on Probabilistic Word Embeddings (PWE) can be applied to Neural Information Retrieval (NeuIR). We illustrate PWE pros for text retrieval, and identify the core issues which prevent a full exploitation of their potential. In particular, we focus on the application of elliptical probabilistic embeddings, a type of PWE, to a NeuIR system (i.e., MatchPyramid). The main contributions of this paper are: (i) an analysis of the pros and cons of PWE in NeuIR; (ii) an in-depth comparison of PWE against pre-trained Word2Vec, FastText and WordNet word embeddings; (iii) an extension of the MatchPyramid model to take advantage of broader word relations information from WordNet; (iv) a topic-level evaluation of the MatchPyramid ranking models employing the considered word embeddings. Finally, we discuss some lessons learned and outline some open research problems to employ PWE in NeuIR systems more effectively."
NOT_RELEVANT;ACM Digital Library;Analyzing the impact of natural language processing over feature location in models;Lape\~{n}a, Ra\'{u}l and Font, Jaime and Pastor, \'{O}scar and Cetina, Carlos;2017;10.1145/3136040.3136052;NOT_FOUND;Feature Location (FL) is a common task in the Software Engineering field, specially in maintenance and evolution of software products. The results of FL depend in a great manner in the style in which Feature Descriptions and software artifacts are written. Therefore, Natural Language Processing (NLP) techniques are used to process them. Through this paper, we analyze the influence of the most common NLP techniques over FL in Conceptual Models through Latent Semantic Indexing, and the influence of human participation when embedding domain knowledge in the process. We evaluated the techniques in a real-world industrial case study in the rolling stocks domain.
NOT_RELEVANT;ACM Digital Library;Modeling and Mining Domain Shared Knowledge for Sentiment Analysis;Zhou, Guang-You and Huang, Jimmy Xiangji;2017;10.1145/3091995;NOT_FOUND;Sentiment classification aims to automatically predict sentiment polarity (e.g., positive or negative) of user generated sentiment data (e.g., reviews, blogs). In real applications, these user-generated sentiment data can span so many different domains that it is difficult to label the training data for all of them. Therefore, we study the problem of sentiment classification adaptation task in this article. That is, a system is trained to label reviews from one source domain but is meant to be used on the target domain. One of the biggest challenges for sentiment classification adaptation task is how to deal with the problem when two data distributions between the source domain and target domain are significantly different from one another. However, our observation is that there might exist some domain shared knowledge among certain input dimensions of different domains. In this article, we present a novel method for modeling and mining the domain shared knowledge from different sentiment review domains via a joint non-negative matrix factorization–based framework. In this proposed framework, we attempt to learn the domain shared knowledge and the domain-specific information from different sentiment review domains with several various regularization constraints. The advantage of the proposed method can promote the correspondence under the topic space between the source domain and the target domain, which can significantly reduce the data distribution gap across two domains. We conduct extensive experiments on two real-world balanced data sets from Amazon product reviews for sentence-level and document-level binary sentiment classification. Experimental results show that our proposed approach significantly outperforms several strong baselines and achieves an accuracy that is competitive with the most well-known methods for sentiment classification adaptation.
NOT_RELEVANT;ACM Digital Library;DI-2022: The Third Document Intelligence Workshop;Nenkova, Ani and Burdick, Douglas and Han, Benjamin and Lewis, Dave and Tata, Sandeep and Tecuci, Dan;2022;10.1145/3534678.3542919;NOT_FOUND;Business documents are central to the operation of all organizations, and they come in all shapes and sizes: project reports, planning documents, technical specifications, financial statements, meeting minutes, legal agreements, contracts, resumes, purchase orders, invoices, and many more. The ability to read, understand and interpret these documents, referred to here as Document Intelligence (DI), is challenging due to not only many domains of knowledge involved, but also their complex formats and structures, internal and external cross references deployed, and even less-than-ideal quality of scans and OCR oftentimes performed on them. This workshop aims to explore and advance the current state of research and practice in answering these challenges.
NOT_RELEVANT;ACM Digital Library;DI-2021: The Second Document Intelligence Workshop;Han, Benjamin and Burdick, Douglas and Lewis, Dave and Lu, Yijuan and Motahari, Hamid and Tata, Sandeep;2021;10.1145/3447548.3469454;NOT_FOUND;Business documents are central to the operation of all organizations, and they come in all shapes and sizes: project reports, planning documents, technical specifications, financial statements, meeting minutes, legal agreements, contracts, resumes, purchase orders, invoices, and many more. The ability to read, understand and interpret these documents, referred to here as Document Intelligence (DI), is challenging due to not only many domains of knowledge involved, but also their complex formats and structures, internal and external cross references deployed, and even less-than-ideal quality of scans and OCR oftentimes performed on them. This workshop aims to explore and advance the current state of research and practice in answering these challenges.
NOT_RELEVANT;ACM Digital Library;Music4All-Onion -- A Large-Scale Multi-faceted Content-Centric Music Recommendation Dataset;Moscati, Marta and Parada-Cabaleiro, Emilia and Deldjoo, Yashar and Zangerle, Eva and Schedl, Markus;2022;10.1145/3511808.3557656;NOT_FOUND;When we appreciate a piece of music, it is most naturally because of its content, including rhythmic, tonal, and timbral elements as well as its lyrics and semantics. This suggests that the human affinity for music is inherently content-driven. This kind of information is, however, still frequently neglected by mainstream recommendation models based on collaborative filtering that rely solely on user-item interactions to recommend items to users. A major reason for this neglect is the lack of standardized datasets that provide both collaborative and content information. The work at hand addresses this shortcoming by introducing Music4All-Onion, a large-scale, multi-modal music dataset. The dataset expands the Music4All dataset by including 26 additional audio, video, and metadata characteristics for 109,269 music pieces. In addition, it provides a set of 252,984,396 listening records of 119,140 users, extracted from the online music platform Last.fm, which allows leveraging user-item interactions as well. We organize distinct item content features in an onion model according to their semantics, and perform a comprehensive examination of the impact of different layers of this model (e.g., audio features, user-generated content, and derivative content) on content-driven music recommendation, demonstrating how various content features influence accuracy, novelty, and fairness of music recommendation systems. In summary, with Music4All-Onion, we seek to bridge the gap between collaborative filtering music recommender systems and content-centric music recommendation requirements.
NOT_RELEVANT;ACM Digital Library;Toward a big data analysis system for historical newspaper collections research;Satheesan, Sandeep Puthanveetil and Bhavya and Davies, Adam and Craig, Alan B. and Zhang, Yu and Zhai, ChengXiang;2022;10.1145/3539781.3539795;NOT_FOUND;"The availability and generation of digitized newspaper collections have provided researchers in several domains with a powerful tool to advance their research. More specifically, digitized historical newspapers give us a magnifying glass into the past. In this paper, we propose a scalable and customizable big data analysis system that enables researchers to study complex questions about our society as depicted in news media for the past few centuries by applying cutting-edge text analysis tools to large historical newspaper collections. We discuss our experience with building a preliminary version of such a system, including how we have addressed the following challenges: processing millions of digitized newspaper pages from various publications worldwide, which amount to hundreds of terabytes of data; applying article segmentation and Optical Character Recognition (OCR) to historical newspapers, which vary between and within publications over time; retrieving relevant information to answer research questions from such data collections by applying human-in-the-loop machine learning; and enabling users to analyze topic evolution and semantic dynamics with multiple compatible analysis operators. We also present some preliminary results of using the proposed system to study the social construction of juvenile delinquency in the United States and discuss important remaining challenges to be tackled in the future."
NOT_RELEVANT;ACM Digital Library;Building Stemmers for the Polish Language;Karanikolas, Nikitas N.;2016;10.1145/3003733.3003792;NOT_FOUND;"In this paper we examine the applicability of our ""supervised learning methodology that builds stemmers given minor target language knowledge"" (shortly: stemmer builder) for building a stemmer for the Polish language. We examine if the existing features of our stemmer builder can support the necessities of the Polish language for building an acceptable stemmer."
NOT_RELEVANT;ACM Digital Library;A Novel Document Summarization System for Albanian Language;Trandafili, Evis and Paci, Hakik and Karaj, Elona;2019;10.1145/3345252.3345275;NOT_FOUND;"Summarization is a Natural Language Processing application that may seem trivial to a person, but in a time where the quantity of information provided is continuously growing, the possibility of implementing a ""helper"" in order to summarize it, has become a necessity. Most of the existing scientific studies in automatic text summarization has been paying attention primarily to English with only some recent attempts in other major languages. To the best of our knowledge, no prior approaches handle automatic summarization for Albanian documents. This paper is proposed to fill this gap by implementing a novel extractive summarization system, designed specifically for Albanian Language. We showed experimentally that the enrichment of the summarization system with language-dependent elements improves the systems' performance and the compression rate."
NOT_RELEVANT;ACM Digital Library;Retrieval and Clustering of Medicines Within Healthcare Data Records;Wallace, Duncan and Kechadi, M-Tahar;2016;10.1145/3010089.3010101;NOT_FOUND;Electronic Health Records (EHRs) are typically designed to electronically document all information that is administratively and clinically relevant in a patient's use of a healthcare facility. This paper intends to improve discoverability of medications which may exist within the narrative-based free-text notes of patients' health care data records. This led us to introduce a context sensitive approach to retrieve candidate pharmaceuticals. Additionally, a combination of contraction promotion and clustering based upon edit distance will be utilised to increase the precision of this process.
NOT_RELEVANT;ACM Digital Library;ANNE: Improving Source Code Search using Entity Retrieval Approach;Vinayakarao, Venkatesh and Sarma, Anita and Purandare, Rahul and Jain, Shuktika and Jain, Saumya;2017;10.1145/3018661.3018691;NOT_FOUND;"Code search with natural language terms performs poorly because programming concepts do not always lexically match their syntactic forms. For example, in Java, the programming concept ""array"" does not match with its syntactic representation of ""[ ]"". Code search engines can assist developers more effectively over natural language queries if such mappings existed for a variety of programming languages. In this work, we present a programming language agnostic technique to discover such mappings between syntactic forms and natural language terms representing programming concepts. We use the questions and answers in Stack Overflow to create this mapping. We implement our approach in a tool called ANNE. To evaluate its effectiveness, we conduct a user study in an academic setting in which teaching assistants use ANNE to search for code snippets in student submissions. With the use of ANNE, we find that the participants are 29% quicker with no significant drop in correctness and completeness."
MAYBE_RELEVANT;ACM Digital Library;Legal content fusion for legal information retrieval;Heo, Seongwan and Hong, Kihyun and Rhim, Young-Yik;2017;10.1145/3086512.3086549;NOT_FOUND;With recent increasing attention to legal information processing, legal information retrieval (IR) has become one of the active research fields. However, there are still many hindrances obtaining rigorous results in legal IR applications in comparison with IR applications for general document retrieval. It is mainly due to the characteristics of legal information such as the complicated structure of legal contents and usage of legal jargon. In this paper, we present a legal IR method, which is a structure-wise IR approach. e presented method in this study focuses on analyzing the contents of legal documents and applying the content contributions to the IR processing. We demonstrate the performance of the proposed IR method with the COILEE data set, which are derived from Japanese bar exams.
NOT_RELEVANT;ACM Digital Library;Augmenting Media Literacy with Automatic Characterization of News along Pragmatic Dimensions;Boon, Miriam L.;2017;10.1145/3022198.3024948;NOT_FOUND;Media literacy allows individuals to better interpret the information they need to absorb to contribute to our democratic, knowledge-based society. I propose that by automatically notifying readers of an article's problematic pragmatic characteristics, an application could augment their media literacy. I describe two characteristics for which I have been building automatic classifiers -- factiness and tropes as narrative frames -- and discuss the status of each project.
NOT_RELEVANT;ACM Digital Library;Recommending Scientific Papers: The Role of Citation Contexts;Bertin, Marc and Atanassova, Iana;2018;10.1145/3240117.3240123;NOT_FOUND;This paper addresses the problem of building recommender systems for scientific papers based on the linguistic and contextual analysis of citation contexts. We explain the importance of taking into consideration citation contexts and the different methodologies that exist as well as the ways that citations impact recommender systems. We also discuss the limits of using citation contexts to generate recommendations.
NOT_RELEVANT;ACM Digital Library;Total recall, language processing, and software engineering;Yu, Zhe and Menzies, Tim;2018;10.1145/3283812.3283818;NOT_FOUND;"A broad class of software engineering problems can be generalized as the ""total recall problem"". This short paper claims that identifying and exploring the total recall problems in software engineering is an important task with wide applicability.  To make that case, we show that by applying and adapting the state of the art active learning and natural language processing algorithms for solving the total recall problem, two important software engineering tasks can also be addressed : (a) supporting large literature reviews and (b) identifying software security vulnerabilities. Furthermore, we conjecture that (c) test case prioritization and (d) static warning identification can also be generalized as and benefit from the total recall problem.  The widespread applicability of ""total recall"" to software engineering suggests that there exists some underlying framework that encompasses not just natural language processing, but a wide range of important software engineering tasks."
NOT_RELEVANT;ACM Digital Library;A study of factuality, objectivity and relevance: three desiderata in large-scale information retrieval?;Lioma, Christina and Larsen, Birger and Lu, Wei and Huang, Yong;2016;10.1145/3006299.3006315;NOT_FOUND;"Much of the information processed by Information Retrieval (IR) systems is unreliable, biased, and generally untrust-worthy [15, 45, 48]. Yet, factuality &amp; objectivity detection is not a standard component of IR systems, even though it has been possible in Natural Language Processing (NLP) in the last decade. Motivated by this, we ask if and how factuality &amp; objectivity detection may benefit IR. We answer this in two parts. First, we use state-of-the-art NLP to compute the probability of document factuality &amp; objectivity in two TREC collections, and analyse its relation to document relevance. We find that factuality is strongly and positively correlated to document relevance, but objectivity is not. Second, we study the impact of factuality &amp; objectivity to retrieval effectiveness by treating them as query independent features that we combine with a competitive language modelling baseline. Experiments with 450 TREC queries show that factuality improves precision by more than 10% over strong baselines, especially for the type of uncurated data typically used in web search; objectivity gives mixed results. An overall clear trend is that document factuality &amp; objectivity is much more beneficial to IR when searching uncurated (e.g. web) documents vs. curated (e.g. state documentation and newswire articles).To our knowledge, this is the first study of factuality &amp; objectivity for back-end IR, contributing novel findings about the relation between relevance and factuality/objectivity, and statistically significant gains to retrieval effectiveness in the competitive web search task."
NOT_RELEVANT;ACM Digital Library;X5Learn: A Personalised Learning Companion at the Intersection of AI and HCI;Perez-Ortiz, Maria and Dormann, Claire and Rogers, Yvonne and Bulathwela, Sahan and Kreitmayer, Stefan and Yilmaz, Emine and Noss, Richard and Shawe-Taylor, John;2021;10.1145/3397482.3450721;NOT_FOUND;X5Learn (available at https://x5learn.org ) is a human-centered AI-powered platform for supporting access to free online educational resources. X5Learn provides users with a number of educational tools for interacting with open educational videos, and a set of tools adapted to suit the pedagogical preferences of users. It is intended to support both teachers and students, alike. For teachers, it provides a powerful platform to reuse, revise, remix, and redistribute open courseware produced by others. These can be videos, pdfs, exercises and other online material. For students, it provides a scaffolded and informative interface to select content to watch, read, make notes and write reviews, as well as a powerful personalised recommendation system that can optimise learning paths and adjust to the user’s learning preferences. What makes X5Learn stand out from other educational platforms, is how it combines human-centered design with AI algorithms and software tools with the goal of making it intuitive and easy to use, as well as making the AI transparent to the user. We present the core search tool of X5Learn, intended to support exploring open educational materials.
NOT_RELEVANT;ACM Digital Library;ICDAR'20: Intelligent Cross-Data Analysis and Retrieval;Dao, Minh-Son and Fjeld, Morten and Biljecki, Filip and Yavanoglu, Uraz and Dong, Mianxiong;2020;10.1145/3372278.3388041;NOT_FOUND;"The First International Workshop on ""Intelligence Cross-Data Analytics and Retrieval"" (ICDAR'20) welcomes any theoretical and practical works on intelligence cross-data analytics and retrieval to bring the smart-sustainable society to human beings. We have witnessed the era of big data where almost any event that happens is recorded and stored either distributedly or centrally. The utmost requirement here is that data came from different sources, and various domains must be harmonically analyzed to get their insights immediately towards giving the ability to be retrieved thoroughly. These emerging requirements lead to the need for interdisciplinary and multidisciplinary contributions that address different aspects of the problem, such as data collection, storage, protection, processing, and transmission, as well as knowledge discovery, retrieval, and security and privacy. Hence, the goal of the workshop is to attract researchers and experts in the areas of multimedia information retrieval, machine learning, AI, data science, event-based processing and analysis, multimodal multimedia content analysis, lifelog data analysis, urban computing, environmental science, atmospheric science, and security and privacy to tackle the issues as mentioned earlier."
NOT_RELEVANT;ACM Digital Library;Extracting spatial information from social media in support of agricultural management decisions;Golubovic, Nevena and Krintz, Chandra and Wolski, Rich and Lafia, Sara and Hervey, Thomas and Kuhn, Werner;2016;10.1145/3003464.3003468;NOT_FOUND;Farmers face pressure to respond to unpredictable weather, the spread of pests, and other variable events on their farms. This paper proposes a framework for data aggregation from diverse sources that extracts named places impacted by events relevant to agricultural practices. Our vision is to couple natural language processing, geocoding, and existing geographic information retrieval techniques to increase the value of already-available data through aggregation, filtering, validation, and notifications, helping farmers make timely and informed decisions with greater ease.
NOT_RELEVANT;ACM Digital Library;Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2018);Kumar Chandrasekaran, Muthu and Jaidka, Kokil and Mayr, Philipp;2018;10.1145/3209978.3210194;NOT_FOUND;The large scale of scholarly publications poses a challenge for scholars in information seeking and sensemaking. Information retrieval~(IR), bibliometric and natural language processing (NLP) techniques could enhance scholarly search, retrieval and user experience but are not yet widely used. To this purpose, we propose the third iteration of the Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL). The workshop is intended to stimulate IR, NLP researchers and Digital Library professionals to elaborate on new approaches in natural language processing, information retrieval, scientometrics, text mining and recommendation techniques that can advance the state-of-the-art in scholarly document understanding, analysis, and retrieval at scale. The BIRNDL workshop will incorporate multiple invited talks, paper sessions, a poster session and the 4th edition of the Computational Linguistics (CL) Scientific Summarization Shared Task.
NOT_RELEVANT;ACM Digital Library;"Nobody Said it Would be Easy: A Decade of R&amp;D Projects in Information Access from Thomson over Reuters to Refinitiv";Leidner, Jochen L.;2019;10.1145/3331184.3331444;NOT_FOUND;"In this talk, I survey a small, non-random sample of research projects in information access carried out as part of the Thomson Reuters family of companies over the course of a 10+-year period. I analyse into how these projects are similar and different when compared to academic research efforts and attempt a critical (and personal, so certainly subjective) assessment of what academia can do for industry, and what industry can do for research in terms of R&amp;D efforts. I will conclude with some advice for academic-industry collaboration initiatives in several areas of vertical information services (legal, finance, pharma and regulatory/compliance) as well as news."
NOT_RELEVANT;ACM Digital Library;Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2016);Cabanac, Guillaume and Chandrasekaran, Muthu Kumar and Frommholz, Ingo and Jaidka, Kokil and Kan, Min-Yen and Mayr, Philipp and Wolfram, Dietmar;2016;10.1145/2910896.2926734;NOT_FOUND;The large scale of scholarly publications poses a challenge for scholars in information-seeking and sensemaking. Bibliometric, information retrieval~(IR), text mining and NLP techniques could help in these activities, but are not yet widely used in digital libraries. This workshop is intended to stimulate IR researchers and digital library professionals to elaborate on new approaches in natural language processing, information retrieval, scientometric and recommendation techniques which can advance the state-of-the-art in scholarly document understanding, analysis and retrieval at scale.
NOT_RELEVANT;ACM Digital Library;Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2017);Chandrasekaran, Muthu Kumar and Jaidka, Kokil and Mayr, Philipp;2017;10.1145/3077136.3084370;NOT_FOUND;The large scale of scholarly publications poses a challenge for scholars in information seeking and sensemaking. Bibliometrics, information retrieval (IR), text mining and NLP techniques could help in these search and look-up activities, but are not yet widely used. This workshop is intended to stimulate IR researchers and digital library professionals to elaborate on new approaches in natural language processing, information retrieval, scientometrics, text mining and recommendation techniques that can advance the state-of-the-art in scholarly document understanding, analysis, and retrieval at scale. The BIRNDL workshop at SIGIR 2017 will incorporate an invited talk, paper sessions and the third edition of the Computational Linguistics (CL) Scientific Summarization Shared Task.
NOT_RELEVANT;ACM Digital Library;DopeLearning: A Computational Approach to Rap Lyrics Generation;Malmi, Eric and Takala, Pyry and Toivonen, Hannu and Raiko, Tapani and Gionis, Aristides;2016;10.1145/2939672.2939679;NOT_FOUND;Writing rap lyrics requires both creativity to construct a meaningful, interesting story and lyrical skills to produce complex rhyme patterns, which form the cornerstone of good flow. We present a rap lyrics generation method that captures both of these aspects. First, we develop a prediction model to identify the next line of existing lyrics from a set of candidate next lines. This model is based on two machine-learning techniques: the RankSVM algorithm and a deep neural network model with a novel structure. Results show that the prediction model can identify the true next line among 299 randomly selected lines with an accuracy of 17%, i.e., over 50 times more likely than by random. Second, we employ the prediction model to combine lines from existing songs, producing lyrics with rhyme and a meaning. An evaluation of the produced lyrics shows that in terms of quantitative rhyme density, the method outperforms the best human rappers by 21%. The rap lyrics generator has been deployed as an online tool called DeepBeat, and the performance of the tool has been assessed by analyzing its usage logs. This analysis shows that machine-learned rankings correlate with user preferences.
NOT_RELEVANT;ACM Digital Library;TEXTOMAP: determining geographical window for texts;Brun, Geoffrey and Domingu\`{e}s, Catherine and Van Damme, M.-D.;2015;10.1145/2837689.2837703;NOT_FOUND;In newspapers or scholar manuals, numerous texts are accompanied by maps. In these map/text couples, maps give a spatial portrayal of the text issues, thus they make the spatial issues easier to understand. TEXTOMAP aims to design the geographical window of the text, based on the notion of important toponyms according to text issues. The important toponym selection is based on indicators which may be spatial, linguistic or semantic. Examples of geographical window calculation are shown and compared with the corresponding CLAVIN geographical focus. The work is in progress and perspectives are offered.
NOT_RELEVANT;ACM Digital Library;A Supervised Method to Enhance Vocabulary with the Creation of Domain Specific Lexica;Fernandes, Paulo and Furquim, Luis O. C. and Lopes, Lucelene;2013;10.1109/WI-IAT.2013.168;NOT_FOUND;This paper proposes a method to enhance lexica by processing domain specific corpora. The proposed method relies on the identification of the more relevant unknown terms in each domain corpus. The innovative points of the proposed approach is to automatically detect unknown terms using MTMDD technology to handle lexical structures, and to automatically rank and identify domain specific terms using gini and tf-dcf indices. The proposed method is experimented in six corpora in order to illustrate its benefits.
NOT_RELEVANT;ACM Digital Library;Learning the multilingual translation representations for question retrieval in community question answering via non-negative matrix factorization;Zhou, Guangyou and Xie, Zhiwen and He, Tingting and Zhao, Jun and Hu, Xiaohua Tony;2016;10.1109/TASLP.2016.2544661;NOT_FOUND;Community question answering (CQA) has become an increasingly popular research topic. In this paper, we focus on the problem of question retrieval. Question retrieval in CQA can automatically find the most relevant and recent questions that have been solved by other users. However, the word ambiguity and word mismatch problems bring about new challenges for question retrieval in CQA. State-of-the-art approaches address these issues by implicitly expanding the queried questions with additional words or phrases using monolingual translation models. While useful, the effectiveness of these models is highly dependent on the availability of quality parallel monolingual corpora (e.g., question--answer pairs) in the absence of which they are troubled by noise issues. In this work, we propose an alternative way to address the word ambiguity and word mismatch problems by taking advantage of potentially rich semantic information drawn from other languages. Our proposed method employs statistical machine translation to improve question retrieval and enriches the question representation with the translated words from other languages via non-negative matrix factorization. Experiments conducted on real CQA data sets show that our proposed approach is promising.
NOT_RELEVANT;ACM Digital Library;Documents Search Using Semantics Criteria;Cotelo, Santiago and Makowski, Alejandro and Chiruzzo, Luis and Wonsever, Dina;2014;10.1145/2663712.2666187;NOT_FOUND;Current Information Retrieval systems generally search documents using a keywords model, which is often not expressive enough for the user. In this paper we describe some directions for improving an Information Retrieval system by letting the user specify different semantics constraints in her query, using a language based on a simplified version of first-order logic. The user can write queries that express the association between objects and attributes, temporal constraints and negation of attributes, and also perform synonyms expansion of queries. In order to evaluate the relevance of a candidate document with respect to the query, the dependency parse tree of the document is used, as well as other linguistic resources. The system was evaluated using a set of queries and a corpus extracted from the British newspaper The Times. The results are compared against the newspaper's own search engine and they look promising, showing an important improvement in precision in the first documents returned by the query.
NOT_RELEVANT;ACM Digital Library;GraphRep: Boosting Text Mining, NLP and Information Retrieval with Graphs;Vazirgiannis, Michalis and Malliaros, Fragkiskos D. and Nikolentzos, Giannis;2018;10.1145/3269206.3274273;NOT_FOUND;"Graphs have been widely used as modeling tools in Natural Language Processing (NLP), Text Mining (TM) and Information Retrieval (IR). Traditionally, the unigram bag-of-words representation is applied; that way, a document is represented as a multiset of its terms, disregarding dependencies between the terms. Although several variants and extensions of this modeling approach have been proposed, the main weakness comes from the underlying term independence assumption; the order of the terms within a document is completely disregarded and any relationship between terms is not taken into account in the final task. To deal with this problem, the research community has explored various representations, and to this direction, graphs constitute a well-developed model for text representation. The goal of this tutorial is to offer a comprehensive presentation of recent methods that rely on graph-based text representations to deal with various tasks in Text Mining, NLP and IR."
NOT_RELEVANT;ACM Digital Library;Multi-Pointer Co-Attention Networks for Recommendation;Tay, Yi and Luu, Anh Tuan and Hui, Siu Cheung;2018;10.1145/3219819.3220086;NOT_FOUND;Many recent state-of-the-art recommender systems such as D-ATT, TransNet and DeepCoNN exploit reviews for representation learning. This paper proposes a new neural architecture for recommendation with reviews. Our model operates on a multi-hierarchical paradigm and is based on the intuition that not all reviews are created equal, i.e., only a selected few are important. The importance, however, should be dynamically inferred depending on the current target. To this end, we propose a review-by-review pointer-based learning scheme that extracts important reviews from user and item reviews and subsequently matches them in a word-by-word fashion. This enables not only the most informative reviews to be utilized for prediction but also a deeper word-level interaction. Our pointer-based method operates with a gumbel-softmax based pointer mechanism that enables the incorporation of discrete vectors within differentiable neural architectures. Our pointer mechanism is co-attentive in nature, learning pointers which are co-dependent on user-item relationships. Finally, we propose a multi-pointer learning scheme that learns to combine multiple views of user-item interactions. We demonstrate the effectiveness of our proposed model via extensive experiments on 24 benchmark datasets from Amazon and Yelp. Empirical results show that our approach significantly outperforms existing state-of-the-art models, with up to 19% and 71% relative improvement when compared to TransNet and DeepCoNN respectively. We study the behavior of our multi-pointer learning mechanism, shedding light on 'evidence aggregation' patterns in review-based recommender systems.
NOT_RELEVANT;ACM Digital Library;Employing issues and commits for in-code sentence based use case identification and remodularization;Berta, Peter and Bystrick\'{y}, Michal and Krempask\'{y}, Michal and Vrani\'{c}, Valentino;2017;10.1145/3123779.3123792;NOT_FOUND;Use case driven modularization improves code comprehension and maintenance and provides another view on software alongside object-oriented modularization. However, approaches enabling use case driven modularization require to modularize code manually. In this paper, we propose an approach to employing issues and commits for in-code sentence based use case identification and remodularization. The approach aims at providing use case based perspective on the existing code. The sentences of use case steps are compared to sentences of issue descriptions, while the sentences generated from the source code of issue commits are compared to sentences generated from the corresponding methods in source code in order to quantify the similarity between use case steps and methods in source code using different similarity calculation algorithms. The resulting level of similarity is used to remodularize source code according to use cases. We conducted a study on the OpenCart open source e-shop employing 16 use cases. The approach achieved the recall of 3.37% and precision of 75%. The success of the approach strongly depends on issues and commits assigned to them. The results would be better especially for the code that natively employs use case driven modularization.
NOT_RELEVANT;ACM Digital Library;Workshop summary for the 2013 international workshop on mining unstructured big data using natural language processing;Liu, Xiaozhong and Chen, Miao and Ding, Ying and Song, Min;2013;10.1145/2505515.2505810;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;ACM Digital Library;READFAST: high-relevance search-engine for big text;Gubanov, Michael and Pyayt, Anna;2013;10.1145/2505515.2508215;NOT_FOUND;Relevance of search-results is a key factor for any search engine. In order to return and rank the Web-pages that are most relevant to the query, contemporary search engines use complex ranking functions that depend on hundreds of features. For example, presence or absence of the query keywords on the page, their proximity, frequencies, HTML markup are just a few to name. Additional features might include fonts, tags, hyperlinks, metadata, and parts of the Web-page description. All this information is used by the search-engine to rank HTML Web pages returned to the user, but is unfortunately absent in free text that has no HTML markup, tags, hyperlinks, and any other metadata, except implicit natural language structure.Here we demonstrate one of the first Big text search engines that leverages hidden structure of the natural language sentences in order to process user queries and return more relevant search-results than a standard keyword-search. It provides a structured index extracted from the text using Natural Language Processing (NLP) that can be used to browse and query free text.
NOT_RELEVANT;ACM Digital Library;Text Compression for Myanmar Information Retrieval;Lin, Nay and A, Kudinov Vitaly and Soe, Yan Naing;2019;10.1145/3342827.3342830;NOT_FOUND;Myanmar word segmentation is an important task for construction of dictionary file for Myanmar information retrieval and Myanmar text compression. Although Myanmar word segmentation using dictionary and orthography has been existed for Myanmar language, the performance of word segmentation depends on the coverage of the dictionary and training dataset and can cause out of vocabulary (OOV) problem, leading to lower precision and recall in information retrieval. And to compress Myanmar text, words in text needs to be recognized first. In this paper, we propose a new method for Myanmar word segmentation by local statistical dataset without the use of any additional data (e.g., training corpus) and new compressed Myanmar Information Retrieval (MIR) model which used End Tagged Dense Code (ETDC) text compressed method. The experimental results showed that the method can improve evaluation of vocabulary file with precision 75%, recall 87%, F-measure 80% and average compression ratio is 32% of texts for Myanmar language.
NOT_RELEVANT;ACM Digital Library;Portfolio: Searching for relevant functions and their usages in millions of lines of code;Mcmillan, Collin and Poshyvanyk, Denys and Grechanik, Mark and Xie, Qing and Fu, Chen;2013;10.1145/2522920.2522930;NOT_FOUND;"Different studies show that programmers are more interested in finding definitions of functions and their uses than variables, statements, or ordinary code fragments. Therefore, developers require support in finding relevant functions and determining how these functions are used. Unfortunately, existing code search engines do not provide enough of this support to developers, thus reducing the effectiveness of code reuse. We provide this support to programmers in a code search system called Portfolio that retrieves and visualizes relevant functions and their usages. We have built Portfolio using a combination of models that address surfing behavior of programmers and sharing related concepts among functions. We conducted two experiments: first, an experiment with 49 C/C++ programmers to compare Portfolio to Google Code Search and Koders using a standard methodology for evaluating information-retrieval-based engines; and second, an experiment with 19 Java programmers to compare Portfolio to Koders. The results show with strong statistical significance that users find more relevant functions with higher precision with Portfolio than with Google Code Search and Koders. We also show that by using PageRank, Portfolio is able to rank returned relevant functions more efficiently."
NOT_RELEVANT;ACM Digital Library;The automated retrieval console (ARC): open source software for streamlining the process of natural language processing;D'Avolio, Leonard W. and Nguyen, Thien and Fiore, Louis;2010;10.1145/1882992.1883065;NOT_FOUND;Open source natural language processing (NLP) frameworks have made it easier for NLP developers and researchers to develop more reusable and modular components and to capitalize on the work of others. With the Automated Retrieval Console (ARC) we attempt to build upon this foundation by streamlining the many processes surrounding the development, evaluation, and deployment of natural language processing technologies. Toward this end, ARC offers graphical user interfaces to facilitate corpus import, reference set creation, annotation, and inter-annotator agreement calculation. To speed task-specific information extraction development, ARC combines NLP-generated features from UIMA pipelines with machine learning classifiers and calculates performance statistics against a reference set. We also use ARC to explore automated algorithm creation for specific information extraction tasks in an effort to reduce the need for custom code and rules development. We present a detailed description of the ideas implemented in this proof-of-concept and a brief overview of two empirical evaluations.
NOT_RELEVANT;ACM Digital Library;Building Taxonomies based on Human-Machine Teaming: Cyber Security as an Example;Mahaini, Mohamad Imad and Li, Shujun and Sa\u{g}lam, Rahime Belen;2019;10.1145/3339252.3339282;NOT_FOUND;Taxonomies and ontologies are handy tools in many application domains such as knowledge systematization and automatic reasoning. In the cyber security field, many researchers have proposed such taxonomies and ontologies, most of which were built based on manual work. Some researchers proposed the use of computing tools to automate the building process, but mainly on very narrow sub-areas of cyber security. Thus, there is a lack of general cyber security taxonomies and ontologies, possibly due to the difficulties of manually curating keywords and concepts for such a diverse, inter-disciplinary and dynamically evolving field.This paper presents a new human-machine teaming based process to build taxonomies, which allows human experts to work with automated natural language processing (NLP) and information retrieval (IR) tools to co-develop a taxonomy from a set of relevant textual documents. The proposed process could be generalized to support non-textual documents and to build (more complicated) ontologies as well. Using the cyber security as an example, we demonstrate how the proposed taxonomy building process has allowed us to build a general cyber security taxonomy covering a wide range of data-driven keywords (topics) with a reasonable amount of human effort.
NOT_RELEVANT;ACM Digital Library; 'How may I help you'-spoken queries for technical assistance;Wilson, Dale-Marie and Martin, Aqueasha M. and Gilbert, Juan E.;2010;10.1145/1900008.1900068;NOT_FOUND;Spoken dialog systems, including interactive assistants, have emerged as a viable option for presenting technical communication. Thus has contributed to interests in improving the effectiveness and design of such systems through natural language. Traditional methods of natural language processing include parts-of-speech tagging, syntactic parsing, and statistical models. This paper introduces a new conversational question answering methodology, Answer First (A1) that bypasses traditional methods and removes the need for preprocessing of queries.
NOT_RELEVANT;ACM Digital Library;GeoTxt: a web API to leverage place references in text;"Karimzadeh, Morteza and Huang, Wenyi and Banerjee, Siddhartha and Wallgr\""{u}n, Jan Oliver and Hardisty, Frank and Pezanowski, Scott and Mitra, Prasenjit and MacEachren, Alan M.";2013;10.1145/2533888.2533942;NOT_FOUND;Associating place name mentions in unstructured text with their actual references in geographic space is vital to enable spatial queries and analysis. In this paper, we introduce GeoTxt, a web API plus human-usable web tool designed and implemented to tackle three components of place-reference processing from text, namely: extraction, disambiguation, and geolocation of place names mentioned in unstructured text. Current GeoTxt development is focused particularly on support for processing short microblog posts.
NOT_RELEVANT;ACM Digital Library;Towards a more efficient and personalised advertisement content in on-line social networks;Gal\'{a}n Garc\'{\i}a, Patxi and Laorden Gom\'{e}z, Carlos and Garc\'{\i}a Bringas, Pablo;2012;10.1145/2389686.2389706;NOT_FOUND;Knowing what potential clients want, is the most important issue for companies. The current situation of social communication is generating a lot of information about users, such as favourite sites, food, politic tendencies, hopes or needs. This information has an incalculable value for marketing interests but, unfortunately, it is not trivial to process it. One way to obtain this information without disturbing the users is to store their searches, used words on the Internet and clicked items, to construct specific profiles. But, the problem with these techniques is that they do not retrieve current information about the targeted user because they gather information that may or may not be up to date. In light of this background, we propose a methodology to obtain up to date information about user's interests, likes and needs by analysing users conversations in social networks and instant messaging systems to generate personalised and interesting advertisement with better impact and higher success rates.
MAYBE_RELEVANT;ACM Digital Library;Combining the Best of Two Worlds: NLP and IR for Intranet Search;Adindla, Suma and Kruschwitz, Udo;2011;10.1109/WI-IAT.2011.187;NOT_FOUND;Natural language processing (NLP) is becoming much more robust and applicable in realistic applications. One area in which NLP has still not been fully exploited is information retrieval (IR). In particular we are interested in search over intranets and other local Web sites. We see dialogue-driven search which is based on a largely automated knowledge extraction process as one of the next big steps. Instead of replying with a set of documents for a user query the system would allow the user to navigate through the extracted knowledge base by making use of a simple dialogue manager. Here we support this idea with a first task-based evaluation that we conducted on a university intranet. We automatically extracted entities like person names, organizations and locations as well as relations between entities and added visual graphs to the search results whenever a user query could be mapped into this knowledge base. We found that users are willing to interact and use those visual interfaces. We also found that users preferred such a system that guides a user through the result set over a baseline approach. The results represent an important first step towards full NLP-driven intranet search.
NOT_RELEVANT;ACM Digital Library;Text2Geo: from textual data to geospatial information;Tahrat, Sabiha and Kergosien, Eric and Bringay, Sandra and Roche, Mathieu and Teisseire, Maguelonne;2013;10.1145/2479787.2479796;NOT_FOUND;In this paper, we focus on methods for extracting spatial information in text documents. After presenting textual description of space and manual annotation of named entities, mainly location and organization, we present our proposal Text2Geo. It is a hybrid method which combines information extraction approach based on patterns with a supervised classification approach to explore context. We discuss some results obtained on the dataset of Thau lagoon.
NOT_RELEVANT;ACM Digital Library;Natural language processing for information retrieval: the time is ripe (again);Lease, Matthew;2007;10.1145/1316874.1316876;NOT_FOUND;Paraphrasing van Rijsbergen [37], the time is ripe for another attempt at using natural language processing (NLP) for information retrieval (IR). This paper introduces my dissertation study, which will explore methods for integrating modern NLP with state-of-the-art IR techniques. In addition to text, I will also apply retrieval to conversational speech data, which poses a unique set of considerations in comparison to text. Greater use of NLP has potential to improve both text and speech retrieval.
NOT_RELEVANT;ACM Digital Library;Extracting spatiotemporal and semantic events from documents;Wang, Wei and Stewart, Kathleen;2013;10.1145/2533888.2533941;NOT_FOUND;The methods demonstrated through this work support automated annotation of spatiotemporal semantic event information associated with natural hazards, such as power outages, cancellations, and emergency response associated with hurricanes, floods, or drought as reported in web news reports.
NOT_RELEVANT;ACM Digital Library;A vector space analysis of swedish patent claims with different linguistic indices;Andersson, Linda;2010;10.1145/1871888.1871898;NOT_FOUND;"The purpose of this study was twofold, first to examine if it is possible to use a general automatic retrieval model, the Vector Space Model (VSM), in order to discover similarities between Swedish patent claims; and second to examine whether an addition morphological decompounding module at the pre-processing level improves the result. In the present study, a comparison between three different topic sets consisting of patent claims was compared against an entire collection of 30,117 claims. The VSM was evaluated with and without additional morphological decompounding modules. The results indicate that decompounding will influence the performance of the retrieval model in a positive way. However, the sublanguage of patent claims and the errors made during the Optical Character Recognition (OCR) process were harmful towards the overall performance of the Natural Language Processing (NLP) applications as well as for the retrieval model."
NOT_RELEVANT;ACM Digital Library;The Web as a Source of Evidence for Filtering Candidate Answers to Natural Language Questions;Bonnefoy, Ludovic and Bellot, Patrice and Benoit, Michel;2011;10.1109/WI-IAT.2011.226;NOT_FOUND;Identifying and extracting named entities from web pages has been the subject of many researches. In this paper, we propose and evaluate some new unsupervised language modeling approaches to determine the membership level of a candidate answer, a named entity, to a natural language question to a very fine-grained conceptual class of entity. We propose to address this issue by using the Web or DBPedia hierarchy as sources of evidence. Then, this level of membership can be used to improve the ranking of candidate answers in a question-answering task. Lastly, we present the results we obtained by participating in TREC 2010 Entity track.
NOT_RELEVANT;ACM Digital Library;Populating Ontologies in the eTourism Domain;Ruiz-Mart\'{\i}nez, Juana M. and Mi\~{n}arro-Gim\'{e}nez, Jos\'{e} A. and Guill\'{e}n-C\'{a}rceles, Laura and Castellanos-Nieves, Dagoberto and Valencia-Garc\'{\i}a, Rafael and Garc\'{\i}a-S\'{a}nchez, Francisco and Fern\'{a}ndez-Breis, Jesualdo T. and Mart\'{\i}nez-B\'{e}jar, Rodrigo;2008;10.1109/WIIAT.2008.278;NOT_FOUND;The Semantic Web vision is based on structuring the knowledge that is present in the current Web so that it is understandable by machines without human intervention. Ontologies are the backbone technology for the Semantic Web. Thus, the realization of the Semantic Web vision largely depends on the design and instantiation of ontologies. While several methodologies for designing ontologies and automating ontology learning have been proposed, ontology population has not received much attention so far. This paper presents a methodology for populating ontologies from natural language web documents. For this, Semantic Web Technologies and Natural Language Technologies have been used. This approach has been applied in the eTourism domain.
NOT_RELEVANT;ACM Digital Library;Automated identification of relevant new information in clinical narrative;Zhang, Rui and Pakhomov, Serguei and Melton, Genevieve B.;2012;10.1145/2110363.2110467;NOT_FOUND;The ability to explore and visualize clinical information is important for clinicians when reviewing and cognitively synthesizing electronic clinical documents for new patients contained in electronic health record (EHR) systems. In this study, we explore the use of language models for detecting new and potentially relevant information within an individual patient's collection of clinical documents using an expert-based reference standard for evaluation. We achieved good accuracy with a heterogeneous system based on a modified n-gram language model with statistically-derived and classic stop word removal and lexical normalization, as well as heuristic rules. This technique also identified relevant new information not identified with the expert-derived reference standard. These methods appear promising for providing an automated means to improve the use of electronic documents by clinicians.
NOT_RELEVANT;ACM Digital Library;Development and Enhancement of a Stemmer for the Greek Language;Ntais, Georgios and Saroukos, Spyridon and Berki, Eleni and Dalianis, Hercules;2016;10.1145/3003733.3003775;NOT_FOUND;Although there are three stemmers published for the Greek language, only the one presented in this paper and called Ntais' stemmer is freely open and available, together with its enhancements and extensions according to Saroukos' algorithm. The primary algorithm (Ntais' algorithm) uses only capital letters and works with better performance than other past stemming algorithms for the Greek language, giving 92.1 percent correct results. Further extensions of the proposed stemming system (e.g. from capital to small letters) and more evaluation methods are presented according to a new and improved algorithm, Saroukos' algorithm. Stemmer performance metrics are further used for evaluating the existing stemming system and algorithm and show how its accuracy and completeness are enhanced. The improvements were possible by providing an alternative implementation in the programming language PHP, which offers more syntactical rules and exceptions. The two versions of the stemming algorithm are tested and compared.
NOT_RELEVANT;ACM Digital Library;Critical analysis of WSD algorithms;Dwivedi, S. K. and Rastogi, Parul;2009;10.1145/1523103.1523117;NOT_FOUND;Word Sense Disambiguation is the core of many natural language processing (NLP) tasks. WSD is a significant technique and has been an interest and concern for the various researchers from the long time. WSD is considered as a successful technique in the last several years. Though success rate of WSD technique is quiet high it has certain limitation of unavailability of standardized evaluation system. The basic aim of the paper is to investigate the critical aspects of various WSD approaches. The paper will also provide a detail of the success rate of the various approaches and also their usage in various application areas.
NOT_RELEVANT;ACM Digital Library;Data Readiness Level for Unstructured Data;Lu, Yang and Fang, Xing and Zhan, Justin;2014;10.1145/2640087.2644192;NOT_FOUND;When time or computational resources is a constraint, dealing with large amount of data can be painful for many organizations. In this paper, we proposed a new concept called Data Readiness Level(DRL). It will measures the readiness of a file very quickly. We define readiness as ready for immediate analytical purposes. DRL is pair-wised measure, one is PKSS value, another is PVI value. One can see PKSS as the degree of similarity to the objective. PVI exhibits how much valuable information the file has. In the real word, not every data contains valuable information. Even if it is, it is not guaranteed that it will be relevant to the objective. With the aid of DRL, users can simply analyze those data with higher DRL values when time is a constraint. We collected 40,000 PDF files by hand from IEEE Xplore digital library and spriger. The experimental result was quite encouraging.
NOT_RELEVANT;ACM Digital Library;Data Readiness Level For Unstructured Data With A Focus On Unindexed Text Data;Lu, Yang and Fang, Xing and Zhan, Justin;2014;10.1145/2640087.2644160;NOT_FOUND;As we entered into the so called big data era, the amount of data organizations are dealing with are greater than ever before. When time or computational resources is a constraint, dealing with large amount of data can be painful for many organizations. In this paper, we proposed a new concept called Data Readiness Level(DRL). It will measures the readiness of a file very quickly. We define readiness as ready for immediate analytical purposes. DRL is pair-wised measure, one is PKSS value, another is PVI value. One can see PKSS as the degree of similarity to the objective. PVI exhibits how much valuable information the file has. In the real word, not every data contains valuable information. Even if it is, it is not guaranteed that it will be relevant the objective. For example, an atmospheric data from NOAA often times contain many valuable information. However, if our objective is to search psychology literatures, then the atmospheric data is not what we really want. With the aid of DRL, users can simply analyze those data with higher DRL values when time is a constraint. We collected 40,000 PDF files by hand from IEEE Xplore digital library. Overall, the experimental result was quite remarkable.
NOT_RELEVANT;ACM Digital Library;Smart search engine using artificial intelligence;Modi, A. and Bhandari, A. and Desai, K. and Shah, N.;2011;10.1145/1980022.1980174;NOT_FOUND;Due to the increasing information on the Web it becomes very difficult for a single search engine to provide comprehensive Web coverage, thus increasing the difficulty for an average user to search information on the web. We have proposed an idea for smart Meta search engine which will have a more comprehensive coverage of the web by targeting different search engines for different search categories. The user's search query will be modified using a knowledge base and appropriate search engine will be selected. The results returned from these search engines will be merged and ranked. These results will be classified into various categories using a classification algorithm and then the result will be displayed sorted into categories. It will be beneficial to the user who has an obscure idea about the information he/she wanted to search.
NOT_RELEVANT;ACM Digital Library;A survey of types of text noise and techniques to handle noisy text;Subramaniam, L. Venkata and Roy, Shourya and Faruquie, Tanveer A. and Negi, Sumit;2009;10.1145/1568296.1568315;NOT_FOUND;Often, in the real world noise is ubiquitous in text communications. Text produced by processing signals intended for human use are often noisy for automated computer processing. Automatic speech recognition, optical character recognition and machine translation all introduce processing noise. Also digital text produced in informal settings such as online chat, SMS, emails, message boards, newsgroups, blogs, wikis and web pages contain considerable noise. In this paper, we present a survey of the existing measures for noise in text. We also cover application areas that ingest this noisy text for various tasks like Information Retrieval and Information Extraction.
NOT_RELEVANT;ACM Digital Library;Managing syntactic variation in text retrieval;Vilares, Jes\'{u}s and G\'{o}mez-Rodr\'{\i}guez, Carlos and Alonso, Miguel A.;2005;10.1145/1096601.1096643;NOT_FOUND;Information Retrieval systems are limited by the linguistic variation of language. The use of Natural Language Processing techniques to manage this problem has been studied for a long time, but mainly focusing on English. In this paper we deal with European languages, taking Spanish as a case in point. Two different sources of syntactic information, queries and documents, are studied in order to increase the performance of Information Retrieval systems.
NOT_RELEVANT;ACM Digital Library;Mixed-script query labelling using supervised learning and ad hoc retrieval using sub word indexing;Mukherjee, Abhinav and Ravi, Anirudh and Datta, Kaustav;2014;10.1145/2824864.2824873;NOT_FOUND;"Much of the user generated content on the internet is written in their transliterated form instead of in their indigenous script. Due to this search engines receive a large number of transliterated search queries.This paper presents our approach to handle labelling of queries and ad hoc retrieval of documents based on these queries, as part of the FIRE2014 shared task on transliterated search. The content of each document is written in either the native Devanagari script or its transliterated form in Roman script or a combination of both. The queries to retrieve these documents can also be in mixed script. The task is challenging primarily due to the spelling variations that occur in the transliterated form of search queries. This particular problem is addressed by using back transliteration to reduce spelling variations, and a set of hand-tailored rules for consonant mapping. Sub-word indexing is done to take care of breaking and joining of transliterated words. Implementation of query labelling of the mixed script content was done using a supervised learning approach where an SVM classifier was trained using character n-grams as features for language identification. A Na\""{\i}ve Bayes classifier was used for classifying transliterated words that can belong to both Hindi and English when looked at individually.The 2 runs submitted by our team (BITS-Lipyantaran) performs best across all metrics for Subtask 2 among all the teams that participated, with a MRR score of 0.8171 and MAP score of 0.6421."
NOT_RELEVANT;ACM Digital Library;Multi-resolution disambiguation of term occurrences;Amitay, Einat and Nelken, Rani and Niblack, Wayne and Sivan, Ron and Soffer, Aya;2003;10.1145/956863.956913;NOT_FOUND;We describe a system for extracting mentions of terms such as company and product names, in a large and noisy corpus of documents, such as the World Wide Web. Since natural language terms are highly ambiguous, a significant challenge in this task is disambiguating which occurrences of each term are truly related to the right meaning, and which are not. We describe our approach for disambiguation, and show that it achieves very high accuracy with only limited training. This serves as a necessary first step for applications that strive to do analytics on term mentions.
NOT_RELEVANT;ACM Digital Library;Multilingual extraction of semantic indexes;Roussey, Catherine and Calabretto, Sylvie and Harrathi, Farah;2007;10.1145/1283880.1283882;NOT_FOUND;This article deals with multilingual document indexing. We propose an indexing method based on several stages. First of all the most important terms of the document are extracted using general characteristics of languages and statistical methods. Thus, term extraction stages can be applied to any document whatever the document language is. Secondly, our indexing method uses a multilingual ontology in order to find the most relevant concepts representing the document content. Our method can be applied to a multilingual corpus containing document written in different languages. This indexing procedure is part of a Multilingual Document System untitled SyDoM, which manages XML documents.
NOT_RELEVANT;ACM Digital Library;A machine transliteration model based on correspondence between graphemes and phonemes;Oh, Jong-Hoon and Choi, Key-Sun and Isahara, Hitoshi;2006;10.1145/1194936.1194938;NOT_FOUND;Machine transliteration is an automatic method for converting words in one language into phonetically equivalent ones in another language. There has been growing interest in the use of machine transliteration to assist machine translation and information retrieval. Three types of machine transliteration models---grapheme-based, phoneme-based, and hybrid---have been proposed. Surprisingly, there have been few reports of efforts to utilize the correspondence between source graphemes and source phonemes, although this correspondence plays an important role in machine transliteration. Furthermore, little work has been reported on ways to dynamically handle source graphemes and phonemes. In this paper, we propose a transliteration model that dynamically uses both graphemes and phonemes, particularly the correspondence between them. With this model, we have achieved better performance---improvements of about 15 to 41% in English-to-Korean transliteration and about 16 to 44% in English-to-Japanese transliteration---than has been reported for other models.
NOT_RELEVANT;ACM Digital Library;Exploiting syntactic structure of queries in a language modeling approach to IR;Srikanth, Munirathnam and Srihari, Rohini;2003;10.1145/956863.956952;NOT_FOUND;Natural Language Processing (NLP) techniques have been explored to enhance the performance of Information Retrieval (IR) methods with varied results. Most efforts in using NLP techniques have been to identify better index terms for representing documents. This use in the indexing phase of IR has implicit effect on retrieval performance. However, the explicit use of NLP techniques during the retrieval or information seeking phase has been restricted to interactive or dialogue systems. Recent advances in IR are based on using Statistical Language Models (SLM) to represent documents and ranking them based on their model generating a given user query. This paper presents a novel method for using NLP techniques on user queries, specifically, a syntactic parse of a query, in the statistical language modeling approach to IR. In the proposed method, named Concept Language Models, a query is viewed as a sequence of concepts and a concept as a sequence terms. The paper presents different approximations to estimate the concept and term probabilities and compute the query likelihood estimate for documents. Some empirical results on TREC test collections comparing Concept Language Models with smoothed N-gram language models are presented.
NOT_RELEVANT;ACM Digital Library;Intelligent interactive multimedia system: layered ontological video contexts in a folksonomy driven environment;Dal Mas, Massimiliano;2013;10.1145/2479787.2479829;NOT_FOUND;This paper describes a partially developed method on going project for enabling an Intelligent Interactive Multimedia System based on ontological interaction on video clip shown on ubiquitous systems as a computer monitor, mobile or tablet. The paper aims to sketch a theoretical framework on a method for extracting and tracking objects in videos, based on various semantic attributes. It tackles a novel space (cyber-physical) of interaction between human and machine, in a creative way. We use a layered representation based on semantics-driven information to obtain spatiotemporal attributes of objects. The interface is created by extracting object information from the video with a Human Based Computation to obtain a richer semantics of attribute to bridge the semantic gap between words describing an image and its visual features. Users can navigate and manipulate objects displayed on video by associating semantic attributes and comments evaluated by the data and sentiment extraction. Folksonomy tags are extracted from users' comments to be used in a dynamical driven system (Folksodriven). We show some example applications of the proposed method like: advertisement inside the objects displayed on a video, an interface based on objects of interest video navigation, mask layer on an object of interest and a visual interaction for Smart City.
NOT_RELEVANT;ACM Digital Library;Retrieval from captioned image databases using natural language processing;Elworthy, David;2000;10.1145/354756.354850;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;ACM Digital Library;Web-a-where: geotagging web content;Amitay, Einat and Har'El, Nadav and Sivan, Ron and Soffer, Aya;2004;10.1145/1008992.1009040;NOT_FOUND;We describe Web-a-Where, a system for associating geography with Web pages. Web-a-Where locates mentions of places and determines the place each name refers to. In addition, it assigns to each page a geographic focus --- a locality that the page discusses as a whole. The tagging process is simple and fast, aimed to be applied to large collections of Web pages and to facilitate a variety of location-based applications and data analyses.Geotagging involves arbitrating two types of ambiguities: geo/non-geo and geo/geo. A geo/non-geo ambiguity occurs when a place name also has a non-geographic meaning, such as a person name (e.g., Berlin) or a common word (Turkey). Geo/geo ambiguity arises when distinct places have the same name, as in London, England vs. London, Ontario.An implementation of the tagger within the framework of the WebFountain data mining system is described, and evaluated on several corpora of real Web pages. Precision of up to 82% on individual geotags is achieved. We also evaluate the relative contribution of various heuristics the tagger employs, and evaluate the focus-finding algorithm using a corpus pretagged with localities, showing that as many as 91% of the foci reported are correct up to the country level.
NOT_RELEVANT;ACM Digital Library;Language processing technologies for electronic rulemaking: a project highlight;Shulman, Stuart and Hovy, Eduard and Callan, Jamie and Zavestoski, Stephen;2005;NOT_FOUND;NOT_FOUND;In this project, we are developing new text processing tools that help people perform advanced analysis of large collections of text commentary. This problem is increasingly faced by the United States federal government's regulation writers who formulate the rules and regulations that define the details of laws enacted by Congress. Our research focuses on text clustering, text searching using information retrieval, near-duplicate detection, opinion identification, stakeholder characterization, and extractive summarization, as well as the impact of such tools on the process of rulemaking itself. Versions of a Rule-Writer's Workbench will be built by Computer Science researchers at ISI and CMU, deployed annually for experimental use by our government partners, and evaluated by social science researchers from the Library and Information Science and Sociology departments at the Universities of Pittsburgh and San Francisco respectively. This three-year project started in October 2004 and is funded under the National Science Foundation's Digital Government program.
NOT_RELEVANT;ACM Digital Library;Domain-specific FAQ retrieval using independent aspects;Wu, Chung-Hsien and Yeh, Jui-Feng and Chen, Ming-Jun;2005;10.1145/1066078.1066079;NOT_FOUND;"This investigation presents an approach to domain-specific FAQ (frequently-asked question) retrieval using independent aspects. The data analysis classifies the questions in the collected QA (question-answer) pairs into ten question types in accordance with question stems. The answers in the QA pairs are then paragraphed and clustered using latent semantic analysis and the K-means algorithm. For semantic representation of the aspects, a domain-specific ontology is constructed based on WordNet and HowNet. A probabilistic mixture model is then used to interpret the query and QA pairs based on independent aspects; hence the retrieval process can be viewed as the maximum likelihood estimation problem. The expectation-maximization (EM) algorithm is employed to estimate the optimal mixing weights in the probabilistic mixture model. Experimental results indicate that the proposed approach outperformed the FAQ-Finder system in medical FAQ retrieval."
NOT_RELEVANT;ACM Digital Library;Extracting taxonomic relationships from on-line definitional sources using LEXING;Klavans, Judith and Whitman, Brian;2001;10.1145/379437.379675;NOT_FOUND;We present a system which extracts the genus word and phrase from free -form definition text, entitled LEXING, for Lexical Information from Glossaries. The extractions will be used to build automatically a lexical knowledge base from on-line domain specific glossary sources. We combine statistical and semantic processes to extract these terms, and  demonstrate that this combination allows us to predict the genus even in difficult situations such as empty head definitions or verb definitions. We also discuss the use of “linking prepositions” for use in skipping past empty head genus phrases. This system is part of a project to extract ontological information for energy glossary information.
NOT_RELEVANT;ACM Digital Library;WordView: understanding words in context;Normore, Lorraine and Bendig, Mark and Godby, Carol Jean;1998;10.1145/291080.291119;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;ACM Digital Library;Intelligent network news reader;Isahara, Hitoshi and Ozaku, Hiromi;1997;10.1145/238218.238332;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;ACM Digital Library;Circumstance-based categorization analysis of knowledge management systems for the japanese market;Sano, Makoto and Evans, David A.;2004;10.1145/1031171.1031268;NOT_FOUND;"We conducted a survey of thirty of the approximately 1,700 customers of Justsystem Corporation's knowledge-management applications. Our goal was to discover the kinds of functions that customers hoped to address in their next-generation use of knowledge management technology and to assess the core processes that we will need to deploy in our products to address their desired solutions. In particular, we sought to analyze our customers' requirements along dimensions that take account of both the context of use of the application and its stage in the cycle of knowledge creation and use. As part of our analysis, we were able to classify all customer cases as focused by one or more of three Goals, supported by one or more of eleven technology Means. To establish appropriate categories of use, we exploited the stages of the &lt;i&gt;SECI Model&lt;/i&gt;, several other transactional categories of knowledge use, and whether activities were targeted at internal or external users. Through the analysis, we found the typical technology components (Means) for each stage of knowledge creation and use associated with each set of goals. We consider such analysis essential to the task of designing next-generation knowledge-management applications and critical to overcoming the unfortunate tendency of developers to devise solutions that bear little relation to the true needs of users."
NOT_RELEVANT;ACM Digital Library;Text mining as integration of several related research areas: report on KDD's workshop on text mining 2000;Grobelnik, Marko and Mladenic, Dunja and Milic-Frayling, Natasa;2000;10.1145/380995.381051;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;ACM Digital Library;Enhancing information retrieval by automatic acquisition of textual relations using genetic programming;"Bergstr\""{o}m, Agneta and Jaksetic, Patricija and Nordin, Peter";2000;10.1145/325737.325763;NOT_FOUND;We have explored a novel method to find textual relations in electronic documents using genetic programming and semantic networks. This can be used for enhancing information retrieval and simplifying user interfaces. The automatic extraction of relations from text enables easier updating of electronic dictionaries and may reduce interface area both for search input and hit output on small screens such as cell phones and PDAs (Personal Digital Assistants).
NOT_RELEVANT;ACM Digital Library;Effect of relationships between words on Japanese information retrieval;Matsumura, Atsushi and Takasu, Atsuhiro and Adachi, Jun;2006;10.1145/1194936.1194942;NOT_FOUND;Two Japanese-language information retrieval (IR) methods that enhance retrieval effectiveness by utilizing the relationships between words are proposed. The first method uses dependency relationships between words in a sentence. The second method uses proximity relationships, particularly information about the ordered co-occurrence of words in a sentence, to approximate the dependency relationships between them. A Structured Index has been constructed for these two methods, which represents the dependency relationships between words in a sentence as a set of binary trees. The Structured Index is created by morphological analysis and dependency analysis based on simple template matching and compound noun analysis derived from word statistics. Through retrieval experiments using the Japanese test collection for information retrieval systems (NTCIR-1, the NACSIS Test Collection for IR systems), it is shown that these two methods offer superior retrieval effectiveness compared with the TF--IDF method, and are effective with different databases and diverse search topics sets. There is little difference in retrieval effectiveness between these two methods.
NOT_RELEVANT;ACM Digital Library;Probabilistic question answering on the web;Radev, Dragomir and Fan, Weiguo and Qi, Hong and Wu, Harris and Grewal, Amardeep;2002;10.1145/511446.511500;NOT_FOUND;Web-based search engines such as Google and NorthernLight return documents that are relevant to a user query, not answers to user questions. We have developed an architecture that augments existing search engines so that they support natural language question answering. The process entails five steps: query modulation, document retrieval, passage extraction, phrase extraction, and answer ranking. In this paper we describe some probabilistic approaches to the last three of these stages. We show how our techniques apply to a number of existing search engines and we also present results contrasting three different methods for question answering. Our algorithm, probabilistic phrase reranking (PPR) using proximity and question type features achieves a total reciprocal document rank of .20 on the TREC 8 corpus. Our techniques have been implemented as a Web-accessible system, called NSIR.
NOT_RELEVANT;ACM Digital Library;Expans\~{a}o de consulta por pseudo realimenta\c{c}\~{a}o no modelo TR+ para recupera\c{c}\~{a}o de informa\c{c}\~{a}o;Borges, Thyago Bohrer and Gonzalez, Marco and Lima, Vera L\'{u}cia Strube;2008;10.1145/1809980.1810073;NOT_FOUND;This work presents the specification of experiments that apply query expansion techniques with pseudo relevance feedback to TR+ Model in information retrieval. The TR+ Model uses terms and binary lexical relations (BLRs) for indexing and searching of texts in Portuguese. The experiments add (or remove) new terms or BLRs to the original query in order to study the effects of those changes in document retrieval.
NOT_RELEVANT;ACM Digital Library;Vagueness and uncertainty in information retrieval: how can fuzzy sets help?;Kraft, Donald H. and Pasi, Gabriella and Bordogna, Gloria;2006;10.1145/1364742.1364746;NOT_FOUND;The field of fuzzy information systems has grown and is maturing. In this paper, some applications of fuzzy set theory to information retrieval are described, as well as the more recent outcomes of research in this field. Fuzzy set theory is applied to information retrieval with the main aim being to define flexible systems, i.e., systems that can represent and manage the vagueness and subjectivity which characterizes the process of information representation and retrieval, one of the main objectives of artificial intelligence.
NOT_RELEVANT;ACM Digital Library;TExtractor: a multilingual terminology extraction tool;Valderr\'{a}banos, Antonio S. and Belskis, Alexander and Iraola, Luis;2002;NOT_FOUND;NOT_FOUND;This demonstration presents a tool (TExtractor) employed for enriching terminology sets in four languages: English, French, German and Spanish. We present the associated linguistic resources and the experimental results obtained in the medical domain. TExtractor has been developed within project LIQUID (IST-2000-25324), which aims at developing a cost-effective solution for the problem of cross-language information retrieval (CLIR) in multilingual document bases in technical and scientific domains.
NOT_RELEVANT;ACM Digital Library;The infocious web search engine: improving web searching through linguistic analysis;Ntoulas, Alexandros and Chao, Gerald and Cho, Junghoo;2005;10.1145/1062745.1062765;NOT_FOUND;In this paper we present the Infocious Web search engine [23]. Our goal in creating Infocious is to improve the way people find information on the Web by resolving ambiguities present in natural language text. This is achieved by performing linguistic analysis on the content of the Web pages we index, which is a departure from existing Web search engines that return results mainly based on keyword matching. This additional step of linguistic processing gives Infocious two main advantages. First, Infocious gains a deeper understanding of the content of Web pages so it can better match users' queries with indexed documents and therefore can improve relevancy of the returned results. Second, based on its linguistic processing, Infocious can organize and present the results to the user in more intuitive ways. In this paper we present the linguistic processing technologies that we incorporated in Infocious and how they are applied in helping users find information on the Web more efficiently. We discuss the various components in the architecture of Infocious and how each of them benefits from the added linguistic processing. Finally, we experimentally evaluate the performance of a component which leverages linguistic information in order to categorize Web pages.
NOT_RELEVANT;ACM Digital Library;Comparative study of monolingual and multilingual search models for use with asian languages;Savoy, Jacques;2005;10.1145/1105696.1105701;NOT_FOUND;Based on the NTCIR-4 test-collection, our first objective is to present an overview of the retrieval effectiveness of nine vector-space and two probabilistic models that perform monolingual searches in the Chinese, Japanese, Korean, and English languages. Our second goal is to analyze the relative merits of the various automated and freely available toolsto translate the English-language topics into Chinese, Japanese, or Korean, and then submit the resultant query in order to retrieve pertinent documents written in one of the three Asian languages. We also demonstrate how bilingual searches could be improved by applying both the combined query translation strategies and data-fusion approaches. Finally, we address basic problems related to multilingual searches, in which queries written in English are used to search documents written in the English, Chinese, Japanese, and Korean languages.
NOT_RELEVANT;ACM Digital Library;TalkMine and the adaptive recommendation project;Rocha, Luis Mateus;1999;10.1145/313238.313416;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;ACM Digital Library;Referential communication in AUTONOTE, a personal information retrieval system;Linn, William E. and Reitman, Walter;1971;10.1145/800184.810472;NOT_FOUND;"This paper is concerned with the mechanisms of human intelligence and natural language communication, and with the design of an interactive computer program that incorporates and utilizes analogous mechanisms to improve man-machine communication. AUTONOTE2 is an improved personal information retrieval system. It includes, in addition to AUTONOTE (a presently running system), (1) mechanisms allowing the user to employ certain kinds of noun phrases to describe the items he wishes to store and retrieve; and (2) mechanisms enabling the system to maintain a map of what the user is referring to. Though extremely limited by comparison with the analogous human capabilities, these mechanisms add significantly to the descriptive power available to the user, and to the ease and efficiency of communication with the system. Furthermore, because AUTONOTE is a file oriented system, these additions add little to the present low cost of using the system. AUTONOTE2 also suggests new directions for software development for man-machine interaction, and it provides a practical testing ground for ideas about intelligence derived from observation of natural intelligence."
NOT_RELEVANT;ACM Digital Library;Query using inferential processing implemented with inverted hashed files;McKinney, M. H.;1977;10.1145/800179.810186;NOT_FOUND;A methodology is presented for a prototype query processor that can infer the semantics of a query and thus allow a very natural-language-like request from a user. The inference is accomplished by fully inverting all attribute values, attribute names, and system keywords. All atoms of a query are treated in the same manner until the semantics are either positively inferred by the system or explicitly declared by the user. The implementation approach uses a common hash area containing: the attribute names, the inverted attribute values, the key words allowed by the query processor, and any synonyms that have been declared by the user. The methodology includes provision for performing updates, adds, and deletions as well as query. Advantages of the approach are the simplicity of the query language for the user and the elegance of the semantic interpretation by the system.
NOT_RELEVANT;ACM Digital Library;Artificial intelligence techniques in the interface to a Digital Video Library;Hauptmann, Alexander G. and Witbrock, Michael J. and Christel, Michael G.;1997;10.1145/1120212.1120214;NOT_FOUND;For the huge amounts of audio and video material that could usefully be included in digital libraries, the cost of producing human-generated annotations and meta-data is prohibitive. In the Informedia Digital Video Library, the production of meta-data supporting the library interface is automated using techniques from Artificial Intelligence (AI). By applying speech recognition, natural language processing and image analysis, the interface helps users locate the information they want and navigate or browse the digital video library more effectively. Specific AI-based interface components include automatic titles, filmstrips, video skims, word location marking and representative frames for shots.
NOT_RELEVANT;ACM Digital Library;Automatic parsing for content analysis;Damerau, Frederick J.;1970;10.1145/362384.362495;NOT_FOUND;Although automatic syntactic and semantic analysis is not yet possible for all of an unrestricted natural language text, some applications, of which content analysis is one, do not have such a stringent coverage requirement. Preliminary studies show that the Harvard Syntactic Analyzer can produce correct and unambiguous identification of the subject and object of certain verbs for approximately half of the relevant occurences. This provides a degree of coverage for content analysis variables which compares favorably to manual methods, in which only a sample of the total available text is normally processed.
NOT_RELEVANT;ACM Digital Library;An English language question answering system for a large relational database;Waltz, David L.;1978;10.1145/359545.359550;NOT_FOUND;"By typing requests in English, casual users will be able to obtain explicit answers from a large relational database of aircraft flight and maintenance data using a system called PLANES. The design and implementation of this system is described and illustrated with detailed examples of the operation of system components and examples of overall system operation. The language processing portion of the system uses a number of augmented transition networks, each of which matches phrases with a specific meaning, along with context registers (history keepers) and concept case frames; these are used for judging meaningfulness of questions, generating dialogue for clarifying partially understood questions, and resolving ellipsis and pronoun reference problems. Other system components construct a formal query for the relational database, and optimize the order of searching relations. Methods are discussed for handling vague or complex questions and for providing browsing ability. Also included are discussions of important issues in programming natural language systems for limited domains, and the relationship of this system to others."
NOT_RELEVANT;ACM Digital Library;The converse natural language data management system: current status and plans;Kellogg, Charles and Burger, John and Diller, Timothy and Fogt, Kenneth;1971;10.1145/511285.511290;NOT_FOUND;This paper presents an overview of research in progress in which the principal aim is the achievement of more natural and expressive modes of on-line communication with complexly structured data bases. A natural-language compiler has been constructed that accepts sentences in a user-extendable English subset, produces surface and deep-structure syntactic analyses, and uses a network of concepts to construct semantic interpretations formalized as computable procedures. The procedures are evaluated by a data management system that updates, modifies, and searches data bases that can be formalized as finite models of states of affairs. The system has been designed and programmed to handle large vocabularies and large collections of facts efficiently. Plans for extending the research vehicle to interface with a deductive inference component and a voice input-output effort are briefly described.
NOT_RELEVANT;ACM Digital Library;Computer-aided design;Ross, Douglas T.;1961;10.1145/366532.366554;NOT_FOUND;This project is engaged in (a) a program of research into the application of the concepts and techniques of modern data processing to the design of mechanical parts, and (b) the further development of automatic programming systems for numerically controlled machine tools. The project is a cooperative venture between the Computer Applications Group of the Electronic Systems Laboratory and the Design and Graphics Division of the Mechanical Engineering Department, and is sponsored by the Manufacturing Methods Division of the USAF Air Material Command through Contract AF-33(600)-40604.
MAYBE_RELEVANT;Web of Science;KnowledgeNavigator: leveraging large language models for enhanced reasoning over knowledge graph;"Guo, TZ; Yang, QW; Wang, C; Liu, YY; Li, P; Tang, JW; Li, DP; Wen, YY";2024;10.1007/s40747-024-01527-8;NOT_FOUND;Large language models have achieved outstanding performance on various downstream tasks with their advanced understanding of natural language and zero-shot capability. However, they struggle with knowledge constraints, particularly in tasks requiring complex reasoning or extended logical sequences. These limitations can affect their performance in question answering by leading to inaccuracies and hallucinations. This paper proposes a novel framework called KnowledgeNavigator that leverages large language models on knowledge graphs to achieve accurate and interpretable multi-hop reasoning. Especially with an analysis-retrieval-reasoning process, KnowledgeNavigator searches the optimal path iteratively to retrieve external knowledge and guide the reasoning to reliable answers. KnowledgeNavigator treats knowledge graphs and large language models as flexible components that can be switched between different tasks without additional costs. Experiments on three benchmarks demonstrate that KnowledgeNavigator significantly improves the performance of large language models in question answering and outperforms all large language models-based baselines.
NOT_RELEVANT;Web of Science;Advances in information retrieval collection on the European conference on information retrieval 2023;"Kamps, J; Goeuriot, L; Crestani, F";2024;10.1007/s10791-024-09442-9;NOT_FOUND;This paper introduces the Collection on ECIR 2023. The 45th European Conference on Information Retrieval (ECIR 2023) was held in Dublin, Ireland, during April 2-6, 2023. The conference was the largest ECIR ever, and brought together hundreds of researchers from Europe and abroad. A selection of papers shortlisted for the best paper awards was asked to submit expanded versions appearing in this Discover Computing (formerly the Information Retrieval Journal) Collection on ECIR 2023. First, an analytic paper on incorporating first stage retrieval status values as input in neural cross-encoder re-rankers. Second, new models and new data for a new task of temporal natural language inference. Third, a weak supervision approach to video retrieval overcoming the need for large-scale human labeled training data. Together, these papers showcase the breadth and diversity of current research on information retrieval.
NOT_RELEVANT;Web of Science;Recent automatic text summarization techniques: a survey;"Gambhir, M; Gupta, V";2017;10.1007/s10462-016-9475-9;NOT_FOUND;As information is available in abundance for every topic on internet, condensing the important information in the form of summary would benefit a number of users. Hence, there is growing interest among the research community for developing new approaches to automatically summarize the text. Automatic text summarization system generates a summary, i.e. short length text that includes all the important information of the document. Since the advent of text summarization in 1950s, researchers have been trying to improve techniques for generating summaries so that machine generated summary matches with the human made summary. Summary can be generated through extractive as well as abstractive methods. Abstractive methods are highly complex as they need extensive natural language processing. Therefore, research community is focusing more on extractive summaries, trying to achieve more coherent and meaningful summaries. During a decade, several extractive approaches have been developed for automatic summary generation that implements a number of machine learning and optimization techniques. This paper presents a comprehensive survey of recent text summarization extractive approaches developed in the last decade. Their needs are identified and their advantages and disadvantages are listed in a comparative manner. A few abstractive and multilingual text summarization approaches are also covered. Summary evaluation is another challenging issue in this research field. Therefore, intrinsic as well as extrinsic both the methods of summary evaluation are described in detail along with text summarization evaluation conferences and workshops. Furthermore, evaluation results of extractive summarization approaches are presented on some shared DUC datasets. Finally this paper concludes with the discussion of useful future directions that can help researchers to identify areas where further research is needed.
NOT_RELEVANT;Web of Science;Multi-head attention graph convolutional network model: End-to-end entity and relation joint extraction based on multi-head attention graph convolutional network;"Tao, ZH; Ouyang, CP; Liu, YB; Chung, TLE; Cao, YX";2023;10.1049/cit2.12086;NOT_FOUND;At present, the entity and relation joint extraction task has attracted more and more scholars' attention in the field of natural language processing (NLP). However, most of their methods rely on NLP tools to construct dependency trees to obtain sentence structure information. The adjacency matrix constructed by the dependency tree can convey syntactic information. Dependency trees obtained through NLP tools are too dependent on the tools and may not be very accurate in contextual semantic description. At the same time, a large amount of irrelevant information will cause redundancy. This paper presents a novel end-to-end entity and relation joint extraction based on the multi-head attention graph convolutional network model (MAGCN), which does not rely on external tools. MAGCN generates an adjacency matrix through a multi-head attention mechanism to form an attention graph convolutional network model, uses head selection to identify multiple relations, and effectively improve the prediction result of overlapping relations. The authors extensively experiment and prove the method's effectiveness on three public datasets: NYT, WebNLG, and CoNLL04. The results show that the authors' method outperforms the state-of-the-art research results for the task of entities and relation extraction.
NOT_RELEVANT;Web of Science;A multistage retrieval system for health-related misinformation detection;"Fernández-Pichel, M; Losada, DE; Pichel, JC";2022;10.1016/j.engappai.2022.105211;NOT_FOUND;Web search is widely used to find online medical advice. As such, health-related information access requires retrieval algorithms capable of promoting reliable documents and filtering out unreliable ones. To this end, different types of components, such as query-document matching features, passage relevance estimation and AI-based reliability estimators, need to be combined. In this paper, we propose an entire pipeline for misinformation detection, based on the fusion of multiple content-based features. We present experiments which study the influence of each pipeline stage for the target task. Our technological solution incorporates signals from technologies derived from diverse research fields, including search, deep learning for natural language processing, as well as advanced supervised and unsupervised learning. To combine evidence, different score fusion strategies are compared, including unsupervised rank fusion techniques and learning-to-rank methods. The reference framework for empirically validating our solution is the TREC Health Misinformation Track, which provides several challenging subtasks that foster research on the identification of reliable and correct information for health-related decision making tasks. More specifically, we address a total recall task, the goal of which is to identify all the documents conveying incorrect information for a specific set of topics, and an ad-hoc retrieval task, aiming to rank credible and correct information over incorrect information. All variants are evaluated with an assorted set of effectiveness metrics, which includes standard search measures, such as R-Precision, Average Precision or Normalised Discounted Cumulative Gain, and innovative metrics based on the compatibility between the ranked output and two reference rankings composed of helpful and harmful documents, respectively. Our experiments demonstrate the effectiveness of the proposed pipeline stages and indicate that sophisticated supervised fusion methods do not fare better than simpler fusion alternatives. Additionally, for reliability estimation, unsupervised textual similarity performs better than textual classification based on supervised learning. The results also show that the presented approach is highly competitive when compared with state-of-the-art solutions for the same problem.
NOT_RELEVANT;Web of Science;AI-Assisted Deep NLP-Based Approach for Prediction of Fake News From Social Media Users;"Devarajan, GG; Nagarajan, SM; Amanullah, SI; Mary, SASA; Bashir, AK";2024;10.1109/TCSS.2023.3259480;NOT_FOUND;"Social networking websites are now considered to be the best platforms for the dissemination of news articles. However, information sharing in social media platforms leads to explosion of fake news. Traditional detection methods were focusing on content analysis, while the current researchers examining social features of the news. In this work, we proposed a novel artificial intelligence (AI)-assisted fake news detection with deep natural language processing (NLP) model. The proposed work is characterized in four layers: publisher layer, social media networking layer, enabled edge layer, and cloud layer. In this work, four steps were carried out: 1) data acquisition; 2) information retrieval (IR); 3) NLP-based data processing and feature extraction; and 4) deep learning-based classification model that classifies news articles as fake or real using credibility score of publishers, users, messages, headlines, and so on. Three datasets, such as Buzzface, FakeNewsNet, and Twitter, were used for evaluation of the proposed model, and simulation results were computed. This proposed model obtained an average accuracy of 99.72% and an $F1$ score of 98.33%, which outperforms other existing methods."
MAYBE_RELEVANT;Web of Science;Dynamic prompt-based virtual assistant framework for BIM information search;"Zheng, JW; Fischer, M";2023;10.1016/j.autcon.2023.105067;NOT_FOUND;Efficient information search from building information models (BIMs) requires deep BIM knowledge or extensive engineering efforts for building natural language (NL)-based interfaces. To address this challenge, this paper introduces a dynamic prompt-based virtual assistant framework dubbed BIMS-GPT that integrates generative pre-trained transformer (GPT) technologies, supporting NL-based BIM search. To understand users' NL queries, extract relevant information from BIM databases, and deliver NL responses along with 3D visualizations, a dynamic prompt-based process was developed. In a case study, BIMS-GPT's functionality is demonstrated through a virtual assistant prototype for a hospital building. When evaluated with a BIM query dataset, the approach achieves accuracy rates of 99.5% for classifying NL queries with incorporating 2% of the data in prompts. This paper contributes to the advancement of effective and versatile virtual assistants for BIMs in the construction industry as it significantly enhances BIM accessibility while reducing the engineering and training data prerequisites for processing NL queries.
NOT_RELEVANT;Web of Science;Question-answering framework for building codes using fine-tuned and distilled pre-trained transformer models;"Xue, XR; Zhang, JS; Chen, YF";2024;10.1016/j.autcon.2024.105730;NOT_FOUND;Building code compliance checking is considered a bottleneck in construction projects, which calls for a novel approach to building code query and information retrieval. To address this research gap, the paper presents a question and answering framework comprising: (1) a 'retriever' for efficient context retrieval from building codes in response to an inquiry, and (2) a 'reader' for precise context interpretation and answer generation. The 'retriever', based on the BM25 algorithm, achieved a top-1 precision, recall, and F1-score of 0.95, 0.95, and 0.95, and a top-5 precision, recall, and F1-score of 0.97, 1.00, and 0.99, respectively. The 'reader', utilizing the transformer-based xlm-roberta-base-squad2-distilled model, achieved a top-4 accuracy of 0.95 and a top-1 F1-score of 0.84. A fine-tuning and model distillation process was used and shown to provide high performance on limited amount of training data, overcoming a common barrier in the development of domain-specific (e.g., construction) deep learning models.
NOT_RELEVANT;Web of Science;Why are these similar? Investigating item similarity types in a large digital library;"Gonzalez-Agirre, A; Rigau, G; Agirre, E; Aletras, N; Stevenson, M";2016;10.1002/asi.23482;NOT_FOUND;We introduce a new problem, identifying the type of relation that holds between a pair of similar items in a digital library. Being able to provide a reason why items are similar has applications in recommendation, personalization, and search. We investigate the problem within the context of Europeana, a large digital library containing items related to cultural heritage. A range of types of similarity in this collection were identified. A set of 1,500 pairs of items from the collection were annotated using crowdsourcing. A high intertagger agreement (average 71.5 Pearson correlation) was obtained and demonstrates that the task is well defined. We also present several approaches to automatically identifying the type of similarity. The best system applies linear regression and achieves a mean Pearson correlation of 71.3, close to human performance. The problem formulation and data set described here were used in a public evaluation exercise, the *SEM shared task on Semantic Textual Similarity. The task attracted the participation of 6 teams, who submitted 14 system runs. All annotations, evaluation scripts, and system runs are freely available.(1)
NOT_RELEVANT;Web of Science;A simple and fast method for Named Entity context extraction from patents;"Puccetti, G; Chiarello, F; Fantoni, G";2021;10.1016/j.eswa.2021.115570;NOT_FOUND;The process of extracting relevant technical information from patents or technical literature is as valuable as it is challenging. It deals with highly relevant information extraction from a corpus of documents with particular structure, and a mix of technical and legal jargon. Patents are the wider free source of technical information where homogeneous entities can be found. From a technical perspective the approaches refer to Named Entity Recognition (NER) and make use of Machine Learning techniques for Natural Language Processing (NLP). However, due to the large amount of data, to the complexity of the lexicon, the peculiarity of the structure and the scarcity of the examples to be used to feed the machine learning system, new approaches should be studied. NER methods are increasing their performances in many contexts, but a gap still exists when dealing with technical documentation. The aim of this work is to create an automatic training sets for NER systems by exploiting the nature and structure of patents, an open and massive source of technical documentation. In particular, we focus on collecting the context where users of the invention appear within patents. We then measure to which extent we achieve our goal and discuss how much our method is generalizable to other entities and documents.
NOT_RELEVANT;Web of Science;An Efficient Corpus-Based Stemmer;"Singh, J; Gupta, V";2017;10.1007/s12559-017-9479-z;NOT_FOUND;Word stemming is a linguistic process in which the various inflected word forms are matched to their base form. It is among the basic text pre-processing approaches used in Natural Language Processing and Information Retrieval. Stemming is employed at the text pre-processing stage to solve the issue of vocabulary mismatch or to reduce the size of the word vocabulary, and consequently also the dimensionality of training data for statistical models. In this article, we present a fully unsupervised corpus-based text stemming method which clusters morphologically related words based on lexical knowledge. The proposed method performs cognitive-inspired computing to discover morphologically related words from the corpus without any human intervention or language-specific knowledge. The performance of the proposed method is evaluated in inflection removal (approximating lemmas) and Information Retrieval tasks. The retrieval experiments in four different languages using standard Text Retrieval Conference, Cross-Language Evaluation Forum, and Forum for Information Retrieval Evaluation collections show that the proposed stemming method performs significantly better than no stemming. In the case of highly inflectional languages, Marathi and Hungarian, the improvement in Mean Average Precision is nearly 50% as compared to unstemmed words. Moreover, the proposed unsupervised stemming method outperforms state-of-the-art strong language-independent and rule-based stemming methods in all the languages. Besides Information Retrieval, the proposed stemming method also performs significantly better in inflection removal experiments. The proposed unsupervised language-independent stemming method can be used as a multipurpose tool for various tasks such as the approximation of lemmas, improving retrieval performance or other Natural Language Processing applications.
NOT_RELEVANT;Web of Science;Tashaphyne0.4: a new arabic light stemmer based on rhyzome modeling approach;"Al-Khatib, RM; Zerrouki, T; Abu Shquier, MM; Balla, A";2023;10.1007/s10791-023-09429-y;NOT_FOUND;Stemming algorithms are crucial tools for enhancing the information retrieval process in natural language processing. This paper presents a novel Arabic light stemming algorithm called Tashaphyne0.4, the idea behind this algorithm is to extract the most precise 'roots', and 'stems' from words of an Arabic text. Thus, the proposed algorithm acts as rooter, stemmer, and segmentation tools at the same time. Our approach involves tri-fold phases (i.e., Preparation, Stems-Extractor, and Root-Extractor). Tashaphyne0.4 has shown better results than six other stemmers (i.e., Khoja, ISRI, Motaz/Light10, Tashaphyne0.3, FARASA, and Assem stemmers). The comparison is performed using four different Arabic comprehensive-benchmarks datasets. In conclusion, our proposed stemmer achieved remarkable results and outperformed other competitive stemmers in extracting 'Roots' and 'Stems'.
NOT_RELEVANT;Web of Science;Complexities, variations, and errors of numbering within clinical notes: the potential impact on information extraction and cohort-identification;"Hanauer, DA; Mei, QZ; Vydiswaran, VGV; Singh, K; Landis-Lewis, Z; Weng, CH";2019;10.1186/s12911-019-0784-1;NOT_FOUND;BackgroundNumbers and numerical concepts appear frequently in free text clinical notes from electronic health records. Knowledge of the frequent lexical variations of these numerical concepts, and their accurate identification, is important for many information extraction tasks. This paper describes an analysis of the variation in how numbers and numerical concepts are represented in clinical notes.MethodsWe used an inverted index of approximately 100 million notes to obtain the frequency of various permutations of numbers and numerical concepts, including the use of Roman numerals, numbers spelled as English words, and invalid dates, among others. Overall, twelve types of lexical variants were analyzed.ResultsWe found substantial variation in how these concepts were represented in the notes, including multiple data quality issues. We also demonstrate that not considering these variations could have substantial real-world implications for cohort identification tasks, with one case missing >80% of potential patients.ConclusionsNumbering within clinical notes can be variable, and not taking these variations into account could result in missing or inaccurate information for natural language processing and information retrieval tasks.
NOT_RELEVANT;Web of Science;Descriptive document clustering via discriminant learning in a co-embedded space of multilevel similarities;"Mu, TT; Goulermas, JY; Korkontzelos, I; Ananiadou, S";2016;10.1002/asi.23374;NOT_FOUND;Descriptive document clustering aims at discovering clusters of semantically interrelated documents together with meaningful labels to summarize the content of each document cluster. In this work, we propose a novel descriptive clustering framework, referred to as CEDL. It relies on the formulation and generation of 2 types of heterogeneous objects, which correspond to documents and candidate phrases, using multilevel similarity information. CEDL is composed of 5 main processing stages. First, it simultaneously maps the documents and candidate phrases into a common co-embedded space that preserves higher-order, neighbor-based proximities between the combined sets of documents and phrases. Then, it discovers an approximate cluster structure of documents in the common space. The third stage extracts promising topic phrases by constructing a discriminant model where documents along with their cluster memberships are used as training instances. Subsequently, the final cluster labels are selected from the topic phrases using a ranking scheme using multiple scores based on the extracted co-embedding information and the discriminant output. The final stage polishes the initial clusters to reduce noise and accommodate the multitopic nature of documents. The effectiveness and competitiveness of CEDL is demonstrated qualitatively and quantitatively with experiments using document databases from different application fields.
NOT_RELEVANT;Web of Science;Software for a multimedia encyclopedia;Mülner, H;2002;NOT_FOUND;NOT_FOUND;This paper discusses some experiences with the development of features based on natural language processing for a multimedia encyclopedia.
NOT_RELEVANT;Web of Science;Graph-based term weighting for information retrieval;"Blanco, R; Lioma, C";2012;10.1007/s10791-011-9172-x;NOT_FOUND;A standard approach to Information Retrieval (IR) is to model text as a bag of words. Alternatively, text can be modelled as a graph, whose vertices represent words, and whose edges represent relations between the words, defined on the basis of any meaningful statistical or linguistic relation. Given such a text graph, graph theoretic computations can be applied to measure various properties of the graph, and hence of the text. This work explores the usefulness of such graph-based text representations for IR. Specifically, we propose a principled graph-theoretic approach of (1) computing term weights and (2) integrating discourse aspects into retrieval. Given a text graph, whose vertices denote terms linked by co-occurrence and grammatical modification, we use graph ranking computations (e.g. PageRank Page et al. in The pagerank citation ranking: Bringing order to the Web. Technical report, Stanford Digital Library Technologies Project, 1998) to derive weights for each vertex, i.e. term weights, which we use to rank documents against queries. We reason that our graph-based term weights do not necessarily need to be normalised by document length (unlike existing term weights) because they are already scaled by their graph-ranking computation. This is a departure from existing IR ranking functions, and we experimentally show that it performs comparably to a tuned ranking baseline, such as BM25 (Robertson et al. in NIST Special Publication 500-236: TREC-4, 1995). In addition, we integrate into ranking graph properties, such as the average path length, or clustering coefficient, which represent different aspects of the topology of the graph, and by extension of the document represented as a graph. Integrating such properties into ranking allows us to consider issues such as discourse coherence, flow and density during retrieval. We experimentally show that this type of ranking performs comparably to BM25, and can even outperform it, across different TREC (Voorhees and Harman in TREC: Experiment and evaluation in information retrieval, MIT Press, 2005) datasets and evaluation measures.
NOT_RELEVANT;Web of Science;Structuring Tweets for improving Twitter search;"Luo, ZC; Yu, Y; Osborne, M; Wang, T";2015;10.1002/asi.23332;NOT_FOUND;Spam and wildly varying documents make searching in Twitter challenging. Most Twitter search systems generally treat a Tweet as a plain text when modeling relevance. However, a series of conventions allows users to Tweet in structural ways using a combination of different blocks of texts. These blocks include plain texts, hashtags, links, mentions, etc. Each block encodes a variety of communicative intent and the sequence of these blocks captures changing discourse. Previous work shows that exploiting the structural information can improve the structured documents (e.g., web pages) retrieval. In this study we utilize the structure of Tweets, induced by these blocks, for Twitter retrieval and Twitter opinion retrieval. For Twitter retrieval, a set of features, derived from the blocks of text and their combinations, is used into a learning-to-rank scenario. We show that structuring Tweets can achieve state-of-the-art performance. Our approach does not rely on social media features, but when we do add this additional information, performance improves significantly. For Twitter opinion retrieval, we explore the question of whether structural information derived from the body of Tweets and opinionatedness ratings of Tweets can improve performance. Experimental results show that retrieval using a novel unsupervised opinionatedness feature based on structuring Tweets achieves comparable performance with a supervised method using manually tagged Tweets. Topic-related specific structured Tweet sets are shown to help with query-dependent opinion retrieval.
NOT_RELEVANT;Web of Science;Applications of natural language processing in software traceability: A systematic mapping study?;"Pauzi, Z; Capiluppi, A";2023;10.1016/j.jss.2023.111616;NOT_FOUND;A key part of software evolution and maintenance is the continuous integration from collaborative efforts, often resulting in complex traceability challenges between software artifacts: features and modules remain scattered in the source code, and traceability links become harder to recover. In this paper, we perform a systematic mapping study dealing with recent research recovering these links through information retrieval, with a particular focus on natural language processing (NLP). Our search strategy gathered a total of 96 papers in focus of our study, covering a period from 2013 to 2021. We conducted trend analysis on NLP techniques and tools involved, and traceability efforts (applying NLP) across the software development life cycle (SDLC). Based on our study, we have identified the following key issues, barriers, and setbacks: syntax convention, configuration, translation, explainability, properties representation, tacit knowledge dependency, scalability, and data availability. Based on these, we consolidated the following open challenges: representation similarity across artifacts, the effectiveness of NLP for traceability, and achieving scalable, adaptive, and explainable models. To address these challenges, we recommend a holistic framework for NLP solutions to achieve effective traceability and efforts in achieving interoperability and explainability in NLP models for traceability. (c) 2023 The Author(s). Published by Elsevier Inc. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).
NOT_RELEVANT;Web of Science;Association of metastatic pattern in breast cancer with tumor and patient-specific factors: a nationwide autopsy study using artificial intelligence;"Kazemzadeh, F; Snoek, JAA; Voorham, QJ; van Oijen, MGH; Hugen, N; Nagtegaal, ID";2024;10.1007/s12282-023-01534-6;NOT_FOUND;BackgroundMetastatic spread is characterized by considerable heterogeneity in most cancers. With increasing treatment options for patients with metastatic disease, there is a need for insight into metastatic patterns of spread in breast cancer patients using large-scale studies.MethodsRecords of 2622 metastatic breast cancer patients who underwent autopsy (1974-2010) were retrieved from the nationwide Dutch pathology databank (PALGA). Natural language processing (NLP) and manual information extraction (IE) were applied to identify the tumors, patient characteristics, and locations of metastases.ResultsThe accuracy (0.90) and recall (0.94) of the NLP model outperformed manual IE (on 132 randomly selected patients). Adenocarcinoma no special type more frequently metastasizes to the lung (55.7%) and liver (51.8%), whereas, invasive lobular carcinoma mostly spread to the bone (54.4%) and liver (43.8%), respectively. Patients with tumor grade III had a higher chance of developing bone metastases (61.6%). In a subgroup of patients, we found that ER+/HER2+ patients were more likely to metastasize to the liver and bone, compared to ER-/HER2+ patients.ConclusionThis is the first large-scale study that demonstrates that artificial intelligence methods are efficient for IE from Dutch databanks. Different histological subtypes show different frequencies and combinations of metastatic sites which may reflect the underlying biology of metastatic breast cancer.
NOT_RELEVANT;Web of Science;Spatiotemporal Information Extraction from a Historic Expedition Gazetteer;"Bekele, MK; de By, RA; Singh, G";2016;10.3390/ijgi5120221;NOT_FOUND;Historic expeditions are events that are flavored by exploratory, scientific, military or geographic characteristics. Such events are often documented in literature, journey notes or personal diaries. A typical historic expedition involves multiple site visits and their descriptions contain spatiotemporal and attributive contexts. Expeditions involve movements in space that can be represented by triplet features (location, time and description). However, such features are implicit and innate parts of textual documents. Extracting the geospatial information from these documents requires understanding the contextualized entities in the text. To this end, we developed a semi-automated framework that has multiple Information Retrieval and Natural Language Processing components to extract the spatiotemporal information from a two-volume historic expedition gazetteer. Our framework has three basic components, namely, the Text Preprocessor, the Gazetteer Processing Machine and the JAPE (Java Annotation Pattern Engine) Transducer. We used the Brazilian Ornithological Gazetteer as an experimental dataset and extracted the spatial and temporal entities from entries that refer to three expeditioners' site visits (which took place between 1910 and 1926) and mapped the trajectory of each expedition using the extracted information. Finally, one of the mapped trajectories was manually compared with a historical reference map of that expedition to assess the reliability of our framework.
NOT_RELEVANT;Web of Science;Lyrics segmentation via bimodal text-audio representation;"Fell, M; Nechaev, Y; Meseguer-Brocal, G; Cabrio, E; Gandon, F; Peeters, G";2022;10.1017/S1351324921000024;NOT_FOUND;Song lyrics contain repeated patterns that have been proven to facilitate automated lyrics segmentation, with the final goal of detecting the building blocks (e.g., chorus, verse) of a song text. Our contribution in this article is twofold. First, we introduce a convolutional neural network (CNN)-based model that learns to segment the lyrics based on their repetitive text structure. We experiment with novel features to reveal different kinds of repetitions in the lyrics, for instance based on phonetical and syntactical properties. Second, using a novel corpus where the song text is synchronized to the audio of the song, we show that the text and audio modalities capture complementary structure of the lyrics and that combining both is beneficial for lyrics segmentation performance. For the purely text-based lyrics segmentation on a dataset of 103k lyrics, we achieve an F-score of 67.4%, improving on the state of the art (59.2% F-score). On the synchronized text-audio dataset of 4.8k songs, we show that the additional audio features improve segmentation performance to 75.3% F-score, significantly outperforming the purely text-based approaches.
NOT_RELEVANT;Web of Science;ARmed question answering system;"Fehri, H; Dardour, S; Haddar, K";2022;10.1002/cpe.7054;NOT_FOUND;Question answering, retrieving an exact answer to a question posed in natural language, is an issue which has widely been studied in the open domain over the last decades. This, however, remains a real challenge in the medical domain as most existing systems only support a limited amount of question and answer types. The problem with proposed methods for Arabic language in the medical domain is that there is often a conflict between the extracted answer and user's requirements. This conflict is related to ambiguity. Nevertheless, the method we propose has successfully tackled this problem. Thus, in this article, we introduce ARmed, a system for automatically answering medical questions for Arabic language. ARmed consists of corpora study, pre-processing, question analysis, documents/passages retrieval, and answer extraction. Compared with the previous studies, ARmed has the potential to handle a large number of questions and answer types. The experimental results show that ARmed achieves interesting results.
NOT_RELEVANT;Web of Science;A systematic review of text stemming techniques;"Singh, J; Gupta, V";2017;10.1007/s10462-016-9498-2;NOT_FOUND;"Stemming is a program that matches the morphological variants of the word to its root word. Stemming is extensively used as a pre-processing tool in the field of natural language processing, information retrieval, and language modeling. Though a lot of advancements have been made in the field, yet organized arrangement of the previous work and efforts are lacking in this field. In this paper, we present a review of the text stemming theory, algorithms, and applications. It first describes the existing literature relevant to text stemming by classifying it according to certain key parameters; then it describes the deep analysis of some well-known stemming algorithms on standard data sets. In the end, the current state-of-the-art and certain open issues related to unsupervised stemming are presented. The main aim of this paper is to provide an extensive and useful understanding of the important aspects of text stemming. The open issues and analysis of the current stemming techniques will help the researchers to think of new lines to conduct research in future."
NOT_RELEVANT;Web of Science;Automatically Categorizing Software Technologies;"Nassif, M; Treude, C; Robillard, MP";2020;10.1109/TSE.2018.2836450;NOT_FOUND;Informal language and the absence of a standard taxonomy for software technologies make it difficult to reliably analyze technology trends on discussion forums and other on-line venues. We propose an automated approach called $\mathrm{Witt}$ Witt for the categorization of software technologies (an expanded version of the hypernym discovery problem). $\mathrm{Witt}$ Witt takes as input a phrase describing a software technology or concept and returns a general category that describes it (e.g., integrated development environment), along with attributes that further qualify it (commercial, php, etc.). By extension, the approach enables the dynamic creation of lists of all technologies of a given type (e.g., web application frameworks). Our approach relies on Stack Overflow and Wikipedia, and involves numerous original domain adaptations and a new solution to the problem of normalizing automatically-detected hypernyms. We compared $\mathrm{Witt}$ Witt with six independent taxonomy tools and found that, when applied to software terms, $\mathrm{Witt}$ Witt demonstrated better coverage than all evaluated alternative solutions, without a corresponding degradation in false positive rate.
NOT_RELEVANT;Web of Science;Text mining for neuroanatomy using White Text with an updated corpus and a new web application;"French, L; Liu, P; Marais, O; Koreman, T; Tseng, L; Lai, A; Pavlidis, P";2015;10.3389/fninf.2015.00013;NOT_FOUND;We describe the WhiteText project, and its progress towards automatically extracting statements of neuroanatomical connectivity from text. We review progress to date on the three main steps of the project: recognition of brain region mentions, standardization of brain region mentions to neuroanatomical nomenclature, and connectivity statement extraction. We further describe a new version of our manually curated corpus that adds 2,111 connectivity statements from 1,828 additional abstracts. Cross validation classification within the new corpus replicates results on our original corpus, recalling 67% of connectivity statements at 51% precision. The resulting merged corpus provides 5,208 connectivity statements that can be used to seed species-specific connectivity matrices and to better train automated techniques. Finally, we present a new web application that allows fast interactive browsing of the over 70,000 sentences indexed by the system, as a tool for accessing the data and assisting in further curation. Software and data are freely available at http://www.chibi.ubc.caNVhiteText/.
NOT_RELEVANT;Web of Science;Identification, expansion, and disambiguation of acronyms in biomedical texts;"Bracewell, DB; Russell, S; Wu, AS";2005;NOT_FOUND;NOT_FOUND;With the ever growing amount of biomedical literature there is an increasing desire to use sophisticated language processing algorithms to mine these texts. In order to use these algorithms we must first deal with acronyms, abbreviations, and misspellings. In this paper we look at identifying, expanding, and disambiguating acronyms in biomedical texts. We break the task up into three modular steps: Identification, Expansion, and Disambiguation. For Identification we use a hybrid approach that is composed of a naive Bayesian classifier and a couple of handcrafted rules. We are able to achieve results of 99.96% accuracy with a small training set. We break the expansion up into two categories, local and global expansion. For local expansion we use windowing and longest common subsequence to generate the possible expansion. Global expansion requires an acronym database. To disambiguate the different candidate expansions we use WordNet and semantic similarity, Overall we obtain a recall and precision of over 91%.
NOT_RELEVANT;Web of Science;A fast and compact elimination method of empty elements from a double-array structure;"Oono, M; Atlam, ES; Fuketa, M; Morita, K; Aoe, J";2003;10.1002/spe.545;NOT_FOUND;A double-array is a well-known data structure to implement the trie. However, the space efficiency of the double-array degrades with the number of key deletions because the double-array keeps empty elements produced by the key deletion. This paper presents a fast and compact elimination method of empty elements using properties of the trie nodes that have no siblings. The present elimination method is implemented by C language. From simulation results for large sets of keys, the present elimination method is about 30-330 times faster than the conventional elimination method and maintains high space efficiency. Copyright (C) 2003 John Wiley Sons, Ltd.
NOT_RELEVANT;Web of Science;Acronyms as an Integral Part of Multi-Word Term Recognition - A Token of Appreciation;Spasic, I;2018;10.1109/ACCESS.2018.2807122;NOT_FOUND;Term conflation is the process of linking together different variants of the same term. In automatic term recognition approaches, all term variants should be aggregated into a single normalized term representative, which is associated with a single domain-specific concept as a latent variable. In a previous study, we described FlexiTerm, an unsupervised method for recognition of multiword terms from a domain-specific corpus. It uses a range of methods to normalize three types of term variation orthographic, morphological, and syntactic variations. Acronyms, which represent a highly productive type of term variation, were not supported. In this paper, we describe how the functionality of FlexiTerm has been extended to recognize acronyms and incorporate them into the term conflation process. The main contribution of this paper is not acronym recognition per se, but rather its integration with other types of term variation into the term conflation process. We evaluated the effects of term conflation in the context of information retrieval as one of its most prominent applications. On average, relative recall increased by 32 points, whereas index compression factor increased by 7% points. Therefore, evidence suggests that integration of acronyms provides nontrivial improvement of term conflation.
MAYBE_RELEVANT;Web of Science;Dense-to-Question and Sparse-to-Answer: Hybrid Retriever System for Industrial Frequently Asked Questions;"Seo, J; Lee, T; Moon, H; Park, C; Eo, S; Aiyanyo, ID; Park, K; So, A; Ahn, S; Park, J";2022;10.3390/math10081335;NOT_FOUND;"The term Frequently asked questions (FAQ) refers to a query that is asked repeatedly and produces a manually constructed response. It is one of the most important factors influencing customer repurchase and brand loyalty; thus, most industry domains invest heavily in it. This has led to deep-learning-based retrieval models being studied. However, training a model and creating a database specializing in each industry domain comes at a high cost, especially when using a chatbot-based conversation system, as a large amount of resources must be continuously input for the FAQ system's maintenance. It is also difficult for small- and medium-sized companies and national institutions to build individualized training data and databases and obtain satisfactory results. As a result, based on the deep learning information retrieval module, we propose a method of returning responses to customer inquiries using only data that can be easily obtained from companies. We hybridize dense embedding and sparse embedding in this work to make it more robust in professional terms, and we propose new functions to adjust the weight ratio and scale the results returned by the two modules."
NOT_RELEVANT;Web of Science;Predicting spotify audio features from Last.fm tags;"Castillo, JR; Flores, MJ; Leray, P";2023;10.1007/s11042-023-17160-5;NOT_FOUND;Music information retrieval (MIR) is an interdisciplinary research field that focuses on the extraction, processing, and knowledge discovery of information contained in music. While previous studies have utilized Spotify audio features and Last.fm tags as input values for classification tasks, such as music genre recognition, their potential as target values has remained unexplored. In this article, we address this notable gap in the research landscape by proposing a novel approach to predict Spotify audio features based on a set of Last.fm tags. By predicting audio features, we aim to explore the relationship between subjective perception and concrete musical features, shedding light on patterns and hidden correlations between how music is perceived, consumed, and discovered. Additionally, the predicted audio features can be leveraged in recommendation systems to provide users with explainable recommendations, bridging the gap between algorithmic suggestions and user understanding. Our experiments involve training models such as GPT-2, XGBRegressor, and Bayesian Ridge regressor to predict Spotify audio features from Last.fm tags. Through our findings, we contribute to the advancement of MIR research by demonstrating the potential of Last.fm tags as target values and paving the way for future research on the connection between subjective and objective music characterization. Our approach holds promise for both listeners and researchers, offering new insights into the intricate relationship between perception and audio signal in music. Our study aims to explore the feasibility and efficacy of this unique approach, where we intentionally refrain from using traditional audio-based or metadata-driven methods.
NOT_RELEVANT;Web of Science;Single document keyword extraction for Internet news articles;"Bracewell, DB; Yan, J; Ren, F";2008;NOT_FOUND;NOT_FOUND;Keywords are a fundamental part of information retrieval (IR) and as such they have been studied extensively. They are used for everything from, searching to describing a document. A Keyword extraction algorithm can be defined as a combination of a keyword representation and a selection/weighting scheme. The most common selection/weighting schemes are based on collection statistics or using supervised machine learning algorithms. In these cases, keywords can, typically, only be extracted from documents that belong to a collection or using a large amount of annotated training data. The importance of extracting keywords without a document collection has been gradually increasing due to the Internet. In, this paper, a keyword, extraction algorithm designed with news in mind that requires neither a document collection or training data is presented. It, uses noun phrases as its keyword representation and takes in document statistics to derive its weighting scheme. Through experimentation it is shown. that the quality of the keywords extracted from, the proposed algorithm are better than. standard algorithms for both information retrieval and humans.
NOT_RELEVANT;Web of Science;Towards a better signal detection and knowledge management in pharmacovigilance: The VigiTermes project;"Bousquet, C; Amardheil, F; Daube, JM; Delamarre, D; Duclos, C; Lanne, SG; Jaulent, MC; Louët, ALL; Toussaint, Y";2011;10.1016/j.irbm.2011.01.037;NOT_FOUND;Objectives. - Prevention of adverse drug reactions (ADRs) has become an important public health issue. Pharmacovigilance is dedicated to the identification, analysis and prevention of risks related to ADRs. The objective of the VigiTerms project is to develop innovative methods for earlier detection of ADRs. Material and methods. - We used methods related to knowledge engineering, multi lingual engineering, natural language processing and text mining. Results. - The main result of the project is a software architecture for search and analysis of articles describing ADRs on the PubMed server. This is the first implementation of such an application for pharmacovigilance. Other results are related to statistical signal detection, modelling of drugs and ADRs with ontologies, information retrieval in pharmacovigilance databases and natural language processing applied to case reports in Japanese. Discussion. - Developments are underway to make the results of the project in operational form for use by regulatory authorities and pharmaceutical companies. Improved graphical user interface should facilitate decision support for professionals in charge of pharmacovigilance issues. Conclusion. - Our ambition is to continue the integration of components on the common platform to offer pharmacovigilance teams the most complete and effective tools for detecting, tracking and consolidation of pharmacovigilance signals. (C) 2011 Elsevier Masson SAS. All rights reserved.
NOT_RELEVANT;Web of Science;Crop Information Retrieval Framework Based on LDW-Ontology and SNM-BERT Techniques;"Ezhilarasi, K; Hussain, DM; Sowmiya, M; Krishnamoorthy, N";2023;10.5755/j01.itc.52.3.31945;NOT_FOUND;"Currently, on the Internet, the information about agriculture is augmenting extremely; thus, searching for precise, relevant data of various details is highly complicated. To deal with particular difficulties like lower relevancy rate, false detection of retrieval resources, poor similarity rate, unstructured data format, multivariate data, irrelevant spelling, and higher computation time, an intelligent Information Retrieval (IR) system is required. An IR Framework centered on Levenshtein Distance Weight-centric Ontology (LDW-Ontology) and Sutskever Nesterov Momentum-centred Bidirectional Encoder Representation from Transformer (SNM-BERT) methodologies is presented here to overcome the complications as mentioned earlier. Firstly, the data is pre-processed, transmuting the unstructured data into a structured format, thus mitigating the error probabilities. Then, the LDW-Crop Ontology construction is done regarding the structured data. In the methodology presented, significance, frequency, and the suggestion of word in mind are considered to build Crop ontology. In the MongoDB database, the data being constructed are amassed. Then, by utilizing SNM-BERT, the data is trained for IR regarding clustered input produced by Inter Quartile Pruning Range-centred Hierarchical Divisive Clustering (IQPR-HDC) model. The LDW is computed for the provided user query; subsequently, the similarity evaluation outcomes are obtained from the database. The experiential evaluation displays that when analogized with the prevailing methodologies, a better accuracy of 94 % for simple queries and 92% for complex queries is achieved. Along with retrieval rate with lower computation time is achieved by the proposed methodology."
NOT_RELEVANT;Web of Science;DR-LINK in TIPSTER III;"Liddy, ED; Diamond, T; McKenna, M";2000;10.1023/A:1009986331526;NOT_FOUND;A Natural Language Processing based Information Retrieval System that was one of the original systems developed in Phase I of TIPSTER, was the basis of research in TIPSTER III the goal of which was to add two extended capabilities to the core system. Following a description of the multiple levels of linguistic processing that were developed for the original DR-LINK System, details are provided on research into query specific data fusion and query-specific cross-document summarization. Experimental results show that there is potential for improving retrieval through query-specific fusion and that analysts found the Detailed Multiple Document Summary to be extremely useful for almost every query, while the Thumbnail sketch was useful in approximately 50% of the queries.
NOT_RELEVANT;Web of Science;A Short Introduction to Learning to Rank;Li, H;2011;10.1587/transinf.E94.D.1854;NOT_FOUND;Learning to rank refers to machine learning techniques for training the model in a ranking task. Learning to rank is useful for many applications in Information Retrieval, Natural Language Processing, and Data Mining. Intensive studies have been conducted on the problem and significant progress has been made [1], [2]. This short paper gives an introduction to learning to rank, and it specifically explains the fundamental problems, existing approaches, and future work of learning to rank. Several learning to rank methods using SVM techniques are described in details.
NOT_RELEVANT;Web of Science;A fast local citation recommendation algorithm scalable to multi-topics *;"Yin, MJ; Wang, BY; Ling, CR";2024;10.1016/j.eswa.2023.122031;NOT_FOUND;In the era of rapid paper publications in various venues, automatic citation recommendations would be highly useful to researchers when they write papers. Local citation recommendation aims to recommend possible papers to cite given local citation contexts. Previous work mainly computes the similarity score between citation contexts and cited papers on a one-to-one basis, which is quite time-consuming. We train a pair of neural network encoders that map citation contexts and all possible cited papers to the same vector space, respectively. After that, we index the positions of all cited papers in the vector space. This makes our process for searching recommended papers considerably faster. On the other hand, existing methods tend to recommend papers that are highly similar to each other, which makes recommendations lack diversity. Therefore, we extend our algorithm to perform multi-topic recommendations. We generate multi-topic training examples based on the index we mentioned earlier. Furthermore, we specially design a multi-group contrastive learning method to train our model so that it can distinguish different topics. Empirical experiments show that our model outperforms previous methods by a wide margin. Our model is also light weighted and has been deployed online so that researchers can use it to obtain recommended citations for their own paper in real-time.
NOT_RELEVANT;Web of Science;Information is essential for competitive and cost-effective public procurement;"Gorgun, MK; Kutlu, M; Tas, BKO";2022;10.1177/01655515221141042;NOT_FOUND;Public authorities promote transparent public procurement practices to increase competition and reduce public procurement costs. In this article, we focus on public procurement of the European Union (EU). We employ a multidisciplinary approach to analyse economic effects of information in public procurement. We quantify the information content of 2,390,630 EU public procurement notices published in 22 different languages using natural language processing techniques. Subsequently, we examine the impact of the information content on public procurement outcomes. We find that higher information levels have significant positive effects. Competition is considerably higher when notices contain more information. On average, contract prices would be 6%-8% lower if notices were to contain adequate information. EU governments could save up to euro 80 billion if all public procurement notices were to have detailed information. Based on our comprehensive analysis, we believe that authorities should regulate the information content of notices to promote competition and cost-effectiveness in public procurement.
NOT_RELEVANT;Web of Science;A novel unsupervised corpus-based stemming technique using lexicon and corpus statistics;"Singh, J; Gupta, V";2019;10.1016/j.knosys.2019.05.025;NOT_FOUND;Word Stemming is a widely used mechanism in the fields of Natural Language Processing, Information Retrieval, and Language Modeling. Language-independent stemmers discover classes of morphologically related words from the ambient corpus without using any language related rules. In this article, we proposed a fully unsupervised language-independent text stemming technique that clusters morphologically related words from the corpus of the language using both lexical and co-occurrence features such as lexical similarity, suffix knowledge, and co-occurrence similarity. The method applies to a wide range of inflectional languages as it identifies morphological variants formed through different linguistic processes such as affixation, compounding, conversion, etc. The proposed approach has been tested in Information Retrieval application for four languages (English, Marathi, Hungarian, and Bengali) using standard TREC, CLEF, and FIRE test collections. A significant improvement over word-based retrieval, five other corpus-based stemmers, and rule-based stemmers has been achieved in all the languages. Besides, information retrieval, the proposed approach has also been tested in text classification and inflection removal tasks. Our algorithm excelled over other baseline methods in all the test scenarios. Thus, we successfully achieved the objective of developing a multipurpose stemming algorithm that cannot only be used for information retrieval task but also for non-traditional tasks such as text classification, sentiment analysis, inflection removal, etc. (C) 2019 Elsevier B.V. All rights reserved.
NOT_RELEVANT;Web of Science;Using Topic Identification in Chinese Information Retrieval;"Yeh, CL; Chen, YC";2009;NOT_FOUND;NOT_FOUND;Information retrieval is to identify documents, from text collections, which are relevant with respect to some query. In current information retrieval systems, users can query with an unordered set of keywords, a question or a sentence. A list of document links matching the query can be retrieved and ordered by relevancy between the query and the documents. In this article, we are concerned with a hypothesis that the discourse-level element, topic, could be used to contribute the calculations of information retrieval. Due to the phenomenon of zero anaphora frequently occurring in Chinese texts, the topics may be omitted and are not expressed on the surface text. The key elements of the centering model of local discourse coherence are employed to extract structures of discourse segments. We propose a topic identification method using the local discourse structure to recover the omissions of topics and identify the topics of documents in the text collection. Then the topic information is inserted into the text for creating better indices. The experiment results are demonstrated on a test collection which is taken from Chinese Information Retrieval Benchmark, version 3.0.
NOT_RELEVANT;Web of Science;Machine transliteration and transliterated text retrieval: a survey;"Prabhakar, DK; Pal, S";2018;10.1007/s12046-018-0828-8;NOT_FOUND;Users of the WWW across the globe are increasing rapidly. According to Internet live stats there are more than 3 billion Internet users worldwide today and the number of non-English native speakers is quite high there. A large proportion of these non-English speakers access the Internet in their native languages but use the Roman script to express themselves through various communication channels like messages and posts. With the advent of Web 2.0, user-generated content is increasing on the Web at a very rapid rate. A substantial proportion of this content is transliterated data. To leverage this huge information repository, there is a matching effort to process transliterated text. In this article, we survey the recent body of work in the field of transliteration. We start with a definition and discussion of the different types of transliteration followed by various deterministic and non-deterministic approaches used to tackle transliteration-related issues in machine translation and information retrieval. Finally, we study the performance of those techniques and present a comparative analysis of them.
NOT_RELEVANT;Web of Science;A knowledge acquisition methodology to ontology construction for information retrieval from medical documents;"Valencia-Garcia, R; Fernández-Breis, JT; Ruiz-Martínez, JM; García-Sánchez, F; Martínez-Béjar, R";2008;10.1111/j.1468-0394.2008.00464.x;NOT_FOUND;Vast amounts of medical information reside within text documents, so that the automatic retrieval of such information would certainly be beneficial for clinical activities. The need for overcoming the bottleneck provoked by the manual construction of ontologies has generated several studies and research on obtaining semi-automatic methods to build ontologies. Most techniques for learning domain ontologies from free text have important limitations. Thus, they can extract concepts so that only taxonomies are generally produced although there are other types of semantic relations relevant in knowledge modelling. This paper presents a language-independent approach for extracting knowledge from medical natural language documents. The knowledge is represented by means of ontologies that can have multiple semantic relationships among concepts.
NOT_RELEVANT;Web of Science;Developing an NLP and IR-based algorithm for analyzing gene-disease relationships;"Yen, YT; Chen, B; Chiu, HW; Lee, YC; Li, YC; Hsu, CY";2006;10.1055/s-0038-1634069;NOT_FOUND;Objectives: High-throughput techniques such as cDNA microarray, oligonucleotide arrays, and serial analysis of gone expression (SAGE) have been developed and used to automatically screen huge amounts of gene expression data. However, researchers usually spend lots of time and money on discovering gene-disease relationships by utilizing these techniques. We prototypically implemented an algorithm that can provide some kind of predicted results for biological researchers before they proceed with experiments, and it is very helpful for them to discover gene-disease relationships more efficiently. Methods. Due to the fast development of computer technology, many information retrieval techniques have been applied to analyze huge digital biomedical databases available worldwide. Therefore we highly expect that we can apply information retrieval (IR) technique to extract useful information for the relationship of specific diseases and genes from MEDLINE articles. Furthermore, we also applied natural language processing (NLP) methods to do the semantic analysis for the relevant articles to discover the relationships between genes and diseases. Results. We have extracted gone symbols from our literature collection according to disease MeSH classifications. We have also built an IR-based retrieval system, Biomedical Literature Retrieval System (BLRS) and applied the N-gram model to extract the relationship features which can reveal the relationship between genes and diseases. Finally, a relationship network of a specific disease has been built to represent the gene-disease relationships. Conclusions: A relationship feature is a functional word that con reveal the relationship between one single gene and a disease, By incorporating many modern IR techniques, we found that BLRS is a very powerful information discovery tool for literature searching. A relationship network which contains the information on gene symbol, relationship feature, and disease MeSH term can provide an integrated view to discover gene-disease relationships.
NOT_RELEVANT;Web of Science;A Comparative Study on R Packages for Text Mining;"Hellín, CJ; Valledor, A; Cuadrado-Gallego, JJ; Tayebi, A; Gómez, J";2023;10.1109/ACCESS.2023.3310818;NOT_FOUND;The term Text Mining, which is given to the set of techniques used for the extraction, cleaning and processing of the information in texts, has become useful to provide valuable information to other algorithms and widely used with statistical and machine learning methods. By enabling the extraction of useful insights from textual data, Text Mining has become a potent tool in decision-making and knowledge discovery across many areas, including health care, government, education and industry. R is a mature open-source programming environment that has overstepped its initial scope of application for statistical computing and graphics to be used in pretty all the Data Science knowledge Area Groups. The objective of this paper is to present review and benchmarking analysis of packages for text mining techniques with R in computational systems. The paper reviews thirteen different packages comparing them on their execution time and memory used, for which new tests have been specifically designed. The results of this approach have been intended to be used over the most common tasks carried out when analyzing texts, and comparisons included allow R users to know which packages are best for each task and to improve their performance. Text mining package (tm) stands out particularly in Tokenization and Stemming techniques, while fastTextR is the best choice for Topic Modeling and Normalization. Also in the case of the Term Frequency-Inverse Document Frequency (TF-IDF) technique, the textir package is a clear choice. The other packages will depend on whether the technique is applied to a document-term matrix (DTM) or to plain text. In addition, there are packages that perform better in runtime than in memory usage and vice versa, making the choice more difficult. Packages such as udpipe can achieve better results working in parallel. Future works will include the same analysis for parallel computing, hybrid approaches, and novel algorithms.
NOT_RELEVANT;Web of Science;AN EFFICIENT IMPLEMENTATION OF TRIE STRUCTURES;"AOE, JI; MORIMOTO, K; SATO, T";1992;10.1002/spe.4380220902;NOT_FOUND;A new internal array structure, called a double-array, implementing a trie structure is presented. The double-array combines the fast access of a matrix form with the compactness of a list form. The algorithms for retrieval, insertion and deletion are introduced through examples. Although insertion is rather slow, it is still practical, and both the deletion and the retrieval time can be improved from the list form. From the comparison with the list for various large sets of keys, it is shown that the size of the double-array can be about 17 per cent smaller than that of the list, and that the retrieval speed of the double-array can be from 3.1 to 5.1 times faster than that of the list.
NOT_RELEVANT;Web of Science;Estimating term domain relevance through term frequency, disjoint corpora frequency - tf-dcf;"Lopes, L; Fernandes, P; Vieira, R";2016;10.1016/j.knosys.2015.12.015;NOT_FOUND;This paper proposes a new relevance index for terms extracted from domain corpora. We call it term frequency, disjoint corpora frequency (tf-dcf), and it is based on the absolute frequency of each term tempered by its frequency in other (contrasting) corpora. Conceptual differences and mathematical computation of the proposed index are discussed in respect with other similar approaches that also take contrasting corpora into account. To illustrate the efficiency of our index, this paper evaluates tf-dcf against other similar approaches. Finally, other experiments are made in order to analyze the tf-dcf behavior according to the characteristics of contrasting corpora. (C) 2016 Elsevier B.V. All rights reserved.
NOT_RELEVANT;Web of Science;PE-MSC: partial entailment-based minimum set cover for text summarization;"Gupta, A; Kaur, M; Mittal, S; Garg, S";2021;10.1007/s10115-020-01537-1;NOT_FOUND;The notion of Textual Entailment (TE) is an established indicator of text connectedness. It captures semantic relationships between texts. Recently, it has been used successfully for determining sentence salience in many text summarization methods. However, it has been reported in previous works that the standard textual entailment is not ideal for measuring sentence salience. This is because textual entailment relationships between sentences are quite rare in real-world texts. Therefore, we suggest using partial TE to accomplish the task of recognizing standard TE. We present the single document summarization problem as an optimization problem which is solved using a weighted Minimum Set Cover (wMSC) algorithm. In this method, sentences are broken into fragments and Partial TE is used to form sets of fragments. Finally, wMSC is applied to the sets to obtain the minimum set cover, which corresponds to the summary of the document. The results achieved on the DUC 2002 dataset using ROUGE and other quality metrics show that the proposed method outperforms the state of the art.
NOT_RELEVANT;Web of Science;Relevance Assessment of Crowdsourced Data (CSD) Using Semantics and Geographic Information Retrieval (GIR) Techniques;"Koswatte, S; McDougall, K; Liu, XY";2018;10.3390/ijgi7070256;NOT_FOUND;Crowdsourced data (CSD) generated by citizens is becoming more popular as its potential utilization in many applications increases due to its currency and availability. However, the quality of CSD, including its relevance, is often questioned as the data is not generated by professionals nor follows standard data-collection procedures. The quality of CSD can be assessed according to a range of characteristics including its relevance. In this paper, information relevance has been explored through using geographic information retrieval (GIR) techniques to identify the most highly relevant information from a set of crowdsourced data. This research tested a relevance assessment approach for CSD by adapting relevance assessment techniques available in the GIR domain. Thematic and geographic relevance were assessed by analyzing the frequency of selected terms which appeared in CSD reports using natural language processing techniques. The study analyzed crowdsourced reports from the 2011 Australian flood's Crowdmap to examine a proof of concept on relevance assessment using a subset of this dataset based on a defined set of queries. The results determined that the thematic and geographic specificities of the queries were 0.44 and 0.67, respectively, which indicated the queries used were more geographically specific than thematically specific. The Spearman's rho value of 0.62 indicated that the final ranked relevance lists showed reasonable agreement with a manually classified list and confirmed the potential of the approach for CSD relevance assessment. In particular, this research has contributed to the field of CSD relevance assessment through an integrated thematic and geographic relevance ranking process by using a user-query specificity approach to improve the final ranking.
NOT_RELEVANT;Web of Science;Venue Topic Model-enhanced Joint Graph Modelling for Citation Recommendation in Scholarly Big Data;"Wang, W; Gong, ZG; Ren, J; Xia, F; Lv, ZH; Wei, W";2021;10.1145/3404995;NOT_FOUND;Natural language processing technologies, such as topic models, have been proven to be effective for scholarly recommendation tasks with the ability to deal with content information. Recently, venue recommendation is becoming an increasingly important research task due to the unprecedented number of publication venues. However, traditional methods focus on either the author's local network or author-venue similarity, where the multiple relationships between scholars and venues are overlooked, especially the venue-venue interaction. To solve this problem, we propose an author topic model-enhanced joint graph modeling approach that consists of venue topic modeling, venue-specific topic influence modeling, and scholar preference modeling. We first model the venue topic with Latent Dirichlet Allocation. Then, we model the venue-specific topic influence in an asymmetric and low-dimensional way by considering the topic similarity between venues, the top-influence of venues, and the top-susceptibility of venues. The top-influence characterizes venues' capacity of exerting topic influence on other venues. The top-susceptibility captures venues' propensity of being topically influenced by other venues. Extensive experiments on two real-world datasets show that our proposed joint graph modeling approach outperforms the state-of-the-art methods.
NOT_RELEVANT;Web of Science;Neural relational inference for disaster multimedia retrieval;"Fadel, SG; Torres, RD";2020;10.1007/s11042-020-09272-z;NOT_FOUND;Events around the world are increasingly documented on social media, especially by the people experiencing them, as these platforms become more popular over time. As a consequence, social media turns into a valuable source of data for understanding those events. Due to their destructive potential, natural disasters are among events of particular interest to response operations and environmental monitoring agencies. However, this amount of information also makes it challenging to identify relevant content pertaining to those events. In this paper, we use a relational neural network model for identifying this type of content. The model is particularly suitable for unstructured text, that is, text with no particular arrangement of words, such as tags, which is commonplace in social media data. In addition, our method can be combined with a CNN for handling multimodal data where text and visual data are available. We perform experiments in three different scenarios, where different modalities are evaluated: visual, textual, and both. Our method achieves competitive performance in both modalities by themselves, while significantly outperforms the baseline on the multimodal scenario. We also demonstrate the behavior of the proposed method in different applications by performing additional experiments in the CUB-200-2011 multimodal dataset.
NOT_RELEVANT;Web of Science;Processing social media in real-time;"Spina, D; Zubiaga, A; Sheth, A; Strohmaier, M";2019;10.1016/j.ipm.2018.06.006;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;Web of Science;Editorial: Methods and applications of natural language processing in psychiatry research;"Wang, L; Li, SY; Chen, H; Zhou, YY";2022;10.3389/fpsyt.2022.972799;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;Web of Science;Enriching Domain Concepts with Qualitative Attributes: A Text Mining based Approach;"Behera, NK; Mahalakshmi, GS";2020;10.34028/iajit/17/6/10;NOT_FOUND;Attributes, whether qualitative or non-qualitative are the formal description of any real-world entity and are crucial in modern knowledge representation models like ontology. Though ample evidence for the amount of research done for mining non-qualitative attributes (like part-of relation) extraction from text as well as the Web is available in the wealth of literature, on the other side limited research can be found relating to qualitative attribute (i.e., size, color, taste etc.,) mining. Herein this research article an analytical framework has been proposed to retrieve qualitative attribute values from unstructured domain text. The research objective covers two aspects of information retrieval (1) acquiring quality values from unstructured text and (2) then assigning attribute to them by comparing the Google derived meaning or context of attributes as well as quality value (adjectives). The goal has been accomplished by using a framework which integrates Vector Space Modelling (VSM) with a probabilistic Multinomial Naive Bayes (MNB) classifier. Performance Evaluation has been carried out on two data sets (1) HeiPLAS Development Data set (106 adjective-noun exemplary phrases) and (2) a text data set in Medicinal Plant Domain (MPD). System is found to perform better with probabilistic approach compared to the existing pattern-based framework in the state of art.
NOT_RELEVANT;Web of Science;Automatic Retrieval of Bone Fracture Knowledge Using Natural Language Processing;"Do, BH; Wu, AS; Maley, J; Biswal, S";2013;10.1007/s10278-012-9531-1;NOT_FOUND;Natural language processing (NLP) techniques to extract data from unstructured text into formal computer representations are valuable for creating robust, scalable methods to mine data in medical documents and radiology reports. As voice recognition (VR) becomes more prevalent in radiology practice, there is opportunity for implementing NLP in real time for decision-support applications such as context-aware information retrieval. For example, as the radiologist dictates a report, an NLP algorithm can extract concepts from the text and retrieve relevant classification or diagnosis criteria or calculate disease probability. NLP can work in parallel with VR to potentially facilitate evidence-based reporting (for example, automatically retrieving the Bosniak classification when the radiologist describes a kidney cyst). For these reasons, we developed and validated an NLP system which extracts fracture and anatomy concepts from unstructured text and retrieves relevant bone fracture knowledge. We implement our NLP in an HTML5 web application to demonstrate a proof-of-concept feedback NLP system which retrieves bone fracture knowledge in real time.
NOT_RELEVANT;Web of Science;Evaluating and reducing the effect of data corruption when applying bag of words approaches to medical records;"Ruch, P; Baud, R; Geissbühler, A";2002;10.1016/S1386-5056(02)00057-6;NOT_FOUND;"Unlike journal corpora, which are supposed to be carefully reviewed before being published, the quality of documents in a patient record are often corrupted by misspelled words and conventional graphies or abbreviations. After a survey of the domain, the paper focuses on evaluating the effect of such corruption on an information retrieval (IR) engine. The IR system uses a classical bag of words approach, with stems as representation items and term frequency-inverse document frequency (tf-idf) as weighting schema; we pay special attention to the normalization factor. First results shows that even low corruption levels (3%) do affect retrieval effectiveness (4-7%), whereas higher corruption levels can affect retrieval effectiveness by 25%. Then, we show that the use of an improved automatic spelling correction system, applied on the corrupted collection, can almost restore the retrieval effectiveness of the engine. (C) 2002 Elsevier Science Ireland Ltd. All rights reserved."
NOT_RELEVANT;Web of Science;HealthRecSys: A semantic content-based recommender system to complement health videos;"Bocanegra, CLS; Ramos, JLS; Rizo, C; Civit, A; Fernandez-Luque, L";2017;10.1186/s12911-017-0431-7;NOT_FOUND;"Background: The Internet, and its popularity, continues to grow at an unprecedented pace. Watching videos online is very popular; it is estimated that 500 h of video are uploaded onto YouTube, a video-sharing service, every minute and that, by 2019, video formats will comprise more than 80% of Internet traffic. Health-related videos are very popular on YouTube, but their quality is always a matter of concern. One approach to enhancing the quality of online videos is to provide additional educational health content, such as websites, to support health consumers. This study investigates the feasibility of building a content-based recommender system that links health consumers to reputable health educational websites from MedlinePlus for a given health video from YouTube. Methods: The dataset for this study includes a collection of health-related videos and their available metadata. Semantic technologies (such as SNOMED-CT and Bio-ontology) were used to recommend health websites from MedlinePlus. A total of 26 healths professionals participated in evaluating 253 recommended links for a total of 53 videos about general health, hypertension, or diabetes. The relevance of the recommended health websites from MedlinePlus to the videos was measured using information retrieval metrics such as the normalized discounted cumulative gain and precision at K. Results: The majority of websites recommended by our system for health videos were relevant, based on ratings by health professionals. The normalized discounted cumulative gain was between 46% and 90% for the different topics. Conclusions: Our study demonstrates the feasibility of using a semantic content-based recommender system to enrich YouTube health videos. Evaluation with end-users, in addition to healthcare professionals, will be required to identify the acceptance of these recommendations in a nonsimulated information-seeking context."
NOT_RELEVANT;Web of Science;Leveraging statistical information in fine-grained financial sentiment analysis;"Zhang, H; Li, ZX; Xie, HR; Lau, RYK; Cheng, G; Li, Q; Zhang, DA";2022;10.1007/s11280-021-00993-1;NOT_FOUND;The recent development of deep learning-based natural language processing (NLP) methods has fostered many downstream applications in various fields. As one of the applications in the financial industry, fine-grained financial sentiment analysis (FSA) aims to understand the sentimental orientation, i.e., bullish or bearish, of financial texts by predicting the polarity score and has been widely applied in the financial industry stock-related opinion mining. Because of the lack of a large-scale labeled dataset and the domain-dependent nature, FSA is challenging. Previous works mainly focus on constructing and exploiting handcrafted lexicons that encode expert knowledge to enhance the semantic features in decision making, which yields improvements but are expensive to acquire. This paper proposes a lightweight regression model incorporating the statistical distribution of a term over the polarity range, say between - 1 and 1, to address the fine-grained FSA task. More concretely, we first count each word's appearance at different polarity intervals and produce a statistic-based representation for each text, which will be encoded as a corpus-level statistical feature vector by an autoencoder. Subsequently, the obtained feature vector will be integrated with the semantic feature vector in the regression model. Our experiments show such a model can produce significant improvements compared with the baseline models on two FSA subsets, i.e., news headlines and microblogs, without a computational overhead. Furthermore, we notice the signs that lexicon-based approaches have neglected can play an important role in FSA.
NOT_RELEVANT;Web of Science;A survey on Urdu and Urdu like language stemmers and stemming techniques;"Jabbar, A; Iqbal, S; Khan, MUG; Hussain, S";2018;10.1007/s10462-016-9527-1;NOT_FOUND;Stemming is one of the basic steps in natural language processing applications such as information retrieval, parts of speech tagging, syntactic parsing and machine translation, etc. It is a morphological process that intends to convert the inflected forms of a word into its root form. Urdu is a morphologically rich language, emerged from different languages, that includes prefix, suffix, infix, co-suffix and circumfixes in inflected and multi-gram words that need to be edited in order to convert them into their stems. This editing (insertion, deletion and substitution) makes the stemming process difficult due to language morphological richness and inclusion of words of foreign languages like Persian and Arabic. In this paper, we present a comprehensive review of different algorithms and techniques of stemming Urdu text and also considering the syntax, morphological similarity and other common features and stemming approaches used in Urdu like languages, i.e. Arabic and Persian analyzed, extract main features, merits and shortcomings of the used stemming approaches. In this paper, we also discuss stemming errors, basic difference between stemming and lemmatization and coin a metric for classification of stemming algorithms. In the final phase, we have presented the future work directions.
NOT_RELEVANT;Web of Science;Automated Story Selection for Color Commentary in Sports;"Lee, G; Bulitko, V; Ludvig, EA";2014;10.1109/TCIAIG.2013.2275199;NOT_FOUND;Automated sports commentary is a form of automated narrative. Sports commentary exists to keep the viewer informed and entertained. One way to entertain the viewer is by telling brief stories relevant to the game in progress. We present a system called the sports commentary recommendation system (SCoReS) that can automatically suggest stories for commentators to tell during games. Through several user studies, we compared commentary using SCoReS to three other types of commentary and show that SCoReS adds significantly to the broadcast across several enjoyment metrics. We also collected interview data from professional sports commentators who positively evaluated a demonstration of the system. We conclude that SCoReS can be a useful broadcast tool, effective at selecting stories that add to the enjoyment and watchability of sports. SCoReS is a step toward automating sports commentary and, thus, automating narrative.
NOT_RELEVANT;Web of Science;First steps in building a model for the retrieval of court decisions;"Moens, MF; De Busser, R";2002;10.1006/ijhc.2002.1029;NOT_FOUND;The MOSAIC project investigates a retrieval model for court decisions based on structured and unstructured (natural language) information in legal cases. This paper focuses on how relevant information in court decisions can function as a key for retrieval and on the automated construction of case representations. Techniques of automated concept learning and rhetorical structure identification are among the most promising ones. (C) 2002 Elsevier Science Ltd. All rights reserved.
NOT_RELEVANT;Web of Science;Spatiotemporal and semantic information extraction from Web news reports about natural hazards;"Wang, W; Stewart, K";2015;10.1016/j.compenvurbsys.2014.11.001;NOT_FOUND;In the field of geographic information science, modeling geographic dynamics based on spatiotemporal information extracted from the Web, especially unconstructed data such as online news reports, is a growing area of research. Extracting spatiotemporal and semantic information from a set of Web documents enables us to build a rich representation of geographic knowledge described in text, capturing where, when, or what events have occurred. This work investigates the role ontologies play as a key component in the process of semantic information extraction. We show how ontologies can be used in conjunction with natural language gazetteers in order to process semantic information about hazard events and augment spatiotemporal extraction with semantics. We are interested in capturing the spatiotemporal patterns of hazard-related events from online news reports to track the occurrences and evolution of natural hazards, such as severe storms. A hazard ontology has been created to assist the spatiotemporal information extraction process, especially with the automatic detection of different kinds of events at multiple granularities from unstructured texts revealing relationships between the events over space-time. The extraction and retrieval of semantic information about event dynamics provides information about the progression of events using both natural and human perspectives. (C) 2014 Elsevier Ltd. All rights reserved.
NOT_RELEVANT;Web of Science;Retrieval Contrastive Learning for Aspect-Level Sentiment Classification;"Jian, ZQ; Li, JJ; Wu, QQ; Yao, JF";2024;10.1016/j.ipm.2023.103539;NOT_FOUND;"Aspect-Level Sentiment Classification (ALSC) aims to assign specific sentiments to a sentence toward different aspects, which is one of the crucial challenges in the field of Natural Language Processing (NLP). Despite numerous approaches being proposed and obtaining prominent results, the majority of them focus on leveraging the relationships between the aspect and opinion words in a single instance while ignoring correlations with other instances, which will make models inevitably become trapped in local optima due to the absence of a global viewpoint. Instance representation derived from a single instance, on the one hand, the contained information is insufficient due to the lack of descriptions from other perspectives; on the other hand, its stored knowledge is redundant since the inability to filter extraneous content. To obtain a polished instance representation, we developed a Retrieval Contrastive Learning (RCL) framework to subtly extract intrinsic knowledge across instances. RCL consists of two modules: (a) obtaining retrieval instances by sparse retriever and dense retriever, and (b) extracting and learning the knowledge of the retrieval instances by using Contrastive Learning (CL). To demonstrate the superiority of RCL, five ALSC models are employed to conduct comprehensive experiments on three widely-known benchmarks. Compared with the baselines, ALSC models achieve substantial improvements when trained with RCL. Especially, ABSA-DeBERTa with RCL obtains new state-of-the-art results, which outperform the advanced methods by 0.92%, 0.23%, and 0.47% in terms of Macro F1 gains on Laptops, Restaurants, and Twitter, respectively."
NOT_RELEVANT;Web of Science;An IR-Based Approach Utilizing Query Expansion for Plagiarism Detection in MEDLINE;"Nawab, RMA; Stevenson, M; Clough, P";2017;10.1109/TCBB.2016.2542803;NOT_FOUND;The identification of duplicated and plagiarized passages of text has become an increasingly active area of research. In this paper, we investigate methods for plagiarism detection that aim to identify potential sources of plagiarism from MEDLINE, particularly when the original text has been modified through the replacement of words or phrases. A scalable approach based on Information Retrieval is used to perform candidate document selection-the identification of a subset of potential source documents given a suspicious text-from MEDLINE. Query expansion is performed using the ULMS Metathesaurus to deal with situations in which original documents are obfuscated. Various approaches to Word Sense Disambiguation are investigated to deal with cases where there are multiple Concept Unique Identifiers (CUIs) for a given term. Results using the proposed IR-based approach outperform a state-of-the-art baseline based on Kullback-Leibler Distance.
NOT_RELEVANT;Web of Science;A WORDNET.PT-BASED TERMINOLOGY FOR INFORMATION RETRIEVAL SYSTEMS;"Nhacuongue, JA; Dutra, ML";2020;NOT_FOUND;NOT_FOUND;The article results from post-doctoral research conducted in Universidade Federal de Santa Catarina. The goal is to propose information retrieval strategies based on natural language processing, to extract semantic relations from WordNet.Pt, and use them to represent documents and users' search expressions. The approach is qualitative, exploratory and applied to ambiguity problems in information retrieval. As for the procedures used, it is a bibliographic search. The discussion is motivated by the problem of low precision and high recall in user searches, influenced both by the absence of semantic correspondence between search expressions and terms used in indexing and by the lack of determination of the semantic similarity between document terms that, even being lexicographically different, have the same meaning. The research core is justified by the advantage of developing systems that combine natural language and controlled language, for an interactive search. Although in a partial way, the research points to important results in the solution of lexical ambiguity, through semantic relationships in the representation of documents and user search. On the one hand, this success guarantees the restriction of the search space and, consequently, precision. On the other hand, the expansion of consultations by suggesting equivalent terms from controlled vocabularies and the natural language and its variants.
NOT_RELEVANT;Web of Science;Extracting and analyzing ejection fraction values from electronic echocardiography reports in a large health maintenance organization;"Xie, FG; Zheng, CY; Yuh-Jer Shen, A; Chen, WS";2017;10.1177/1460458216651917;NOT_FOUND;The left ventricular ejection fraction value is an important prognostic indicator of cardiovascular outcomes including morbidity and mortality and is often used clinically to indicate severity of heart disease. However, it is usually reported in free-text echocardiography reports. We developed and validated a computerized algorithm to extract ejection fraction values from echocardiography reports and applied the algorithm to a large volume of unstructured echocardiography reports between 1995 and 2011 in a large health maintenance organization. A total of 621,856 echocardiography reports with a description of ejection fraction values or systolic functions were identified, of which 70 percent contained numeric ejection fraction values and the rest (30%) were text descriptions explicitly indicating the systolic left ventricular function. The 12.1 percent (16.0% for male and 8.4% for female) of these extracted ejection fraction values are < 45 percent. Validation conducted based on a random sample of 200 reports yielded 95.0 percent sensitivity and 96.9 percent positive predictive value.
NOT_RELEVANT;Web of Science;THE LBI-METHOD FOR AUTOMATED INDEXING OF DIAGNOSES BY USING SNOMED .1. DESIGN AND REALIZATION;"BRIGL, B; MIETH, M; HAUX, R; GLUCK, E";1994;10.1016/0020-7101(94)90122-8;NOT_FOUND;We present a simple, formal, lexicon-based method for automated indexing of diagnoses based on the Systematized Nomenclature of Medicine (SNOMED II), called the LBI-method. Part 1 gives an introduction to the LBI-method and presents its realization as application system SALBIDH. The underlying model states that a diagnosis is represented by a set of indices of any nomenclature. The LBI-method is defined as a composition of functions, which in turn define the 3 steps of the LBI-method: preprocessing, morphological analysis, and semantic analysis. Part 2 will focus on the design and the results of an evaluation study to judge the quality of the LBI-method. In this evaluation study the quality of automated indexing was examined as well as the quality of the retrieval of patient data by using automated indexed diagnoses.
NOT_RELEVANT;Web of Science;Scientific Attention to Sustainability and SDGs: Meta-Analysis of Academic Papers;"Asatani, K; Takeda, H; Yamano, H; Sakata, I";2020;10.3390/en13040975;NOT_FOUND;"Scientific research plays an important role in the achievement of a sustainable society. However, grasping the trends in sustainability research is difficult because studies are not devised and conducted in a top-down manner with Sustainable Development Goals (SDGs). To understand the bottom-up research activities, we analyzed over 300,000 publications concerned with sustainability by using citation network analysis and natural language processing. The results suggest that sustainability science's diverse and dynamic changes have been occurring over the last few years; several new topics, such as nanocellulose and global health, have begun to attract widespread scientific attention. We further examined the relationship between sustainability research subjects and SDGs and found significant correspondence between the two. Moreover, we extracted SDG topics that were discussed following a convergent approach in academic studies, such as inclusive society and early childhood development, by observing the convergence of terms in the citation network. These results are valuable for government officials, private companies, and academic researchers, empowering them to understand current academic progress along with research attention devoted to SDGs."
NOT_RELEVANT;Web of Science;Predicting taxi demand hotspots using automated Internet Search Queries;"Markou, I; Kaiser, K; Pereira, FC";2019;10.1016/j.trc.2019.03.001;NOT_FOUND;Disruptions due to special events are a well-known challenge in transport operations, since the transport system is typically designed for habitual demand. Part of the problem relates to the difficulty in collecting comprehensive and reliable information early enough to prepare mitigation measures. A tool that automatically scans the internet for events and predicts their impact would strongly support transport management in many cities in the world. This study addresses the challenges related to retrieving and analyzing web documents about real world events, and using them for demand explanation (if related to a past event) and prediction (if a future one). Transport demand is predicted with a supervised topic modeling algorithm by utilizing information about social events retrieved using various strategies, which made use of search aggregation, natural language processing, and query expansion. It was found that a two-step process produced the highest accuracy for transport demand prediction, where different (but related) queries are used to retrieve an initial set of documents, and then, based on these documents, a final query is constructed that obtains the set of predictive documents. These are then used to model the most discriminating topics related to the transport demand. A framework was proposed that sequentially handles all stages of data gathering, enrichment, and prediction with the intention of generating automated search queries.
NOT_RELEVANT;Web of Science;Pattern and semantic analysis to improve unsupervised techniques for opinion target identification;"Khan, K; Ullah, A; Baharudin, B";2016;NOT_FOUND;NOT_FOUND;"This research employs patterns and semantic analysis to improve the existing unsupervised opinion targets extraction technique. Two steps are employed to identify opinion targets: candidate selection and opinion targets selection. For candidate selection; a combined lexical based syntactic pattern is identified. For opinion targets selection, a hybrid approach that combines the existing likelihood ratio test technique with semantic based relatedness is proposed. The existing approach basically extracts frequently observed targets in text. However, analysis shows that not all target features occur frequently in the texts. Hence the hybrid technique is proposed to extract both frequent and infrequent targets. The proposed algorithm employs incremental approach to improve the performance of existing unsupervised mining of features by extracting infrequent features through semantic relatedness with frequent features based on lexical dictionary. Empirical results show that the hybrid technique with combined patterns outperforms the existing techniques."
NOT_RELEVANT;Web of Science;A novel method for providing relational databases with rich semantics and natural language processing;"Hamaz, K; Benchikha, F";2017;10.1108/JEIM-01-2015-0005;NOT_FOUND;Purpose - With the development of systems and applications, the number of users interacting with databases has increased considerably. The relational database model is still considered as the most used model for data storage and manipulation. However, it does not offer any semantic support for the stored data which can facilitate data access for the users. Indeed, a large number of users are intimidated when retrieving data because they are non-technical or have little technical knowledge. To overcome this problem, researchers are continuously developing new techniques for Natural Language Interfaces to Databases (NLIDB). Nowadays, the usage of existing NLIDBs is not widespread due to their deficiencies in understanding natural language (NL) queries. In this sense, the purpose of this paper is to propose a novel method for an intelligent understanding of NL queries using semantically enriched database sources. Design/methodology/approach - First a reverse engineering process is applied to extract relational database hidden semantics. In the second step, the extracted semantics are enriched further using a domain ontology. After this, all semantics are stored in the same relational database. The phase of processing NL queries uses the stored semantics to generate a semantic tree. Findings - The evaluation part of the work shows the advantages of using a semantically enriched database source to understand NL queries. Additionally, enriching a relational database has given more flexibility to understand contextual and synonymous words that may be used in a NL query. Originality/value - Existing NLIDBs are not yet a standard option for interfacing a relational database due to their lack for understanding NL queries. Indeed, the techniques used in the literature have their limits. This paper handles those limits by identifying the NL elements by their semantic nature in order to generate a semantic tree. This last is a key solution towards an intelligent understanding of NL queries to relational databases.
NOT_RELEVANT;Web of Science;Excavating the mother lode of human-generated text: A systematic review of research that uses the wikipedia corpus;"Mehdi, M; Okoli, C; Mesgari, M; Nielsen, FÅ; Lanamäki, A";2017;10.1016/j.ipm.2016.07.003;NOT_FOUND;Although primarily an encyclopedia, Wikipedia's expansive content provides a knowledge base that has been continuously exploited by researchers in a wide variety of domains. This article systematically reviews the scholarly studies that have used Wikipedia as a data source, and investigates the means by which Wikipedia has been employed in three main computer science research areas: information retrieval, natural language processing, and ontology building. We report and discuss the research trends of the identified and examined studies. We further identify and classify a list of tools that can be used to extract data from Wikipedia, and compile a list of currently available data sets extracted from Wikipedia. (C) 2016 Published by Elsevier Ltd.
NOT_RELEVANT;Web of Science;THE LBI-METHOD FOR AUTOMATED INDEXING OF DIAGNOSES BY USING SNOMED .2. EVALUATION;"BRIGL, B; MIETH, M; HAUX, R; GLUCK, E";1995;10.1016/0020-7101(94)01062-6;NOT_FOUND;We present a simple, formal, lexicon-based method for automated indexing of diagnoses based on the Systematized Nomenclature of Medicine (SNOMED), called LBI-method. Part 1 gave an introduction to the LBI-method and presented its realisation as application system SALBIDH. Part 2 presents the design and the results of an evaluation study to judge the quality of the LBI-method. In this evaluation study the quality of automated indexing as well as the quality of the retrieval of patient data by using automated indexed diagnoses was examined. The results show that the retrieval based on SNOMED indices is at least as good as the retrieval based on ICD classes despite a lot of indexing errors. From this we gather that our system is not yet good enough for immediate routine use but that an appropriate indexing quality and, as a result, a higher retrieval quality can be achieved after few improvements of the LBI-method, especially after revision of the lexicons.
NOT_RELEVANT;Web of Science;Video Moment Localization Network Based on Text Multi-semantic Clues Guidance;"Wu, GL; Xu, TJ";2023;NOT_FOUND;NOT_FOUND;With the rapid development of the Internet and information technology, people are able to create multimedia data such as pictures or videos anytime and anywhere. Efficient multimedia processing tools are needed for the vast video data. The video moment localization task aims to locate the video moment which best matches the query in the untrimmed video. Existing text-guided methods only consider single-scale text features, which cannot fully represent the semantic features of text, and also do not consider the masking of crucial information in the video by text information when using text to guide the extraction of video features. To solve the above problems, we propose a video moment localization network based on text multi-semantic clues guidance. Specifically, we first design a text encoder based on fusion gate to better capture the semantic information in the text through multi-semantic clues composed of word embedding, local features and global features. Then text guidance module guides the extraction of video features by text semantic features to highlight the video features related to text semantics. Experimental results on two datasets, Charades-STA and ActivityNet Captions, show that our approach provides significant improvements over state-of-the-art methods.
NOT_RELEVANT;Web of Science;Fast insertion methods of a double-array structure;"Morita, K; Fuketa, M; Yamakawa, Y; Aoe, J";2001;"10.1002/1097-024X(200101)31:1<43::AID-SPE356>3.0.CO;2-R";NOT_FOUND;A double-array is a compact and fast data structure for a trie, but it degrades the speed of insertion for a large set of keys. In this paper, two kinds of methods for improving insertion are presented. The basic functions for retrieval, insertion and deletion are implemented in the C language, Comparing with the original double-array for large sets of keys, the improved double-array is about six to 320 times faster than that for insertion. Copyright (C) 2001 John Wiley & Sons, Ltd.
NOT_RELEVANT;Web of Science;Project-Based As-Needed Information Retrieval from Unstructured AEC Documents;"Fan, HQ; Xue, F; Li, H";2015;10.1061/(ASCE)ME.1943-5479.0000341;NOT_FOUND;With the increasing complexity of architecture, engineering, and construction (AEC) projects and fast track execution of project works, written documents are becoming more and more important for project coordination, communication, and works control. Finding all the relevant information from unstructured construction documents is critical to various management tasks such as work planning, progress control, and claims. A framework is proposed in this research to retrieve project-wide as-needed information from AEC documents. Through this framework, improvement in the levels of precision and recall in the information retrieval process can be made effective through the use of a project-specific term dictionary and dependency grammar parsing information of textual documents. Their effectiveness is demonstrated through a series of experimental tests conducted on a real life building redevelopment project with different information retrieval and ranking strategies. The results and findings are presented in this paper along with discussion on the related issues on research and system development. (C) 2014 American Society of Civil Engineers.
NOT_RELEVANT;Web of Science;LAAP: Learning the Argument of An Entity with Event Prompts for document-level event extraction;"Xu, JH; Yang, C; Kang, XJ";2025;10.1016/j.neucom.2024.128584;NOT_FOUND;Document-level Event Extraction (DEE) aims to identify event types within a document and extract their corresponding arguments, which is essential for structured information provision in various NLP applications. Unlike sentence-level extraction, DEE requires handling events and arguments scattered across a document. Existing methods often focus on intricate feature interactions, neglecting explicit argument-entity relationships. We introduce a novel method, Learning the Argument of an Entity with Event Prompts (LAAP), which constructs event prompts for type detection, incorporating sentence placeholders to elicit event-specific information. Additionally, we propose an entity argument learning strategy that narrows down entity types to find the most suitable one. Experiments on the ChFinAnn and three other public datasets show that our method surpasses state-of-the-art approaches in accuracy and effectiveness.
NOT_RELEVANT;Web of Science;Image Classification in Arabic: Exploring Direct English to Arabic Translations;Alsudais, A;2019;10.1109/ACCESS.2019.2926924;NOT_FOUND;"Image classification is an ongoing research challenge. Most of the current research focuses on image classification in English with very little research in Arabic. Expanding image classification to Arabic has several applications and benefits. This paper investigates the accuracy of direct translations of English labels that are available in ImageNet, a database of images labeled in English that is commonly used in computer vision research, to Arabic. A dataset comprised of 2,887 labeled images was constructed by randomly selecting images from ImageNet. All of the labels were translated to Arabic using an online translation service. The accuracy of each translation was evaluated by a human judge. Results indicated that 65.6% of the generated Arabic labels were accurate with the highest results achieved when the labels consisted of only one word. This study makes three important contributions to the image classification literature: (1) it determines a baseline level of accuracy for image classification in Arabic algorithms; (2) it provides 1,910,935 images classified with accurate Arabic labels (based on accurately labeling 1,895 images that consist of 1,643 unique synsets); and (3) it measures the accuracy of translations of image labels in ImageNet to Arabic."
NOT_RELEVANT;Web of Science;Automatic extraction of new words based on Google News corpora for supporting lexicon-based Chinese word segmentation systems;"Hong, CM; Chen, CM; Chiu, CY";2009;10.1016/j.eswa.2008.02.013;NOT_FOUND;Chinese word segmentation is an essential step in a processing of Chinese natural language because it is beneficial to the Chinese text mining and information retrieval. Currently, the lexicon-based Chinese word segmentation scheme is widely adopted, which call correctly identify Chinese sentences as distinct words from Chinese language texts in real-word applications. However, the word identification ability of the lexicon-based scheme is highly dependent with a well prepared lexicon with sufficient amount of lexical entries which covers all of the Chinese words. In particular, this scheme cannot perform Chinese word segmentation process well for highly changeable texts with time, Such as newspaper articles and web documents. This is because highly changeable documents often contain many new words that cannot be identified by a lexicon-based Chinese word segmentation system with a constant lexicon. Moreover, to maintain a lexicon by manpower is an inefficient and time-consuming job. Therefore, this study proposes it novel statistics-based scheme for extraction of new words based on the categorized corpora of Google News retrieved automatically from the Google News site to promote the word identification ability for lexicon-based Chinese word segmentation systems. Since corpora of news almost contain all words used in daily life, to extract news words from corpora of news and to incrementally add them into lexicon for lexicon-based Chinese word segmentation systems provide benefits in terms of automatically constructing a professional lexicon and enhancing word identification capability. Compared to another proposed scheme of new word extraction, the experimental results indicated that the proposed extraction scheme of new words not only more correctly retrieves new words from the categorized corpora of Google News, but also obtains larger amount of new words. Moreover, the proposed scheme of new word extraction has been applied to automatically expand the lexicon of the Chinese word segmentation system ECScanner (A Chinese Lexicon Scanner with Lexicon Extension). Currently, the ECScanner has been published on the Web to provide Chinese word segmentation service based on Web service. Experimental results also confirmed that ECScanner is superior to CKIP (Chinese knowledge information processing) in identifying meaningful Chinese words. (C) 2008 Elsevier Ltd. All rights reserved.
NOT_RELEVANT;Web of Science;Unveiling the inventive process from patents by extracting problems, solutions and advantages with natural language processing;"Giordano, V; Puccetti, G; Chiarello, F; Pavanello, T; Fantoni, G";2023;10.1016/j.eswa.2023.120499;NOT_FOUND;"Patents are the main means for disclosing an invention. These documents encompass many steps of the inventive process starting with the definition of the problem to be solved and ending with the identification of a solution. In this study we focus on three fundamental concepts of the inventive process: (A) technical problems; (B) solutions; and (C) advantageous effects of the invention, which, based on the WIPO guidelines, any patent should include. We propose a system based on Natural Language Processing (NLP) pipeline that uses transformer language models to identify technical problems, solutions and advantageous effects from patents. We use a training dataset composed of 480,000 patents sentences contained in sections manually labelled by inventors or attorneys. Our model reaches a F1 score of 90%. The model is evaluated on a random set of patents to assess its deployability in a real-world scenario. The proposed model can be used as a novel tool for prior art mapping, novel ideas generation and technological evolution identification and can help to disclose valuable information hidden in patent documents."
NOT_RELEVANT;Web of Science;An empirical study of the design choices for local citation recommendation systems;"Medic, Z; Snajder, J";2022;10.1016/j.eswa.2022.116852;NOT_FOUND;As the number of published research articles grows on a daily basis, it is becoming increasingly difficult for scientists to keep up with the published work. Local citation recommendation (LCR) systems, which produce a list of relevant articles to be cited in a given text passage, could help alleviate the burden on scientists and facilitate research. While research on LCR is gaining popularity, building such systems involves a number of important design choices that are often overlooked. We present an empirical study of the impact of the three design choices in two-stage LCR systems consisting of a prefiltering and a reranking phase. In particular, we investigate (1) the impact of the prefiltering models' parameters on the model's performance, as well as the impact of (2) the training regime and (3) negative sampling strategy on the performance of the reranking model. We evaluate various combinations of these parameters on two datasets commonly used for LCR and demonstrate that specific combinations improve the model's performance over the widely used standard approaches. Specifically, we demonstrate that (1) optimizing prefiltering models' parameters improves R@1000 in the range of 3% to 12% in absolute value, (2) using the strict training regime improves both R@10 and MRR (up to a maximum of 3.4% and 2.6%, respectively) in all combinations of dataset and prefiltering model, and (3) a careful choice of negative examples can further improve both R@10 and MRR (up to a maximum of 11.9% and 8%, respectively) in both datasets used Our results show that the design choices we considered are important and should be given greater consideration when building LCR systems.
NOT_RELEVANT;Web of Science;Integrating an incident dataset with a question and answering language model to assist hazard identification: Comparison of an extractive and generative model;"Ricketts, J; Guo, WS; Pelham, J; Barry, D";2024;10.1177/1748006X241272831;NOT_FOUND;Robust hazard identification (HAZID) relies upon extensive knowledge of the system being analysed, the technical aspects, and how it will be used operationally. Typically, this knowledge is held by human participants who can draw out answers in natural language to hazard related questions based upon their own experience. However, several threats exist to this, such as high staff turnover, a poor learning from incidents capability or even insufficient Information Technology resources. Alternatively, incident databases hold vast amounts of hazard information that can be transformed into a source of knowledge. As mitigation to the aforementioned issues, this paper presents a Question and Answering (Q&A) Bidirectional Encoder Representations from Transformers (BERT) language model trained upon aviation incidents and a unique Q&A dataset. The model can extract answers to typical HAZID questions, based upon factual incident reports. Alongside this extractive approach, the paper also explores the use of a generative Large Language Model combined with an incident dataset. Both models proved a useful addition to HAZID activities based upon the Structured What If Technique (SWIFT), answering safety-themed questions based upon a retrieved context of incident reports that semantically matched the query. For the purposes of HAZID, it was suggested that the generative option is preferable based upon its ease of implementation, lower resource requirements and quality of responses. Additionally, it is shown that it is possible for organisations to train and create their own custom models for HAZID purposes. Future work may wish to consider the application of models that can hypothesize scenarios based upon incident reports, building further understanding to the relationships between causes, hazards and consequences.
NOT_RELEVANT;Web of Science;Finding relevant free-text radiology reports at scale with IBM Watson Content Analytics: a feasibility study in the UK NHS;"Piotrkowicz, A; Johnson, O; Hall, G";2019;10.1186/s13326-019-0213-5;NOT_FOUND;Background Significant amounts of health data are stored as free-text within clinical reports, letters, discharge summaries and notes. Busy clinicians have limited time to read such large amounts of free-text and are at risk of information overload and consequently missing information vital to patient care. Automatically identifying relevant information at the point of care has the potential to reduce these risks but represents a considerable research challenge. One software solution that has been proposed in industry is the IBM Watson analytics suite which includes rule-based analytics capable of processing large document collections at scale. Results In this paper we present an overview of IBM Watson Content Analytics and a feasibility study using Content Analytics with a large-scale corpus of clinical free-text reports within a UK National Health Service (NHS) context. We created dictionaries and rules for identifying positive incidence of hydronephrosis and brain metastasis from 5.6 m radiology reports and were able to achieve 94% precision, 95% recall and 89% precision, 94% recall respectively on a sample of manually annotated reports. With minor changes for US English we applied the same rule set to an open access corpus of 0.5 m radiology reports from a US hospital and achieved 93% precision, 94% recall and 84% precision, 88% recall respectively. Conclusions We were able to implement IBM Watson within a UK NHS context and demonstrate effective results that could provide clinicians with an automatic safety net which highlights clinically important information within free-text documents. Our results suggest that currently available technologies such as IBM Watson Content Analytics already have the potential to address information overload and improve clinical safety and that solutions developed in one hospital and country may be transportable to different hospitals and countries. Our study was limited to exploring technical aspects of the feasibility of one industry solution and we recognise that healthcare text analytics research is a fast-moving field. That said, we believe our study suggests that text analytics is sufficiently advanced to be implemented within industry solutions that can improve clinical safety.
NOT_RELEVANT;Web of Science;Open set evaluation of web genre identification;"Pritsos, D; Stamatatos, E";2018;10.1007/s10579-018-9418-y;NOT_FOUND;Web genre detection is a task that can enhance information retrieval systems by providing rich descriptions of documents and enabling more specialized queries. Most of previous studies in this field adopt the closed-set scenario where a given palette comprises all available genre labels. However this is not a realistic setup since web genres are constantly enriched with new labels and existing web genres are evolving in time. Open-set classification, where some pages used in the evaluation phase do not belong to any of the known genres, is a more realistic setup for this task. In this case, all pages not belonging to known genres can be seen as noise. This paper focuses on systematic evaluation of open-set web genre identification when the noise is either structured or unstructured. Two open-set methods combined with alternative text representation schemes and similarity measures are tested based on two benchmark corpora. Moreover, we adopt the openness test for web genre identification that enables the observation of effectiveness for a varying number of known/unknown labels.
NOT_RELEVANT;Web of Science;Heterogeneous text graph for comprehensive multilingual sentiment analysis: capturing shortand longdistance semantics;"Mercha, E; Benbrahim, H; Erradi, M";2024;10.7717/peerj-cs.1876;NOT_FOUND;Multilingual sentiment analysis (MSA) involves the task of comprehending people's opinions, sentiments, and emotions in multilingual written texts. This task has garnered considerable attention due to its importance in extracting insights for decision -making across diverse fields such as marketing, finance, and politics. Several studies have explored MSA using deep learning methods. Nonetheless, a majority of these studies depend on sequential -based approaches, which focus on capturing short -distance semantics within adjacent word sequences, but they overlook longdistance semantics, which can provide more profound insights for analysis. In this work, we propose an approach for multilingual sentiment analysis, namely MSAGCN, leveraging a graph convolutional network to effectively capture both shortand long-distance semantics. MSA-GCN involves the comprehensive modeling of the multilingual sentiment analysis corpus through a unified heterogeneous text graph. Subsequently, a slightly deep graph convolutional network is employed to acquire predictive representations for all nodes by encouraging the transfer learning across languages. Extensive experiments are carried out on various language combinations using different benchmark datasets to assess the efficiency of the proposed approach. These datasets include Multilingual Amazon Reviews Corpus (MARC), Internet Movie Database (IMDB), Allocine, and Muchocine. The achieved results reveal that MSA-GCN significantly outperformed all baseline models in almost all datasets with a p -value < 0.05 based on student t -test. In addition, such approach shows prominent results in a variety of language combinations, revealing the robustness of the approach against language variation.
NOT_RELEVANT;Web of Science;Technology identification from patent texts: A novel named entity recognition method;"Puccetti, G; Giordano, V; Spada, I; Chiarello, F; Fantoni, G";2023;10.1016/j.techfore.2022.122160;NOT_FOUND;Identifying technologies is a key element for mapping a domain and its evolution. It allows managers and decision makers to anticipate trends for an accurate forecast and effective foresight. Researchers and practitioners are taking advantage of the rapid growth of the publicly accessible sources to map technological domains. Among these sources, patents are the widest technical open access database used in the literature and in practice. Nowadays, Natural Language Processing (NLP) techniques enable new methods for the analysis of patent texts. Among these techniques, in this paper we explore the use of Named Entity Recognition (NER) with the purpose to identify the technologies mentioned in patents' text. We compare three different NER methods, gazetteer-based, rule-based and deep learning-based (e.g. BERT), measuring their performances in terms of precision, recall and computational time. We test the approaches on 1600 patents from four assorted IPC classes as case studies. Our NER systems collected over 4500 fine-grained technologies, achieving the best results thanks to the combination of the three methodologies. The proposed method overcomes the literature thanks to the ability to filter generic technological terms. Our study delineates a valid technology identification tool that can be integrated in any text analysis pipeline to support academics and companies in investigating a technological domain.
NOT_RELEVANT;Web of Science;Automatic identification of light stop words for Persian information retrieval systems;"Sadeghi, M; Vegas, J";2014;10.1177/0165551514530655;NOT_FOUND;Stop word identification is one of the most important tasks for many text processing applications such as information retrieval. Stop words occur too frequently in documents in a collection and do not contribute significantly to determining the context or information about the documents. These words are worthless as index terms and should be removed during indexing as well as before querying by an information retrieval system. In this paper, we propose an automatic aggregated methodology based on term frequency, normalized inverse document frequency and information model to extract the light stop words from Persian text. We define a light stop word' as a stop word that has few letters and is not a compound word. In the Persian language, a complete stop word list can be derived by combining the light stop words. The evaluation results, using a standard corpus, show a good percentage of coincidence between the Persian and English stop words and a significant improvement in the number of index terms. Specifically, the first 32 Persian light stop words have a great impact on the index size reduction and the set of stop words can reduce the number of index terms by about 27%.
NOT_RELEVANT;Web of Science;Machine learning based english-to-Korean transliteration using grapheme and phoneme information;"Oh, JH; Choi, KS";2005;10.1093/ietisy/e88-d.7.1737;NOT_FOUND;Machine transliteration is an automatic method to generate characters or words in one alphabetical system for the corresponding characters in another alphabetical system. Machine transliteration can play an important role in natural language application such as information retrieval and machine translation, especially for handling proper nouns and technical terms. The previous works focus on either a grapheme-based or phoneme-based method. However, transliteration is an orthographical and phonetic converting process. Therefore, both grapheme and phoneme information should be considered in machine transliteration. In this paper, we propose a grapheme and phoneme-based transliteration model and compare it with previous grapheme-based and phoneme-based models using several machine learning techniques. Our method shows about 13 similar to 78% performance improvement.
NOT_RELEVANT;Web of Science;CuentosIE: can a chatbot about tales with a messagehelp to teach emotional intelligence?;"Ferrández, A; Lavigne-Cerván, R; Peral, J; Navarro-Soria, I; Lloret, A; Gil, D; Rocamora, C";2024;10.7717/peerj-cs.1866;NOT_FOUND;In this article, we present CuentosIE (TalesEI: chatbot of tales with a message to develop Emotional Intelligence), an educational chatbot on emotions that also provides teachers and psychologists with a tool to monitor their students/patients through indicators and data compiled by CuentosIE. The use of tales with a messageis justified by their simplicity and easy understanding, thanks to their moral or associated metaphors. The main contributions of CuentosIE are the selection, collection, and classification of a set of highly specialized tales, as well as the provision of tools (searching, reading comprehension, chatting, recommending, and classifying) that are useful for both educating users about emotions and monitoring their emotional development. The preliminary evaluation of the tool has obtained encouraging results, which provides an affirmative answer to the question posed in the title of the article.
MAYBE_RELEVANT;Web of Science;Information retrieval systems adapted to the biomedical domain;"Marrero, M; Sánchez-Cuadrado, S; Urbano, J; Morato, J; Moreiro, JA";2010;10.3145/epi.2010.may.04;NOT_FOUND;The terminology used in biomedicine has lexical characteristics that have required the elaboration of terminological resources and information retrieval systems with specific functionalities. The main characteristics are the high rates of synonymy and homonymy, due to phenomena such as the pro-liferation of polysemic acronyms and their interaction with common language. Information retrieval systems in the biomedical domain use techniques oriented to the treatment of these lexical peculiarities. In this paper we review some of these techniques, such as the application of Natural Language Processing (BioNLP), the incorporation of lexical-semantic resources, and the application of Named Entity Recognition (BioNER). Finally, we present the evaluation methods adopted to assess the suitability of these techniques for retrieving biomedical resources.
NOT_RELEVANT;Web of Science;Aspects of Swedish morphology and semantics from the perspective of mono- and cross-language information retrieval;"Hedlund, T; Pirkola, A; Järvelin, K";2001;10.1016/S0306-4573(00)00024-8;NOT_FOUND;This paper analyzes the features of the Swedish language from the viewpoint of mono- and cross-language information retrieval (CLIR), The study was motivated by the fact that Swedish is known poorly from the IR perspective. This paper shows that Swedish has unique features, in particular gender features, the use of fogemorphemes in the formation of compound words, and a high frequency of homographic words. Especially in dictionary-based CLIR, correct word normalization and compound splitting are essential. It was shown in this study, however, that publicly available morphological analysis tools used for normalization and compound splitting have pitfalls that might decrease the effectiveness of TR and CLIR, A comparative study was performed to test the degree of lexical ambiguity in Swedish, Finnish and English. The results suggest that part-of-speech tagging might be useful in Swedish IR due to the high frequency of homographic words. (C) 2000 Elsevier Science Ltd. All rights reserved.
NOT_RELEVANT;Web of Science;Digital Support for Archaeology;"Boon, P; Van der Maaten, L; Paijmans, H; Postma, E; Lange, G";2009;10.1179/174327909X441108;NOT_FOUND;We describe an interdisciplinary approach in which computer scientists develop techniques to support archaeology. In the Reading Images for the Cultural Heritage ( RICH) project, a variety of methods have been developed to support archaeologists in the visualization, categorization, and characterization of archaeological objects, such as medieval glass, coins, ceramics, and seeds. The methods are based on image processing and machine learning algorithms that are tailored to the task at hand. We describe the algorithms and illustrate their application on archaeological datasets. The virtues and pitfalls of the interdisciplinary approach to archaeology are discussed.
MAYBE_RELEVANT;Web of Science;Retrieval-Based Diagnostic Decision Support: Mixed Methods Study;"Abdullahi, T; Mercurio, L; Singh, R; Eickhoff, C";2024;10.2196/50209;NOT_FOUND;Background: Diagnostic errors pose significant health risks and contribute to patient mortality. With the growing accessibility of electronic health records, machine learning models offer a promising avenue for enhancing diagnosis quality. Current research has primarily focused on a limited set of diseases with ample training data, neglecting diagnostic scenarios with limited data availability. Objective: This study aims to develop an information retrieval (IR)-based framework that accommodates data sparsity to facilitate broader diagnostic decision support. Methods: We introduced an IR-based diagnostic decision support framework called CliniqIR. It uses clinical text records, the Unified Medical Language System Metathesaurus, and 33 million PubMed abstracts to classify a broad spectrum of diagnoses independent of training data availability. CliniqIR is designed to be compatible with any IR framework. Therefore, we implemented it using both dense and sparse retrieval approaches. We compared CliniqIR's performance to that of pretrained clinical transformer models such as Clinical Bidirectional Encoder Representations from Transformers (ClinicalBERT) in supervised and zero-shot settings. Subsequently, we combined the strength of supervised fine-tuned ClinicalBERT and CliniqIR to build an ensemble Results: On a complex diagnosis data set (DC3) without any training data, CliniqIR models returned the correct diagnosis within their top 3 predictions. On the Medical Information Mart for Intensive Care III data set, CliniqIR models surpassed ClinicalBERT in predicting diagnoses with <5 training samples by an average difference in mean reciprocal rank of 0.10. In a zero-shot setting where models received no disease-specific training, CliniqIR still outperformed the pretrained transformer models with a greater mean reciprocal rank of at least 0.10. Furthermore, in most conditions, our ensemble framework surpassed the performance of its individual components, demonstrating its enhanced ability to make precise diagnostic predictions. Conclusions: Our experiments highlight the importance of IR in leveraging unstructured knowledge resources to identify infrequently encountered diagnoses. In addition, our ensemble framework benefits from combining the complementary strengths of the supervised and retrieval-based models to diagnose a broad spectrum of diseases.
NOT_RELEVANT;Web of Science;A NATURAL-LANGUAGE PROCESSING BASED GROUP DECISION-SUPPORT SYSTEM;"CONLON, SP; REITHEL, BJ; AIKEN, MW; SHIRANI, AI";1994;10.1016/0167-9236(94)90002-7;NOT_FOUND;Group Decision Support Systems (GDSSs) are currently used primarily as surface-level discussion tools. That is, current GDSSs do not allow group members to easily access information in deeper levels, such as the data base, model base, and application programs. This paper describes ways in which GDSSs can be improved by using a natural language interface to allow group members to communicate with deeper-level information systems using human languages. The system consists of database, model base, application programs and natural language interface system. The system is designed to both route questions to appropriate subsystems and translate these questions into the computer language controlling these subsystems. Finally, experimental results demonstrate the feasibility of the technique.
NOT_RELEVANT;Web of Science;Advances in Information Retrieval. Proceedings of the 32nd European Conference on IR Research (ECIR 2010);Thornley, C;2012;NOT_FOUND;NOT_FOUND;NOT_FOUND
NOT_RELEVANT;Web of Science;A Review on Large Language Models: Architectures, Applications, Taxonomies, Open Issues and Challenges;"Raiaan, MAK; Mukta, MSH; Fatema, K; Fahad, NM; Sakib, S; Mim, MMJ; Ahmad, J; Ali, ME; Azam, S";2024;10.1109/ACCESS.2024.3365742;NOT_FOUND;Large Language Models (LLMs) recently demonstrated extraordinary capability in various natural language processing (NLP) tasks including language translation, text generation, question answering, etc. Moreover, LLMs are new and essential part of computerized language processing, having the ability to understand complex verbal patterns and generate coherent and appropriate replies in a given context. Though this success of LLMs has prompted a substantial increase in research contributions, rapid growth has made it difficult to understand the overall impact of these improvements. Since a plethora of research on LLMs have been appeared within a short time, it is quite impossible to track all of these and get an overview of the current state of research in this area. Consequently, the research community would benefit from a short but thorough review of the recent changes in this area. This article thoroughly overviews LLMs, including their history, architectures, transformers, resources, training methods, applications, impacts, challenges, etc. This paper begins by discussing the fundamental concepts of LLMs with its traditional pipeline of the LLMs training phase. Then the paper provides an overview of the existing works, the history of LLMs, their evolution over time, the architecture of transformers in LLMs, the different resources of LLMs, and the different training methods that have been used to train them. The paper also demonstrates the datasets utilized in the studies. After that, the paper discusses the wide range of applications of LLMs, including biomedical and healthcare, education, social, business, and agriculture. The study also illustrates how LLMs create an impact on society and shape the future of AI and how they can be used to solve real-world problems. Finally, the paper also explores open issues and challenges to deploy LLMs in real-world scenario. Our review paper aims to help practitioners, researchers, and experts thoroughly understand the evolution of LLMs, pre-trained architectures, applications, challenges, and future goals.
NOT_RELEVANT;Web of Science;Automatic acquisition of morphological knowledge for medical language processing;"Zweigenbaum, P; Grabar, N";1999;NOT_FOUND;NOT_FOUND;Medical words exhibit a rich and productive morphology. Morphological knowledge is therefore very important for any medical language processing application. We propose a simple and powerful method to acquire automatically such knowledge. It takes advantage of commonly available lists of synonym terms to bootstrap the acquisition process. We experimented it on the SNOMED International Microglossary fur pathology in its French version. The families of morphologically related words that we obtained were useful for query expansion in a coding assistant. Since the method does not rely on a priori linguistic knowledge, it is applicable to other languages such as English.
NOT_RELEVANT;Web of Science;OIE4PA: open information extraction for the public administration;"Siciliani, L; Ghizzota, E; Basile, P; Lops, P";2024;10.1007/s10844-023-00814-z;NOT_FOUND;Tenders are powerful means of investment of public funds and represent a strategic development resource. Despite the efforts made so far by governments at national and international levels to digitalise documents related to the Public Administration sector, most of the information is still available in an unstructured format only. With the aim of bridging this gap, we present OIE4PA, our latest study on extracting and classifying relations from tenders of the Public Administration. Our work focuses on the Italian language, where the availability of linguistic resources to perform Natural Language Processing tasks is considerably limited. Nevertheless, OIE4PA adopts a multilingual approach so it can be applied to several languages by providing appropriate training data. Rather than purely training a classifier on a portion of the extracted relations, the backbone idea of our learning strategy is to put a supervised method based on self-training to the proof and to assess whether or not it improves the performance of the classifier. For evaluation purposes, we built a dataset composed of 2,000 triples which have been manually annotated by two human experts. The in-vitro evaluation shows that OIE4PA achieves a MacroF1\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$_1$$\end{document} equal to 0.89 and a 91%\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$\%$$\end{document} accuracy. In addition, OIE4PA was used as the pillar of a prototype search engine, which has been evaluated through an in-vivo experiment with positive feedback from 32 final users, obtaining a SUS score equal to 83.98.
NOT_RELEVANT;Web of Science;A comparative review of Urdu stemmers: Approaches and challenges;"Jabbar, A; ul Islam, S; Hussain, S; Akhunzada, A; Ilahi, M";2019;10.1016/j.cosrev.2019.100195;NOT_FOUND;With the advent of globalization epoch, the Internet-based resources for Urdu are increasing in depth and breadth at a higher pace than ever and thus require a mechanism for computational processing of Urdu text. Information retrieval (IR) systems have now become the major tool for seeking varied information on the web. It uses variant forms of the word transformed through stemmer. Broadly speaking, current Urdu stemmers can be categorized into two major categories: linguistic-based stemmers and statistical stemmers. In this paper, the authors explain the applications where stemming is used as a first step and highlight the challenges in Urdu text stemming. This is the first comparative study of the state-of-the-art Urdu stemmers, based on various distinct features such as used approach, main idea, limitations, the rules or affixes, data set, evaluation criteria and claimed accuracy. A comparative analysis, among state-of-the-art Urdu stemmers, is performed by using the standard data set. Finally, we outline the relevant research gaps in the literature and suggest recommendations for future research on Urdu text stemming. (C) 2019 Elsevier Inc. All rights reserved.
NOT_RELEVANT;Web of Science;DISCOURSE ANALYSIS FOR A LEGAL EXPERT SYSTEM;"LEHMANN, H; GUENTHNER, F";1991;10.1007/BF00124145;NOT_FOUND;This paper deals with discourse analysis, with specific reference to the Linguistic and Logic Based Legal Expert System, LEX. In the LEX project we concentrated on a few arbitrarily selected court decisions, extracted the case descriptions, and then added the necessary background knowledge to our prototype expert system to analyze the case descriptions and to deduce the answers to some juridical questions. In this paper we present and comment on a typical discourse representation structure for an accident description in the corpus we studied.
NOT_RELEVANT;Web of Science;A customised grammar framework for query classification;"Mohasseb, A; Bader-El-Den, M; Cocea, M";2019;10.1016/j.eswa.2019.06.010;NOT_FOUND;In real-life classification problems, prior information about the problem and expert knowledge about the domain are often used to obtain reliable and consistent solutions. This is especially true in fields where the data is ambiguous, such as text, in which the same words can be used in seemingly similar texts, but have a different meaning. A promising avenue for text classification is machine learning, which has been shown to perform well in a variety of applications including query classification and sentiment analysis. Many of the proposed approaches rely on the bag-of-words representation, which loses the information about the structure of the text. In this paper, we propose a Customised Grammar Framework for text classification, which exploits domain-related information and a new way to represent text as a series of syntactic categories forming syntactic patterns. The framework employs a formal grammar approach for transforming the text into the syntactic patterns representation. We applied the framework for the query classification problem and our results show that our approach outperforms previous ones in terms of classification performance. (C) 2019 Published by Elsevier Ltd.
NOT_RELEVANT;Web of Science;Quantum Entanglement in Corpuses of Documents;"Beltran, L; Geriente, S";2019;10.1007/s10699-018-9570-2;NOT_FOUND;We show that data collected from corpuses of documents violate the Clauser-Horne-Shimony-Holt version of Bell's inequality (CHSH inequality) and therefore indicate the presence of quantum entanglement in their structure. We obtain this result by considering two concepts and their combination and coincidence operations consisting of searches of co-occurrences of exemplars of these concepts in specific corpuses of documents. Measuring the frequencies of these co-occurrences and calculating the relative frequencies as approximate probabilities entering in the CHSH inequality, we obtain manifest violations of the latter for all considered corpuses of documents. In comparing these violations with those analogously obtained in an earlier work for the same combined concepts in psychological coincidence experiments with human participants, also violating the CHSH inequality, we identify the entanglement as being carried by the meaning connection between the two considered concepts within the combination they form. We explain the stronger violation for the corpuses of documents, as compared to the violation in the psychology experiments, as being due to the superior meaning domain of the human mind and, on the other side, to the latter reaching a broader domain of meaning and being possibly also actively influenced during the experimentation. We mention some of the issues to be analyzed in future work such as the violations of the CHSH inequality being larger than the Cirel'son bound' for all of the considered corpuses of documents.
NOT_RELEVANT;Web of Science;Biomedical named entity recognition and linking datasets: survey and our recent development;"Huang, MS; Lai, PT; Lin, PY; You, YT; Tsai, RTH; Hsu, WL";2020;10.1093/bib/bbaa054;NOT_FOUND;"Natural language processing (NLP) is widely applied in biological domains to retrieve information from publications. Systems to address numerous applications exist, such as biomedical named entity recognition (BNER), named entity normalization (NEN) and protein-protein interaction extraction (PPIE). High-quality datasets can assist the development of robust and reliable systems; however, due to the endless applications and evolving techniques, the annotations of benchmark datasets may become outdated and inappropriate. In this study, we first review commonlyused BNER datasets and their potential annotation problems such as inconsistency and low portability. Then, we introduce a revised version of the JNLPBA dataset that solves potential problems in the original and use state-of-the-art named entity recognition systems to evaluate its portability to different kinds of biomedical literature, including protein-protein interaction and biology events. Lastly, we introduce an ensembled biomedical entity dataset (EBED) by extending the revised JNLPBA dataset with PubMed Central full-text paragraphs, figure captions and patent abstracts. This EBED is a multi-task dataset that covers annotations including gene, disease and chemical entities. In total, it contains 85000 entity mentions, 25000 entity mentions with database identifiers and 5000 attribute tags. To demonstrate the usage of the EBED, we review the BNER track from the AI CUP Biomedical Paper Analysis challenge."
NOT_RELEVANT;Web of Science;A Method to Detect Chorus Sections in Lyrics Text;"Watanabe, K; Goto, M";2023;10.1587/transinf.2022EDP7139;NOT_FOUND;"This paper addresses the novel task of detecting chorus sections in English and Japanese lyrics text. Although chorus-section detection using audio signals has been studied, whether chorus sections can be detected from text-only lyrics is an open issue. Another open issue is whether patterns of repeating lyric lines such as those appearing in chorus sections depend on language. To investigate these issues, we propose a neural-network-based model for sequence labeling. It can learn phrase repetition and linguistic features to detect chorus sections in lyrics text. It is, however, difficult to train this model since there was no dataset of lyrics with chorus-section annotations as there was no prior work on this task. We therefore generate a large amount of training data with such annotations by leveraging pairs of musical audio signals and their corresponding manually time-aligned lyrics; we first automatically detect chorus sections from the audio signals and then use their temporal positions to transfer them to the line-level chorus-section annotations for the lyrics. Experimental results show that the proposed model with the generated data contributes to detecting the chorus sections, that the model trained on Japanese lyrics can detect chorus sections surprisingly well in English lyrics, and that patterns of repeating lyric lines are language-independent."
NOT_RELEVANT;Web of Science;GIS-KG: building a large-scale hierarchical knowledge graph for geographic information science;"Du, JX; Wang, SH; Ye, XY; Sinton, DS; Kemp, K";2022;10.1080/13658816.2021.2005795;NOT_FOUND;An organized knowledge base can facilitate the exploration of existing knowledge and the detection of emerging topics in a domain. Knowledge about and around Geographic Information Science and its associated system technologies (GIS) is complex, extensive and emerging rapidly. Taking the challenge, we built a GIS knowledge graph (GIS-KG) by (1) merging existing GIS bodies of knowledge to create a hierarchical ontology and then (2) applying deep-learning methods to map GIS publications to the ontology. We conducted several experiments on information retrieval to evaluate the novelty and effectiveness of the GIS-KG. Results showed the robust support of GIS-KG for knowledge search of existing GIS topics and potential to explore emerging research themes.
NOT_RELEVANT;Web of Science;Query expansion with a medical ontology to improve a multimodal information retrieval system;"Díaz-Galiano, MC; Martín-Valdivia, MT; Ureña-López, LA";2009;10.1016/j.compbiomed.2009.01.012;NOT_FOUND;Searching biomedical information in a large collection of medical data is a complex task. The use of tools and biomedical resources could ease the retrieval of the information desired. In this paper, we use the medical ontology MeSH to improve a Multimodal Information Retrieval System by expanding the user's query with medical terms. in order to accomplish our experiments, we have used the dataset provided by ImageCLEFmed task organizers for years 2005 and 2006. This dataset is composed of a multimodal collection (images and text) of clinical cases. a list of queries for each year, and a list of relevance judgments for each query to evaluate the results. The results from the experiments show that the use of a medical ontology to expand the queries greatly improves the results. Crown Copyright (c) 2009 Published by Elsevier Ltd. All rights reserved.
NOT_RELEVANT;Web of Science;Searching for musical features using natural language queries: the C@merata evaluations at MediaEval;"Sutcliffe, R; Hovy, E; Collins, T; Wan, S; Crawford, T; Root, DL";2019;10.1007/s10579-018-9422-2;NOT_FOUND;Musicological texts about classical music frequently include detailed technical discussions concerning the works being analysed. These references can be specific (e.g. C sharp in the treble clef) or general (fugal passage, Thor's Hammer). Experts can usually identify the features in question in music scores but a means of performing this task automatically could be very useful for experts and beginners alike. Following work on textual question answering over many years as co-organisers of the QA tasks at the Cross Language Evaluation Forum, we decided in 2013 to propose a new type of task where the input would be a natural language phrase, together with a music score in MusicXML, and the required output would be one or more matching passages in the score. We report here on 3years of the C@merata task at MediaEval. We describe the design of the task, the evaluation methods we devised for it, the approaches adopted by participant systems and the results obtained. Finally, we assess the progress which has been made in aligning natural language text with music and map out the main steps for the future. The novel aspects of this work are: (1) the task itself, linking musical references to actual music scores, (2) the evaluation methods we devised, based on modified versions of precision and recall, applied to demarcated musical passages, and (3) the progress which has been made in analysing and interpreting detailed technical references to music within texts.
NOT_RELEVANT;Web of Science;Application of Natural Language Processing and Network Analysis Techniques to Post-market Reports for the Evaluation of Dose-related Anti-Thymocyte Globulin Safety Patterns;"Botsis, T; Foster, M; Arya, N; Kreimeyer, K; Pandey, A; Arya, D";2017;10.4338/ACI-2016-10-RA-0169;NOT_FOUND;Objective: To evaluate the feasibility of automated dose and adverse event information retrieval in supporting the identification of safety patterns. Methods: We extracted all rabbit Anti-Thymocyte Globulin (rATG) reports submitted to the United States Food and Drug Administration Adverse Event Reporting System (FAERS) from the product's initial licensure in April 16, 1984 through February 8, 2016. We processed the narratives using the Medication Extraction (MedEx) and the Event-based Text-mining of Health Electronic Records (ETHER) systems and retrieved the appropriate medication, clinical, and temporal information. When necessary, the extracted information was manually curated. This process resulted in a high quality dataset that was analyzed with the Pattern-based and Advanced Network Analyzer for Clinical Evaluation and Assessment (PANACEA) to explore the association of rATG dosing with post-transplant lymphoproliferative disorder (PTLD). Results: Although manual curation was necessary to improve the data quality, MedEx and ETHER supported the extraction of the appropriate information. We created a final dataset of 1,380 cases with complete information for rATG dosing and date of administration. Analysis in PANACEA found that PTLD was associated with cumulative doses of rATG > 8 mg/kg, even in periods where most of the submissions to FAERS reported low doses of rATG. Conclusion: We demonstrated the feasibility of investigating a dose-related safety pattern for a particular product in FAERS using a set of automated tools.
NOT_RELEVANT;Web of Science;Clinical language search algorithm from free-text: facilitating appropriate imaging;"Chaudhari, GR; Chillakuru, YR; Chen, TL; Pedoia, V; Vu, TH; Hess, CP; Seo, YH; Sohn, JH";2022;10.1186/s12880-022-00740-6;NOT_FOUND;Background The comprehensiveness and maintenance of the American College of Radiology (ACR) Appropriateness Criteria (AC) makes it a unique resource for evidence-based clinical imaging decision support, but it is underutilized by clinicians. To facilitate the use of imaging recommendations, we develop a natural language processing (NLP) search algorithm that automatically matches clinical indications that physicians write into imaging orders to appropriate AC imaging recommendations. Methods We apply a hybrid model of semantic similarity from a sent2vec model trained on 223 million scientific sentences, combined with term frequency inverse document frequency features. AC documents are ranked based on their embeddings' cosine distance to query. For model testing, we compiled a dataset of simulated simple and complex indications for each AC document (n = 410) and another with clinical indications from randomly sampled radiology reports (n = 100). We compare our algorithm to a custom google search engine. Results On the simulated indications, our algorithm ranked ground truth documents as top 3 for 98% of simple queries and 85% of complex queries. Similarly, on the randomly sampled radiology report dataset, the algorithm ranked 86% of indications with a single match as top 3. Vague and distracting phrases present in the free-text indications were main sources of errors. Our algorithm provides more relevant results than a custom Google search engine, especially for complex queries. Conclusions We have developed and evaluated an NLP algorithm that matches clinical indications to appropriate AC guidelines. This approach can be integrated into imaging ordering systems for automated access to guidelines.
MAYBE_RELEVANT;Web of Science;Answering engineers' questions using semantic annotations;"Kim, S; Bracewell, RH; Wallace, KM";2007;10.1017/S0890060407070205;NOT_FOUND;Question-answering (QA) systems have proven to be helpful, especially to those who feel uncomfortable entering keywords, sometimes extended with search symbols such as +, *, and so forth. In developing such systems, the main focus has been on the enhanced retrieval performance of searches, and recent trends in QA systems center on the extraction of exact answers. However, when their usability was evaluated, some users indicated that they found it difficult to accept the answers because of the absence of supporting context and rationale. Current approaches to address this problem include providing answers with linking paragraphs or with summarizing extensions. Both methods are believed to be sufficient to answer questions seeking the names of objects or quantities that have only a single answer, However, neither method addresses the situation when an answer requires the comparison and integration of information appearing in multiple documents or in several places in a single document. This paper argues that coherent answer generation is crucial for such questions, and that the key to this coherence is to analyze texts to a level beyond sentence annotations. To demonstrate this idea, a prototype has been developed based on rhetorical structure theory, and a preliminary evaluation has been carried out. The evaluation indicates that users prefer to see the extended answers that can be generated using such semantic annotations, provided that additional context and rationale information are made available.
NOT_RELEVANT;Web of Science;Content analysis-based documentation and exploration of research articles;Phyo, SS;2022;10.1108/DTA-07-2020-0146;NOT_FOUND;"Purpose With the wealth of information available on the World Wide Web, it is difficult for anyone from a general user to the researcher to easily fulfill their information need. The main challenge is to categorize the documents systematically and also take into account more valuable data such as semantic information. The purpose of this paper is to develop a concept-based search system that leverages the external knowledge resources as the background knowledge for getting the accurate and efficient meaningful search results. Design/methodology/approach The paper introduces the approach which is based on formal concept analysis (FCA) with the semantic information to support the document management in information retrieval (IR). To describe the semantic information of the documents, the system uses the popular knowledge resources WordNet and Wikipedia. By using FCA, the system creates the concept lattice as the concept hierarchy of the document and proposes the navigation algorithm for retrieving the hierarchy based on the user query. Findings The semantic information of the document is based on the two external popular knowledge resources; the authors find that it will be more efficient to deal with the semantic mismatch problems of user need. Originality/value The navigation algorithm proposed in this research is applied to the scientific articles of the National Science Foundation (NSF). The proposed system can enhance the integration and exploration of the scientific articles for the advancement of the Scientific and Engineering Research Community."
NOT_RELEVANT;Web of Science;Learning the Multilingual Translation Representations for Question Retrieval in Community Question Answering via Non-Negative Matrix Factorization;"Zhou, GY; Xie, ZW; He, TT; Zhao, J; Hu, XT";2016;10.1109/TASLP.2016.2544661;NOT_FOUND;Community question answering (CQA) has become an increasingly popular research topic. In this paper, we focus on the problem of question retrieval. Question retrieval in CQA can automatically find the most relevant and recent questions that have been solved by other users. However, the word ambiguity and word mismatch problems bring about new challenges for question retrieval in CQA. State-of-the-art approaches address these issues by implicitly expanding the queried questions with additional words or phrases using monolingual translation models. While useful, the effectiveness of these models is highly dependent on the availability of quality parallel monolingual corpora (e. g., question-answer pairs) in the absence of which they are troubled by noise issues. In this work, we propose an alternative way to address the word ambiguity and word mismatch problems by taking advantage of potentially rich semantic information drawn from other languages. Our proposed method employs statistical machine translation to improve question retrieval and enriches the question representation with the translated words from other languages via non-negative matrix factorization. Experiments conducted on real CQA data sets show that our proposed approach is promising.
NOT_RELEVANT;Web of Science;Rhetorical Structure Theory for polarity estimation: An experimental study;"Chenlo, JM; Hogenboom, A; Losada, DE";2014;10.1016/j.datak.2014.07.009;NOT_FOUND;Sentiment analysis tools often rely on counts of sentiment-carrying words, ignoring structural aspects of content. Natural Language Processing has been fruitfully exploited in text mining, but advanced discourse processing is still nonpervasive for mining opinions. Some studies, however, extracted opinions based on the discursive role of text segments. The merits of such computationally intensive analyses have thus far been assessed in very specific, small-scale scenarios. In this paper, we investigate the usefulness of Rhetorical Structure Theory in various sentiment analysis tasks on different types of information sources. First, we demonstrate how to perform a large-scale ranking of individual blog posts in terms of their overall polarity, by exploiting the rhetorical structure of a few key evaluative sentences. In order to further validate our findings, we additionally explore the potential of Rhetorical Structure Theory in sentence-level polarity classification of news and product reviews. Our most valuable polarity classification features turn out to capture the way in which polar terms are used, rather than the sentiment-carrying words per se. (C) 2014 Elsevier B.V. All rights reserved.
NOT_RELEVANT;Web of Science;Event graphs for information retrieval and multi-document summarization;"Glavas, G; Snajder, J";2014;10.1016/j.eswa.2014.04.004;NOT_FOUND;With the number of documents describing real-world events and event-oriented information needs rapidly growing on a daily basis, the need for efficient retrieval and concise presentation of event-related information is becoming apparent. Nonetheless, the majority of information retrieval and text summarization methods rely on shallow document representations that do not account for the semantics of events. In this article, we present event graphs, a novel event-based document representation model that filters and structures the information about events described in text. To construct the event graphs, we combine machine learning and rule-based models to extract sentence-level event mentions and determine the temporal relations between them. Building on event graphs, we present novel models for information retrieval and multi-document summarization. The information retrieval model measures the similarity between queries and documents by computing graph kernels over event graphs. The extractive multi-document summarization model selects sentences based on the relevance of the individual event mentions and the temporal structure of events. Experimental evaluation shows that our retrieval model significantly outperforms well-established retrieval models on event-oriented test collections, while the summarization model outperforms competitive models from shared multi-document summarization tasks. (C) 2014 Elsevier Ltd. All rights reserved.
NOT_RELEVANT;Web of Science;What identifies different age cohorts in Yahoo! Answers?;"Figueroa, A; Timilsina, M";2021;10.1016/j.knosys.2021.107278;NOT_FOUND;For different kinds of online platforms, understanding demographics has shown to be instrumental in improving user experience, especially for personalizing and contextualizing content. Needless to say, there has been a number of studies delving into demographics in online social media platforms including Facebook and Twitter. However, only a mere handful of works have explored demographic factors behind community question-answering platforms despite their massive amount of members. For this reason, we decided to undertake a study of Yahoo! Answers members, namely as it relates to age demographics. To this end, we automatically built and annotated a large-scale corpus comprising metadata and textual inputs produced by ca. 650,000 community fellows. We profit from this collection by conducting both an exploratory/statistical analysis and predictive modelling. In the former, we explored the correlation between distinct age groups and some variables that, intuitively, can seem to be highly correlated with some cohorts. Interestingly enough, this analysis revealed that Millennials are answering questions prompted by their succeeding age group (GEN Z). In the latter, we assessed the prediction rate of various traditional statistical methods and neural networks classifiers coupled with numerous combinations of assorted textual and metadata features. Overall, best classifiers finished with an MRR of up to 0.862, and were modelled by means of FastText and Maximum Entropy (MaxEnt). In terms of informative attributes, user asking/answering activity patterns and sentimentally charged words provide telltale clues about which age group a community peer belongs to. (C) 2021 Elsevier B.V. All rights reserved.
NOT_RELEVANT;Web of Science;Geographical localization of web domains and organization addresses recognition by employing natural language processing, Pattern Matching and clustering;"Nesi, P; Pantaleo, G; Tenti, M";2016;10.1016/j.engappai.2016.01.011;NOT_FOUND;Nowadays, the World Wide Web is growing at increasing rate and speed, and consequently the online available resources populating Internet represent a large source of knowledge for various business and research interests. For instance, over the past years, increasing attention has been focused on retrieving information related to geographical location of places and entities, which is largely contained in web pages and documents. However, such resources are represented in a wide variety of generally unstructured formats, and this actually does not help final users to find desired information items. The automatic annotation and comprehension of toponyms, location names and addresses (at different resolution and granularity levels) can deliver significant benefits for the whole web community by improving search engines filtering capabilities and intelligent data mining systems. The present paper addresses the problem of gathering geographical information from unstructured text in web pages and documents. In the specific, the proposed method aims at extracting geographical location (at street number resolution) of commercial companies and services, by annotating geo-related information from their web domains. The annotation process is based on Natural Language Processing (NLP) techniques for text comprehension, and relies on Pattern Matching and Hierarchical Cluster Analysis for recognizing and dis-ambiguating geographical entities. Geotagging performances have been assessed by evaluating Precision, Recall and F-Measure of the proposed system output (represented in form of semantic RDF triples) against both a geo-annotated reference database and a semantic Smart City repository. (C) 2016 Elsevier Ltd. All rights reserved.
NOT_RELEVANT;Web of Science;Decoding peak emotional responses to music from computational acoustic and lyrical features;Mori, K;2022;10.1016/j.cognition.2021.105010;NOT_FOUND;Music can evoke strong emotions. Research has suggested that pleasurable chills (shivering) and tears (weeping) are peak emotional responses to music. The present study examines whether computational acoustic and lyrical features can decode chills and tears. The experiment comprises 186 pieces of self-selected music to evoke emotional responses from 54 Japanese participants. Machine learning analysis with L2-norm-regularization regression revealed the decoding accuracy and specified well-defined features. In Study 1, time-series acoustic features significantly decoded emotional chills, tears, and the absence of chills or tears by using information within a few seconds before and after the onset of the three responses. The classification results showed three significant periods, indicating that complex anticipation-resolution mechanisms lead to chills and tears. Evoking chills was particularly associated with rhythm uncertainty, while evoking tears was related to harmony. Violating rhythm expectancy may have been a trigger for chills, while the harmonious overlapping of acoustic spectra may have played a role in evoking tears. In Study 2, acoustic and lyrical features from the entire piece decoded tears but not chill frequency. Mixed emotions stemming from happiness were associated with major chords, while lyric content related to sad farewells can contribute to the prediction of emotional tears, indicating that distinctive emotions in music may evoke a tear response. When considered in tandem with theoretical studies, the violation of rhythm may biologically boost both the pleasure- and fight-related physiological response of chills, whereas tears may be evolutionarily embedded in the social bonding effect of musical harmony and play a unique role in emotional regulation.
NOT_RELEVANT;Web of Science;Modeling and Learning Distributed Word Representation with Metadata for Question Retrieval;"Zhou, GY; Huang, JXJ";2017;10.1109/TKDE.2017.2665625;NOT_FOUND;Community question answering (cQA) has become an important issue due to the popularity of cQA archives on the Web. This paper focuses on addressing the lexical gap problem in question retrieval. Question retrieval in cQA archives aims to find the existing questions that are semantically equivalent or relevant to the queried questions. However, the lexical gap problem brings a new challenge for question retrieval in cQA. In this paper, we propose to model and learn distributed word representations with metadata of category information within cQA pages for question retrieval using two novel category powered models. One is a basic category powered model called MB-NET and the other one is an enhanced category powered model called ME-NET which can better learn the distributed word representations and alleviate the lexical gap problem. To deal with the variable size of word representation vectors, we employ the framework of fisher kernel to transform them into the fixed-length vectors. Experimental results on large-scale English and Chinese cQA data sets show that our proposed approaches can significantly outperform state-of-the-art retrieval models for question retrieval in cQA. Moreover, we further conduct our approaches on large-scale automatic evaluation experiments. The evaluation results show that promising and significant performance improvements can be achieved.
NOT_RELEVANT;Web of Science;Document Retrieval System for Biomedical Question Answering;"Bolat, H; Sen, B";2024;10.3390/app14062613;NOT_FOUND;Featured Application In the biomedical field, accessing data by classical methods is getting more difficult day by day, as it is in any other field, due to the data growth rate. Different methods are needed to access the desired data more quickly. In particular, more specific methods need to be developed for question answering systems. In this study, a model is proposed for the document retrieval and answer extraction modules which are a part of biomedical question answering systems.Abstract In this paper, we describe our biomedical document retrieval system and answers extraction module, which is part of the biomedical question answering system. Approximately 26.5 million PubMed articles are indexed as a corpus with the Apache Lucene text search engine. Our proposed system consists of three parts. The first part is the question analysis module, which analyzes the question and enriches it with biomedical concepts related to its wording. The second part of the system is the document retrieval module. In this step, the proposed system is tested using different information retrieval models, like the Vector Space Model, Okapi BM25, and Query Likelihood. The third part is the document re-ranking module, which is responsible for re-arranging the documents retrieved in the previous step. For this study, we tested our proposed system with 6B training questions from the BioASQ challenge task. We obtained the best MAP score on the document retrieval phase when we used Query Likelihood with the Dirichlet Smoothing model. We used the sequential dependence model at the re-rank phase, but this model produced a worse MAP score than the previous phase. In similarity calculation, we included the Named Entity Recognition (NER), UMLS Concept Unique Identifiers (CUI), and UMLS Semantic Types of the words in the question to find the sentences containing the answer. Using this approach, we observed a performance enhancement of roughly 25% for the top 20 outcomes, surpassing another method employed in this study, which relies solely on textual similarity.
NOT_RELEVANT;Web of Science;Swat: A system for detecting salient Wikipedia entities in texts;"Ponza, M; Ferragina, P; Piccinno, F";2019;10.1111/coin.12216;NOT_FOUND;We study the problem of entity salience by proposing the design and implementation of Swat, a system that identifies the salient Wikipedia entities occurring in an input document. Swat consists of several modules that are able to detect and classify on-the-fly Wikipedia entities as salient or not, based on a large number of syntactic, semantic, and latent features properly extracted via a supervised process, which has been trained over millions of examples drawn from the New York Times corpus. The validation process is performed through a large experimental assessment, eventually showing that Swat improves known solutions over all publicly available datasets. We release Swat via an API that we describe and comment in the paper to ease its use in other software.
NOT_RELEVANT;Web of Science;An evidence-based iterative content trust algorithm for the credibility of online news;"Zeng, GS; Wang, W";2009;10.1002/cpe.1415;NOT_FOUND;People encounter more information than they can possibly use every day. But all information is not necessarily of equal value. In many cases, certain information appears to be better, or more trustworthy, than other information. And the challenge that most people then face is to judge which information is more credible. In this paper we propose a new problem called Corroboration Trust, which studies how to find credible news events by seeking more than one source to verify information on a given topic. We design an evidence-based corroboration trust algorithm called TrustNewsFinder, which utilizes the relationships between news articles and related evidence information (person, location, time and keywords about the news). A news article is trustworthy if it provides many pieces of trustworthy evidence, and a piece of evidence is likely to be true if it is provided by many trustworthy news articles. Our experiments show that TrustNewsFinder successfully finds true events among conflicting information and identifies trustworthy news better than the popular search engines. Copyright (C) 2009 John Wiley & Sons, Ltd.
NOT_RELEVANT;Web of Science;Quantitative similarity assessment of construction projects using WBS-based metrics;"Torkanfar, N; Azar, ER";2020;10.1016/j.aei.2020.101179;NOT_FOUND;Lessons learned from completed projects are valuable resources for planning of new projects. A quantitative similarity measurement between construction projects can improve knowledge reuse practices. The information and documents of a similar past project can be retrieved to resolve the challenges in a new project. This paper introduces a novel method for measuring the similarity of construction projects based on semantic comparison of their work breakdown structure (WBS). WBS of a project should theoretically encompass a hierarchical decomposition of the total scope of project's works, thus it could be used as an appropriate representative of the projects. The proposed method measures the semantic similarity between WBS of projects by means of natural language processing techniques. This method was implemented based on three metrics: node, structural, and total similarity. Each of these metrics calculate a quantitative similarity score between 0 and 1. The method was assessed using fifteen test samples with promising results in compliance with similarity properties. In addition, precision and recall of the method were evaluated in retrieving similar past projects. The results illustrate that the structural similarity slightly outperforms the other metrics.
NOT_RELEVANT;Web of Science;Integrating discriminant and descriptive information for dimension reduction and classification;"Yu, J; Tian, Q; Rui, T; Huang, TS";2007;10.1109/TCSVT.2007.890861;NOT_FOUND;In this paper, a novel hybrid dimension reduction technique for classification is proposed based on the hybrid analysis of principal component analysis (PCA) and linear discriminant analysis (LDA). LDA is known for capturing the most discriminant features of the data in the projected space while PCA is known for preserving the most descriptive ones after projection. Our hybrid technique integrates discriminant and descriptive information and finds a richer set of alternatives beyond LDA and PCA in a 2-D parametric space, which fits a specific classification task and data distribution better. Theoretical study shows that our technique also alleviates the singularity problem of scatter matrix, which is caused by small training set, and increases the effective dimension of the projected subspace. In order to find the hybrid features adaptively and avoid exhaustive parameter searching, we further propose a boosted hybrid analysis method that incorporates a nonlinear boosting process to enhance a set of hybrid classifiers and combine them into a more accurate one. Compared with the other techniques that aim at combining PCA and LDA, our approaches are novel because our method finds alternatives to LDA and PCA in a 2-D parameter space and the boosting process provides enhancement and robust combination of the classifiers. Extensive experiments are conducted on benchmark and real,image databases to compare our proposed methods with the state-of-the-art linear and nonlinear discriminant analysis techniques. The results show the superior performance of our hybrid analysis methods.
MAYBE_RELEVANT;Web of Science;Dipe-D: A tool for knowledge-based query formulation in information retrieval;Van der Pol, R;2003;10.1023/A:1022944313947;NOT_FOUND;"The paper reports the development of Dipe-D, a knowledge-based procedure for the formulation of Boolean queries in information retrieval. Dipe-D creates a query in two steps: ( 1) the user's information need is developed interactively, while identifying the concepts of the information need, and subsequently ( 2) the collection of concepts identified is automatically transformed into a Boolean query. In the first step, the subject area - as represented in a knowledge base - is explored by the user. He does this by means of specifying the ( concepts that meet his) information need in an artificial language and looking through the solution as provided by the computer. The specification language allows one to specify concepts by their features, both in precise terms as well as vaguely. By repeating the process of specifying the information need and exploring the resulting concepts, the user may precisely single out the concepts that describe his information need. In the second step, the program provides the designations ( and variants) for the concepts identified, and connects them by appropriate operators. Dipe-D is meant to improve on existing procedures that identify the concepts less systematically, create a query manually, and then sometimes expand that query. Experiments are reported on each of the two steps; they indicate that the first step identifies only but not all the relevant concepts, and the second step performs ( at least) as good as human beings do."
NOT_RELEVANT;Web of Science;On metonymy recognition for geographic information retrieval;"Leveling, J; Hartrumpf, S";2008;10.1080/13658810701626244;NOT_FOUND;Metonymically used location names (toponyms) refer to other, related entities and thus possess a meaning different from their literal, geographic sense. Metonymic uses are to be treated differently to improve the performance of geographic information retrieval (GIR). Statistics on toponym senses show that 75.06% of all location names are used in their literal sense, 17.05% are used metonymically, and 7.89% have a mixed sense. This article presents a method for disambiguating location names in texts between literal and metonymic senses, based on shallow features. The evaluation of this method is two-fold. First, we use a memory-based learner (TiMBL) to train a classifier and determine standard evaluation measures such as F-score and accuracy. The classifier achieved an F-score of 0.842 and an accuracy of 0.846 for identifying toponym senses in a subset of the CoNLL (Conference on Natural Language Learning) data. Second, we perform retrieval experiments based on the GeoCLEF data (newspaper article corpus and queries) from 2005 and 2006. We compare searching location names in a database index containing both their literal and metonymic senses with searching in an index containing their literal senses only. Evaluation results indicate that removing metonymic senses from the index yields a higher mean average precision (MAP) for GIR. In total, we observed a significant gain in MAP: an increase from 0.0704 to 0.0715 MAP for the GeoCLEF 2005 data, and an increase from 0.1944 to 0.2100 MAP for the GeoCLEF 2006 data.
NOT_RELEVANT;Web of Science;Semantic structures of timbre emerging from social and acoustic descriptions of music;"Ferrer, R; Eerola, T";2011;10.1186/1687-4722-2011-11;NOT_FOUND;The perceptual attributes of timbre have inspired a considerable amount of multidisciplinary research, but because of the complexity of the phenomena, the approach has traditionally been confined to laboratory conditions, much to the detriment of its ecological validity. In this study, we present a purely bottom-up approach for mapping the concepts that emerge from sound qualities. A social media (http://www.last.fm) is used to obtain a wide sample of verbal descriptions of music (in the form of tags) that go beyond the commonly studied concept of genre, and from this the underlying semantic structure of this sample is extracted. The structure that is thereby obtained is then evaluated through a careful investigation of the acoustic features that characterize it. The results outline the degree to which such structures in music (connected to affects, instrumentation and performance characteristics) have particular timbral characteristics. Samples representing these semantic structures were then submitted to a similarity rating experiment to validate the findings. The outcome of this experiment strengthened the discovered links between the semantic structures and their perceived timbral qualities. The findings of both the computational and behavioural parts of the experiment imply that it is therefore possible to derive useful and meaningful structures from free verbal descriptions of music, that transcend musical genres, and that such descriptions can be linked to a set of acoustic features. This approach not only provides insights into the definition of timbre from an ecological perspective, but could also be implemented to develop applications in music information research that organize music collections according to both semantic and sound qualities.
MAYBE_RELEVANT;AIS;AI in Universities and Libraries;"Aaron TAY; Aaron Tay";2023;http://ink.library.smu.edu.sg/library_research;http://ink.library.smu.edu.sg/library_research/212;NOT_FOUND
NOT_RELEVANT;AIS;SEMANTIC-FIRST DECISION TREE NATURALIZATION FOR ENHANCED CONVERSATIONAL ARTIFICIAL INTELLIGENCE;James J. Teeter II, Ph.D.;2024;http://www.tdcommons.org/dpubs_series;http://www.tdcommons.org/dpubs_series/7234;NOT_FOUND
NOT_RELEVANT;AIS;Retrospective Analysis and Prediction: Artificial Intelligence and Its Applications in Libraries;Ping Fu;2018;http://digitalcommons.cwu.edu/libraryfac;http://digitalcommons.cwu.edu/libraryfac/58;NOT_FOUND
NOT_RELEVANT;AIS;Emerging Multidisciplinary Research Across Database Management Systems;"Anisoara Nica; Fabian M. Suchanek; Aparna Varde";2010;http://digitalcommons.montclair.edu/compusci-facpubs;http://digitalcommons.montclair.edu/compusci-facpubs/253;NOT_FOUND
NOT_RELEVANT;AIS;Law, Artificial Intelligence, and Natural Language Processing: A Funny Thing Happened on the Way to My Search Results;Paul D. Callister;2020;http://irlaw.umkc.edu/faculty_works;http://irlaw.umkc.edu/faculty_works/24;NOT_FOUND
NOT_RELEVANT;AIS;ChatGPT's Epoch in Rheumatological Diagnostics: A Critical Assessment in the Context of Sjögren's Syndrome.;"Bilal Irfan; Aneela Yaqoob";2023;http://scholarlyworks.beaumont.org/infectious_diseases_articles;http://scholarlyworks.beaumont.org/infectious_diseases_articles/66;NOT_FOUND
NOT_RELEVANT;AIS;Leveraging Generative AI and Large Language Models: A Comprehensive Roadmap for Healthcare Integration;"Ping Yu; Hua Xu; Xia Hu; Chao Deng";2023;http://ro.uow.edu.au/test2021;http://ro.uow.edu.au/test2021/9753;NOT_FOUND
NOT_RELEVANT;AIS;Language Modeling Approaches to Information Retrieval;"Protima Banerjee; Hyoil Han";2009;http://mds.marshall.edu/wdcs_faculty;http://mds.marshall.edu/wdcs_faculty/4;NOT_FOUND
NOT_RELEVANT;AIS;Explainable Semantic Retrieval Using Dual Encoder Large Language Models;"Marco Bonechi; IP Spring LLC";2024;http://www.tdcommons.org/dpubs_series;http://www.tdcommons.org/dpubs_series/6676;NOT_FOUND
MAYBE_RELEVANT;AIS;Large language model powered agents for information retrieval;"An ZHANG; Yang DENG; Yankai LIN; Xu CHEN; Ji-Rong WEN; Tat-Seng CHUA";2024;http://ink.library.smu.edu.sg/sis_research;http://ink.library.smu.edu.sg/sis_research/9104;NOT_FOUND
NOT_RELEVANT;AIS;Ontology based semantic annotation of Urdu language web documents;Quratulain Rajput;2014;http://ir.iba.edu.pk/faculty-research-series;http://ir.iba.edu.pk/faculty-research-series/31;NOT_FOUND
NOT_RELEVANT;AIS;Generative AI for Software Metadata: Overview of the Information Retrieval in Software Engineering Track at FIRE 2023;"Srijoni Majumdar; Soumen Paul; Bhargav Dave; Debjyoti Paul; Ayan Bandyopadhyay; Samiran Chattopadhyay; Partha Pratim Das; Paul D. Clough; Prasenjit Majumder";2023;http://digitalcommons.isical.ac.in/conf-articles;http://digitalcommons.isical.ac.in/conf-articles/519;NOT_FOUND
NOT_RELEVANT;AIS;Cross-Language Information Retrieval using Dutch Query Translation;"Anne R. Diekema; Wen-Yuan Hsiao";2000;http://surface.syr.edu/istpub;http://surface.syr.edu/istpub/18;NOT_FOUND
NOT_RELEVANT;AIS;Cross-Language Information Retrieval using Dutch Query Translation;"Anne R. Diekama; Anne Diekema; W. Hsiao";2000;http://digitalcommons.usu.edu/itls_facpub;http://digitalcommons.usu.edu/itls_facpub/32;NOT_FOUND
NOT_RELEVANT;AIS;Semantic Networks and Associative Databases: Two Approaches to Knowledge Representation and Reasoning;"Ee Peng LIM; Ee-Peng LIM; Vladimir CHERKASSY";1992;http://ink.library.smu.edu.sg/sis_research;http://ink.library.smu.edu.sg/sis_research/109;NOT_FOUND
NOT_RELEVANT;AIS;Information Extraction from Text;Jing JIANG;2012;http://ink.library.smu.edu.sg/sis_research;http://ink.library.smu.edu.sg/sis_research/1711;NOT_FOUND
NOT_RELEVANT;AIS;The Power of Selecting Key Blocks with Local Pre-ranking for Long Document Information Retrieval;"Minghan Li; Diana Nicoleta Popa; Johan Chagnon; Yagmur Gizem Cinar; Eric Gaussier";2023;http://ro.uow.edu.au/test2021;http://ro.uow.edu.au/test2021/8400;NOT_FOUND
NOT_RELEVANT;AIS;Patterns of Productivity in Information Retrieval (IR) Literature: A Study of the Scientometric Distributions;"Pramod Kumar Singh; Sangita Gupta";2021;http://digitalcommons.unl.edu/libphilprac;http://digitalcommons.unl.edu/libphilprac/5332;NOT_FOUND
NOT_RELEVANT;AIS;Extraction of Word Set for Increasing Human-Computer Interaction in Information Retrieval;"Eiko Yamamoto; Hitoshi Isahara";2008;http://aisel.aisnet.org/confirm2008;http://aisel.aisnet.org/confirm2008/28;NOT_FOUND
NOT_RELEVANT;AIS;Where was COVID-19 first discovered? Designing a question-answering system for pandemic situations;"Johannes Graf; Gino Lancho; Patrick Zschech; Kai Heinrich";2022;http://aisel.aisnet.org/ecis2022_rp;http://aisel.aisnet.org/ecis2022_rp/104;NOT_FOUND
NOT_RELEVANT;AIS;Deep Contextualized Biomedical Abbreviation Expansion;"Qiao Jin; Jinling Liu; Xinghua Lu";2019;http://scholarsmine.mst.edu/engman_syseng_facwork;http://scholarsmine.mst.edu/engman_syseng_facwork/787;NOT_FOUND
NOT_RELEVANT;AIS;Transfer Learning for Arabic Named Entity Recognition with Deep Neural Networks;"Mohammad Al-Smadi; Saad Al-Zboon; Yaser Jararweh; Patrick Juola";2020;http://dsc.duq.edu/faculty;http://dsc.duq.edu/faculty/364;NOT_FOUND
NOT_RELEVANT;AIS;Spellchecking for Children in Web Search: A Natural Language Interface Case-Study;"Casey Kennington; Jerry Alan Fails; Jerry Alan  Fails; Katherine Landau Wright; Maria Soledad Pera";2021;http://scholarworks.boisestate.edu/cs_facpubs;http://scholarworks.boisestate.edu/cs_facpubs/300;NOT_FOUND
NOT_RELEVANT;AIS;An empirical study of bugs in machine learning systems;"Ferdian THUNG; Thung Ferdian; Shaowei WANG; Shaowei Wang; David LO; Lingxiao JIANG";2012;http://ink.library.smu.edu.sg/sis_research;http://ink.library.smu.edu.sg/sis_research/1587;NOT_FOUND
NOT_RELEVANT;AIS;Using ChatGPT for Medical Information Seeking;"Anna Soboleva; Annette Mills";2023;http://aisel.aisnet.org/acis2023;http://aisel.aisnet.org/acis2023/126;NOT_FOUND
MAYBE_RELEVANT;AIS;AN OVERVIEW OF CHATBOTS USAGE IN RECRUITMENT AND SELECTION PRACTICES;"Babak Barghi; Eva Gallardo-Gallardo; Vicence Fernandez; Vicenç Fernández-Alarcón";2022;http://aisel.aisnet.org/mcis2022;http://aisel.aisnet.org/mcis2022/7;NOT_FOUND
NOT_RELEVANT;AIS;A semantic method for textual entailment;"Andrew Neel; Max Garzon; Vasile Rus";2008;http://digitalcommons.memphis.edu/facpubs;http://digitalcommons.memphis.edu/facpubs/2413;NOT_FOUND
NOT_RELEVANT;AIS;Link Analysis in Complex Networks by Structural Method and Machine Learning;Aliakbar Abdurahimov;2023;http://dclibrary.mbzuai.ac.ae/mbzsp;http://dclibrary.mbzuai.ac.ae/mbzsp/81;NOT_FOUND
NOT_RELEVANT;AIS;Sem-TF-IDF: A Simple Semantic Approach to Generalize TF-IDF by Employing Instruction Tuned Large Language Models;"Wei Chen; IP Spring LLC; Bo Lin";2024;http://www.tdcommons.org/dpubs_series;http://www.tdcommons.org/dpubs_series/6675;NOT_FOUND
NOT_RELEVANT;AIS;Relating big data to local natural hazards: lessons learned from data mining the Twitter API for volunteered geographic information on earthquakes, wildfires, and prescribed fires in the contiguous United States;"Jessica L. McCarty; Jessica McCarty; Eugene Levin; Evgueny Levin; K. Arthur Endsley; Samuel T. Aden; James Bialas";2015;http://digitalcommons.mtu.edu/mtri_p;http://digitalcommons.mtu.edu/mtri_p/209;NOT_FOUND
NOT_RELEVANT;AIS;Mapping prescribed burns and wildfires from Twitter with natural language processing and information retrieval techniques;"Kevin A. Endsley; K. A. Endsley; Jessica L. McCarty; Jessica McCarty";2013;http://digitalcommons.mtu.edu/mtri_p;http://digitalcommons.mtu.edu/mtri_p/165;NOT_FOUND
NOT_RELEVANT;AIS;Evaluation of Featureless Strategies for Relative Simplicity Prediction;"Devan Karsann; Devan Karsann Mr.";2019;http://scholarworks.boisestate.edu/under_conf_2019;http://scholarworks.boisestate.edu/under_conf_2019/82;NOT_FOUND
NOT_RELEVANT;AIS;Multilingual Information Access (MLIA) Tools on Google and WorldCat: Bi/Multilingual University Students’ Experience and Perceptions;"P. Nzomo; Liwen Vaughan; Isola Ajiferuke; Pam McKenzie";2019;http://ir.lib.uwo.ca/fimspub;http://ir.lib.uwo.ca/fimspub/268;NOT_FOUND
NOT_RELEVANT;AIS;Artificial Intelligence(AI) application in Library Systems in Iran: A taxonomy study;"Asefeh Asemi; Adeleh Asemi";2018;http://digitalcommons.unl.edu/libphilprac;http://digitalcommons.unl.edu/libphilprac/1840;NOT_FOUND
NOT_RELEVANT;AIS;A unified approach to non-negative matrix factorization and probabilistic latent semantic indexing;"Karthik Devarajan; Guoli Wang; Nader Ebrahimi; nader Ebrahimi";2011;http://biostats.bepress.com/cobra;http://biostats.bepress.com/cobra/art80;NOT_FOUND
NOT_RELEVANT;AIS;hpcNMF: A high-performance toolbox for non-negative matrix factorization;"Karthik Devarajan; Guoli Wang";2016;http://biostats.bepress.com/cobra;http://biostats.bepress.com/cobra/art115;NOT_FOUND
NOT_RELEVANT;AIS;What makes the story forward?: Inferring commonsense explanations as prompts for future event generation;"Li LIN; Yixin CAO; Lifu HUANG; Shu Ang LI; Xuming HU; Lijie WEN; Jianmin WANG";2022;http://ink.library.smu.edu.sg/sis_research;http://ink.library.smu.edu.sg/sis_research/7229;NOT_FOUND
NOT_RELEVANT;AIS;MSQ-BioBERT: Ambiguity Resolution to Enhance BioBERT Medical Question-Answering;"Muzhe Guo; Muhao Guo; Edward T. Dougherty; Fang Jin";2023;http://docs.rwu.edu/fcas_fp;http://docs.rwu.edu/fcas_fp/981;NOT_FOUND
NOT_RELEVANT;AIS;Plackett-Luce Regression Mixture model for heterogeneous rankings;"Maksim TKACHENKO; MAKSIM TKACHENKO; Hady W. LAUW";2016;http://ink.library.smu.edu.sg/sis_research;http://ink.library.smu.edu.sg/sis_research/3354;NOT_FOUND
NOT_RELEVANT;AIS;A comprehensive survey on relation extraction: Recent advances and new frontiers;"Xiaoyan ZHAO; Yang DENG; Min YANG; Lingzhi WANG; Rui ZHANG; Hong CHENG; Wai LAM; Ying SHEN; Ruifeng XU";2024;http://ink.library.smu.edu.sg/sis_research;http://ink.library.smu.edu.sg/sis_research/9098;NOT_FOUND
NOT_RELEVANT;AIS;Deep code comment generation with hybrid lexical and syntactical information;"Xing HU; Ge LI; Xin XIA; David LO; Zhi JIN";2019;http://ink.library.smu.edu.sg/sis_research;http://ink.library.smu.edu.sg/sis_research/4407;NOT_FOUND
NOT_RELEVANT;AIS;Application and Use of Artificial Intelligence (AI) for Library Services Delivery in Academic Libraries in Kwara State, Nigeria;Abdullahi Olayinka Isiaka;2023;http://digitalcommons.unl.edu/libphilprac;http://digitalcommons.unl.edu/libphilprac/7998;NOT_FOUND
MAYBE_RELEVANT;AIS;Beyond factuality: A comprehensive evaluation of large language models as knowledge generators;"Liang CHEN; Yang DENG; Yatao BIAN; Zeyu QIN; Bingzhe WU; Tat-Seng CHUA; Kam-Fai WONG";2023;http://ink.library.smu.edu.sg/sis_research;http://ink.library.smu.edu.sg/sis_research/9117;NOT_FOUND
NOT_RELEVANT;AIS;A comprehensive evaluation of large language models on legal judgment prediction;"Ruihao SHUI; Yixin CAO; Xiang WANG; Tat-Seng CHUA";2023;http://ink.library.smu.edu.sg/sis_research;http://ink.library.smu.edu.sg/sis_research/8396;NOT_FOUND
NOT_RELEVANT;AIS;Exploiting Topical Perceptions Over Multi-Lingual Text For Hashtag Suggestion On Twitter;"Amara Tariq; Asim Karim; Fernando Gomez; Hassan Foroosh";2013;http://stars.library.ucf.edu/scopus2010;http://stars.library.ucf.edu/scopus2010/5953;NOT_FOUND
NOT_RELEVANT;AIS;Analyzing the Strengths, Weaknesses, Opportunities, and Threats of AI in Libraries;Parbat Chhetri;2023;http://digitalcommons.unl.edu/libphilprac;http://digitalcommons.unl.edu/libphilprac/7808;NOT_FOUND
NOT_RELEVANT;AIS;Quench MLA Semantics-Preserving Markup Language for Knowledge Representation in Quenching;"Aparna Varde; Mohammed Maniruzzaman; Richard D. Sisson";2013;http://digitalcommons.montclair.edu/compusci-facpubs;http://digitalcommons.montclair.edu/compusci-facpubs/498;NOT_FOUND
NOT_RELEVANT;AIS;ChatGPT: A Conceptual Review of Applications and Utility in the Field of Medicine;"Shiavax J Rao; Ameesh Isath; Parvathy Krishnan; Jonathan A Tangsrivimol; Hafeez Ul Hassan Virk; Zhen Wang; Benjamin S Glicksberg; Chayakrit Krittanawong";2024;http://touroscholar.touro.edu/nymc_res_pubs;http://touroscholar.touro.edu/nymc_res_pubs/392;NOT_FOUND
NOT_RELEVANT;AIS;Semantic methods for textual entailment: How much world knowledge is enough?;"Andrew Neel; Max Garzon";2010;http://digitalcommons.memphis.edu/facpubs;http://digitalcommons.memphis.edu/facpubs/3196;NOT_FOUND
NOT_RELEVANT;AIS;An Open Dialogue Between Neuromusicology and Computational Modelling Methods;"Sujas Bhardwaj; Kaustuv Kanti Ganguli; Shantala Hegde";2024;http://zuscholars.zu.ac.ae/works;http://zuscholars.zu.ac.ae/works/6461;NOT_FOUND
NOT_RELEVANT;AIS;Using data-driven sublanguage pattern mining to induce knowledge models: application in medical image reports knowledge representation;"Yiqing Zhao; Nooshin J. Fesharaki; Hongfang Liu; Jake Luo";2018;http://dc.uwm.edu/healthinfo_facart;http://dc.uwm.edu/healthinfo_facart/2;NOT_FOUND
NOT_RELEVANT;AIS;Abstract Sentence Classification Acceptance;"Connor Stead; Conno Stead; Stephen Smith; Peter Busch; Savanid Vatanasakdakul";2023;http://aisel.aisnet.org/acis2023;http://aisel.aisnet.org/acis2023/120;NOT_FOUND
NOT_RELEVANT;AIS;Detecting semantic uncertainty by learning hedge cues in sentences using an HMM;"Xiujun LI; Wei GAO; Jude SHAVLIK";2017;http://ink.library.smu.edu.sg/sis_research;http://ink.library.smu.edu.sg/sis_research/4643;NOT_FOUND
NOT_RELEVANT;AIS;Learnings from a pilot hybrid question answering system: CQAS: Case study based on a Singapore government agency's customer service centre;"Hui Shan LEE; SHANKARARAMAN, Venky; Venky SHANKARARAMAN; Eng Lieh OUH; Eng Lieh Ouh";2022;http://ink.library.smu.edu.sg/sis_research;http://ink.library.smu.edu.sg/sis_research/7601;NOT_FOUND
NOT_RELEVANT;AIS;On the effectiveness of pretrained models for API learning;"Mohammad Abdul HADI; IMAM NUR BANI YUSUF; Thung Ferdian; Gia Kien LUONG; Lingxiao JIANG; Fatemeh H. FARD; David LO";2022;http://ink.library.smu.edu.sg/sis_research;http://ink.library.smu.edu.sg/sis_research/7642;NOT_FOUND
NOT_RELEVANT;AIS;Human-centered AI for software engineering: Requirements, reflection, and road ahead;David LO;2023;http://ink.library.smu.edu.sg/sis_research;http://ink.library.smu.edu.sg/sis_research/8622;NOT_FOUND
NOT_RELEVANT;AIS;Recommender Systems Research;Saverio Perugini;2005;http://ecommons.udayton.edu/cps_fac_pub;http://ecommons.udayton.edu/cps_fac_pub/32;NOT_FOUND
NOT_RELEVANT;AIS;Automatically Extracting Meaning from Legal Texts: Opportunities and Challenges;Kevin D. Ashley;2019;http://scholarship.law.pitt.edu/fac_articles;http://scholarship.law.pitt.edu/fac_articles/524;NOT_FOUND
NOT_RELEVANT;AIS;Fighting Human Trafficking During and Post COVID-19: A Design Science-based Approach;"Somnath Mukherjee; James R. Van Scotter; Curtis Hampton";2021;http://aisel.aisnet.org/treos_amcis2021;http://aisel.aisnet.org/treos_amcis2021/29;NOT_FOUND
NOT_RELEVANT;AIS;IMPROVING PREFERENCE RECOMMENDATION AND CUSTOMIZATION IN REAL WORLD HIGHLY CONFIGURABLE SOFTWARE SYSTEMS;Dongpu Jin;2014;http://digitalcommons.unl.edu/computerscidiss;http://digitalcommons.unl.edu/computerscidiss/84;NOT_FOUND
NOT_RELEVANT;AIS;IMPROVING PREFERENCE RECOMMENDATION AND CUSTOMIZATION IN REAL WORLD HIGHLY CONFIGURABLE SOFTWARE SYSTEMS;Dongpu Jin;2014;http://digitalcommons.unl.edu/embargotheses;http://digitalcommons.unl.edu/embargotheses/71;NOT_FOUND
MAYBE_RELEVANT;AIS;Leveraging Large Language Models for Simplified Patient Summary Generation, Literature Retrieval and Medical Information Summarization: A Health CASCADE Study;"Georgios Balaskas; Homer Papadopoulos; Antonis Korakis";2024;http://aisel.aisnet.org/hicss-57;http://aisel.aisnet.org/hicss-57/hc/ecosystems/4;NOT_FOUND
NOT_RELEVANT;AIS;Optimizing Campus Chat-bot Experience Using PUAA: Integrating Large Language Model (LLM) into University AI Assistants;"Sijan Panday; Zurab Sabakhtarishvili; Clayton Jensen";2024;http://orc.library.atu.edu/atu_rs;http://orc.library.atu.edu/atu_rs/2024/2024/12;NOT_FOUND
NOT_RELEVANT;AIS;Identifying Authorship from Linguistic Text Patterns;"Joshua Madden; Veda C. Storey; Richard Baskerville; Richard L. Baskerville";2019;http://aisel.aisnet.org/hicss-52;http://aisel.aisnet.org/hicss-52/os/design_science_research/6;NOT_FOUND
MAYBE_RELEVANT;AIS;Generative Assistants for Handling Incidents in IT Service Management;"Rainer Schmidt; Rainer Alt; ALFRED ZIMMERMANN; Alfred Zimmermann";2024;http://aisel.aisnet.org/icis2024;http://aisel.aisnet.org/icis2024/digtech_fow/digtech_fow/19;NOT_FOUND
NOT_RELEVANT;AIS;Ensuring Accuracy and Engagement in Robots using Large Language Models;Logan Reichling;2023;http://digitalcommons.onu.edu/student_research_colloquium;http://digitalcommons.onu.edu/student_research_colloquium/2023/papers/20;NOT_FOUND
NOT_RELEVANT;AIS;GMR-229 Semantic Search using Sentence Transformers;"Roshni Satish; Arpana Challa";2024;http://digitalcommons.kennesaw.edu/cday;http://digitalcommons.kennesaw.edu/cday/Fall_2024/Masters_Research/9;NOT_FOUND
NOT_RELEVANT;AIS;Transforming Generative Large Language Models' Limitations into Strengths using Gestalt: A Synergetic Approach to Mathematical Problem-Solving with Computational Engines;"Cayden Dunn; Navid Hashemi Tonekaboni";2024;http://aisel.aisnet.org/hicss-57;http://aisel.aisnet.org/hicss-57/ks/design/2;NOT_FOUND
NOT_RELEVANT;AIS;Question answering enhanced with a window function;"J. Jovanovska; I. Bozhinova; K. Zdravkova";2017;http://knowledgecenter.ubt-uni.net/conference;http://knowledgecenter.ubt-uni.net/conference/2017/all-events/101;NOT_FOUND
NOT_RELEVANT;AIS;Natural Language Inputs for Medication Reconciliation: A Feasibility Study;"Marquise Hopson; Neal K. Sikka; Charles Jankowski; Anne Thyme-Gobbel";2016;http://hsrc.himmelfarb.gwu.edu/gw_research_days;http://hsrc.himmelfarb.gwu.edu/gw_research_days/2016/SMHS/155;NOT_FOUND
NOT_RELEVANT;AIS;UNVEILING THE ORIGINS OF SOURCE CODE THROUGH AUTHORSHIP ATTRIBUTION: A COMPARATIVE STUDY OF AI AND HUMAN CODING PATTERNS;Shamma Humaid Alalawi;2024;http://scholarworks.uaeu.ac.ae/thesis_dissertation_defense;http://scholarworks.uaeu.ac.ae/thesis_dissertation_defense/2024-2023/thesis_defenses/69;NOT_FOUND
NOT_RELEVANT;AIS;Toward Automatic Fake News Classification;"Souvick Ghosh; Chirag Shah";2019;http://aisel.aisnet.org/hicss-52;http://aisel.aisnet.org/hicss-52/dsm/data_mining/6;NOT_FOUND
NOT_RELEVANT;AIS;The semantics of similarity in geographic information retrieval;"Krzysztof Janowicz; Martin Raubal; Werner Kuhn";2012;http://digitalcommons.library.umaine.edu/josis;http://digitalcommons.library.umaine.edu/josis/vol2011/iss2/3;NOT_FOUND
NOT_RELEVANT;AIS;Similarity Evaluation Based on Contextual Modelling;"Mohamed H. Haggag; Marwa M. A. ELFattah; Ahmed Mohammed Ahmed";2020;http://digitalcommons.aaru.edu.jo/fcij;http://digitalcommons.aaru.edu.jo/fcij/vol4/iss1/5;NOT_FOUND
MAYBE_RELEVANT;AIS;SEMANTIC NESTED SEARCH ENGINE VIA INTEGRATION ONTOLOGY WITH MULTI-AGENT SYSTEM;"Eman Elsayed; AbdAllah ELHabashy; Raghda khaled";2019;http://absb.researchcommons.org/journal;http://absb.researchcommons.org/journal/vol30/iss1/9;NOT_FOUND
NOT_RELEVANT;AIS;Intuitive Direction Concepts;"Alexander Klippel; Jan Oliver Wallgrün; Jinlong Yang; Kevin Sparks";2015;http://newprairiepress.org/biyclc;http://newprairiepress.org/biyclc/vol10/iss1/3;NOT_FOUND
NOT_RELEVANT;AIS;The Present and Future of Internet Search;"Wendy Lucas; William Schiano; Katherine Crosett";2001;http://aisel.aisnet.org/cais;http://aisel.aisnet.org/cais/vol5/iss1/8;NOT_FOUND
NOT_RELEVANT;AIS;Teaching Law and Digital Age Legal Practice with an AI and Law Seminar;Kevin D. Ashley;2013;http://scholarship.kentlaw.iit.edu/cklawreview;http://scholarship.kentlaw.iit.edu/cklawreview/vol88/iss3/7;NOT_FOUND
NOT_RELEVANT;AIS;Law in the Age of Exabytes: Some further Thoughts on ‘Information Inflation’ and Current Issues in E-Discovery Search;Jason R. Baron;2011;http://scholarship.richmond.edu/jolt;http://scholarship.richmond.edu/jolt/vol17/iss3/3;NOT_FOUND
NOT_RELEVANT;AIS;A NOVEL ARABIC CORPUS FOR TEXT CLASSIFICATION USING DEEP LEARNING AND WORD EMBEDDING;"Roua A. Abou Khachfeh; Roua Abou Khachfeh; Islam El Kabani; Ziad Osman";2021;http://digitalcommons.bau.edu.lb/stjournal;http://digitalcommons.bau.edu.lb/stjournal/vol3/iss1/10;NOT_FOUND
