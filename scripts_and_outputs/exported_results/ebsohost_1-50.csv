Article Title,Author,Journal Title,ISSN,ISBN,Publication Date,Volume,Issue,First Page,Page Count,Accession Number,DOI,Publisher,Doctype,Subjects,Keywords,Abstract,PLink
"A case study of duplications detection for educational domain thorough ad hoc search and identification NLP-based method.","Mikhaylov, S.N.; Chuikova, V.V.; Sokolova, Marina V.; Potapenko, A.M.","Expert Systems",="02664720",,="Aug2017","34","4","n/a","11","124518582","10.1111/exsy.12200","Wiley-Blackwell","Article","NATURAL language processing; ELECTRONIC records; WEB services; COMPUTER software; Computer, computer peripheral and pre-packaged software merchant wholesalers; Computer and Computer Peripheral Equipment and Software Merchant Wholesalers; Computer and software stores; Software publishers (except video game publishers); All Other Miscellaneous Schools and Instruction; Educational Support Services; Administration of Education Programs; Data Processing, Hosting, and Related Services; TEACHING; EDUCATION","evaluation; information resource; information retrieval; natural language processing; visual interface for knowledge representation","During the organization and planning of lecture courses for a discipline, its content may be overlapped and partially delivered in more than one course. Sometimes this action causes time loss through unnecessary repeating. This paper introduces an automated tool for duplications detections adapting methods of natural language processing used for Web search. The experiment for unstructured electronic document repositories clustering for thematic duplicate identification in different documents in the case of educational domain is presented. A prototype of this Web service-based software search engine is being designed and discussed. The experiment aimed to identify thematic duplicates of various courses within one of the teaching disciplines is also presented. [ABSTRACT FROM AUTHOR] Copyright of Expert Systems is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=124518582&site=bsi-live"
"A Novel classification framework for the Thirukkural for building an efficient search system.","Ramalingam, Anita; Navaneethakrishnan, Subalalitha Chinnaudayar","Journal of Intelligent & Fuzzy Systems",="10641246",,="2022","42","3","2397","12","156139289","10.3233/JIFS-211667","IOS Press","Article","INFORMATION retrieval; RANDOM forest algorithms; SUPPORT vector machines; NAIVE Bayes classification; KEYWORD searching","information retrieval; morphological analysis; multinomial naive bayes classifier; Natural language processing; text classification; the Thirukkural","Thirukkural, a Tamil classic literature, which was written in 300 BCE is a didactic literature. Though Thirukkural comprises 1330 couplets which are organized into three sections and 133 chapters, in order to retrieve meaningful Thirukkural for a given query in search systems, a better organization of the Thirukkural is needed. This paper lays such a foundation by classifying the Thirukkural into ten new categories called superclasses that is helpful for building a better Information Retrieval (IR) system. The classifier is trained using Multinomial Naïve Bayes algorithm. Each superclass is further classified into two subcategories based on the didactic information. The proposed classification framework is evaluated using precision, recall and F-score metrics and achieved an overall F-score of 82.33% and a comparison analysis has been done with the Support Vector Machine, Logistic Regression and Random Forest algorithms. An IR system is built on top of the proposed system and the performance comparison has been done with the Google search and a locally built keyword search. The proposed classification framework has achieved a mean average precision score of 89%, whereas the Google search and keyword search have yielded 59% and 68% respectively. [ABSTRACT FROM AUTHOR] Copyright of Journal of Intelligent & Fuzzy Systems is the property of IOS Press and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=156139289&site=bsi-live"
"A novel method for providing relational databases with rich semantics and natural language processing.","Hamaz, Kamal; Benchikha, Fouzia","Journal of Enterprise Information Management",="17410398",,="2017","30","3","503","23","122548771","10.1108/JEIM-01-2015-0005","Emerald Publishing Limited","Article","NATURAL language processing; RELATIONAL databases; SEMANTICS","Enrichment; Information retrieval; Natural language processing; Ontologies; Relational databases; Reverse engineering","Purpose With the development of systems and applications, the number of users interacting with databases has increased considerably. The relational database model is still considered as the most used model for data storage and manipulation. However, it does not offer any semantic support for the stored data which can facilitate data access for the users. Indeed, a large number of users are intimidated when retrieving data because they are non-technical or have little technical knowledge. To overcome this problem, researchers are continuously developing new techniques for Natural Language Interfaces to Databases (NLIDB). Nowadays, the usage of existing NLIDBs is not widespread due to their deficiencies in understanding natural language (NL) queries. In this sense, the purpose of this paper is to propose a novel method for an intelligent understanding of NL queries using semantically enriched database sources.Design/methodology/approach First a reverse engineering process is applied to extract relational database hidden semantics. In the second step, the extracted semantics are enriched further using a domain ontology. After this, all semantics are stored in the same relational database. The phase of processing NL queries uses the stored semantics to generate a semantic tree.Findings The evaluation part of the work shows the advantages of using a semantically enriched database source to understand NL queries. Additionally, enriching a relational database has given more flexibility to understand contextual and synonymous words that may be used in a NL query.Originality/value Existing NLIDBs are not yet a standard option for interfacing a relational database due to their lack for understanding NL queries. Indeed, the techniques used in the literature have their limits. This paper handles those limits by identifying the NL elements by their semantic nature in order to generate a semantic tree. This last is a key solution towards an intelligent understanding of NL queries to relational databases. [ABSTRACT FROM AUTHOR] Copyright of Journal of Enterprise Information Management is the property of Emerald Publishing Limited and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=122548771&site=bsi-live"
"A Review and Future Perspectives of Arabic Question Answering Systems.","Ray, Santosh K.; Shaalan, Khaled","IEEE Transactions on Knowledge & Data Engineering",="10414347",,="Dec2016","28","12","3169","22","119353200","10.1109/TKDE.2016.2607201","IEEE","Article","NATURAL language processing; INTERNET; Wired Telecommunications Carriers; Internet Publishing and Broadcasting and Web Search Portals; QUESTION answering systems; ARABIC language; COMPUTATIONAL linguistics","Computer architecture; Document retrieval; Internet; Knowledge based systems; Knowledge discovery; literature review; Natural language processing; query expansion; question answering systems; Search engines; Standards; tools for arabic information retrieval","Question Answering Systems (QASs) have emerged as a good alternative for information seekers to retrieve precise information over the Internet. A good amount of research has been done to improve the performance of QASs across several languages, including European and Asian languages. However, Arabic, a morphologically rich Semitic language spoken by over 422 million people, has not seen similar development in the field of question answering. This article reviews the developments taking place in Arabic QASs as well as the challenges faced by researchers in developing Arabic QASs. After conducting an extensive literature survey of a number of English and Arabic QASs, this article classifies them according to several criteria. The most commonly used architecture for the development of an Arabic QAS, known as pipeline architecture, has been presented. In order to encourage and support the new researchers and scholars in conducting research in Arabic QASs, a list of techniques, tools, and computational linguistic resources, required to implement the components of the presented pipelined architecture, are described in this article in a simple and persuasive manner. Finally, the gap analysis between the research in Arabic and English QASs has been performed and accordingly, some future directions for research in Arabic QASs have been proposed. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Knowledge & Data Engineering is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=119353200&site=bsi-live"
"A Review of Modern Fashion Recommender Systems.","DELDJOO, YASHAR; NAZARY, FATEMEH; RAMISA, ARNAU; MCAULEY, JULIAN; PELLEGRINI, GIOVANNI; BELLOGIN, ALEJANDRO; DI NOIA, TOMMASO","ACM Computing Surveys",="03600300",,="Apr2024","56","4","1","37","174004917","10.1145/3624733","Association for Computing Machinery","Article",,"artificial intelligence; computer vision; e-commerce; fashion retail; information retrieval; machine learning; Recommender systems; text mining",,"https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=174004917&site=bsi-live"
"Accounting fraud detection using contextual language learning.","Bhattacharya, Indranil; Mickovic, Ana","International Journal of Accounting Information Systems",="14670895",,="Jun2024","53",,"N.PAG","1","177747955","10.1016/j.accinf.2024.100682","Elsevier B.V.","Article","ACCOUNTING fraud; FORENSIC accounting; ACCOUNTING software; UNITED States. Securities & Exchange Commission; Other Accounting Services; Regulation, Licensing, and Inspection of Miscellaneous Commercial Sectors; FRAUD investigation; CONTEXTUAL learning; LANGUAGE models","Accounting fraud detection; BERT; Information Retrieval; Natural Language Processing","• Financial reporting text is an important source of information for fraud detection. • Application of BERT in accounting fraud detection setting. • Contextual learning improves accounting fraud detection relative to benchmarks. • Final model outperforms existing textual and quantitative benchmark models. • We provide practical insights for financial investigators. Accounting fraud is a widespread problem that causes significant damage in the economic market. Detection and investigation of fraudulent firms require a large amount of time, money, and effort for corporate monitors and regulators. In this study, we explore how textual contents from financial reports help in detecting accounting fraud. Pre-trained contextual language learning models, such as BERT, have significantly advanced natural language processing in recent years. We fine-tune the BERT model on Management Discussion and Analysis (MD&A) sections of annual 10-K reports from the Securities and Exchange Commission (SEC) database. Our final model outperforms the textual benchmark model and the quantitative benchmark model from the previous literature by 15% and 12%, respectively. Further, our model identifies five times more fraudulent firm-year observations than the textual benchmark by investigating the same number of firms, and three times more than the quantitative benchmark. Optimizing this investigation process, where more fraudulent observations are detected in the same size of the investigation sample, would be of great economic significance for regulators, investors, financial analysts, and auditors. [ABSTRACT FROM AUTHOR] Copyright of International Journal of Accounting Information Systems is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=177747955&site=bsi-live"
"An augmented semantic search tool for multilingual news analytics.","Harikumar, Sandhya; Sathyajit, Rohit; Karumudi, Gnana Venkata Naga Sai Kalyan","Journal of Intelligent & Fuzzy Systems",="10641246",,="2022","43","6","8315","13","160553638","10.3233/JIFS-221184","IOS Press","Article","INFORMATION retrieval; NATURAL language processing; SEARCH engines; KEYWORD searching; ENGLISH language","Latent dirichlet allocation(LDA); multilingual; natural language processing(NLP); News analytics; semantic information retrieval","News feeds generate colossal amount of data consisting of important information hidden in the intricacies. State of the art methods are still at infancy in providing a very generic and publicly available solution to skim through the important information in the news from various sources and an ability to search using specific keywords in different languages. This paper focuses on designing a tool to extract semantic details from news articles published through various internet sources in various languages. The semantic information is stored within DBMS for ease of organizing and retrieving the data. Further, a querying facility to search through entire articles based on the keyword or date-based search is also proposed to view the crisp content. The news articles in English, and two Indian languages - Hindi and Malayalam are considered for experimentation. The proposed strategy consists of two main components namely, Generative model creation and Query engine. Generative model aims to extract important entities and keywords along with their relevance to the article and other similar articles using Latent Dirichlet Allocation(LDA) and Named Entity Recognition(NER). Query engine is to facilitate on the fly retrieval of semantic content from the database, based on user keyword. The search engine, along with database indexing, reduces the access time to the database thereby retrieving the information in less time. Experimental results show that the proposed method is effective in terms of quality of information and time consumed for information retrieval. [ABSTRACT FROM AUTHOR] Copyright of Journal of Intelligent & Fuzzy Systems is the property of IOS Press and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=160553638&site=bsi-live"
"An ensemble clustering approach for topic discovery using implicit text segmentation.","Memon, Muhammad Qasim; Lu, Yu; Chen, Penghe; Memon, Aasma; Pathan, Muhammad Salman; Zardari, Zulfiqar Ali","Journal of Information Science",="01655515",,="Aug2021","47","4","431","27","151171329","10.1177/0165551520911590","Sage Publications, Ltd.","Article","INFORMATION retrieval; NATURAL language processing; WORD frequency","Information retrieval; natural language processing; ontological similarity; text clustering; text mining; text segmentation","Text segmentation (TS) is the process of dividing multi-topic text collections into cohesive segments using topic boundaries. Similarly, text clustering has been renowned as a major concern when it comes to multi-topic text collections, as they are distinguished by sub-topic structure and their contents are not associated with each other. Existing clustering approaches follow the TS method which relies on word frequencies and may not be suitable to cluster multi-topic text collections. In this work, we propose a new ensemble clustering approach (ECA) is a novel topic-modelling-based clustering approach, which induces the combination of TS and text clustering. We improvised a LDA-onto (LDA-ontology) is a TS-based model, which presents a deterioration of a document into segments (i.e. sub-documents), wherein each sub-document is associated with exactly one sub-topic. We deal with the problem of clustering when it comes to a document that is intrinsically related to various topics and its topical structure is missing. ECA is tested through well-known datasets in order to provide a comprehensive presentation and validation of clustering algorithms using LDA-onto. ECA exhibits the semantic relations of keywords in sub-documents and resultant clusters belong to original documents that they contain. Moreover, present research sheds the light on clustering performances and it indicates that there is no difference over performances (in terms of F -measure) when the number of topics changes. Our findings give above par results in order to analyse the problem of text clustering in a broader spectrum without applying dimension reduction techniques over high sparse data. Specifically, ECA provides an efficient and significant framework than the traditional and segment-based approach, such that achieved results are statistically significant with an average improvement of over 10.2%. For the most part, proposed framework can be evaluated in applications where meaningful data retrieval is useful, such as document summarization, text retrieval, novelty and topic detection. [ABSTRACT FROM AUTHOR] Copyright of Journal of Information Science is the property of Sage Publications, Ltd. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=151171329&site=bsi-live"
"An improved Urdu stemming algorithm for text mining based on multi-step hybrid approach.","Jabbar, Abdul; Iqbal, Sajid; Akhunzada, Adnan; Abbas, Qaisar","Journal of Experimental & Theoretical Artificial Intelligence",="0952813X",,="Sep2018","30","5","703","21","132054653","10.1080/0952813X.2018.1467495","Taylor & Francis Ltd","Article","ARTIFICIAL intelligence; ALGORITHMS; DATA mining; ELECTRONIC data processing; Data Processing, Hosting, and Related Services; MACHINE learning","affixes; information retrieval; lemmatization; natural language processing; stemming; text mining; Urdu; Urdu stemmer","Stemming is the basic operation in Natural language processing (NLP) to remove derivational and inflectional affixes without performing a morphological analysis. This practice is essential to extract the root or stem. In NLP domains, the stemmer is used to improve the process of information retrieval (IR), text classifications (TC), text mining (TM) and related applications. In particular, Urdu stemmers utilize only uni-gram words from the input text by ignoring bigrams, trigrams, and n-gram words. To improve the process and efficiency of stemming, bigrams and trigram words must be included. Despite this fact, there are a few developed methods for Urdu stemmers in the past studies. Therefore, in this paper, we proposed an improved Urdu stemmer, using hybrid approach divided into multi-step operation, to deal with unigram, bigram, and trigram features as well. To evaluate the proposed Urdu stemming method, we have used two corpora; word corpus and text corpus. Moreover, two different evaluation metrics have been applied to measure the performance of the proposed algorithm. The proposed algorithm achieved an accuracy of 92.97% and compression rate of 55%. These experimental results indicate that the proposed system can be used to increase the effectiveness and efficiency of the Urdu stemmer for better information retrieval and text mining applications. [ABSTRACT FROM AUTHOR] Copyright of Journal of Experimental & Theoretical Artificial Intelligence is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=132054653&site=bsi-live"
"An open source and modular search engine for biomedical literature retrieval.","Almeida, Hayda; Jean‐Louis, Ludovic; Meurs, Marie‐Jean","Computational Intelligence",="08247935",,="Feb2018","34","1","200","19","128227532","10.1111/coin.12125","Wiley-Blackwell","Article","OPEN source software; SEARCH engines; NATURAL language processing; DATA mining; Research and development in the physical, engineering and life sciences; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); BIOLOGICAL research","biomedical literature, document index, full‐text search, information retrieval, natural language processing, natural language query, search engine","Abstract: This work presents the bioMine system, a full‐text natural language search engine for biomedical literature. bioMine provides search capabilities based on the full‐text content of documents belonging to a database composed of scientific articles and allows users to submit their search queries using natural language. Beyond the text content of articles, the system engine also uses article metadata, empowering the search by considering extra information from picture and table captions. bioMine is publicly released as an open‐source system under the MIT license. [ABSTRACT FROM AUTHOR] Copyright of Computational Intelligence is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=128227532&site=bsi-live"
"Applications of natural language processing in software traceability: A systematic mapping study.","Pauzi, Zaki; Capiluppi, Andrea","Journal of Systems & Software",="01641212",,="Apr2023","198",,"N.PAG","1","161845363","10.1016/j.jss.2023.111616","Elsevier B.V.","Article","NATURAL language processing; COMPUTER software; INFORMATION retrieval; Computer, computer peripheral and pre-packaged software merchant wholesalers; Computer and Computer Peripheral Equipment and Software Merchant Wholesalers; Computer and software stores; Software publishers (except video game publishers); TACIT knowledge; SOFTWARE maintenance; SOURCE code; TREND analysis","Information retrieval; Natural language processing; Software traceability","A key part of software evolution and maintenance is the continuous integration from collaborative efforts, often resulting in complex traceability challenges between software artifacts: features and modules remain scattered in the source code, and traceability links become harder to recover. In this paper, we perform a systematic mapping study dealing with recent research recovering these links through information retrieval, with a particular focus on natural language processing (NLP). Our search strategy gathered a total of 96 papers in focus of our study, covering a period from 2013 to 2021. We conducted trend analysis on NLP techniques and tools involved, and traceability efforts (applying NLP) across the software development life cycle (SDLC). Based on our study, we have identified the following key issues, barriers, and setbacks: syntax convention, configuration, translation, explainability, properties representation, tacit knowledge dependency, scalability, and data availability. Based on these, we consolidated the following open challenges: representation similarity across artifacts, the effectiveness of NLP for traceability, and achieving scalable, adaptive, and explainable models. To address these challenges, we recommend a holistic framework for NLP solutions to achieve effective traceability and efforts in achieving interoperability and explainability in NLP models for traceability. [Display omitted] • Our search strategy across multiple library databases gathered a total of 96 papers. • Trend analysis was conducted throughout the years 2013 to 2021. • Our study highlighted open challenges from key issues, barriers, and setbacks. • We proposed two key recommendations to address these open challenges. [ABSTRACT FROM AUTHOR] Copyright of Journal of Systems & Software is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=161845363&site=bsi-live"
"Automated arabic text classification with P- Stemmer, machine learning, and a tailored news article taxonomy.","Kanan, Tarek; Fox, Edward A.","Journal of the Association for Information Science & Technology",="23301635",,="Nov2016","67","11","2667","17","118731689","10.1002/asi.23609","Wiley-Blackwell","Article","NATURAL language processing; PROBABILITY theory; RESEARCH funding; MIDDLE East; Book stores and news dealers; News Dealers and Newsstands; Book, Periodical, and Newspaper Merchant Wholesalers; Book, periodical and newspaper merchant wholesalers; Newspaper Publishers; LANGUAGE classification; ARABS; ELECTRONIC journals; NEWSPAPERS; MANN Whitney U Test","digital libraries; information retrieval; natural language processing","Arabic news articles in electronic collections are difficult to study. Browsing by category is rarely supported. Although helpful machine-learning methods have been applied successfully to similar situations for English news articles, limited research has been completed to yield suitable solutions for Arabic news. In connection with a Qatar National Research Fund ( QNRF)-funded project to build digital library community and infrastructure in Qatar, we developed software for browsing a collection of about 237,000 Arabic news articles, which should be applicable to other Arabic news collections. We designed a simple taxonomy for Arabic news stories that is suitable for the needs of Qatar and other nations, is compatible with the subject codes of the International Press Telecommunications Council, and was enhanced with the aid of a librarian expert as well as five Arabic-speaking volunteers. We developed tailored stemming (i.e., a new Arabic light stemmer called P- Stemmer) and automatic classification methods (the best being binary Support Vector Machines classifiers) to work with the taxonomy. Using evaluation techniques commonly used in the information retrieval community, including 10-fold cross-validation and the Wilcoxon signed-rank test, we showed that our approach to stemming and classification is superior to state-of-the-art techniques. [ABSTRACT FROM AUTHOR] Copyright of Journal of the Association for Information Science & Technology is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=118731689&site=bsi-live"
"Automatic smart contract comment generation via large language models and in-context learning.","Zhao, Junjie; Chen, Xiang; Yang, Guang; Shen, Yiheng","Information & Software Technology",="09505849",,="Apr2024","168",,"N.PAG","1","175296066","10.1016/j.infsof.2024.107405","Elsevier B.V.","Article",,"Demonstration selection; In-context learning; Information retrieval; Large language model; Smart contract comment","Designing effective automatic smart contract comment generation approaches can facilitate developers' comprehension, boosting smart contract development and improving vulnerability detection. The previous approaches can be divided into two categories: fine-tuning paradigm-based approaches and information retrieval-based approaches. However, for the fine-tuning paradigm-based approaches, the performance may be limited by the quality of the gathered dataset for the downstream task and they may have knowledge-forgetting issues, which can reduce the generality of the fine-tuned model. While for the information retrieval-based approaches, it is difficult for them to generate high-quality comments if similar code does not exist in the historical repository. Therefore we want to utilize the domain knowledge related to smart contract code comment generation in large language models (LLMs) to alleviate the disadvantages of these two types of approaches. In this study, we propose an approach SCCLLM based on LLMs and in-context learning. Specifically, in the demonstration selection phase, SCCLLM retrieves the top- k code snippets from the historical corpus by considering syntax, semantics, and lexical information. In the in-context learning phase, SCCLLM utilizes the retrieved code snippets as demonstrations for in-context learning, which can help to utilize the related knowledge for this task in the LLMs. In the LLMs inference phase, the input is the target smart contract code snippet, and the output is the corresponding comment generated by the LLMs. We select a large corpus from a smart contract community Etherscan.io as our experimental subject. Extensive experimental results show the effectiveness of SCCLLM when compared with baselines in automatic evaluation and human evaluation. We also show the rationality of our customized demonstration selection strategy in SCCLLM by ablation studies. Our study shows using LLMs and in-context learning is a promising direction for automatic smart contract comment generation, which calls for more follow-up studies. [ABSTRACT FROM AUTHOR] Copyright of Information & Software Technology is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=175296066&site=bsi-live"
"BIMASR: Framework for Voice-Based BIM Information Retrieval.","Shin, Sangyun; Issa, Raja R. A.","Journal of Construction Engineering & Management",="07339364",,="Oct2021","147","10","1","18","152144732","10.1061/(ASCE)CO.1943-7862.0002138","American Society of Civil Engineers","Article","AUTOMATIC speech recognition; NATURAL language processing; INFORMATION retrieval; BUILDING information modeling; SPEECH perception","Building information modeling; Domain ontology; Information retrieval; Natural language processing; Speech recognition; Structured query language","Voice is the most convenient means for human beings to communicate with others, even if the objects of their communication are not other humans but machines or computers. Many industries, and even the architecture, engineering, construction, and operations (AECO) industry, have attempted to study and apply speech recognition systems in their operations to improve work efficiency and productivity. However, previous studies on speech recognition had two limitations: they used keywords requiring basic knowledge of building information modeling (BIM) commands for using them and in searching BIM data, they relied on the Industry Foundation Classes (IFC) format, which involves converting BIM data to IFC. Such methods did not conduce to direct retrieval in BIM software. In the latter case, data search was possible, but data manipulation was not. To improve on the limitations of previous studies, this study developed a building information modeling automatic speech recognition (BIMASR) framework that requires no knowledge of BIM commands, which allows for the input of natural language (NL)-based questions into BIM software using human voice to search and manipulate data. The framework consists of three modules: one for voice recognition, one for natural language processing (syntax and semantic analysis), and one for BIM data preprocessing and interworking with relational databases. The manipulation of BIM data with NL-based speech recognition converts the BIM operating environment from an expert-oriented into a user-oriented environment. This conversion allows for more BIM interaction and the popularization of BIM use and enhances the use of BIM in dynamic environments such as virtual reality, augmented reality, and holograms, where conventional input devices are typically absent. [ABSTRACT FROM AUTHOR] Copyright of Journal of Construction Engineering & Management is the property of American Society of Civil Engineers and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=152144732&site=bsi-live"
"Biomedical named entity recognition and linking datasets: survey and our recent development.","Huang, Ming-Siang; Lai, Po-Ting; Lin, Pei-Yen; You, Yu-Ting; Tsai, Richard Tzong-Han; Hsu, Wen-Lian","Briefings in Bioinformatics",="14675463",,="Nov2020","21","6","2219","20","147531223","10.1093/bib/bbaa054","Oxford University Press / USA","Article","NATURAL language processing; INTERNET servers; Data Processing, Hosting, and Related Services; PROTEIN-protein interactions; NAMED-entity recognition","biological information retrieval; biomedical dataset; biomedical natural language processing; named entity recognition","Natural language processing (NLP) is widely applied in biological domains to retrieve information from publications. Systems to address numerous applications exist, such as biomedical named entity recognition (BNER), named entity normalization (NEN) and protein–protein interaction extraction (PPIE). High-quality datasets can assist the development of robust and reliable systems; however, due to the endless applications and evolving techniques, the annotations of benchmark datasets may become outdated and inappropriate. In this study, we first review commonlyused BNER datasets and their potential annotation problems such as inconsistency and low portability. Then, we introduce a revised version of the JNLPBA dataset that solves potential problems in the original and use state-of-the-art named entity recognition systems to evaluate its portability to different kinds of biomedical literature, including protein–protein interaction and biology events. Lastly, we introduce an ensembled biomedical entity dataset (EBED) by extending the revised JNLPBA dataset with PubMed Central full-text paragraphs, figure captions and patent abstracts. This EBED is a multi-task dataset that covers annotations including gene, disease and chemical entities. In total, it contains 85000 entity mentions, 25000 entity mentions with database identifiers and 5000 attribute tags. To demonstrate the usage of the EBED, we review the BNER track from the AI CUP Biomedical Paper Analysis challenge. Availability: The revised JNLPBA dataset is available at https://iasl-btm.iis.sinica.edu.tw/BNER/Content/Re vised_JNLPBA.zip. The EBED dataset is available at https://iasl-btm.iis.sinica.edu.tw/BNER/Content/AICUP _EBED_dataset.rar. Contact: Email: thtsai@g.ncu.edu.tw , Tel. 886-3-4227151 ext. 35203, Fax: 886-3-422-2681 Email: hsu@iis.sinica.edu.tw , Tel. 886-2-2788-3799 ext. 2211, Fax: 886-2-2782-4814 Supplementary information: Supplementary data are available at Briefings in Bioinformatics online. [ABSTRACT FROM AUTHOR] Copyright of Briefings in Bioinformatics is the property of Oxford University Press / USA and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=147531223&site=bsi-live"
"Building siamese attention-augmented recurrent convolutional neural networks for document similarity scoring.","Han, Sifei; Shi, Lingyun; Richie, Russell; Tsui, Fuchiang R.","Information Sciences",="00200255",,="Nov2022","615",,"90","13","160251254","10.1016/j.ins.2022.10.032","Elsevier B.V.","Article","ARTIFICIAL neural networks; NATURAL language processing; JOB resumes; CONVOLUTIONAL neural networks; RECURRENT neural networks","Attention neural network; Deep learning; Information retrieval; Machine learning; Natural language processing; Text similarity","• We proposed a new architecture - the Siamese attention-augmented recurrent convolutional neural network (S-ARCNN). • We compared the performance of S-ARCNN with eight popular models for measuring document similarity. • Our model outperformed the state-of-the-art Transformer based model (Sentence BERT) by over 5% in F1. • Simply fitting an optimal decision threshold can significantly improve pre-trained BERT model for the new task. • S-ARCNN performed best in longer question pairs (length > = 50 words). Automatically measuring document similarity is imperative in natural language processing, with applications ranging from recommendation to duplicate document detection. State-of-the-art approach in document similarity commonly involves deep neural networks, yet there is little study on how different architectures may be combined. Thus, we introduce the Siamese Attention-augmented Recurrent Convolutional Neural Network (S-ARCNN) that combines multiple neural network architectures. In each subnetwork of S-ARCNN, a document passes through a bidirectional Long Short-Term Memory (bi-LSTM) layer, which sends representations to local and global document modules. A local document module uses convolution, pooling, and attention layers, whereas a global document module uses last states of the bi-LSTM. Both local and global features are concatenated to form a single document representation. Using the Quora Question Pairs dataset, we evaluated S-ARCNN, Siamese convolutional neural networks (S-CNNs), Siamese LSTM, and two BERT models. While S-CNNs (82.02% F1) outperformed S-ARCNN (79.83% F1) overall, S-ARCNN slightly outperformed S-CNN on duplicate question pairs with more than 50 words (39.96% vs. 39.42% accuracy). With the potential advantage of S-ARCNN for processing longer documents, S-ARCNN may help researchers identify collaborators with similar research interests, help editors find potential reviewers, or match resumes with job descriptions. [ABSTRACT FROM AUTHOR] Copyright of Information Sciences is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=160251254&site=bsi-live"
"Deep supervised hashing network with integrated regularisation.","Liao, Jianxin; Li, Baoran; Yang, Di; Wang, Jingyu; Qi, Qi; Wang, Jing","IET Image Processing (Wiley-Blackwell)",="17519659",,="Oct2019","13","12","2143","9","148084114","10.1049/iet-ipr.2018.6644","Wiley-Blackwell","Article",,"approximate nearest neighbour search; binary codes; binary hash code; compact binary codes; deep hashing system; deep structures; deep supervised hashing network; DSHIR system; existing end‐to‐end deep hashing systems; existing methods; feature extraction; file organisation; good hash codes; hashing methods; image classification; image representation; image retrieval; indexing; information retrieval; integrated regularisation system; large‐scale multimedia retrieval tasks; learning (artificial intelligence); retrieval efficiency; state‐of‐the‐art systems; storage","Hashing has been widely deployed to approximate nearest neighbour search for large‐scale multimedia retrieval tasks due to storage and retrieval efficiency. State‐of‐the‐art supervised hashing methods for image retrieval construct deep structures to simultaneously learn image representation and generate good hash codes, and the key step among them is simultaneously learned feature representation and binary hash code. Existing methods use similarity and regularity loss to train deep hashing systems, but these two functions usually work together but not cooperative, which may lead to inadequate performance of the whole system. In this study, a new method for training deep hashing system to learn compact binary codes is presented. The deep supervised hashing network with integrated regularisation (DSHIR) system develop the zero division restriction as a new part of the loss function, which settles the problem of cooperatively guiding the system generate similarity preserving binary codes. DSHIR system also modifies the similarity handling loss to better extract features from image data, which promotes the performance compared to existing end‐to‐end deep hashing systems. Experiments show that DSHIR yields about 10 per cent higher mean average precision on CIFAR‐10 dataset, and also promote on other evaluation indexes compared with state‐of‐the‐art systems. [ABSTRACT FROM AUTHOR] Copyright of IET Image Processing (Wiley-Blackwell) is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=148084114&site=bsi-live"
"Detecting new Chinese words from massive domain texts with word embedding.","Qian, Yu; Du, Yang; Deng, Xiongwen; Ma, Baojun; Ye, Qiongwei; Yuan, Hua","Journal of Information Science",="01655515",,="Apr2019","45","2","196","16","135207149","10.1177/0165551518786676","Sage Publications, Ltd.","Article","INFORMATION retrieval; DATA mining; WORD recognition; VOCABULARY; CHINESE language","Natural language processing; new word detection; similarity measurement; textual information retrieval; word embedding","Textual information retrieval (TIR) is based on the relationship between word units. Traditional word segmentation techniques attempt to discern the word units accurately from texts; however, they are unable to appropriately and efficiently identify all new words. Identification of new words, especially in languages such as Chinese, remains a challenge. In recent years, word embedding methods have used numerical word vectors to retain the semantic and correlated information between words in a corpus. In this article, we propose the word-embedding-based method (WEBM), a novel method that combines word embedding and frequent n-gram string mining for discovering new words from domain corpora. First, we mapped all word units in a domain corpus to a high-dimension word vector space. Second, we used a frequent n-gram word string mining method to identify a set of candidates for new words. We designed a pruning strategy based on the word vectors to quantify the possibility of a word string being a new word, thereby allowing the evaluation of candidates based on the similarity of word units in the same string. In a comparative study, our experimental results revealed that WEBM had a great advantage in detecting new words from massive Chinese corpora. [ABSTRACT FROM AUTHOR] Copyright of Journal of Information Science is the property of Sage Publications, Ltd. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=135207149&site=bsi-live"
"Developing intuitive and explainable algorithms through inspiration from human physiology and computational biology.","Turki, Houcemeddine; Taieb, Mohamed Ali Hadj; Aouicha, Mohamed Ben","Briefings in Bioinformatics",="14675463",,="Sep2021","22","5","1","2","152975229","10.1093/bib/bbab081","Oxford University Press / USA","Article","ARTIFICIAL intelligence; ALGORITHMS; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); Research and development in the physical, engineering and life sciences; Research and Development in Biotechnology; COMPUTATIONAL biology; HUMAN physiology; INSPIRATION","biological computing; computational biology; explainable artificial intelligence; human physiology; information retrieval","In this letter, we explain how intuitive and explainable methods inspired from human physiology and computational biology can serve to simplify and ameliorate the way we process and generate knowledge resources. [ABSTRACT FROM AUTHOR] Copyright of Briefings in Bioinformatics is the property of Oxford University Press / USA and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=152975229&site=bsi-live"
"Editorial Artificial Intelligence and Innovation Management.","Tanev, Stoyan; Sandstrom, Gregory","Technology Innovation Management Review",="19270321",,="Dec2019","9","12","3","2","141074995","10.22215/timreview/1286","Carleton University, Talent First Network, Technology Innovation Management Review","Article","ARTIFICIAL intelligence; INNOVATION management; NATURAL language processing; AUSTRIA","3S Process; Advanced Analytics; AI; AI innovationmanagement; AI value chain; AImaturity; Artificial intelligence; Austria; Big Data; business education; connected health; data access; datamanagement; decision-making; design science; design thinking; enterprise platform; environmental scanning; front-end of innovation; governance; Harvard CaseMethod; information mobility; information processing; information retrieval; innovation; innovation search field; latent semantic indexing; opportunity; orchestration; patient- centered; SME",,"https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=141074995&site=bsi-live"
"Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs.","Malkov, Yu A.; Yashunin, D. A.","IEEE Transactions on Pattern Analysis & Machine Intelligence",="01628828",,="Apr2020","42","4","824","13","143315070","10.1109/TPAMI.2018.2889473","IEEE","Article","ALGORITHMS; NEW South Wales; METRIC spaces","approximate search; artificial intelligence; big data; data structures; Graph and tree search strategies; graphs and networks; information search and retrieval; information storage and retrieval; information technology and systems; nearest neighbor search; search process; similarity search","We present a new approach for the approximate K-nearest neighbor search based on navigable small world graphs with controllable hierarchy (Hierarchical NSW, HNSW). The proposed solution is fully graph-based, without any need for additional search structures (typically used at the coarse search stage of the most proximity graph techniques). Hierarchical NSW incrementally builds a multi-layer structure consisting of a hierarchical set of proximity graphs (layers) for nested subsets of the stored elements. The maximum layer in which an element is present is selected randomly with an exponentially decaying probability distribution. This allows producing graphs similar to the previously studied Navigable Small World (NSW) structures while additionally having the links separated by their characteristic distance scales. Starting the search from the upper layer together with utilizing the scale separation boosts the performance compared to NSW and allows a logarithmic complexity scaling. Additional employment of a heuristic for selecting proximity graph neighbors significantly increases performance at high recall and in case of highly clustered data. Performance evaluation has demonstrated that the proposed general metric space search index is able to strongly outperform previous opensource state-of-the-art vector-only approaches. Similarity of the algorithm to the skip list structure allows straightforward balanced distributed implementation. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Pattern Analysis & Machine Intelligence is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=143315070&site=bsi-live"
"Efficient feature extraction model for validation performance improvement of duplicate bug report detection in software bug triage systems.","Soleimani Neysiani, Behzad; Babamir, Seyed Morteza; Aritsugi, Masayoshi","Information & Software Technology",="09505849",,="Oct2020","126",,"N.PAG","1","145736248","10.1016/j.infsof.2020.106344","Elsevier B.V.","Article","NATURAL language processing; COMPUTER software; Software publishers (except video game publishers); Computer and software stores; Computer and Computer Peripheral Equipment and Software Merchant Wholesalers; Computer, computer peripheral and pre-packaged software merchant wholesalers; FEATURE extraction; MODEL validation; MACHINE learning; EXTRACTION techniques","Bug reports; Dimension reduction; Duplicate detection; Feature extraction; Feature selection; Information retrieval; Natural language processing; Textual similarity metric","There are many duplicate bug reports in the semi-structured software repository of various software bug triage systems. The duplicate bug report detection (DBRD) process is a significant problem in software triage systems. The DBRD problem has many issues, such as efficient feature extraction to calculate similarities between bug reports accurately, building a high-performance duplicate detector model, and handling continuous real-time queries. Feature extraction is a technique that converts unstructured data to structured data. The main objective of this study is to improve the validation performance of DBRD using a feature extraction model. This research focuses on feature extraction to build a new general model containing all types of features. Moreover, it introduces a new feature extractor method to describe a new viewpoint of similarity between texts. The proposed method introduces new textual features based on the aggregation of term frequency and inverse document frequency of text fields of bug reports in uni-gram and bi-gram forms. Further, a new hybrid measurement metric is proposed for detecting efficient features, whereby it is used to evaluate the efficiency of all features, including the proposed ones. The validation performance of DBRD was compared for the proposed features and state-of-the-art features. To show the effectiveness of our model, we applied it and other related studies to DBRD of the Android, Eclipse, Mozilla, and Open Office datasets and compared the results. The comparisons showed that our proposed model achieved (i) approximately 2% improvement for accuracy and precision and more than 4.5% and 5.9% improvement for recall and F1-measure , respectively, by applying the linear regression (LR) and decision tree (DT) classifiers and (ii) a performance of 91%−99% (average ~97%) for the four metrics, by applying the DT classifier as the best classifier. Our proposed features improved the validation performance of DBRD concerning runtime performance. The pre-processing methods (primarily stemming) could improve the validation performance of DBRD slightly (up to 0.3%), but rule-based machine learning algorithms are more useful for the DBRD problem. The results showed that our proposed model is more effective both for the datasets for which state-of-the-art approaches were effective (i.e., Mozilla Firefox) and those for which state-of-the-art approaches were less effective (i.e., Android). The results also showed that the combination of all types of features could improve the validation performance of DBRD even for the LR classifier with less validation performance, which can be implemented easily for software bug triage systems. Without using the longest common subsequence (LCS) feature, which is effective but time-consuming, our proposed features could cover the effectiveness of LCS with lower time-complexity and runtime overhead. In addition, a statistical analysis shows that the results are reliable and can be generalized to other datasets or similar classifiers. [ABSTRACT FROM AUTHOR] Copyright of Information & Software Technology is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=145736248&site=bsi-live"
"Excavating the mother lode of human-generated text: A systematic review of research that uses the wikipedia corpus.","Mehdi, Mohamad; Okoli, Chitu; Mesgari, Mostafa; Nielsen, Finn Årup; Lanamäki, Arto","Information Processing & Management",="03064573",,="Mar2017","53","2","505","25","120834017","10.1016/j.ipm.2016.07.003","Elsevier B.V.","Article","INFORMATION retrieval; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); Research and development in the physical, engineering and life sciences; META-analysis; COMPUTER science research; ONTOLOGY; WIKIPEDIA","Information extraction; Information retrieval; Literature review; Natural language processing; Ontologies; Wikipedia","Although primarily an encyclopedia, Wikipedia’s expansive content provides a knowledge base that has been continuously exploited by researchers in a wide variety of domains. This article systematically reviews the scholarly studies that have used Wikipedia as a data source, and investigates the means by which Wikipedia has been employed in three main computer science research areas: information retrieval, natural language processing, and ontology building. We report and discuss the research trends of the identified and examined studies. We further identify and classify a list of tools that can be used to extract data from Wikipedia, and compile a list of currently available data sets extracted from Wikipedia. [ABSTRACT FROM AUTHOR] Copyright of Information Processing & Management is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=120834017&site=bsi-live"
"Exploiting named entity recognition for improving syntactic-based web service discovery.","Lizarralde, Ignacio; Mateos, Cristian; Rodriguez, Juan Manuel; Zunino, Alejandro","Journal of Information Science",="01655515",,="Jun2019","45","3","398","18","136492938","10.1177/0165551518793321","Sage Publications, Ltd.","Article","WEB services; INFORMATION retrieval; COMPUTER software industry; BIG data; DATA mining; Computer, computer peripheral and pre-packaged software merchant wholesalers; Computer and software stores; Computer and Computer Peripheral Equipment and Software Merchant Wholesalers; Software publishers (except video game publishers); Software Publishers; Data Processing, Hosting, and Related Services","Information retrieval; named entity recognition; natural language processing; service-oriented computing; web Services","Web Services have become essential to the software industry as they represent reusable, remotely accessible functionality and data. Since Web Services must be discovered before being consumed, many discovery approaches applying classic Information Retrieval techniques, which store and process textual service descriptions, have arisen. These efforts are affected by term mismatch: a description relevant to a query can be retrieved only if they share many words. We present an approach to improve Web Service discoverability that automatically augments Web Service descriptions and can be used on top of such existing syntactic-based approaches. We exploit Named Entity Recognition to identify entities in descriptions and expand them with information from public text corpora, for example, Wikidata, mitigating term mismatch since it exploits both synonyms and hypernyms. We evaluated our approach together with classical syntactic-based service discovery approaches using a real 1274-service dataset, achieving up to 15.06% better Recall scores, and up to 17% Precision-at-1, 8% Precision-at-2 and 4% Precision-at-3. [ABSTRACT FROM AUTHOR] Copyright of Journal of Information Science is the property of Sage Publications, Ltd. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=136492938&site=bsi-live"
"Feedback evaluations to promote image captioning.","He, Jun; Zhao, Yijia; Sun, Bo; Yu, Lejun","IET Image Processing (Wiley-Blackwell)",="17519659",,="Nov2020","14","13","3021","7","148084462","10.1049/iet-ipr.2019.1317","Wiley-Blackwell","Article",,"ARL; auxiliary retrieval loss; caption‐generating process; captioning model; captioning process; discriminability score; evaluation reward; feature extraction; feedback evaluation method; feedback evaluations; generated caption; image captioning evaluation metrics; image retrieval; information retrieval; learning (artificial intelligence); neural nets; policy gradient problem; retrieval model; text analysis","Image captioning can be treated as a policy gradient problem. A retrieval model to obtain the discriminability score to distinguish between two images, given the caption for one of them, has been proposed previously; the discriminability score and one of the image captioning evaluation metrics were optimised using policy gradient. Based on this, two methods to evaluate the caption and caption‐generating process, referred to as feedback evaluations, are proposed in this study. The results of the evaluations were used to improve the model. First, an auxiliary retrieval loss (ARL) is introduced to evaluate the generated caption to improve the discriminability of the model. ARL has been utilised as a feedback evaluation method because it calculates similarity between the generated caption and convolutional neural network features. With ARL, a higher similarity and better discriminability were achieved. Second, an evaluation reward is introduced to evaluate the captioning process. With ER, the overall evaluation metrics can be improved. A policy gradient was used, and a captioning model could be trained by jointly adjusting the captioning process and captioning itself. The attention long short‐term memory network was trained with ARL and ER successively and it demonstrated state‐of‐the‐art performance on the COCO database. [ABSTRACT FROM AUTHOR] Copyright of IET Image Processing (Wiley-Blackwell) is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=148084462&site=bsi-live"
"Incorporating Deep Median Networks for Arabic Document Retrieval Using Word Embeddings-Based Query Expansion.","Farhan, Yasir Hadi; Tareq, Mustafa Abd; Shakir, Mohanaad; Shannaq, Boumedyen","Journal of Information Science Theory & Practice (JIStaP)",="22879099",,="2024","12","3","36","13","180644236","10.1633/JISTaP.2024.12.3.3","Korea Institute of Science & Technology Information","Article","NATURAL language processing; INFORMATION retrieval; INFORMATION networks; RESEARCH personnel; SYNONYMS","Arabic document retrieval; automatic query expansion; deep median networks; information retrieval; natural language processing; word embedding","The information retrieval (IR) process often encounters a challenge known as query-document vocabulary mismatch, where user queries do not align with document content, impacting search effectiveness. Automatic query expansion (AQE) techniques aim to mitigate this issue by augmenting user queries with related terms or synonyms. Word embedding, particularly Word2Vec, has gained prominence for AQE due to its ability to represent words as real-number vectors. However, AQE methods typically expand individual query terms, potentially leading to query drift if not carefully selected. To address this, researchers propose utilizing median vectors derived from deep median networks to capture query similarity comprehensively. Integrating median vectors into candidate term generation and combining them with the BM25 probabilistic model and two IR strategies (EQE1 and V2Q) yields promising results, outperforming baseline methods in experimental settings. [ABSTRACT FROM AUTHOR] Copyright of Journal of Information Science Theory & Practice (JIStaP) is the property of Korea Institute of Science & Technology Information and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=180644236&site=bsi-live"
"INEX Tweet Contextualization task: Evaluation, results and lesson learned.","Bellot, Patrice; Moriceau, Véronique; Mothe, Josiane; SanJuan, Eric; Tannier, Xavier","Information Processing & Management",="03064573",,="Sep2016","52","5","801","19","116988472","10.1016/j.ipm.2016.03.002","Elsevier B.V.","Article","MICROBLOGS; ANAPHORA (Linguistics); PARTS of speech; SENTENCES (Grammar); WIKIPEDIA","Automatic summarization; Contextual information retrieval; Focus information retrieval; Kullback–Leibler divergence; Natural language processing; Question answering; Short text contextualization; Text informativeness; Text readability; Textual references; Tweet contextualization; Tweet understanding; Wikipedia","Microblogging platforms such as Twitter are increasingly used for on-line client and market analysis. This motivated the proposal of a new track at CLEF INEX lab of Tweet Contextualization . The objective of this task was to help a user to understand a tweet by providing him with a short explanatory summary (500 words). This summary should be built automatically using resources like Wikipedia and generated by extracting relevant passages and aggregating them into a coherent summary. Running for four years, results show that the best systems combine NLP techniques with more traditional methods. More precisely the best performing systems combine passage retrieval, sentence segmentation and scoring, named entity recognition, text part-of-speech (POS) analysis, anaphora detection, diversity content measure as well as sentence reordering. This paper provides a full summary report on the four-year long task. While yearly overviews focused on system results, in this paper we provide a detailed report on the approaches proposed by the participants and which can be considered as the state of the art for this task. As an important result from the 4 years competition, we also describe the open access resources that have been built and collected. The evaluation measures for automatic summarization designed in DUC or MUC were not appropriate to evaluate tweet contextualization, we explain why and depict in detailed the LogSim measure used to evaluate informativeness of produced contexts or summaries. Finally, we also mention the lessons we learned and that it is worth considering when designing a task. [ABSTRACT FROM AUTHOR] Copyright of Information Processing & Management is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=116988472&site=bsi-live"
"Information is essential for competitive and cost-effective public procurement.","Gorgun, Mustafa Kaan; Kutlu, Mucahid; Tas, Bedri Kamil Onur","Journal of Information Science",="01655515",,="Dec2022",,,"1",,"160812237","10.1177/01655515221141042","Sage Publications, Ltd.","Article",,"Economic analysis; information retrieval; natural language processing; public procurement","Public authorities promote transparent public procurement practices to increase competition and reduce public procurement costs. In this article, we focus on public procurement of the European Union (EU). We employ a multidisciplinary approach to analyse economic effects of information in public procurement. We quantify the information content of 2,390,630 EU public procurement notices published in 22 different languages using natural language processing techniques. Subsequently, we examine the impact of the information content on public procurement outcomes. We find that higher information levels have significant positive effects. Competition is considerably higher when notices contain more information. On average, contract prices would be 6%–8% lower if notices were to contain adequate information. EU governments could save up to € 80 billion if all public procurement notices were to have detailed information. Based on our comprehensive analysis, we believe that authorities should regulate the information content of notices to promote competition and cost-effectiveness in public procurement. [ABSTRACT FROM AUTHOR] Copyright of Journal of Information Science is the property of Sage Publications, Ltd. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=160812237&site=bsi-live"
"Integrating large language models and generative artificial intelligence tools into information literacy instruction.","Carroll, Alexander J.; Borycz, Joshua","Journal of Academic Librarianship",="00991333",,="Jul2024","50","4","N.PAG","1","177854578","10.1016/j.acalib.2024.102899","Elsevier B.V.","Article","INFORMATION science; LANGUAGE & languages; GENERATIVE artificial intelligence; INFORMATION literacy; STEM education; ENGINEERING education","Critical thinking; Generative artificial intelligence; Information literacy; Information retrieval; Large language models","Generative artificial intelligence (AI) and large language models (LLMs) have induced a mixture of excitement and panic among educators. However, there is a lack of consensus over how much experience science and engineering students have with using these tools for research-related tasks. Likewise, it is not yet known how educators and information professionals can leverage these tools to teach students strategies for information retrieval and knowledge synthesis. This study assesses the extent of students' use of AI tools in research-related tasks and if information literacy instruction could impact their perception of these tools. Responses to Likert-scale questions indicate that many students did not have extensive experience using LLMs for research-related purposes prior to the information literacy sessions. However, after participating in a didactic lecture and discussion with an engineering librarian that explored how to use these tools effectively and responsibly, many students reported viewing these tools as potentially useful for future assignments. Student responses to open-response questions suggest that librarian-led information literacy training can assist students in developing more sophisticated understandings of the limitations and use cases for artificial intelligence in inquiry-based coursework. [ABSTRACT FROM AUTHOR] Copyright of Journal of Academic Librarianship is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=177854578&site=bsi-live"
"Interactive query expansion for professional search applications.","Russell-Rose, Tony; Gooch, Philip; Kruschwitz, Udo","Business Information Review",="02663821",,="Sep2021","38","3","127","11","153606127","10.1177/02663821211034079","Sage Publications Inc.","Article","INFORMATION professionals; KNOWLEDGE workers; MEDICAL personnel; PATENT lawyers; EMPLOYEE recruitment; Human Resources Consulting Services; Administration of Human Resource Programs (except Education, Public Health, and Veterans' Affairs Programs)","Information retrieval; machine learning; natural language processing; ontologies; professional search; query expansion","Knowledge workers (such as healthcare information professionals, patent agents and recruitment professionals) undertake work tasks where search forms a core part of their duties. In these instances, the search task is often complex and time-consuming and requires specialist expert knowledge to formulate accurate search strategies. Interactive features such as query expansion can play a key role in supporting these tasks. However, generating query suggestions within a professional search context requires that consideration be given to the specialist, structured nature of the search strategies they employ. In this paper, we investigate a variety of query expansion methods applied to a collection of Boolean search strategies used in a variety of real-world professional search tasks. The results demonstrate the utility of context-free distributional language models and the value of using linguistic cues to optimise the balance between precision and recall. [ABSTRACT FROM AUTHOR] Copyright of Business Information Review is the property of Sage Publications Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=153606127&site=bsi-live"
"Knowledge powered by artificial intelligence.","Pichman, Brian","Information Services & Use",="01675265",,="Nov2024",,,"1",,"180869637","10.1177/18758789241299017","IOS Press","Article","ARTIFICIAL intelligence; LEGACY systems; KNOWLEDGE management; GENERATIVE artificial intelligence; LANGUAGE models","AI ethics; bias reduction; customer support AI; generative AI; healthcare AI; knowledge forecasting; knowledge management; large language models; legacy systems; multilingual support; personalized information retrieval; RAG; real-time collaboration; Retrieval Augmented Generation systems","Generative Artificial Intelligence (GenAI) has revolutionized knowledge management, offering unprecedented capabilities for creating, proofing, summarizing, and evaluating documentation. This paper explores how AI, particularly large language models (LLMs), and Retrieval Augmented Generation (RAG) systems, can streamline the development of knowledge articles while addressing ethical concerns such as data ownership and bias. We examine practical applications, including real-time collaboration, multilingual support, personalized information retrieval, and automated knowledge forecasting. Additionally, we explore AI’s role in bridging legacy systems, reducing biases, and enhancing decision-making. Ultimately, AI extends beyond generating content, shaping a more efficient, inclusive, and innovative approach to knowledge management. This article is based upon a presentation given at the 2024 NISO Plus Conference that was held in Baltimore, MD, USA, February 13–14, 2024. [ABSTRACT FROM AUTHOR] Copyright of Information Services & Use is the property of IOS Press and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=180869637&site=bsi-live"
"Leveraging AI-based Decision Support for Opportunity Analysis.","Groher, Wolfgang; Rademacher, Friedrich-Wilhelm; Csillaghy, André","Technology Innovation Management Review",="19270321",,="Dec2019","9","12","29","7","141074992","10.22215/timreview/1289","Carleton University, Talent First Network, Technology Innovation Management Review","Article","ARTIFICIAL intelligence; INFORMATION retrieval; DESIGN science; INFORMATION processing; LATENT semantic analysis; CONCEPTUAL models","artificial intelligence; decision-making; design science; environmental scanning; front-end of innovation; information processing; information retrieval; innovation search field; latent semantic indexing; opportunity","The dynamics and speed of change in corporate environments have increased. At the front-end of innovation, firms are challenged to evaluate growing amounts of information within shorter time frames in order to stay competitive. Either they spend significant time on structured data analysis, at the risk of delayed market launch, or they follow their intuition, at the risk of not meeting market trends. Both scenarios constitute a significant risk for a firm's continued existence.Motivated by this, a conceptual model is presented in this paper that aims at remediating these risks. Grounded on design science methodology, it concentrates on previous assessments of innovation search fields. These innovation search fields assist in environmental scanning and lay the foundation for deciding which opportunities to pursue. The model applies a novel AI-based approach, which draws on natural language processing and information retrieval. To provide decision support, the approach includes market-, technology-, and firm-related criteria. This allows us to replace intuitive decision-making by fact-based considerations. In addition, an often-iterative approach for environmental scanning is replaced by a more straightforward process. Early testing of the conceptual model has shown results of increased quality and speed of decision-making. Further testing and feedback is still required to enhance and calibrate the AI-functionality. Applied in business environments, the approach can contribute to remediate fuzziness in early front-end activities, thus helping direct innovation managers to ""do the right things"". [ABSTRACT FROM AUTHOR] Copyright of Technology Innovation Management Review is the property of Carleton University, Talent First Network, Technology Innovation Management Review and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=141074992&site=bsi-live"
"Listwise approach based on the cross‐correntropy for learning to rank.","Wu, Mintao; Zhu, Jihua; Wang, Jun; Pang, Shanmin; Li, Yaochen","Electronics Letters (Wiley-Blackwell)",="00135194",,="Jul2018","54","15","878","3","148786885","10.1049/el.2018.0815","Wiley-Blackwell","Article",,"cross‐correntropy loss; document retrieval; entropy; gradient descent algorithm; gradient methods; information retrieval; learning (artificial intelligence); learning to rank; ListCCE; listwise approach; listwise loss function; neural network model; training model","The problem of learning to rank is addressed and a novel listwise approach by taking document retrieval as an example is proposed. It first introduces the concept of cross‐correntropy into learning to rank and then proposes the listwise loss function based on the cross‐correntropy between the ranking list given by the label and the one predicted by training model. The use of the cross‐correntropy loss leads to the development of the listwise approach called ListCCE, which employs the gradient descent algorithm to train a neural network model. Experimental results tested on publicly available data sets show that the proposed approach performs better than some existing approaches. [ABSTRACT FROM AUTHOR] Copyright of Electronics Letters (Wiley-Blackwell) is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=148786885&site=bsi-live"
"Machine learning and ontology-based novel semantic document indexing for information retrieval.","Sharma, Anil; Kumar, Suresh","Computers & Industrial Engineering",="03608352",,="Feb2023","176",,"N.PAG","1","161600684","10.1016/j.cie.2022.108940","Elsevier B.V.","Article","INFORMATION retrieval; COMPUTER science; Research and development in the physical, engineering and life sciences; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); ONTOLOGIES (Information retrieval); MACHINE learning; ANALYTIC hierarchy process; INFORMATION needs","Computer science ontology; Concept extraction; Document indexing; Information retrieval; Machine learning; Natural language processing; Semantic web","• Document key phrases to ontology concept mapping with limited or no related concepts in the ontology. • Analytic hierarchy process based application specific concept feature weights. • Document's concept term variations and synonyms mapped on domain ontology. • Average F-measure enhanced by 25% compared to the state-of-the-art. The goal of information retrieval (IR) systems is to find the contents most closely related to the user's information needs from a pool of information. However, conventional IR methods neglect semantic descriptions of document contents and index documents based on the words that they include. When users and indexing systems use different terms to express the same subject, a vocabulary gap emerges. To overcome this limitation and to enhance the effectiveness of the IR systems, this paper introduced a novel hybrid semantic document indexing employing machine learning and domain ontology. The presented technique uses a skip-gram with negative sampling-based machine learning model and a domain ontology to determine the concepts for annotating unstructured documents. The proposed work also introduced multiple feature based novel concept ranking algorithm where statistical, semantic, and scientific named entity features of the concept were used to assign relevance weight to the annotations. The fuzzy analytical hierarchy process was used to derive the parameters of these feature weights. The final step is to rank the concepts according to their relevance to the document. Five benchmark publicly accessible datasets from the computer science domain were used in a series of experiments to validate the results of presented method. Experiment findings showed that the proposed method performs better than state-of-the-art techniques on these datasets, by improving average accuracy by 29%, while an improvement of 25% was recorded in F-measure. The improvement in average accuracy demonstrates that the performance of the proposed approach is better than the state-of-the-art methods in extracting document concepts accurately even when the same concept is referred to by distinct terms in the document and domain ontologies. The proposed system's ability to find similar concepts when the documents possess no concept from domain ontology is demonstrated by the improvement in F-measure, which is attributed to high recall rates of the proposed indexing scheme while maintaining high accuracy. [ABSTRACT FROM AUTHOR] Copyright of Computers & Industrial Engineering is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=161600684&site=bsi-live"
"Measuring the Semantic Uncertainty of News Events for Evolution Potential Estimation.","XIANGFENG LUO; JUNYU XUAN; JIE LU; GUANGQUAN ZHANG","ACM Transactions on Information Systems",="10468188",,="2016","34","4","1","25","116348725","10.1145/2903719","Association for Computing Machinery","Article","DECISION making; PUBLIC relations; UNCERTAINTY (Information theory); STATISTICAL correlation; Public Relations Agencies; SEMANTIC computing","Information search and retrieval; natural language processing; news event; semantic analysis; text mining","The evolution potential estimation of news events can support the decision making of both corporations and governments. For example, a corporation could manage its public relations crisis in a timely manner if a negative news event about this corporation is known with large evolution potential in advance. However, existing state-of-the-art methods are mainly based on time series historical data, which are not suitable for the news events with limited historical data and bursty properties. In this article, we propose a purely content-based method to estimate the evolution potential of the news events. The proposed method considers a news event at a given time point as a system composed of different keywords, and the uncertainty of this system is defined and measured as the Semantic Uncertainty of this news event. At the same time, an uncertainty space is constructed with two extreme states: the most uncertain state and the most certain state. We believe that the Semantic Uncertainty has correlation with the content evolution of the news events, so it can be used to estimate the evolution potential of the news events. In order to verify the proposed method, we present detailed experimental setups and results measuring the correlation of the Semantic Uncertainty with the Content Change of news events using collected news events data. The results show that the correlation does exist and is stronger than the correlation of value from the time-series-based method with the Content Change. Therefore, we can use the Semantic Uncertainty to estimate the evolution potential of news events. [ABSTRACT FROM AUTHOR] Copyright of ACM Transactions on Information Systems is the property of Association for Computing Machinery and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=116348725&site=bsi-live"
"Modeling and Learning Distributed Word Representation with Metadata for Question Retrieval.","Zhou, Guangyou; Huang, Jimmy Xiangji","IEEE Transactions on Knowledge & Data Engineering",="10414347",,="Jun2017","29","6","1226","14","122814210","10.1109/TKDE.2017.2665625","IEEE","Article","METADATA; FACILITATED learning; INTERROGATIVE (Grammar); DOCUMENT type definitions; ARCS Model of Motivational Design","Aggregates; community question answering; Computational modeling; Context modeling; information retrieval; Kernel; Knowledge discovery; Metadata; Natural language processing; question retrieval; Semantics; text mining","Community question answering (cQA) has become an important issue due to the popularity of cQA archives on the Web. This paper focuses on addressing the lexical gap problem in question retrieval. Question retrieval in cQA archives aims to find the existing questions that are semantically equivalent or relevant to the queried questions. However, the lexical gap problem brings a new challenge for question retrieval in cQA. In this paper, we propose to model and learn distributed word representations with metadata of category information within cQA pages for question retrieval using two novel category powered models. One is a basic category powered model called MB-NET and the other one is an enhanced category powered model called ME-NET which can better learn the distributed word representations and alleviate the lexical gap problem. To deal with the variable size of word representation vectors, we employ the framework of fisher kernel to transform them into the fixed-length vectors. Experimental results on large-scale English and Chinese cQA data sets show that our proposed approaches can significantly outperform state-of-the-art retrieval models for question retrieval in cQA. Moreover, we further conduct our approaches on large-scale automatic evaluation experiments. The evaluation results show that promising and significant performance improvements can be achieved. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Knowledge & Data Engineering is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=122814210&site=bsi-live"
"Opera Clustering: K-means on librettos datasets.","Harim Jeong; Joo Hun Yoo","Journal of Internet Computing & Services",="15980170",,="Apr2022","23","2","45","8","156816744","10.7472/jksii.2022.23.2.45","Korean Society for Internet Information","Article","ARTIFICIAL intelligence; NATURAL language processing; K-means clustering; MACHINE learning; EMOTIONS","Classification; Embedding; K-means Clustering; Music Analysis; Music Information Retrieval; Natural Language Processing","With the development of artificial intelligence analysis methods, especially machine learning, various fields are widely expanding their application ranges. However, in the case of classical music, there still remain some difficulties in applying machine learning techniques. Genre classification or music recommendation systems generated by deep learning algorithms are actively used in general music, but not in classical music. In this paper, we attempted to classify opera among classical music. To this end, an experiment was conducted to determine which criteria are most suitable among, composer, period of composition, and emotional atmosphere, which are the basic features of music. To generate emotional labels, we adopted zero-shot classification with four basic emotions, ‘happiness’, ‘sadness’, ‘anger’, and ‘fear.’ After embedding the opera libretto with the doc2vec processing model, the optimal number of clusters is computed based on the result of the elbow method. Decided four centroids are then adopted in k-means clustering to classify unsupervised libretto datasets. We were able to get optimized clustering based on the result of adjusted rand index scores. With these results, we compared them with notated variables of music. As a result, it was confirmed that the four clusterings calculated by machine after training were most similar to the grouping result by period. Additionally, we were able to verify that the emotional similarity between composer and period did not appear significantly. At the end of the study, by knowing the period is the right criteria, we hope that it makes easier for music listeners to find music that suits their tastes. [ABSTRACT FROM AUTHOR] Copyright of Journal of Internet Computing & Services is the property of Korean Society for Internet Information and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=156816744&site=bsi-live"
"Predicting taxi demand hotspots using automated Internet Search Queries.","Markou, Ioulia; Kaiser, Kevin; Pereira, Francisco C.","Transportation Research Part C: Emerging Technologies",="0968090X",,="May2019","102",,"73","14","135915002","10.1016/j.trc.2019.03.001","Elsevier B.V.","Article","INTERNET searching; NATURAL language processing; GLOBALIZATION","Demand prediction; Information retrieval; Natural language processing; Query expansion; Special events","Highlights • Popular events can cause distinct taxi demand hotspots. • Internet search queries are proven useful for the prediction of demand hotspots. • Queries expansion with two new terms return more representative results. • MedLDA performs better than an independent topic modelling and classification process. • Terms with time expressions seem to be good prediction indicators in topics. Abstract Disruptions due to special events are a well-known challenge in transport operations, since the transport system is typically designed for habitual demand. Part of the problem relates to the difficulty in collecting comprehensive and reliable information early enough to prepare mitigation measures. A tool that automatically scans the internet for events and predicts their impact would strongly support transport management in many cities in the world. This study addresses the challenges related to retrieving and analyzing web documents about real world events, and using them for demand explanation (if related to a past event) and prediction (if a future one). Transport demand is predicted with a supervised topic modeling algorithm by utilizing information about social events retrieved using various strategies, which made use of search aggregation, natural language processing, and query expansion. It was found that a two-step process produced the highest accuracy for transport demand prediction, where different (but related) queries are used to retrieve an initial set of documents, and then, based on these documents, a final query is constructed that obtains the set of predictive documents. These are then used to model the most discriminating topics related to the transport demand. A framework was proposed that sequentially handles all stages of data gathering, enrichment, and prediction with the intention of generating automated search queries. [ABSTRACT FROM AUTHOR] Copyright of Transportation Research Part C: Emerging Technologies is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=135915002&site=bsi-live"
"Processing social media in real-time.","Spina, Damiano; Zubiaga, Arkaitz; Sheth, Amit; Strohmaier, Markus","Information Processing & Management",="03064573",,="May2019","56","3","1081","3","135137886","10.1016/j.ipm.2018.06.006","Elsevier B.V.","Article","DATA mining; NATURAL language processing; INFORMATION retrieval; SOCIAL media; SOCIAL context","Data mining; Information retrieval; Natural language processing; Social media mining",,"https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=135137886&site=bsi-live"
"Recognition of cursive video text using a deep learning framework.","Mirza, Ali; Siddiqi, Imran","IET Image Processing (Wiley-Blackwell)",="17519659",,="Dec2020","14","14","3444","12","148084505","10.1049/iet-ipr.2019.1070","Wiley-Blackwell","Article",,"background segmentation; bi‐directional recurrent neural networks; character recognition rate; complex ligatures; content‐based retrieval; context‐dependent shape variations; convolutional networks; convolutional neural network; cursive caption text; cursive scripts; cursive video text; deep learning; end‐to‐end framework; feature extraction; feature sequence extraction; image segmentation; information retrieval; learning (artificial intelligence); mature V‐OCRs; News channel videos; noncursive scripts; optical character recognition; overlapping ligatures; recurrent neural nets; sequence‐to‐sequence mapping; text analysis; text lines extraction; text regions; textual content‐based retrieval system; Urdu text; video frames; video optical character recognition systems; video retrieval; video signal processing; video text recognition","This study focuses on cursive text recognition appearing in videos, using a complete framework of deep neural networks. While mature video optical character recognition systems (V‐OCRs) are available for text in non‐cursive scripts, recognition of cursive scripts is marked by many challenges. These include complex and overlapping ligatures, context‐dependent shape variations and presence of a large number of dots and diacritics. The authors present an analytical technique for recognition of cursive caption text that relies on a combination of convolutional and recurrent neural networks trained in an end‐to‐end framework. Text lines extracted from video frames are preprocessed to segment the background and are fed to a convolutional neural network for feature extraction. The extracted feature sequences are fed to different variants of bi‐directional recurrent neural networks along with the ground truth transcription to learn sequence‐to‐sequence mapping. Finally, a connectionist temporal classification layer is employed to produce the final transcription. Experiments on a data set of more than 40,000 text lines from 11,192 video frames of various News channel videos reported an overall character recognition rate of 97.63%. The proposed work employs Urdu text as a case study but the findings can be generalised to other cursive scripts as well. [ABSTRACT FROM AUTHOR] Copyright of IET Image Processing (Wiley-Blackwell) is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=148084505&site=bsi-live"
"Retrieval Contrastive Learning for Aspect-Level Sentiment Classification.","Jian, Zhongquan; Li, Jiajian; Wu, Qingqiang; Yao, Junfeng","Information Processing & Management",="03064573",,="Jan2024","61","1","N.PAG","1","173784683","10.1016/j.ipm.2023.103539","Elsevier B.V.","Article",,"Aspect-level sentiment classification; Contrastive learning; Information retrieval; Natural language processing","Aspect-Level Sentiment Classification (ALSC) aims to assign specific sentiments to a sentence toward different aspects, which is one of the crucial challenges in the field of Natural Language Processing (NLP). Despite numerous approaches being proposed and obtaining prominent results, the majority of them focus on leveraging the relationships between the aspect and opinion words in a single instance while ignoring correlations with other instances, which will make models inevitably become trapped in local optima due to the absence of a global viewpoint. Instance representation derived from a single instance, on the one hand, the contained information is insufficient due to the lack of descriptions from other perspectives; on the other hand, its stored knowledge is redundant since the inability to filter extraneous content. To obtain a polished instance representation, we developed a Retrieval Contrastive Learning (RCL) framework to subtly extract intrinsic knowledge across instances. RCL consists of two modules: (a) obtaining retrieval instances by sparse retriever and dense retriever, and (b) extracting and learning the knowledge of the retrieval instances by using Contrastive Learning (CL). To demonstrate the superiority of RCL, five ALSC models are employed to conduct comprehensive experiments on three widely-known benchmarks. Compared with the baselines, ALSC models achieve substantial improvements when trained with RCL. Especially, ABSA-DeBERTa with RCL obtains new state-of-the-art results, which outperform the advanced methods by 0.92%, 0.23%, and 0.47% in terms of Macro F1 gains on Laptops, Restaurants, and Twitter, respectively. • We proposed RCL to enable ALSC models to generate polished representations. • RCL has two modules: obtain retrieval instances and learn common features by CL. • The sparse and dense retrievers are used to obtain high-quality retrieval instances. • The ALSC model can be enhanced greatly by training with RCL. • ABSA-DeBERTa obtains new state-of-the-art results by being trained with RCL. [ABSTRACT FROM AUTHOR] Copyright of Information Processing & Management is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=173784683&site=bsi-live"
"Retrieve–Revise–Refine: A novel framework for retrieval of concise entailing legal article set.","Nguyen, Chau; Nguyen, Phuong; Nguyen, Le-Minh","Information Processing & Management",="03064573",,="Jan2025","62","1","N.PAG","1","181036359","10.1016/j.ipm.2024.103949","Elsevier B.V.","Article",,"COLIEE competition; Information retrieval; Large language models; Legal article set retrieval; Retrieval–Revise–Refine framework","The retrieval of entailing legal article sets aims to identify a concise set of legal articles that holds an entailment relationship with a legal query or its negation. Unlike traditional information retrieval that focuses on relevance ranking, this task demands conciseness. However, prior research has inadequately addressed this need by employing traditional methods. To bridge this gap, we propose a three-stage Retrieve–Revise–Refine framework which explicitly addresses the need for conciseness by utilizing both small and large language models (LMs) in distinct yet complementary roles. Empirical evaluations on the COLIEE 2022 and 2023 datasets demonstrate that our framework significantly enhances performance, achieving absolute increases in the macro F2 score by 3.17% and 4.24% over previous state-of-the-art methods, respectively. Specifically, our Retrieve stage, employing various tailored fine-tuning strategies for small LMs, achieved a recall rate exceeding 0.90 in the top-5 results alone—ensuring comprehensive coverage of entailing articles. In the subsequent Revise stage, large LMs narrow this set, improving precision while sacrificing minimal coverage. The Refine stage further enhances precision by leveraging specialized insights from small LMs, resulting in a relative improvement of up to 19.15% in the number of concise article sets retrieved compared to previous methods. Our framework offers a promising direction for further research on specialized methods for retrieving concise sets of entailing legal articles, thereby more effectively meeting the task's demands. • Propose a novel three-stage Retrieve–Revise–Refine framework for concise legal article set retrieval. • Achieve 3.17% and 4.24% higher macro F2 scores on two evaluated datasets. • Retrieve 19.15% more concise sets of legal articles compared to previous methods. • Ablation studies show over 90% coverage within top-5 results in the Retrieval stage. [ABSTRACT FROM AUTHOR] Copyright of Information Processing & Management is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=181036359&site=bsi-live"
"Reviewer assignment algorithms for peer review automation: A survey.","Zhao, Xiquan; Zhang, Yangsen","Information Processing & Management",="03064573",,="Sep2022","59","5","N.PAG","1","158915546","10.1016/j.ipm.2022.103028","Elsevier B.V.","Article","MATHEMATICAL optimization; ALGORITHMS; NATURAL language processing; ASSIGNMENT problems (Programming)","Information retrieval; Matching degree; Natural language processing; Optimization algorithm; Peer review; Reviewer assignment problem","Assigning paper to suitable reviewers is of great significance to ensure the accuracy and fairness of peer review results. In the past three decades, many researchers have made a wealth of achievements on the reviewer assignment problem (RAP). In this survey, we provide a comprehensive review of the primary research achievements on reviewer assignment algorithm from 1992 to 2022. Specially, this survey first discusses the background and necessity of automatic reviewer assignment, and then systematically summarize the existing research work from three aspects, i.e., construction of candidate reviewer database, computation of matching degree between reviewers and papers, and reviewer assignment optimization algorithm, with objective comments on the advantages and disadvantages of the current algorithms. Afterwards, the evaluation metrics and datasets of reviewer assignment algorithm are summarized. To conclude, we prospect the potential research directions of RAP. Since there are few comprehensive survey papers on reviewer assignment algorithm in the past ten years, this survey can serve as a valuable reference for the related researchers and peer review organizers. [ABSTRACT FROM AUTHOR] Copyright of Information Processing & Management is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=158915546&site=bsi-live"
"Swat: A system for detecting salient Wikipedia entities in texts.","Ponza, Marco; Ferragina, Paolo; Piccinno, Francesco","Computational Intelligence",="08247935",,="Nov2019","35","4","858","33","139349679","10.1111/coin.12216","Wiley-Blackwell","Article","NATURAL language processing; LATENT semantic analysis; WIKIPEDIA","entity linking; entity salience; information retrieval; machine learning; natural language processing; Wikipedia","We study the problem of entity salience by proposing the design and implementation of Swat, a system that identifies the salient Wikipedia entities occurring in an input document. Swat consists of several modules that are able to detect and classify on‐the‐fly Wikipedia entities as salient or not, based on a large number of syntactic, semantic, and latent features properly extracted via a supervised process, which has been trained over millions of examples drawn from the New York Times corpus. The validation process is performed through a large experimental assessment, eventually showing that Swat improves known solutions over all publicly available datasets. We release Swat via an API that we describe and comment in the paper to ease its use in other software. [ABSTRACT FROM AUTHOR] Copyright of Computational Intelligence is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=139349679&site=bsi-live"
"Systematic mapping study on question answering frameworks over linked data.","Tasar, Ceren Ocal; Komesli, Murat; Unalir, Murat Osman","IET Software (Wiley-Blackwell)",="17518806",,="Dec2018","12","6","461","12","148479975","10.1049/iet-sen.2018.5105","Wiley-Blackwell","Article",,"database management systems; exclusion criteria; inclusion criteria; input questions; Linked Data; linked data technologies; natural language processing; ontologies (artificial intelligence); question answering (information retrieval); question answering frameworks; research questions; semantic analysis; semantic endpoints; sentence-level recognition; systematic mapping","Employing linked data technologies and semantic endpoints for question answering systems are expanding approaches among the researchers. Therefore, systems that combine syntactic and semantic analysis and enrich input questions by sentence‐level recognition are examined. A systematic mapping study is conducted to identify and analyse the studies from major databases, journals and proceedings of conferences or workshops published between 2010 and 2017. With a set of 14 research questions, inclusion and exclusion criteria are specified. 53 studies are selected as primary studies from an initial set of 845 papers. This study provides a mapping while focusing on the methods and identifying the gaps between required and existing approaches. Popular approaches which have gained the most attention among researchers are given as a conclusion. Moreover, a comparison between the authors' study and related work in the literature is given to point out the differences and the contributions of their study. As the result of the comparison, it is concluded that the study is a novel and original topic on question answering frameworks. [ABSTRACT FROM AUTHOR] Copyright of IET Software (Wiley-Blackwell) is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=148479975&site=bsi-live"
"Text mining approaches for dealing with the rapidly expanding literature on COVID-19.","Wang, Lucy Lu; Lo, Kyle","Briefings in Bioinformatics",="14675463",,="Mar2021","22","2","781","19","149507131","10.1093/bib/bbaa296","Oxford University Press / USA","Article","COVID-19; MEDICAL personnel; INFORMATION overload; SHARED housing","CORD-19; information extraction; information retrieval; natural language processing; question answering; shared tasks; summarization; text mining","More than 50 000 papers have been published about COVID-19 since the beginning of 2020 and several hundred new papers continue to be published every day. This incredible rate of scientific productivity leads to information overload, making it difficult for researchers, clinicians and public health officials to keep up with the latest findings. Automated text mining techniques for searching, reading and summarizing papers are helpful for addressing information overload. In this review, we describe the many resources that have been introduced to support text mining applications over the COVID-19 literature; specifically, we discuss the corpora, modeling resources, systems and shared tasks that have been introduced for COVID-19. We compile a list of 39 systems that provide functionality such as search, discovery, visualization and summarization over the COVID-19 literature. For each system, we provide a qualitative description and assessment of the system's performance, unique data or user interface features and modeling decisions. Many systems focus on search and discovery, though several systems provide novel features, such as the ability to summarize findings over multiple documents or linking between scientific articles and clinical trials. We also describe the public corpora, models and shared tasks that have been introduced to help reduce repeated effort among community members; some of these resources (especially shared tasks) can provide a basis for comparing the performance of different systems. Finally, we summarize promising results and open challenges for text mining the COVID-19 literature. [ABSTRACT FROM AUTHOR] Copyright of Briefings in Bioinformatics is the property of Oxford University Press / USA and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=149507131&site=bsi-live"
"The “General Problem Solver” Does Not Exist: Mortimer Taube and the Art of AI Criticism.","Garvey, Shunryu Colin","IEEE Annals of the History of Computing",="10586180",,="Jan-Mar2021","43","1","60","14","149121999","10.1109/MAHC.2021.3051686","IEEE","Article","ARTIFICIAL intelligence; LIBRARY science; ART criticism; COMMON sense; MUSICAL criticism; GLOBAL Positioning System","Artificial intelligence; Artificial Intelligence (AI); Cognitive Simulation; Computer Systems; Computers; Global Positioning System; History; Indexing; Information Processing; Information retrieval; Libraries; Social Criticism","This article reconfigures the history of artificial intelligence (AI) and its accompanying tradition of criticism by excavating the work of Mortimer Taube, a pioneer in information and library sciences, whose magnum opus, Computers and Common Sense: The Myth of Thinking Machines (1961), has been mostly forgotten. To convey the essence of his distinctive critique, the article focuses on Taube's attack on the general problem solver (GPS), the second major AI program. After examining his analysis of the social construction of this and other ""thinking machines,"" it concludes that, despite technical changes in AI, much of Taube's criticism remains relevant today. Moreover, his status as an ""information processing"" insider who criticized AI on behalf of the public good challenges the boundaries and focus of most critiques of AI from the past half-century. In sum, Taube's work offers an alternative model from which contemporary AI workers and critics can learn much. [ABSTRACT FROM AUTHOR] Copyright of IEEE Annals of the History of Computing is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=149121999&site=bsi-live"
"Unlocking maintenance insights in industrial text through semantic search.","Naqvi, Syed Meesam Raza; Ghufran, Mohammad; Varnier, Christophe; Nicod, Jean-Marc; Javed, Kamran; Zerhouni, Noureddine","Computers in Industry",="01663615",,="May2024","157",,"N.PAG","1","176499791","10.1016/j.compind.2024.104083","Elsevier B.V.","Article",,"Industrial information retrieval; Large Language Models; Maintenance decision support; Navigating human knowledge; Semantic search; Technical Language Processing","Maintenance records in Computerized Maintenance Management Systems (CMMS) contain valuable human knowledge on maintenance activities. These records primarily consist of noisy and unstructured texts written by maintenance experts. The technical nature of the text, combined with a concise writing style and frequent use of abbreviations, makes it difficult to be processed through classical Natural Language Processing (NLP) pipelines. Due to these complexities, this text must be normalized before feeding to classical machine learning models. Developing these custom normalization pipelines requires manual labor and domain expertise and is a time-consuming process that demands constant updates. This leads to the under-utilization of this valuable source of information to generate insights to help with maintenance decision support. This study proposes a Technical Language Processing (TLP) pipeline for semantic search in industrial text using BERT (Bidirectional Encoder Representations), a transformer-based Large Language Model (LLM). The proposed pipeline can automatically process complex unstructured industrial text and does not require custom preprocessing. To adapt the BERT model for the target domain, three unsupervised domain fine-tuning techniques are compared to identify the best strategy for leveraging available tacit knowledge in industrial text. The proposed approach is validated on two industrial maintenance records from the mining and aviation domains. Semantic search results are analyzed from a quantitative and qualitative perspective. Analysis shows that TSDAE, a state-of-the-art unsupervised domain fine-tuning technique, can efficiently identify intricate patterns in the industrial text regardless of associated complexities. BERT model fine-tuned with TSDAE on industrial text achieved a precision of 0.94 and 0.97 for mining excavators and aviation maintenance records, respectively. • Industrial text stores crucial human knowledge about various assets. • The free-form nature of the industrial text makes it difficult to process. • Transformer-based models can leverage tacit knowledge in the industrial text. • LLMs can enable semantic search in complex industrial text with high precision. [ABSTRACT FROM AUTHOR] Copyright of Computers in Industry is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=176499791&site=bsi-live"
"User satisfaction with Arabic COVID-19 apps: Sentiment analysis of users' reviews using machine learning techniques.","Ramzy, Mina; Ibrahim, Bahaa","Information Processing & Management",="03064573",,="May2024","61","3","N.PAG","1","175964535","10.1016/j.ipm.2024.103644","Elsevier B.V.","Article",,"Arabic mHealth apps; COVID-19 apps; Deep learning; Information retrieval; Machine learning; Natural language processing; Neural networks; Sentiment analysis; User's reviews","• We provide a benchmark dataset composed of 114,499 reviews from 18 Arabic COVID-19 Apps. • The ANN algorithm provides the best performance with 89 % accuracy and 89 % F1. • The proposed model can be generalized for Arab sentiment analysis to mobile apps for new COVID-19 strains that may appear in the future. • There are positive sentiments (71 %) among most users of Arabic COVID-19 apps, which reflects its positive role in infection control. Digital technologies such as mobile health (mHealth) apps with a variety of features can be essential tools for controlling pandemics. Therefore, many Arab countries have launched COVID-19 mHealth apps to reduce the spread of infection among their citizens. Recently, empirical studies have shown that user reviews include useful details to develop apps. However, Arab citizens' satisfaction with the COVID-19 mHealth apps has not been examined yet. Our study aims to provide Arabic sentiment analysis of users' reviews to explore their satisfaction with Arabic Covid-19 apps. To achieve this goal, we have provided a benchmark dataset composed of 114,499 reviews from 18 Arabic COVID-19 Apps. Six machine learning (ML) models were tested and compared (Support Vector Machine (SVM), K-Nearest Neighbor (KNN), Naive Bayes (NB), Logistic Regression (LR), Random Forest (RF), and Artificial Neural Network (ANN)) using a representative sample of 8220 reviews, which were annotated manually. Then, the best-performing algorithms were applied to the benchmark dataset to explore the polarity of Arab sentiment toward the apps. In a later step, we conducted a thematic analysis of both positive and negative reviews to determine which factors positively and negatively influence the effectiveness of apps. The findings show that the ANN algorithm provides the best performance with 89 % accuracy and 89 % F1. 71 % of user reviews include positive sentiments, while only 21 % include negative sentiments. Frequently crashes, update issues, and bugs were among the most prominent negative factors that affected the effectiveness of apps from the users' point of view. Finally, we presented a set of recommendations to address the negative factors and improve the effectiveness of Arabic COVID-19 apps. [ABSTRACT FROM AUTHOR] Copyright of Information Processing & Management is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=175964535&site=bsi-live"
"Utilizing BERT for Information Retrieval: Survey, Applications, Resources, and Challenges.","Wang, Jiajia; Huang, Jimmy Xiangji; Tu, Xinhui; Wang, Junmei; Huang, Angela Jennifer; Laskar, Md Tahmid Rahman; Bhuiyan, Amran","ACM Computing Surveys",="03600300",,="Jul2024","56","7","1","33","176628823","10.1145/3648471","Association for Computing Machinery","Article",,"artificial intelligence; BERT; information retrieval; natural language processing",,"https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=176628823&site=bsi-live"
