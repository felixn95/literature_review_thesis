@article{SAZALI20222151,
title = {Characteristics of Malay translated hadith corpus},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {5},
pages = {2151-2160},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2020.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S1319157820304250},
author = {Siti Syakirah Sazali and Nurazzah Abdul Rahman and Zainab Abu Bakar},
keywords = {Malay language, Linguistic analysis, Malay translated hadith corpus, Natural language processing, Corpus linguistic},
abstract = {Annotated corpus can greatly assist in the natural language processing field. For example, computers can understand more of the document context, and indexing and clustering in information retrieval can be done precisely with less or no ambiguity of words. However, there are only a few annotated corpora in Malay language, which are not publicly shared. In this paper, we delve into analysing and annotating Malay translated hadith documents in terms of tagging and entities. There are three phases, which are manual filtering and cleaning, analysing the corpus and creating the benchmark. As the result, an analysis and benchmark of Malay translated hadith corpus were produced in term of part-of-speech and named entities tags that follows the Zipf’s law distribution.}
}
@article{BHATTACHARYA2024100682,
title = {Accounting fraud detection using contextual language learning},
journal = {International Journal of Accounting Information Systems},
volume = {53},
pages = {100682},
year = {2024},
issn = {1467-0895},
doi = {https://doi.org/10.1016/j.accinf.2024.100682},
url = {https://www.sciencedirect.com/science/article/pii/S1467089524000150},
author = {Indranil Bhattacharya and Ana Mickovic},
keywords = {Accounting fraud detection, Natural Language Processing, BERT, Information Retrieval},
abstract = {Accounting fraud is a widespread problem that causes significant damage in the economic market. Detection and investigation of fraudulent firms require a large amount of time, money, and effort for corporate monitors and regulators. In this study, we explore how textual contents from financial reports help in detecting accounting fraud. Pre-trained contextual language learning models, such as BERT, have significantly advanced natural language processing in recent years. We fine-tune the BERT model on Management Discussion and Analysis (MD&A) sections of annual 10-K reports from the Securities and Exchange Commission (SEC) database. Our final model outperforms the textual benchmark model and the quantitative benchmark model from the previous literature by 15% and 12%, respectively. Further, our model identifies five times more fraudulent firm-year observations than the textual benchmark by investigating the same number of firms, and three times more than the quantitative benchmark. Optimizing this investigation process, where more fraudulent observations are detected in the same size of the investigation sample, would be of great economic significance for regulators, investors, financial analysts, and auditors.}
}
@article{YIANNAKOULIAS2024103392,
title = {Spatial intelligence and contextual relevance in AI-driven health information retrieval},
journal = {Applied Geography},
volume = {171},
pages = {103392},
year = {2024},
issn = {0143-6228},
doi = {https://doi.org/10.1016/j.apgeog.2024.103392},
url = {https://www.sciencedirect.com/science/article/pii/S0143622824001978},
author = {Niko Yiannakoulias},
keywords = {Large language models, Spatial artificial intelligence, Health information},
abstract = {The evolution of large language models (LLMs) has already significantly influenced online health information retrieval. As these models gain more widespread use, it is important to understand their ability to contextualize responses based on spatial and geographic information. This study investigates whether LLMs can vary responses based on geographic and spatial context. Using a structured set of prompts submitted to ChatGPT, responses were analyzed to discern patterns based on prompt question and geographic identifiers included in queries. The analysis used word frequency analysis and bidirectional encoder representations from transformers (BERT) embeddings to evaluate the variation in responses concerning geographic specificity. The results provide some evidence that LLMs can generate geographically tailored responses when the query specifies such a need, thereby supporting localized information retrieval. Moreover, prompt responses exhibit an association between spatial distance and word frequency/sentence embedding differences between texts. This result suggests a nuanced representation of spatial information, which could impact user experience by providing more relevant health information based on the user's location. This study lays the groundwork for further exploration into the spatial intelligence of LLMs and their impact on the accessibility of health information online.}
}
@article{MARWAN2023101742,
title = {Leveraging artificial intelligence and mutual authentication to optimize content caching in edge data centers},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {35},
number = {9},
pages = {101742},
year = {2023},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2023.101742},
url = {https://www.sciencedirect.com/science/article/pii/S1319157823002963},
author = {Mbarek Marwan and Feda AlShahwan and Yassine Afoudi and Abdelkarim {Ait Temghart} and Mohamed Lazaar},
keywords = {Reactive caching, Information retrieval, Mutual authentication, Recommendation systems, Feature selection, Machine learning},
abstract = {Available online Edge data centers are designed to meet the stringent QoE requirements of delay-sensitive and computationally intensive services in Content Delivery Network (CDN) and 5G networks. The primary purpose of this paper was to formulate and solve the problem of optimizing many control variables jointly: (i) what contents to store by taking into consideration edge capacity, and (ii) what contents to recommend to each Internet of Everything (IoE) item, based on identity and access management (IAM). In reactive caching policy, we proposed a new Two-Factor Authentication (2FA) scheme founded upon the Elliptic Curve Cryptography (ECC) and one-way hash function for access control. More interestingly, we use Non-negative Matrix Factorization (NMF), Fuzzy C-Means (FCM), Random Forest (RF) and Pearson Correlation (PC) to improve the accuracy and latency of traditional data filtering models. The intelligent recommendation engine we propose is designed to be implemented by cloud for caching and prefetching contents at the edge. The experimental results validate the theoretical guarantees of the proposed solution and its ability to achieve significant performance gains compared to common baseline models.}
}
@article{AI202380,
title = {Information Retrieval meets Large Language Models: A strategic report from Chinese IR community},
journal = {AI Open},
volume = {4},
pages = {80-90},
year = {2023},
issn = {2666-6510},
doi = {https://doi.org/10.1016/j.aiopen.2023.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S2666651023000049},
author = {Qingyao Ai and Ting Bai and Zhao Cao and Yi Chang and Jiawei Chen and Zhumin Chen and Zhiyong Cheng and Shoubin Dong and Zhicheng Dou and Fuli Feng and Shen Gao and Jiafeng Guo and Xiangnan He and Yanyan Lan and Chenliang Li and Yiqun Liu and Ziyu Lyu and Weizhi Ma and Jun Ma and Zhaochun Ren and Pengjie Ren and Zhiqiang Wang and Mingwen Wang and Ji-Rong Wen and Le Wu and Xin Xin and Jun Xu and Dawei Yin and Peng Zhang and Fan Zhang and Weinan Zhang and Min Zhang and Xiaofei Zhu},
keywords = {Information Retrieval, Language Models, Recommendation system},
abstract = {The research field of Information Retrieval (IR) has evolved significantly, expanding beyond traditional search to meet diverse user information needs. Recently, Large Language Models (LLMs) have demonstrated exceptional capabilities in text understanding, generation, and knowledge inference, opening up exciting avenues for IR research. LLMs not only facilitate generative retrieval but also offer improved solutions for user understanding, model evaluation, and user-system interactions. More importantly, the synergistic relationship among IR models, LLMs, and humans forms a new technical paradigm that is more powerful for information seeking. IR models provide real-time and relevant information, LLMs contribute internal knowledge, and humans play a central role of demanders and evaluators to the reliability of information services. Nevertheless, significant challenges exist, including computational costs, credibility concerns, domain-specific limitations, and ethical considerations. To thoroughly discuss the transformative impact of LLMs on IR research, the Chinese IR community conducted a strategic workshop in April 2023, yielding valuable insights. This paper provides a summary of the workshop’s outcomes, including the rethinking of IR’s core values, the mutual enhancement of LLMs and IR, the proposal of a novel IR technical paradigm, and open challenges.}
}
@article{KRUIPER2024102653,
title = {A platform-based Natural Language processing-driven strategy for digitalising regulatory compliance processes for the built environment},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102653},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102653},
url = {https://www.sciencedirect.com/science/article/pii/S147403462400301X},
author = {Ruben Kruiper and Bimal Kumar and Richard Watson and Farhad Sadeghineko and Alasdair Gray and Ioannis Konstas},
keywords = {Digital Regulatory Compliance, Natural Language Processing, Semantic Web, Machine Learning, Knowledge Graph, Automated Compliance Checking},
abstract = {The digitalisation of the regulatory compliance process has been an active area of research for several decades. However, more recently the level of activities in this area has increased considerably. In the UK, the tragic incident of Grenfell fire in 2017 has been a major catalyst for this as a result of the Hackitt report’s recommendations pointing a lot of the blame on the broken regulatory regime in the country. The Hackitt report emphasises the need to overhaul the building regulations, but the approach to do so remains an open research question. Existing work in this space tends to overlook the processing of actual regulatory documents, or limits their scope to solving a relatively small subtask. This paper presents a new comprehensive platform approach to the digitalisation of the regulatory compliance processing. We present i-ReC (intelligent Regulatory Compliance), a platform approach to digitalisation of regulatory compliance that takes into consideration the enormous diversity of all the stakeholders’ activities. A historical perspective on research in this area is first presented to put things in perspective which identifies the challenges in such an endeavour and identifies the gaps in state-of-the-art. After enumerating all the challenges in implementing a platform-based approach to digitalising the regulatory compliance process, the implementation of some parts of the platform is described. Our research demonstrates that the identification and extraction of all relevant requirements from the corpus of several hundred regulatory documents is a key part of the whole process which underlies the entire process from authoring to eventually compliance checking of designs. Some of the issues that need addressing in this endeavour include ambiguous language, inconsistent use of terms, contradicting requirements and handling multi-word expressions. The implementation of these tools is driven by NLP, ML and Semantic Web technologies. A semantic search engine was developed and validated against other popular and comparable engines with a corpus of 420 (out of about 800) documents used in the UK for compliance checking of building designs. In every search scenario, our search engine performed better on all objective criteria. Limitations of the approach are discussed which includes the challenges around licensing for all the documents in the corpus. Further work includes improving the performance of SPaR.txt (the tool created to identify multi-word expressions) as well as the information retrieval engine by increasing the dataset and providing the model with examples from more diverse formats of regulations. There is also a need to develop and align strategies to collect a comprehensive set of domain vocabularies to be combined in a Knowledge Graph.}
}
@article{YE202422,
title = {Next generation of electronic medical record search engines to support chart reviews: A systematic user study and future research direction},
journal = {Journal of Economy and Technology},
volume = {2},
pages = {22-30},
year = {2024},
issn = {2949-9488},
doi = {https://doi.org/10.1016/j.ject.2024.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S2949948824000179},
author = {Cheng Ye and Daniel Fabbri},
keywords = {Electronic medical records, Clinical chart reviews, Search engine, Ranking metrics, User study, Semantic embeddings, Natural language processing, Information retrieval, Medical context vector space},
abstract = {Objective
Little research has been done on the user-centered document ranking approach, especially in a crowdsourcing chart review environment. As the starting point of designing and implementing the next generation of Electronic Medical Record (EMR) search engines, a systematic user study is needed to better understand the users' needs, challenges, and future research directions of EMR search engines.
Materials and methods
One primary observation during the user study is the need for a ranking method to better support the so-called "early stopping" reviewing strategy (i.e., reviewing only a subset of EMRs of one patient to make the final decision) during the clinical chart reviews. The authors proposed two novel user-centered ranking metrics: "critical documents" and "negative guarantee ratio," to better measure the power of a ranking method in supporting the “early stopping” requirements during clinical chart reviews.
Results
The evaluation results show that i) traditional information retrieval metrics, such as the precision-at-K, have limitations in guiding the design and development of EMR search engines to better support clinical chart reviews; ii) there is not a global optimal ranking method that fits the needs of different chart reviews and different users; iii) a learning-to-rank approach cannot guarantee a stable and optimal ranking for different chart reviews and different users; and iv) A user-centered ranking metric, such as the negative guarantee ratio (NGR) metric is able to measure the “early-stopping” performance of ranking methods.
Conclusions
User-centered ranking metrics can better measure the power of ranking methods in supporting clinical chart reviews. Future research should explore more user-centered ranking metrics and evaluate their impact on real-world EMR search engines.}
}
@article{VASSILIADES2024100816,
title = {Extraction of object-action and object-state associations from Knowledge Graphs},
journal = {Journal of Web Semantics},
volume = {81},
pages = {100816},
year = {2024},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100816},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000027},
author = {Alexandros Vassiliades and Theodore Patkos and Vasilis Efthymiou and Antonis Bikakis and Nick Bassiliades and Dimitris Plexousakis},
keywords = {Association extraction, Knowledge Graphs, Semantics-based extraction, Topology-based extraction, Linking entities},
abstract = {Infusing autonomous artificial systems with knowledge about the physical world they inhabit is a critical and long-held aim for the Artificial Intelligence community. Training systems with relevant data is a typical approach; however, finding the data required is not always possible, especially when much of this knowledge is commonsense. In this paper, we present a comparison of topology-based and semantics-based methods for extracting information about object-action and object-state association relations from knowledge graphs, such as ConceptNet, WordNet, ATOMIC, YAGO, WebChild and DBpedia. Moreover, we propose a novel method for extracting information about object-action and object-state associations from knowledge graphs. Our method is composed of a set of techniques for locating, enriching, evaluating, cleaning and exposing knowledge from such resources, relying on semantic similarity methods. Some important aspects of our method are the flexibility in deciding how to deal with the noise that exists in the data, and the capability to determine the importance of a path through training, rather than through manual annotation.}
}
@article{KEJRIWAL2021100052,
title = {A meta-engine for building domain-specific search engines},
journal = {Software Impacts},
volume = {7},
pages = {100052},
year = {2021},
issn = {2665-9638},
doi = {https://doi.org/10.1016/j.simpa.2020.100052},
url = {https://www.sciencedirect.com/science/article/pii/S2665963820300439},
author = {Mayank Kejriwal},
keywords = {Domain-specific search, Knowledge graphs, Information extraction, Complex domains, Knowledge discovery, Information retrieval},
abstract = {In recent years, domain-specific search (DSS) has emerged as a growing and important area of applied research in artificial intelligence (AI) and information retrieval (IR). Over the last 6 years of research, our group has developed a ‘meta-engine’ called myDIG (my Domain-specific Insight Graphs) that provides a relatively easy and customizable workflow for building DSSs without advanced technical training in crawling, information retrieval or user-interfaces. The myDIG system has been applied to some important and difficult use cases (most notably, fighting human trafficking), in addition to being used in classrooms by graduate students for building complex DSSs from scratch.}
}
@article{VENUGOPAL2021100290,
title = {Looking through glass: Knowledge discovery from materials science literature using natural language processing},
journal = {Patterns},
volume = {2},
number = {7},
pages = {100290},
year = {2021},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2021.100290},
url = {https://www.sciencedirect.com/science/article/pii/S2666389921001239},
author = {Vineeth Venugopal and Sourav Sahoo and Mohd Zaki and Manish Agarwal and Nitya Nand Gosvami and N. M. Anoop Krishnan},
keywords = {natural language processing, artificial intelligence, glass science, materials science, knowledge discovery},
abstract = {Summary
Most of the knowledge in materials science literature is in the form of unstructured data such as text and images. Here, we present a framework employing natural language processing, which automates text and image comprehension and precision knowledge extraction from inorganic glasses’ literature. The abstracts are automatically categorized using latent Dirichlet allocation (LDA) to classify and search semantically linked publications. Similarly, a comprehensive summary of images and plots is presented using the caption cluster plot (CCP), providing direct access to images buried in the papers. Finally, we combine the LDA and CCP with chemical elements to present an elemental map, a topical and image-wise distribution of elements occurring in the literature. Overall, the framework presented here can be a generic and powerful tool to extract and disseminate material-specific information on composition–structure–processing–property dataspaces, allowing insights into fundamental problems relevant to the materials science community and accelerated materials discovery.}
}
@article{ALSHALABI2022363,
title = {BPR algorithm: New broken plural rules for an Arabic stemmer},
journal = {Egyptian Informatics Journal},
volume = {23},
number = {3},
pages = {363-371},
year = {2022},
issn = {1110-8665},
doi = {https://doi.org/10.1016/j.eij.2022.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S1110866522000184},
author = {Hamood Alshalabi and Sabrina Tiun and Nazlia Omar and Elham {abdulwahab Anaam} and Yazid Saif},
keywords = {Arabic broken plural, Arabic root-based, Artificial intelligence, Arabic Root Extraction, Arabic morphological analyzer, Arabic corpus, Arabic text processing},
abstract = {One of the most important phases in text processing is stemming, whose aim is to aggregate all variations in a word into one group to aid natural language processing. The morphological structure of the Arabic language is more challenging than that of the English language; thus, it requires superior stemming algorithms for Arabic stemmers to be effective. One of the challenges is the irregular broken plural, which has been a problematic issue in Arabic natural language processing that affects the performance of Arabic information retrieval and other Arabic language engineering applications. Several studies have attempted to develop solutions to irregular plural problems, but the challenge remains, especially in extracting correct Arabic root words. In this paper, the broken plural rule (BPR) algorithm introduces new solutions to solve the problem in which an existing root-based method cannot extract correct roots by using their proposed rules. The BPR algorithm introduces several rules (main rules and subrules) to extract the correct roots of the Arabic irregular broken plural words. To evaluate the effectiveness of the BPR algorithm, we extracted roots from an Arabic standard dataset and applied the BPR algorithm as an enhancement to a root-based Arabic stemmer, ISRI. The obtained results from both evaluations showed encouraging results: (i) Only a few numbers of incorrect roots were stemmed on the large-sized Arabic word dataset. (ii) The enhanced root-based Arabic stemmer, ISRI + BPR, exhibited the best performance compared with the original ISRI stemmer and a well-known Arabic stemmer, ARLS 2. Thus, the proposed BPR algorithm has solved some of the irregular broken plural problems that eventually increase the performance of a root-based Arabic stemmer.}
}
@article{NAJI20243694,
title = {Towards an LLM based approach for medical e-consent},
journal = {Procedia Computer Science},
volume = {246},
pages = {3694-3701},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.187},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924021987},
author = {Mouncef Naji and Maroua Masmoudi and Hajer Baazaoui Zghal},
keywords = {E-consent, Large Language Model, knowledge graph, Healthcare Information Systems},
abstract = {The question of informed and voluntary consent emerges as a matter of significance in healthcare. Obtaining informed consent, encounters many obstacles coupled with systemic, clinician-related, and patient-related factors, demanding interventions at different levels. This paper introduces a novel approach to present personalized consent based on Large Language Models (LLMs). The personalization of information is displayed through the combination of the LLM with a knowledge graph. We focus in our approach on how the knowledge graph enhances and personalize content generation, allowing therefore the acquisition of informed consent. The paper focuses as well on aspects related to hyper-parameters of information retrieval that help giving better prompt to the LLM. Experiments have showcased intresting results in terms of personalization and information retrieval using metrics of Rouge, Faithfulness and Relevance.}
}
@article{GIMENOBALLESTER2024246,
title = {El rol de la inteligencia artificial en la publicación científica: perspectivas desde la farmacia hospitalaria},
journal = {Farmacia Hospitalaria},
volume = {48},
number = {5},
pages = {246-251},
year = {2024},
issn = {1130-6343},
doi = {https://doi.org/10.1016/j.farma.2024.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S1130634324000965},
author = {Vicente Gimeno-Ballester and Cristina Trigo-Vicente},
keywords = {Inteligencia artificial, Publicación científica, Farmacia Hospitalaria, Escritura científica, Chatbots, Herramientas inteligencia artificial, Ética, Investigación, Artificial Intelligence, Scientific Publications, Hospital Pharmacy, Scientific Writing, Chatbots, AI Tools, Ethics, Research},
abstract = {Resumen
El artículo explora el impacto de la inteligencia artificial en la escritura científica, con especial atención a su aplicación en la farmacia hospitalaria. Se analizan herramientas de inteligencia artificial que optimizan la búsqueda de información, el análisis de la literatura, la calidad de la escritura y la redacción de manuscritos. Chatbots como Consensus, junto con plataformas como Scite y SciSpace, facilitan la búsqueda precisa en bases de datos científicas, ofreciendo respuestas con evidencia y referencias. SciSpace permite la generación de tablas comparativas y la formulación de preguntas sobre estudios, mientras que ResearchRabbit mapea la literatura científica para identificar tendencias. DeepL y ProWritingAid mejoran la calidad de la escritura al corregir errores gramaticales, de estilo y plagio. A.R.I.A. optimiza la gestión de referencias, mientras que Jenny AI ayuda a superar el bloqueo del escritor. Librerías de Python como LangChain permiten realizar búsquedas semánticas avanzadas y la creación de agentes. A pesar de sus beneficios, la inteligencia artificial plantea preocupaciones éticas como sesgos, desinformación y plagio. Se destaca la importancia de un uso responsable y la revisión crítica por expertos. En la farmacia hospitalaria, la inteligencia artificial puede mejorar la eficiencia y la precisión en la investigación y la comunicación científica. Los farmacéuticos pueden utilizar estas herramientas para mantenerse actualizados, mejorar la calidad de sus publicaciones, optimizar la gestión de la información y facilitar la toma de decisiones clínicas. En conclusión, la inteligencia artificial es una herramienta poderosa para la farmacia hospitalaria, siempre que se utilice de manera responsable y ética.
The article examines the impact of artificial intelligence on scientific writing, with a particular focus on its application in hospital pharmacy. It analyzes artificial intelligence tools that enhance information retrieval, literature analysis, writing quality, and manuscript drafting. Chatbots like Consensus, along with platforms such as Scite and SciSpace, enable precise searches in scientific databases, providing evidence-based responses and references. SciSpace facilitates the generation of comparative tables and the formulation of queries regarding studies, while ResearchRabbit maps the scientific literature to identify trends. Tools like DeepL and ProWritingAid improve writing quality by correcting grammatical, stylistic, and plagiarism errors. A.R.I.A. enhances reference management, and Jenny AI assists in overcoming writer's block. Python libraries such as LangChain enable advanced semantic searches and the creation of agents. Despite their benefits, artificial intelligence raises ethical concerns including biases, misinformation, and plagiarism. The importance of responsible use and critical review by experts is emphasized. In hospital pharmacy, artificial intelligence can enhance efficiency and precision in research and scientific communication. Pharmacists can use these tools to stay updated, enhance the quality of their publications, optimize information management, and facilitate clinical decision-making. In conclusion, artificial intelligence is a powerful tool for hospital pharmacy, provided it is used responsibly and ethically.}
}
@article{WANG2024100181,
title = {Comparing ChatGPT and clinical nurses’ performances on tracheostomy care: A cross-sectional study},
journal = {International Journal of Nursing Studies Advances},
volume = {6},
pages = {100181},
year = {2024},
issn = {2666-142X},
doi = {https://doi.org/10.1016/j.ijnsa.2024.100181},
url = {https://www.sciencedirect.com/science/article/pii/S2666142X24000080},
author = {Tongyao Wang and Juan Mu and Jialing Chen and Chia-Chin Lin},
keywords = {Generative artificial intelligence, ChatGPT, Education, Tracheostomy, Nursing},
abstract = {Background
The release of ChatGPT for general use in 2023 by OpenAI has significantly expanded the possible applications of generative artificial intelligence in the healthcare sector, particularly in terms of information retrieval by patients, medical and nursing students, and healthcare personnel.
Objective
To compare the performance of ChatGPT-3.5 and ChatGPT-4.0 to clinical nurses on answering questions about tracheostomy care, as well as to determine whether using different prompts to pre-define the scope of the ChatGPT affects the accuracy of their responses.
Design
Cross-sectional study.
Setting
The data collected from the ChatGPT was collected using the ChatGPT-3.5 and 4.0 using access provided by the University of Hong Kong. The data from the clinical nurses working in mainland China was collected using the Qualtrics survey program.
Participants
No participants were needed for collecting the ChatGPT responses. A total of 272 clinical nurses, with 98.5 % of them working in tertiary care hospitals in mainland China, were recruited using a snowball sampling approach.
Method
We used 43 tracheostomy care-related questions in a multiple-choice format to evaluate the performance of ChatGPT-3.5, ChatGPT-4.0, and clinical nurses. ChatGPT-3.5 and GPT-4.0 were both queried three times with the same questions by different prompts: no prompt, patient-friendly prompt, and act-as-nurse prompt. All responses were independently graded by two qualified otorhinolaryngology nurses on a 3-point accuracy scale (correct, partially correct, and incorrect). The Chi-squared test and Fisher exact test with post-hoc Bonferroni adjustment were used to assess the differences in performance between the three groups, as well as the differences in accuracy between different prompts.
Results
ChatGPT-4.0 showed significantly higher accuracy, with 64.3 % of responses rated as ‘correct’, compared to 60.5 % in ChatGPT-3.5 and 36.7 % in clinical nurses (X 2 = 74.192, p < .001). Except for the ‘care for the tracheostomy stoma and surrounding skin’ domain (X2 = 6.227, p = .156), scores from ChatGPT-3.5 and -4.0 were significantly better than nurses’ on domains related to airway humidification, cuff management, tracheostomy tube care, suction techniques, and management of complications. Overall, ChatGPT-4.0 consistently performed well in all domains, achieving over 50 % accuracy in each domain. Alterations to the prompt had no impact on the performance of ChatGPT-3.5 or -4.0.
Conclusion
ChatGPT may serve as a complementary medical information tool for patients and physicians to improve knowledge in tracheostomy care.
Tweetable abstract
ChatGPT-4.0 can answer tracheostomy care questions better than most clinical nurses. There is no reason nurses should not be using it.}
}
@article{ABDULLAH2024764,
title = {Design of automated model for inspecting and evaluating handwritten answer scripts: A pedagogical approach with NLP and deep learning},
journal = {Alexandria Engineering Journal},
volume = {108},
pages = {764-788},
year = {2024},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2024.08.067},
url = {https://www.sciencedirect.com/science/article/pii/S1110016824009530},
author = {A.Sheik Abdullah and S. Geetha and A.B. {Abdul Aziz} and Utkarsh Mishra},
keywords = {Automated answer script evaluation, Natural language processing, Information retrieval models, Machine learning, Deep learning, Image processing, Artificial intelligence, Educational technology, pedagogical data modeling},
abstract = {We address common challenges examiners face, such as accidental question skipping, marking omissions, and potential bias in assessment. These issues often arise due to the necessity of examining scripts in separate sessions, driven by the high volume of examination materials. In response, we propose the implementation of a self-regulating examiner, harnessing contemporary technology to reduce examiner workload and mitigate the possibility of errors. This automated approach aims to ensure fairness and accuracy in evaluating response scripts, offering a promising solution to the challenges encountered by examiners in the field Our study introduces an innovative approach that seamlessly integrates technologies, including Optical Character Recognition (OCR) for text ex- traction, Natural Language Processing (NLP) for keyword analysis, and ma- chine learning for grading. The results of our method are efficiently presented through a user-friendly web application, providing a streamlined and understandable means for examiners to evaluate response scripts.}
}
@article{GIMENOBALLESTER2024T246,
title = {[Translated article] The role of artificial intelligence in scientific publishing: perspectives from hospital pharmacy},
journal = {Farmacia Hospitalaria},
volume = {48},
number = {5},
pages = {T246-T251},
year = {2024},
issn = {1130-6343},
doi = {https://doi.org/10.1016/j.farma.2024.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S1130634324001259},
author = {Vicente Gimeno-Ballester and Cristina Trigo-Vicente},
keywords = {Artificial intelligence, Scientific publications, Hospital pharmacy, Scientific writing, Chatbots, AI tools, Ethics, Research, Inteligencia Artificial, Publicación científica, Farmacia Hospitalaria, Escritura científica, Chatbots, Herramientas inteligencia Artificial, Ética, Investigación},
abstract = {The article examines the impact of artificial intelligence on scientific writing, with a particular focus on its application in hospital pharmacy. It analyses artificial intelligence tools that enhance information retrieval, literature analysis, writing quality, and manuscript drafting. Chatbots like Consensus, along with platforms such as Scite and SciSpace, enable precise searches in scientific databases, providing evidence-based responses and references. SciSpace facilitates the generation of comparative tables and the formulation of queries regarding studies, while ResearchRabbit maps the scientific literature to identify trends. Tools like DeepL and ProWritingAid improve writing quality by correcting grammatical, stylistic, and plagiarism errors. A.R.I.A. enhances reference management, and Jenny AI assists in overcoming writer's block. Python libraries such as langchain enable advanced semantic searches and the creation of agents. Despite their benefits, artificial intelligence raises ethical concerns including biases, misinformation, and plagiarism. The importance of responsible use and critical review by experts is emphasised. In hospital pharmacy, artificial intelligence can enhance efficiency and precision in research and scientific communication. Pharmacists can use these tools to stay updated, enhance the quality of their publications, optimise information management, and facilitate clinical decision-making. In conclusion, artificial intelligence is a powerful tool for hospital pharmacy, provided it is used responsibly and ethically.
Resumen
El artículo explora el impacto de la Inteligencia artificial en la escritura científica, con especial atención a su aplicación en la farmacia hospitalaria. Se analizan herramientas de inteligencia artificial que optimizan la búsqueda de información, el análisis de la literatura, la calidad de la escritura y la redacción de manuscritos. Chatbots como Consensus, junto con plataformas como Scite y SciSpace, facilitan la búsqueda precisa en bases de datos científicas, ofreciendo respuestas con evidencia y referencias. SciSpace permite la generación de tablas comparativas y la formulación de preguntas sobre estudios, mientras que ResearchRabbit mapea la literatura científica para identificar tendencias. DeepL y ProWritingAid mejoran la calidad de la escritura al corregir errores gramaticales, de estilo y plagio. A.R.I.A. optimiza la gestión de referencias, mientras que Jenny AI ayuda a superar el bloqueo del escritor. Librerías de Python como langchain permiten realizar búsquedas semánticas avanzadas y la creación de agentes. A pesar de sus beneficios, la inteligencia artificial plantea preocupaciones éticas como sesgos, desinformación y plagio. Se destaca la importancia de un uso responsable y la revisión crítica por expertos. En la farmacia hospitalaria, la inteligencia artificial puede mejorar la eficiencia y la precisión en la investigación y la comunicación científica. Los farmacéuticos pueden utilizar estas herramientas para mantenerse actualizados, mejorar la calidad de sus publicaciones, optimizar la gestión de la información y facilitar la toma de decisiones clínicas. En conclusión, la inteligencia artificial es una herramienta poderosa para la farmacia hospitalaria, siempre que se utilice de manera responsable y ética.}
}
@article{FISZMAN2009801,
title = {Automatic summarization of MEDLINE citations for evidence-based medical treatment: A topic-oriented evaluation},
journal = {Journal of Biomedical Informatics},
volume = {42},
number = {5},
pages = {801-813},
year = {2009},
note = {Biomedical Natural Language Processing},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2008.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S1532046408001263},
author = {Marcelo Fiszman and Dina Demner-Fushman and Halil Kilicoglu and Thomas C. Rindflesch},
keywords = {Natural language processing, Semantic processing, Automatic summarization, Evidence-based medicine, Knowledge representation, Artificial intelligence, Evaluation},
abstract = {As the number of electronic biomedical textual resources increases, it becomes harder for physicians to find useful answers at the point of care. Information retrieval applications provide access to databases; however, little research has been done on using automatic summarization to help navigate the documents returned by these systems. After presenting a semantic abstraction automatic summarization system for MEDLINE citations, we concentrate on evaluating its ability to identify useful drug interventions for 53 diseases. The evaluation methodology uses existing sources of evidence-based medicine as surrogates for a physician-annotated reference standard. Mean average precision (MAP) and a clinical usefulness score developed for this study were computed as performance metrics. The automatic summarization system significantly outperformed the baseline in both metrics. The MAP gain was 0.17 (p<0.01) and the increase in the overall score of clinical usefulness was 0.39 (p<0.05).}
}
@article{PETHANI2023104282,
title = {Natural language processing for clinical notes in dentistry: A systematic review},
journal = {Journal of Biomedical Informatics},
volume = {138},
pages = {104282},
year = {2023},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2023.104282},
url = {https://www.sciencedirect.com/science/article/pii/S1532046423000035},
author = {Farhana Pethani and Adam G. Dunn},
keywords = {Dental informatics, Dental records, Dentistry, Information sciences, Natural language processing},
abstract = {Objective
To identify and synthesise research on applications of natural language processing (NLP) for information extraction and retrieval from clinical notes in dentistry.
Materials and methods
A predefined search strategy was applied in EMBASE, CINAHL and Medline. Studies eligible for inclusion were those that that described, evaluated, or applied NLP to clinical notes containing either human or simulated patient information. Quality of the study design and reporting was independently assessed based on a set of questions derived from relevant tools including CHecklist for critical Appraisal and data extraction for systematic Reviews of prediction Modelling Studies (CHARMS). A narrative synthesis was conducted to present the results.
Results
Of the 17 included studies, 10 developed and evaluated NLP methods and 7 described applications of NLP-based information retrieval methods in dental records. Studies were published between 2015 and 2021, most were missing key details needed for reproducibility, and there was no consistency in design or reporting. The 10 studies developing or evaluating NLP methods used document classification or entity extraction, and 4 compared NLP methods to non-NLP methods. The quality of reporting on NLP studies in dentistry has modestly improved over time.
Conclusions
Study design heterogeneity and incomplete reporting of studies currently limits our ability to synthesise NLP applications in dental records. Standardisation of reporting and improved connections between NLP methods and applied NLP in dentistry may improve how we can make use of clinical notes from dentistry in population health or decision support systems. Protocol Registration. PROSPERO CRD42021227823.}
}
@article{KIM20241,
title = {Assessing the performance of ChatGPT's responses to questions related to epilepsy: A cross-sectional study on natural language processing and medical information retrieval},
journal = {Seizure: European Journal of Epilepsy},
volume = {114},
pages = {1-8},
year = {2024},
issn = {1059-1311},
doi = {https://doi.org/10.1016/j.seizure.2023.11.013},
url = {https://www.sciencedirect.com/science/article/pii/S105913112300300X},
author = {Hyun-Woo Kim and Dong-Hyeon Shin and Jiyoung Kim and Gha-Hyun Lee and Jae Wook Cho},
keywords = {Epilepsy, Artificial intelligence, ChatGPT, Natural language processing},
abstract = {Background
Epilepsy is a neurological condition marked by frequent seizures and various cognitive and psychological effects. Reliable information is essential for effective treatment. Natural language processing models like ChatGPT are increasingly used in healthcare for information access and data analysis, making it crucial to assess their accuracy.
Objective
This study aimed to investigate the accuracy of ChatGPT in providing educational information related to epilepsy.
Methods
We compared the answers from ChatGPT-4 and ChatGPT-3.5 to 57 common epilepsy questions based on the Korean Epilepsy Society's "Epilepsy Patient and Caregiver Guide." Two epileptologists reviewed the responses, with a third serving as an arbiter in cases of disagreement.
Results
Out of 57 questions, 40 responses from ChatGPT-4 had "sufficient educational value," 16 were "correct but inadequate," and one was "mixed with correct and incorrect" information. No answers were entirely incorrect. GPT-4 generally outperformed GPT-3.5 and was often on par with or better than the official guide.
Conclusions
ChatGPT-4 shows promise as a tool for delivering reliable epilepsy-related information and could help alleviate the educational burden on healthcare professionals. Further research is needed to explore the benefits and limitations of using such models in medical contexts.}
}
@article{ZHAN2021100289,
title = {Structuring clinical text with AI: Old versus new natural language processing techniques evaluated on eight common cardiovascular diseases},
journal = {Patterns},
volume = {2},
number = {7},
pages = {100289},
year = {2021},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2021.100289},
url = {https://www.sciencedirect.com/science/article/pii/S2666389921001227},
author = {Xianghao Zhan and Marie Humbert-Droz and Pritam Mukherjee and Olivier Gevaert},
keywords = {clinical notes, cardiovascular disease, ICD-10 codes, natural language processing, interpretability},
abstract = {Summary
Free-text clinical notes in electronic health records are more difficult for data mining while the structured diagnostic codes can be missing or erroneous. To improve the quality of diagnostic codes, this work extracts diagnostic codes from free-text notes: five old and new word vectorization methods were used to vectorize Stanford progress notes and predict eight ICD-10 codes of common cardiovascular diseases with logistic regression. The models showed good performance, with TF-IDF as the best vectorization model showing the highest AUROC (0.9499–0.9915) and AUPRC (0.2956–0.8072). The models also showed transferability when tested on MIMIC-III data with AUROC from 0.7952 to 0.9790 and AUPRC from 0.2353 to 0.8084. Model interpretability was shown by the important words with clinical meanings matching each disease. This study shows the feasibility of accurately extracting structured diagnostic codes, imputing missing codes, and correcting erroneous codes from free-text clinical notes for information retrieval and downstream machine-learning applications.}
}
@article{GHAFAROLLAHI20241389,
title = {ProtAgents: protein discovery via large language model multi-agent collaborations combining physics and machine learning††Electronic supplementary information (ESI) available: The full records of different conversation experiments along with additional materials are provided as supplementary materials. See DOI: https://doi.org/10.1039/d4dd00013g},
journal = {Digital Discovery},
volume = {3},
number = {7},
pages = {1389-1409},
year = {2024},
issn = {2635-098X},
doi = {https://doi.org/10.1039/d4dd00013g},
url = {https://www.sciencedirect.com/science/article/pii/S2635098X24001153},
author = {Alireza Ghafarollahi and Markus J. Buehler},
abstract = {Designing de novo proteins beyond those found in nature holds significant promise for advancements in both scientific and engineering applications. Current methodologies for protein design often rely on AI-based models, such as surrogate models that address end-to-end problems by linking protein structure to material properties or vice versa. However, these models frequently focus on specific material objectives or structural properties, limiting their flexibility when incorporating out-of-domain knowledge into the design process or comprehensive data analysis is required. In this study, we introduce ProtAgents, a platform for de novo protein design based on Large Language Models (LLMs), where multiple AI agents with distinct capabilities collaboratively address complex tasks within a dynamic environment. The versatility in agent development allows for expertise in diverse domains, including knowledge retrieval, protein structure analysis, physics-based simulations, and results analysis. The dynamic collaboration between agents, empowered by LLMs, provides a versatile approach to tackling protein design and analysis problems, as demonstrated through diverse examples in this study. The problems of interest encompass designing new proteins, analyzing protein structures and obtaining new first-principles data – natural vibrational frequencies – via physics simulations. The concerted effort of the system allows for powerful automated and synergistic design of de novo proteins with targeted mechanical properties. The flexibility in designing the agents, on one hand, and their capacity in autonomous collaboration through the dynamic LLM-based multi-agent environment on the other hand, unleashes great potentials of LLMs in addressing multi-objective materials problems and opens up new avenues for autonomous materials discovery and design.}
}
@article{PS20202541,
title = {Towards an efficient Malayalam Named Entity Recognizer Analysis on the Challenges},
journal = {Procedia Computer Science},
volume = {171},
pages = {2541-2546},
year = {2020},
note = {Third International Conference on Computing and Network Communications (CoCoNet'19)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.04.275},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920312679},
author = {Sreeja {P S} and Anitha S Pillai},
keywords = {Named Entity Recognition, Natural Language Processing, Machine Learning, Recurrent Neural Network, Long short term memory},
abstract = {Named Entity Recognition (NER) also known as entity extraction plays an important role in identifying and classifying the named entities into different categories like name of person, place, organization, things, quantity, monetary value etc. that appear in a document. NER has applications in various Natural Language Processing (NLP) tasks such as information retrieval, question answering system, Machine Translation, Sentiment Analysis. NER systems can be developed using rule based, machine learning or hybrid approach. Now Deep learning is being used to develop efficient NER as these models are capable of learning patterns easily and efficiently. Quite a large number of work has been done in English compared to the work done for Indian Languages. The focus of this paper is to highlight the challenges in building an efficient NER for one of the south Indian language namely Malayalam. The different issues that we need to address in order to develop an efficient NER for Malayalam is presented.}
}
@article{NAKPIH2024100081,
title = {A modified Vector Space Model for semantic information retrieval},
journal = {Natural Language Processing Journal},
volume = {8},
pages = {100081},
year = {2024},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2024.100081},
url = {https://www.sciencedirect.com/science/article/pii/S2949719124000293},
author = {Callistus Ireneous Nakpih},
keywords = {Natural Language Processing, Vector Space Model, Information retrieval, Semantic retrieval, Cosine similarity},
abstract = {In this research, we present a modified Vector Space Model which focuses on the semantic relevance of words for retrieving documents. The modified VSM resolves the problem of the classical model performing only lexical matching of query terms to document terms for retrievals. This problem also restricts the classical model from retrieving documents that do not have exact match of query terms even if they are semantically relevant to the query. In the modified model, we introduced a Query Relevance Update technique, which pads the original query set with semantically relevant document terms for optimised semantic retrieval results. The modified model also includes a novel tf−p which replaces the tf−idf technique of the classical VSM, which is used to compute the Term Frequency weights. The replacement of the tf−idf resolves the problem of the classical model penalising terms that occur across documents with the assumption that they are stop words, which in practice, there are usually such words which carry relevant semantic information for documents’ retrieval. We also extended the cosine similarity function with a proportionality weight pqd, which moderates biases for high frequency of terms in longer documents. The pqd ensures that the frequency of query terms including the updated ones are accounted for in proportionality with documents size for the overall ranking of documents. The simulated results reveal that, the modified VSM does achieve semantic retrieval of documents beyond lexical matching of query and document terms.}
}
@article{FERIDOONI2024100137,
title = {Development of a vascular surgery-specific artificial intelligence chat interface using retrieval-augmented generation: VASC.AI, a specialized vascular surgery chatbot},
journal = {JVS-Vascular Insights},
volume = {2},
pages = {100137},
year = {2024},
issn = {2949-9127},
doi = {https://doi.org/10.1016/j.jvsvi.2024.100137},
url = {https://www.sciencedirect.com/science/article/pii/S2949912724000850},
author = {Tiam Feridooni and Arshia P. Javidan and Daniyal N. Mahmood and Zoya Gomes and Andrew Dueck and Mark Wheatcroft and David Szalay},
keywords = {Artificial intelligence, Large language model, Retrieval-augmented generation, Natural language processing, Medical education, Vascular surgery},
abstract = {Background
Large language models (LLMs) exhibit considerable potential in processing educational content for vascular surgery. However, commercially available LLMs are not optimized for medical education and can generate inaccurate information or “hallucinations.” Retrieval-augmented generation (RAG) is an advanced architecture that integrates specialized vascular surgery data into LLMs. This approach customizes LLMs, potentially decreasing the generation of incorrect information and enhancing their educational usefulness.
Methods
This study evaluated the proficiency of baseline Chat Generative Pre-trained Transformer (GPT)-3.5, ChatGPT-4, and ChatGPT-4o models using 244 text-based multiple-choice questions from six VESAP-5 modules, covering aortoiliac disease, cerebrovascular disease, lower extremity disease, renal and mesenteric disease, vascular medicine, and venous disease. The questions were input directly into each model between November 2023 and May 2024. Incorrect responses were categorized as either logical errors or information errors. Additionally, a vascular surgery-specific LLM, VASC.AI, was developed using OpenAI's application programming interface combined with RAG. This model used a database of >200,000 clinical abstracts, guidelines, and landmark trials, vectorized into embeddings for information retrieval. VASC.AI's proficiency was assessed using the same Vascular Education and Self-Assessment Program questions and compared against the baseline models.
Results
ChatGPT-4o and ChatGPT-4 demonstrated improved performance over ChatGPT-3.5, with ChatGPT-4o achieving an average correct response rate of 77.7% ± 7.6%, ChatGPT-4 at 69.0% ± 4.9%, and ChatGPT-3.5 at 55.3% ± 4.3%. VASC.AI significantly outperformed all baseline models, achieving a correct response rate of 93.8% ± 2.4%. Detailed analysis showed that ChatGPT-3.5 had 34.5% ± 13.9% logical errors and 65.5% ± 13.9% information errors, ChatGPT-4 had 22.3% ± 12.7% logical errors and 76.5% ± 12.7% information errors, and ChatGPT-4o had 25.5% ± 9.7% logical errors and 74.7% ± 9.5% information errors. VASC.AI's incorrect responses were solely due to logical errors, demonstrating the efficacy of RAG in providing accurate, specialized information.
Conclusions
The integration of RAG into LLMs significantly improves their performance in specialized medical fields. VASC.AI, tailored with up-to-date vascular surgery-specific data, outperforms general purpose LLMs in answering complex vascular surgery questions. This approach has the potential to enhance medical education, patient care, and clinical decision-making, representing a significant advancement in the application of AI in vascular surgery.}
}
@article{KAJIWARA2024100251,
title = {AI literacy for ethical use of chatbot: Will students accept AI ethics?},
journal = {Computers and Education: Artificial Intelligence},
volume = {6},
pages = {100251},
year = {2024},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2024.100251},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X24000547},
author = {Yusuke Kajiwara and Kouhei Kawabata},
keywords = {AI literacy, Ethics, Chatbot, Large language models, Technology acceptance, Secondary education},
abstract = {In AI literacy education, there are few examples of education based on AI ethical principles, and limited knowledge exists regarding curriculum design that incorporates AI ethical principles and its effects. Therefore, in this study, we propose a curriculum that teaches the ethical use of large language models (LLM) such as ChatGPT and verify its impact on educational effectiveness and technology acceptance among students aged 12 to 24. The validation results show that the proposed curriculum particularly contributes to the understanding of LLM concepts and their ethical use in decision support. We also demonstrate that experience using ChatGPT influences the level of understanding of ethical usage. Additionally, students aged 12 to 18 may actively adopt ChatGPT responses in decision support, and careful consideration is needed when using LLMs in the 12- to 18-year-old age group. Using technology acceptance model, AI ethical principles were also examined to determine technology acceptance, and it was found that usefulness, justice and fairness, privacy, and data protection directly impact attitudes toward ChatGPT. It has also become clear that students feel uneasy about using their personal information for learning ChatGPT, even if they have consented to the use of their personal information. This result suggests that AI developers and providers need to handle personal information carefully to foster a positive AI attitude.}
}
@article{KHALIFI2018139,
title = {Support Vector Machines for a new Hybrid Information Retrieval System},
journal = {Procedia Computer Science},
volume = {127},
pages = {139-145},
year = {2018},
note = {PROCEEDINGS OF THE FIRST INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING IN DATA SCIENCES, ICDS2017},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.01.108},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918301194},
author = {Hamid Khalifi and Abderrahim Elqadi and Youssef Ghanou},
keywords = {Information retrieval, natural language processing, unsupervised classification, supervised classification, support vector machines},
abstract = {Information Retrieval systems are used to extract, from a large database, relevant information for users. When the type of data is text, the complex nature of the database makes the process of retrieving information more difficult. Generally, such processes reformulate queries according to associations among information items before the query session. In this latter, semantic relationships or other approaches such as machine learning techniques can be applied to select the appropriate results to return. This paper presents a formal model and a new search algorithm. The proposed algorithm is applied to find associations between information items, and then use them to structure search results. It incorporates a natural language preprocessing stage, a statistical representation of short documents and queries and a machine learning model to select relevant results. On a series of experiments through Yahoo dataset, the proposed hybrid information retrieval system returned significantly satisfying results.}
}
@article{BENSGHAIER2020551,
title = {Classification and Analysis of Arabic Natural Language Inference Systems},
journal = {Procedia Computer Science},
volume = {176},
pages = {551-560},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.08.057},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920318810},
author = {Mabrouka Ben-Sghaier and Wided Bakari and Mahmoud Neji},
keywords = {Natural Language Processing, Natural Language Inference, Recognizing textual entailment, Arabic language},
abstract = {In natural language, the same meaning can be expressed by different texts. The process of determining the inference relationship occurring between a text T and a hypothesis H is called Natural Language Inference (NLI). The NLI task aims to provide a generic framework that captures, in a unifying manner, the inference across Natural Language Processing applications such as question answering, summarization, information retrieval, and machine translation. Many tasks and datasets have been created to support the development and evaluation of the ability of the NLI task in different languages. For the Arabic language, interest in this field is gradually increasing. This paper aims to provide an overview of state-of-the-art NLI approaches for Arabic, the relevant knowledge resources and the used tools in order to support a better understanding of this growing field. Moreover, this paper points to classify the proposed approaches for Arabic NLI and compare existing NLI systems.}
}
@article{HARTMANN2024120,
title = {Leveraging AI for Current Research Information Systems: Opportunities and Challenges},
journal = {Procedia Computer Science},
volume = {249},
pages = {120-130},
year = {2024},
note = {16th International Conference on Current Research Information Systems (CRIS 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.11.056},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924032678},
author = {Simone Hartmann and Daniel Niederlechner},
keywords = {AI in Research Management, Ethics in AI, CRIS Enhancement},
abstract = {Integrating Artificial Intelligence (AI) into Current Research Information Systems (CRIS) offers significant opportunities to enhance research management. This paper explores AI's potential to automate data handling, improve analytical capabilities, and enhance user experiences within CRIS. Key areas of impact include data enrichment, advanced information retrieval, trend analysis, and predictive analytics. The paper also addresses the challenges and ethical considerations of AI integration, such as data privacy, security, and algorithmic bias. Insights from a Live Poll at the CRIS2024 conference reveal high familiarity with AI among participants, optimism about its potential, and recognition of implementation challenges. By overcoming these obstacles, AI can transform CRIS, making research management more efficient and effective. The paper concludes by advocating for collaboration and dialogue to guide the responsible integration of AI in CRIS, ensuring alignment with stakeholder interests.}
}
@article{GAKIS202379,
title = {Extraction and normalization of IR indexing terms and phrases in a highly inflectional language},
journal = {Journal of Greek Linguistics},
volume = {23},
number = {1},
pages = {79-96},
year = {2023},
issn = {1566-5844},
doi = {https://doi.org/10.1163/15699846-02301001},
url = {https://www.sciencedirect.com/science/article/pii/S156658442300003X},
author = {Panagiotis Gakis and Theodoros Kokkinos and Christos Tsalidis},
keywords = {natural language processing for Information Retrieval, stemming/morphological analysis, phrase detection and use},
abstract = {Term-based indexing of documents is conventionally implemented by stemmers or their corpus-based improvements, both of which encode implicit linguistic information. Terms are directly derived from document content such that a unique indexing approach is available at indexing run-time. For highly inflectional languages where term variation is high, such techniques are more error-prone. The main focus of the current study is the extraction and normalization of single terms and phrases and the proposal of authenticated control of indexing. The proposed approach relies on the use of explicit linguistic knowledge, appropriately encoded in large language resources. Such control guarantees the highest possible expansion factor for indexing terms as well as indexing consistency. Moreover, it offers a framework where different and eventually contradicting indexing criteria can be practiced, conventional and Natural Language Processing (NLP)-based Information Retrieval (IR) applications can be served, while adaptations can be made for tuning to a specific domain or corpus.}
}
@article{BENFENATI2024586,
title = {A Retrieval-augmented Generation application for Question-Answering in Nutrigenetics Domain},
journal = {Procedia Computer Science},
volume = {246},
pages = {586-595},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.467},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924025092},
author = {Domenico Benfenati and Giovanni Maria {De Filippis} and Antonio Maria Rinaldi and Cristiano Russo and Cristian Tommasino},
keywords = {Retrieval-augmented generation, AI-generated content, Large language models, Information Retrieval, Nutrigenetics, Personalized nutrition},
abstract = {The domain of nutrigenetics investigates the complex relationship between genetic variations and individual dietary responses, encompassing a wide array of disciplines, including genomics, nutrition science, bioinformatics, and personalized medicine. This field is marked by its intricate data landscape, necessitating innovative approaches to effectively manage and interpret the vast volumes of information involved. Given nutrigenetic data sheer volume and complexity, traditional AI models often struggle to maintain comprehensive and up-to-date knowledge. In this paper, we propose an implementation of the Retrieval-Augmented Generation (RAG) strategy to address the question-answering task in nutrigenetic domain. This framework enhances the accuracy and relevancy of outputs produced by an advanced Large Language Model, circumventing the exhaustive model fine-tuning process. As a result, our RAG approach not only alleviates the computational demand but also fortifies against data leakage concerns, particularly critical in the sensitive area of nutrigenetics. The implementation of RAG in the nutrigenetic domain not only addresses the existing challenges but also paves the way for more advanced and efficient exploration of nutrigenetic data. Our proposed workflow could advance the understanding of nutrigenetic interactions and personalized nutrition.}
}
@article{DINGIL2024e33645,
title = {Understanding state-of-the-art situation of transport planning strategies in earthquake-prone areas by using AI-supported literature review methodology},
journal = {Heliyon},
volume = {10},
number = {13},
pages = {e33645},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e33645},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024096762},
author = {Ali Enes Dingil and Ondrej Pribyl},
keywords = {artificial intelligence, Information retrieval, AI-Supported review, Earth-quake prone areas, Seismic risk, Transport planning, Transport system},
abstract = {Aim
This review aims to explore earthquake-based transport strategies in seismic areas, providing state-of-the-art insights into the components necessary to guide urban planners and policymakers in their decision-making processes.
Outputs
The review provides a variety of methodologies and approaches employed for the reinforcement planning and emergency demand management to analyze and evaluate the impact of seismic events on transportation systems, in turn to develop strategies for preparedness, mitigation, response, and recovery phases. The selection of the appropriate approach depends on factors such as the specific transport system, urbanization level and type, built environment, and critical components involved.
Originality and value
Besides providing a distinctive illustration of the integration of transportation and seismic literature as a valuable consolidated resource, this article introduces a novel methodology named ALARM for conducting state-of-the-art reviews on any topic, incorporating AI through the utilization of large language models (LLMs) built upon transformer deep neural networks, along with indexing data structures (in this study mainly OPEN-AI DAVINCI-003 model and vector-storing index). Hence, it is of paramount significance as the first instance of implementing LLMs within academic review standards. This paves the way for the potential integration of AI and human collaboration to become a standard practice under enhanced criteria for comprehending and analyzing specific information.}
}
@article{PASCAZIO2024105428,
title = {Question-answering system for combustion kinetics},
journal = {Proceedings of the Combustion Institute},
volume = {40},
number = {1},
pages = {105428},
year = {2024},
issn = {1540-7489},
doi = {https://doi.org/10.1016/j.proci.2024.105428},
url = {https://www.sciencedirect.com/science/article/pii/S1540748924002360},
author = {Laura Pascazio and Dan Tran and Simon D. Rihm and Jiaru Bai and Sebastian Mosbach and Jethro Akroyd and Markus Kraft},
keywords = {Automated kinetic modeling, Reaction mechanism, Cheminformatics, Artificial intelligence, Knowledge graph},
abstract = {In this paper, we introduce for the first time a natural language question-answering (QA) system specifically designed for the field of combustion kinetics. This system marks a significant step towards achieving the PrIMe vision as outlined by Frenklach in 2007, offering a user-friendly interface that allows researchers and practitioners to easily access and query information about chemical mechanisms. This QA system is a key component of “The World Avatar” (TWA), a dynamic framework built upon semantic web technologies. TWA is characterized by its layered structure, which includes a knowledge graph (KG), software agents, and real-world data integration. These layers collectively create a comprehensive unified system for managing and analyzing complex chemical data from various domains. We detail the enhancements made to TWA’s ontologies (OntoSpecies, OntoKin, and OntoCompChem) to meet specific challenges in chemical kinetics and improve their representation accuracy. By focusing on data provenance and interoperability, our approach ensures transparent and reliable data management that adheres to the FAIR principles, which is vital for precise information retrieval and analysis. The role of software agents in populating these ontologies is highlighted, showcasing how they transform raw data into meaningful structured knowledge and generate new insights within the TWA ecosystem. Additionally, the semantic web technologies’ interoperability feature facilitates data integration and exchange across different platforms and tools, making the data machine-actionable. We instantiated in the KG data on four H2/O2 and five CH4/O2 reaction mechanisms taken from the literature and we then demonstrate the QA system’s capabilities in answering questions related to these reaction mechanisms as a proof of concept. Lastly, we discuss the future directions of the TWA framework, which include not only future extensions of the QA system but also the integration of external tool to automate tasks such as generation of kinetic mechanism, further expanding TWA’s functionality and application in the field of chemical kinetics.}
}
@article{LIU2011163,
title = {Natural Language Processing methods and systems for biomedical ontology learning},
journal = {Journal of Biomedical Informatics},
volume = {44},
number = {1},
pages = {163-179},
year = {2011},
note = {Ontologies for Clinical and Translational Research},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2010.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S153204641000105X},
author = {Kaihong Liu and William R. Hogan and Rebecca S. Crowley},
keywords = {Ontology, Ontology learning from text, Ontology enrichment, Information extraction, Natural Language Processing},
abstract = {While the biomedical informatics community widely acknowledges the utility of domain ontologies, there remain many barriers to their effective use. One important requirement of domain ontologies is that they must achieve a high degree of coverage of the domain concepts and concept relationships. However, the development of these ontologies is typically a manual, time-consuming, and often error-prone process. Limited resources result in missing concepts and relationships as well as difficulty in updating the ontology as knowledge changes. Methodologies developed in the fields of Natural Language Processing, information extraction, information retrieval and machine learning provide techniques for automating the enrichment of an ontology from free-text documents. In this article, we review existing methodologies and developed systems, and discuss how existing methods can benefit the development of biomedical ontologies.}
}
@article{LANDSCHAFT2024105531,
title = {Implementation and evaluation of an additional GPT-4-based reviewer in PRISMA-based medical systematic literature reviews},
journal = {International Journal of Medical Informatics},
volume = {189},
pages = {105531},
year = {2024},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2024.105531},
url = {https://www.sciencedirect.com/science/article/pii/S1386505624001941},
author = {Assaf Landschaft and Dario Antweiler and Sina Mackay and Sabine Kugler and Stefan Rüping and Stefan Wrobel and Timm Höres and Hector Allende-Cid},
keywords = {Systematic literature review, PRISMA, GPT-4 API, AI-based reviewer},
abstract = {Background
PRISMA-based literature reviews require meticulous scrutiny of extensive textual data by multiple reviewers, which is associated with considerable human effort.
Objective
To evaluate feasibility and reliability of using GPT-4 API as a complementary reviewer in systematic literature reviews based on the PRISMA framework.
Methodology
A systematic literature review on the role of natural language processing and Large Language Models (LLMs) in automatic patient-trial matching was conducted using human reviewers and an AI-based reviewer (GPT-4 API). A RAG methodology with LangChain integration was used to process full-text articles. Agreement levels between two human reviewers and GPT-4 API for abstract screening and between a single reviewer and GPT-4 API for full-text parameter extraction were evaluated.
Results
An almost perfect GPT–human reviewer agreement in the abstract screening process (Cohen’s kappa > 0.9) and a lower agreement in the full-text parameter extraction were observed.
Conclusion
As GPT-4 has performed on a par with human reviewers in abstract screening, we conclude that GPT-4 has an exciting potential of being used as a main screening tool for systematic literature reviews, replacing at least one of the human reviewers.}
}
@article{ARSLAN20244722,
title = {Sustainable Digitalization of Business with Multi-Agent RAG and LLM},
journal = {Procedia Computer Science},
volume = {246},
pages = {4722-4731},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.337},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924023627},
author = {Muhammad Arslan and Saba Munawar and Christophe Cruz},
keywords = {Data-driven operations, Information Extraction (IE), Digital transformation, Large Language Models (LLMs), Sustainable Development Goals (SDGs), Multi-Agent RAG},
abstract = {Businesses heavily rely on data sourced from various channels like news articles, financial reports, and consumer reviews to drive their operations, enabling informed decision-making and identifying opportunities. However, traditional manual methods for data extraction are often time-consuming and resource-intensive, prompting the adoption of digital transformation initiatives to enhance efficiency. Yet, concerns persist regarding the sustainability of such initiatives and their alignment with the United Nations (UN)’s Sustainable Development Goals (SDGs). This research aims to explore the integration of Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG) as a sustainable solution for Information Extraction (IE) and processing. The research methodology involves reviewing existing solutions for business decision-making, noting that many systems require training new machine learning models, which are resource-intensive and have significant environmental impacts. Instead, we propose a sustainable business solution using pre-existing LLMs that can work with diverse datasets. We link domain-specific datasets to tailor LLMs to company needs and employ a Multi-Agent architecture to divide tasks such as information retrieval, enrichment, and classification among specialized agents. This approach optimizes the extraction process and improves overall efficiency. Through the utilization of these technologies, businesses can optimize resource utilization, improve decision-making processes, and contribute to sustainable development goals, thereby fostering environmental responsibility within the corporate sector.}
}
@article{TURCHET2024103340,
title = {Musician-AI partnership mediated by emotionally-aware smart musical instruments},
journal = {International Journal of Human-Computer Studies},
volume = {191},
pages = {103340},
year = {2024},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2024.103340},
url = {https://www.sciencedirect.com/science/article/pii/S107158192400123X},
author = {Luca Turchet and Domenico Stefani and Johan Pauwels},
keywords = {Music information retrieval, Music emotion recognition, Smart musical instruments, Transfer learning, Context-aware computing, Trustworthy AI},
abstract = {The integration of emotion recognition capabilities within musical instruments can spur the emergence of novel art formats and services for musicians. This paper proposes the concept of emotionally-aware smart musical instruments, a class of musical devices embedding an artificial intelligence agent able to recognize the emotion contained in the musical signal. This spurs the emergence of novel services for musicians. Two prototypes of emotionally-aware smart piano and smart electric guitar were created, which embedded a recognition method for happiness, sadness, relaxation, aggressiveness and combination thereof. A user study, conducted with eleven pianists and eleven electric guitarists, revealed the strengths and limitations of the developed technology. On average musicians appreciated the proposed concept, who found its value in various musical activities. Most of participants tended to justify the system with respect to erroneous or partially erroneous classifications of the emotions they expressed, reporting to understand the reasons why a given output was produced. Some participants even seemed to trust more the system than their own judgments. Conversely, other participants requested to improve the accuracy, reliability and explainability of the system in order to achieve a higher degree of partnership with it. Our results suggest that, while desirable, perfect prediction of the intended emotion is not an absolute requirement for music emotion recognition to be useful in the construction of smart musical instruments.}
}
@article{DAS2012325,
title = {Extracting Collocations from Bengali Text Corpus},
journal = {Procedia Technology},
volume = {4},
pages = {325-329},
year = {2012},
note = {2nd International Conference on Computer, Communication, Control and Information Technology( C3IT-2012) on February 25 - 26, 2012},
issn = {2212-0173},
doi = {https://doi.org/10.1016/j.protcy.2012.05.049},
url = {https://www.sciencedirect.com/science/article/pii/S2212017312003283},
author = {Bidyut Das},
keywords = {Collocation extraction, Text pre-processing, Fuzzy Bi-gram Index, Natural language processing, Bengali text corpus},
abstract = {Automatic collocation extraction is very important in various applications in the field of natural language processing such as machine translation, word sense disambiguation, information retrieval, and language modelling in speech processing, lexicography and many more. The success of extracting collocations depends on the technique of preprocessing. A systematic pre-processing technique is described in this paper. Then the pre-processed data is used to extract collocation by using two methods: Point-wise Mutual Information and Fuzzy Bi-gram Index. The paper mainly focuses on bi-gram extraction from a Bengali news corpus. Collocations of higher length i.e., n-grams (n>2) are then obtained when the extracted collocations of lower lengths are treated as individual words.}
}
@article{GRECHISHCHEVA2019142,
title = {Risk markers identification in EHR using natural language processing: hemorrhagic and ischemic stroke cases},
journal = {Procedia Computer Science},
volume = {156},
pages = {142-149},
year = {2019},
note = {8th International Young Scientists Conference on Computational Science, YSC2019, 24-28 June 2019, Heraklion, Greece},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.08.189},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919311081},
author = {Sofia Grechishcheva and Egor Efimov and Oleg Metsker},
keywords = {Natural language processing, data extraction, latent semantic analysis, electronic health records, hemorrhagic stroke, ischemic stroke},
abstract = {This article describes the study results in the development of the method of analysis of semi-structured data from electronic health records to improve the quality of data describing patients’ treatment processes. Improving the accuracy of information retrieval from electronic medical records was achieved by using developed problem-solving oriented library. Moreover, the latent-semantic analysis of the electronic health records of chronic patients with chronic heart failure, diabetes mellitus, hypertension was performed. The main tokens characterizing different groups of patients were revealed. The developed library and semantic analysis based on it can be used to accurately automatic extraction of information from semi-structured electronic medical records. Automated markup of medical texts on the Russian language is also possible for the development of artificial intelligence systems and new generation clinical decision support systems.}
}
@article{ALMUZAINI2023101695,
title = {TaSbeeb: A judicial decision support system based on deep learning framework},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {35},
number = {8},
pages = {101695},
year = {2023},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2023.101695},
url = {https://www.sciencedirect.com/science/article/pii/S1319157823002495},
author = {Huda A. Almuzaini and Aqil M. Azmi},
keywords = {Arabic NLP, Deep learning, Legal AI, Judicial decision support system},
abstract = {Since the early 1980s, the legal domain has shown a growing interest in Artificial Intelligence approaches to tackle the increasing number of cases worldwide. TaSbeeb is a deep learning (DL)-based judicial decision support system (JDSS) designed for legal professionals in Saudi courts by retrieving judicial reasoning, Qur’anic verses, and hadiths from a knowledge base. The proposed system consists of three phases: annotation, classification, and information retrieval. To annotate judicial text, we developed Ann-Judicial, a semi-automatic method. To handle the imbalanced corpus for classification, we devised homogeneous and heterogeneous stacking DL models. For information retrieval, we proposed Jud_RoBERTa, a judicial language model. TaSbeeb achieved high accuracy and F-scores in both the classification and information retrieval blocks, showing good accuracy despite complexities in the judicial field and interference between cases. Specifically, the classification phase achieved an accuracy and F-score of 95.8%, while the information retrieval phase achieved an accuracy of 79.8% and F-score of 79.3%. The proposed JDSS has potential for extension to other courts and can be used in judicial inspection. TaSbeeb represents a significant stride towards a more efficient and accurate judicial decision-making process in the Arabic legal system, which has been hindered by a lack of research on Arabic JDSS.}
}
@article{ALBAYRAK2024100409,
title = {Enhancing human phenotype ontology term extraction through synthetic case reports and embedding-based retrieval: A novel approach for improved biomedical data annotation},
journal = {Journal of Pathology Informatics},
pages = {100409},
year = {2024},
issn = {2153-3539},
doi = {https://doi.org/10.1016/j.jpi.2024.100409},
url = {https://www.sciencedirect.com/science/article/pii/S2153353924000488},
author = {Abdulkadir Albayrak and Yao Xiao and Piyush Mukherjee and Sarah S. Barnett and Cherisse A. Marcou and Steven N. Hart},
keywords = {Human phenotype ontology, PhenoTagger, Vector embeddings},
abstract = {With the increasing utilization of exome and genome sequencing in clinical and research genetics, accurate and automated extraction of human phenotype ontology (HPO) terms from clinical texts has become imperative. Traditional methods for HPO term extraction, such as PhenoTagger, often face limitations in coverage and precision. In this study, we propose a novel approach that leverages large language models (LLMs) to generate synthetic sentences with clinical context, which were semantically encoded into vector embeddings. These embeddings are linked to HPO terms, creating a robust knowledgebase that facilitates precise information retrieval. Our method circumvents the known issue of LLM hallucinations by storing and querying these embeddings within a true database, ensuring accurate context matching without the need for a predictive model. We evaluated the performance of three different embedding models, all of which demonstrated substantial improvements over PhenoTagger. Top recall (sensitivity), precision (positive-predictive value, PPV), and F1 are 0.64, 0.64, and 0.64, respectively, which were 31%, 10%, and 21% better than PhenoTagger. Furthermore, optimal performance was achieved when we combined the best performing embedding model with PhenoTagger (a.k.a. Fused model), resulting in recall (sensitivity), precision (PPV), and F1 values of 0.7, 0.7, and 0.7, respectively, which are 10%, 10%, and 10% better than the best embedding models. Our findings underscore the potential of this integrated approach to enhance the precision and reliability of HPO term extraction, offering a scalable and effective solution for biomedical data annotation.}
}
@article{ALZAIDI2024103189,
title = {Enhanced automated text categorization via Aquila optimizer with deep learning for Arabic news articles},
journal = {Ain Shams Engineering Journal},
pages = {103189},
year = {2024},
issn = {2090-4479},
doi = {https://doi.org/10.1016/j.asej.2024.103189},
url = {https://www.sciencedirect.com/science/article/pii/S2090447924005707},
author = {Muhammad Swaileh A. Alzaidi and Alya Alshammari and Abdulkhaleq QA Hassan and Shouki A. Ebad and Hanan Al Sultan and Mohammed A. Alliheedi and Ali Abdulaziz Aljubailan and Khadija Abdullah Alzahrani},
keywords = {Text Categorization, Natural Language Processing, Sentiment Analysis, Aquila Optimizer, Deep Learning},
abstract = {Text Classification is the traditional Natural Language Processing (NLP) task. Text classification (also known as categorization) has become a cutting-edge research area in recent years. However, this task has received less attention in Arabic due to the need for more extensive resources for training Arabic text classifiers. In the area of text classification for Arabic news articles, deep learning (DL) methods, namely recurrent neural network (RNN) and convolutional neural network (CNN), were effectively used. This model is trained on labelled datasets around many news topics to automatically categorize articles into predetermined classes. These DL techniques can efficiently discern the subject matter by leveraging the contextual and semantic data embedded in the Arabic text, enabling accurate classification. This application of DL facilitates effective retrieval and organization of Arabic news articles, which supports tasks such as personalized content recommendations, information retrieval, and summarization. Therefore, this study presents an Enhanced Automated Text Categorization via Aquila Optimizer with Deep Learning for Arabic News Articles (TCAODL-ANA) technique. The TCAODL-ANA technique aims to detect and classify Arabic news articles into seven classes. The TCAODL-ANA technique follows pre-processing and the FastText word embedding process to accomplish this. In addition, the TCAODL-ANA technique utilizes an effective attention-based bidirectional gated recurrent unit (ABiGRU) method to identify various news articles. To enhance the detection results of the ABiGRU method, the AO model is employed for the hyperparameter selection process. A comprehensive simulation evaluation is performed to emphasize the improved performance of the TCAODL-ANA technique. The investigational validation portrayed the superior outcomes of the TCAODL-ANA technique over existing techniques.}
}
@article{GOGOI2021142,
title = {Assamese Word Sense Disambiguation using Cuckoo Search Algorithm},
journal = {Procedia Computer Science},
volume = {189},
pages = {142-147},
year = {2021},
note = {AI in Computational Linguistics},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.05.110},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921012370},
author = {Arjun Gogoi and Nomi Baruah and Lakhya Jyoti Nath},
keywords = {Cosine’s similarity, Global algorithm, Unsupervised, WSD},
abstract = {Natural language processing is associated with human-computer interaction, where several challenges require natural language understanding. The Word sense disambiguation problem comprises the computational assignment of meaning to a word according to a specific context in which it occurs. There are numerous natural language processing applications, such as machine translation, information retrieval, and information extraction, which require this task which takes place at the semantic level. To solve this problem unsupervised computation proposals can be effective since they have been successfully used for many real-world optimization problems. In this paper, we propose to solve the word sense disambiguation problem using the cuckoo search algorithm in the Assamese language. We illustrate the performance of our algorithm by carrying out experiments on an Assamese corpus. And comparing them against an unsupervised genetic algorithm that is implemented in the Assamese language. Results of the experiment show that the cuckoo algorithm can achieve more precision, recall and F-measure, attaining 87.5, 84, and 85.71 percentages respectively.}
}
@article{WAN2023100504,
title = {Implantable QR code subcutaneous microchip using photoacoustic and ultrasound microscopy for secure and convenient individual identification and authentication},
journal = {Photoacoustics},
volume = {31},
pages = {100504},
year = {2023},
issn = {2213-5979},
doi = {https://doi.org/10.1016/j.pacs.2023.100504},
url = {https://www.sciencedirect.com/science/article/pii/S2213597923000575},
author = {Nan Wan and Pengcheng Zhang and Zuheng Liu and Zhe Li and Wei Niu and Xiuye Rui and Shibo Wang and Myeongsu Seong and Pengbo He and Siqi Liang and Jiasheng Zhou and Rui Yang and Sung-Liang Chen},
keywords = {Individual identification and authentication, Implantable devices, Quick response code, Ultrasound microscopy, Acoustic-resolution photoacoustic microscopy},
abstract = {Individual identification and authentication techniques are merged into many aspects of human life with various applications, including access control, payment or banking transfer, and healthcare. Yet conventional identification and authentication methods such as passwords, biometrics, tokens, and smart cards suffer from inconvenience and/or insecurity. Here, inspired by quick response (QR) code and implantable microdevices, implantable and minimally-invasive QR code subcutaneous microchips (QRC-SMs) are proposed to be an effective approach to carry useful and private information, thus enabling individual identification and authentication. Two types of QRC-SMs, QRC-SMs with “hole” and “flat” elements and QRC-SMs with “titanium-coated” and “non-coated” elements, are designed and fabricated to store personal information. Corresponding ultrasound microscopy and photoacoustic microscopy are used for imaging the QR code pattern underneath skin, and open-source artificial intelligence algorithm is applied for QR code detection and recognition. Ex vivo experiments under tissue and in vivo experiments with QRC-SMs implanted in live mice have been performed, demonstrating successful information retrieval from implanted QRC-SMs. QRC-SMs are hidden subcutaneously and invisible to the eyes. They cannot be forgotten, misplaced or lost, and can always be ready for timely medical identification, access control, and payment or banking transfer. Hence, QRC-SMs provide promising routes towards private, secure, and convenient individual identification and authentication.}
}
@article{SNIEGULA2019260,
title = {Study of Named Entity Recognition methods in biomedical field},
journal = {Procedia Computer Science},
volume = {160},
pages = {260-265},
year = {2019},
note = {The 10th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2019) / The 9th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.466},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919316813},
author = {Anna Śniegula and Aneta Poniszewska-Marańda and Łukasz Chomątek},
keywords = {Machine learning, Natural Language Processing, recurrent neural networks, Named Entity Recognition, Conditional Random Fields, UMLS, Long-Short Term Memory},
abstract = {Natural Language Processing (NLP) is very important in modern data processing taking into consideration different sources, forms and purpose of data as well as information in different areas our industry, administration, public and private life. Our studies concern Natural Language Processing techniques in biomedical field. The increasing volume of information stored in medical health record databases both in natural language and in structured forms is creating increasing challenges for information retrieval (IR) technologies. The paper presents the comparison study of chosen Named Entity Recognition techniques for biomedical field.}
}
@article{MA2024105889,
title = {Multi-granularity retrieval of mineral resource geological reports based on multi-feature association},
journal = {Ore Geology Reviews},
volume = {165},
pages = {105889},
year = {2024},
issn = {0169-1368},
doi = {https://doi.org/10.1016/j.oregeorev.2024.105889},
url = {https://www.sciencedirect.com/science/article/pii/S0169136824000222},
author = {Kai Ma and Junyuan Deng and Miao Tian and Liufeng Tao and Junjie Liu and Zhong Xie and Hua Huang and Qinjun Qiu},
keywords = {Multi-granularity association, Mineral resource report, Geological text mining, Natural language processing, Semantic retrieval},
abstract = {Massive geologic report contains all kinds of multimodal geologic data information (geologic text, geologic maps, geologic tables, etc.), which contain a lot of rich geologic basic knowledge and expert experience knowledge about rocks and minerals, stratigraphic structure, geologic age, geographic location, and so on. Accurate retrieval of specific information from massive geologic data has become an important need for geologic information retrieval. However, the majority of existing research primarily revolves around extracting and associating information at a single granularity to facilitate geological semantic retrieval, which ignores many potential semantic associations, leading to ambiguity and fuzziness in semantic retrieval. To solve these problems, this paper proposes a multi-granularity (document-chapter-paragraph) geological information retrieval framework for accurate semantic retrieval. The framework firstly extracts topic feature information, spatiotemporal feature information, figure and table feature information based on the multi-granularity of geological reports. Then, an improved apriori algorithm is used to mine and visualize the associations among the feature information to discover the semantic associations of the geological reports at multiple levels of granularity. Finally, experiments are designed to validate the application of the proposed multi-granularity information retrieval framework on the accurate retrieval of geological reports. The experimental results show that the proposed multi-granularity information retrieval framework in this paper can dig deeper into underlying geo-semantic information and realize accurate retrieval.}
}
@article{KRAFT2003145,
title = {Rules and fuzzy rules in text: concept, extraction and usage},
journal = {International Journal of Approximate Reasoning},
volume = {34},
number = {2},
pages = {145-161},
year = {2003},
note = {Soft Computing Applications to Intelligent Information Retrieval on the Internet},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2003.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X03000896},
author = {D.H. Kraft and M.J. Martı́n-Bautista and J. Chen and D. Sánchez},
keywords = {Rules, Fuzzy logic, Information retrieval, Association rules},
abstract = {Several concepts and techniques have been imported from other disciplines such as Machine Learning and Artificial Intelligence to the field of textual data. In this paper, we focus on the concept of rule and the management of uncertainty in text applications. The different structures considered for the construction of the rules, the extraction of the knowledge base and the applications and usage of these rules are detailed. We include a review of the most relevant works of the different types of rules based on their representation and their application to most of the common tasks of Information Retrieval such as categorization, indexing and classification.}
}
@article{MATHKOR2024559,
title = {Multirole of the internet of medical things (IoMT) in biomedical systems for managing smart healthcare systems: An overview of current and future innovative trends},
journal = {Journal of Infection and Public Health},
volume = {17},
number = {4},
pages = {559-572},
year = {2024},
issn = {1876-0341},
doi = {https://doi.org/10.1016/j.jiph.2024.01.013},
url = {https://www.sciencedirect.com/science/article/pii/S1876034124000194},
author = {Darin Mansor Mathkor and Noof Mathkor and Zaid Bassfar and Farkad Bantun and Petr Slama and Faraz Ahmad and Shafiul Haque},
keywords = {Internet of medical things (IoMT), Biomedical systems, Smart healthcare systems, IoT in healthcare, Real-time patient data},
abstract = {Internet of Medical Things (IoMT) is an emerging subset of Internet of Things (IoT), often called as IoT in healthcare, refers to medical devices and applications with internet connectivity, is exponentially gaining researchers’ attention due to its wide-ranging applicability in biomedical systems for Smart Healthcare systems. IoMT facilitates remote health biomedical system and plays a crucial role within the healthcare industry to enhance precision, reliability, consistency and productivity of electronic devices used for various healthcare purposes. It comprises a conceptualized architecture for providing information retrieval strategies to extract the data from patient records using sensors for biomedical analysis and diagnostics against manifold diseases to provide cost-effective medical solutions, quick hospital treatments, and personalized healthcare. This article provides a comprehensive overview of IoMT with special emphasis on its current and future trends used in biomedical systems, such as deep learning, machine learning, blockchains, artificial intelligence, radio frequency identification, and industry 5.0.}
}
@article{SAWCZYN20211285,
title = {Fact-checking: relevance assessment of references in the Polish political domain},
journal = {Procedia Computer Science},
volume = {192},
pages = {1285-1293},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.08.132},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921016215},
author = {Albert Sawczyn and Jakub Binkowski and Denis Janiak and Łukasz Augustyniak and Tomasz Kajdanowicz},
keywords = {natural language processing, fact-checking, fake news, relevance assessment, information retrieval, deep learning, language modelling, transformers},
abstract = {The prevalence of fake news could be observed in circumstances of emotion-causing events, like elections or pandemics. In fear of the potential impact, many fact-checking organisations were established. However, fact-checking requires a large amount of human labor, and hence there is a strong demand for complete automation of this process. Nevertheless, this milestone has not been achieved yet, even for English. The problem grows for the less popular languages that suffer from a scarcity of available resources. To address this problem for the Polish language domain, we propose a solution for automating one of the fact-checking stages - relevance assessment, which is crucial when searching for evidence. Leveraging recent advancements in natural language processing, we have acquired relevant data and developed classifiers of evidence relevance with respect to claims in Polish. Our approach can assess the evidence relevance with a performance at a level of a 0.778 F1-score.}
}
@article{GOPALAKRISHNAN2019103141,
title = {A survey on literature based discovery approaches in biomedical domain},
journal = {Journal of Biomedical Informatics},
volume = {93},
pages = {103141},
year = {2019},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2019.103141},
url = {https://www.sciencedirect.com/science/article/pii/S1532046419300590},
author = {Vishrawas Gopalakrishnan and Kishlay Jha and Wei Jin and Aidong Zhang},
keywords = {Literature based discovery, MEDLINE, Text-mining, Semantic knowledge, Hypothesis generation},
abstract = {Literature Based Discovery (LBD) refers to the problem of inferring new and interesting knowledge by logically connecting independent fragments of information units through explicit or implicit means. This area of research, which incorporates techniques from Natural Language Processing (NLP), Information Retrieval and Artificial Intelligence, has significant potential to reduce discovery time in biomedical research fields. Formally introduced in 1986, LBD has grown to be a significant and a core task for text mining practitioners in the biomedical domain. Together with its inter-disciplinary nature, this has led researchers across domains to contribute in advancing this field of study. This survey attempts to consolidate and present the evolution of techniques in this area. We cover a variety of techniques and provide a detailed description of the problem setting, the intuition, the advantages and limitations of various influential papers. We also list the current bottlenecks in this field and provide a general direction of research activities for the future. In an effort to be comprehensive and for ease of reference for off-the-shelf users, we also list many publicly available tools for LBD. We hope this survey will act as a guide to both academic and industry (bio)-informaticians, introduce the various methodologies currently employed and also the challenges yet to be tackled.}
}
@article{OZYIRMIDOKUZ2014320,
title = {Mining Unstructured Turkish Economy News Articles},
journal = {Procedia Economics and Finance},
volume = {16},
pages = {320-328},
year = {2014},
note = {21st International Economic Conference of Sibiu 2014, IECS 2014 Prospects of Economic Recovery in a Volatile International Context: Major Obstacles, Initiatives and Projects},
issn = {2212-5671},
doi = {https://doi.org/10.1016/S2212-5671(14)00809-0},
url = {https://www.sciencedirect.com/science/article/pii/S2212567114008090},
author = {Esra Kahya Özyirmidokuz},
keywords = {Knowledge discovery in databases, Text mining, Natural language processing},
abstract = {Text mining is the analysis of unstructured data by combining techniques from knowledge discovery in databases, natural language processing, information retrieval, and machine learning. Text mining allows us to analyze web content dynamically to find meaningful patterns within large collections of textual data. There are too many economic news articles to read. Therefore, it is a necessary to summarize them. In this study, TM is used to analyze the vast amount of text produced in newspaper articles in Turkey. We mine unstructured economy news with natural language processing techniques including tokenization, transform cases, filtering stopwords and stemming. Similarity analysis is also used to determine similar documents. The word vector is extracted. Therefore, economy news is structured into numeric representations that summarize them. In addition, k-means clustering is used. Consequently, the clusters and similarities of the articles are obtained.}
}
@article{PREININGER2021104530,
title = {Differences in information accessed in a pharmacologic knowledge base using a conversational agent vs traditional search methods},
journal = {International Journal of Medical Informatics},
volume = {153},
pages = {104530},
year = {2021},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2021.104530},
url = {https://www.sciencedirect.com/science/article/pii/S1386505621001568},
author = {Anita M. Preininger and Bedda L. Rosario and Adam M. Buchold and Jeff Heiland and Nawshin Kutub and Bryan S. Bohanan and Brett South and Gretchen P. Jackson},
keywords = {Pharmacologic knowledge base, Conversational agent, Artificial intelligence, Natural language processing, Information retrieval},
abstract = {Introduction
Clinicians rely on pharmacologic knowledge bases to answer medication questions and avoid potential adverse drug events. In late 2018, an artificial intelligence-based conversational agent, Watson Assistant (WA), was made available to online subscribers to the pharmacologic knowledge base, Micromedex®. WA allows users to ask medication-related questions in natural language. This study evaluated search method-dependent differences in the frequency of information accessed by traditional methods (keyword search and heading navigation) vs conversational agent search.
Materials and methods
We compared the proportion of information types accessed through the conversational agent to the proportion of analogous information types accessed by traditional methods during the first 6 months of 2020.
Results
Addition of the conversational agent allowed early adopters to access 22 different information types contained in the ‘quick answers’ portion of the knowledge base. These information types were accessed 117,550 times with WA during the study period, compared to 33,649,651 times using traditional search methods. The distribution across information types differed by method employed (c2 test, P < .0001). Single drug/dosing, FDA/non-FDA uses, adverse effects, and drug administration emerged as 4 of the top 5 information types accessed by either method. Intravenous compatibility was accessed more frequently using the conversational agent (7.7% vs. 0.6% for traditional methods), whereas dose adjustments were accessed more frequently via traditional methods (4.8% vs. 1.4% for WA).
Conclusion
In a widely used pharmacologic knowledge base, information accessed through conversational agents versus traditional methods differed. User-centered studies are needed to understand these differences.}
}
@article{RICKY2015459,
title = {A Personal Agents in Ubiquitous Environment: A Survey},
journal = {Procedia Computer Science},
volume = {59},
pages = {459-467},
year = {2015},
note = {International Conference on Computer Science and Computational Intelligence (ICCSCI 2015)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.07.514},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915020438},
author = {Michael Yoseph Ricky and Robin Solala Gulo},
keywords = {Personal Agent, Collaborative System, Intelligence Agent, Adaptive Agent.},
abstract = {A personal agents can be implements in various areas. The previous work has been conducted in website and mobile in corresponding to information retrieval, mobile computing, and artificial intelligence. There are different methods and framework are proposed in previous research to obtain and enhance agent's performance for better recommendations. This research aims to present comparison previous research based on personal agent in different areas for understanding of proposed framework design, architecture and its implementations. Personal agent can be applied to analyse and assisting in completing task especially for solving one purpose, and multi agents system can be applied at education, industrial, commercial, governmental, military, and entertainment applications for solving multi purposes.}
}
@article{IQBAL202192,
title = {Word Embedding based Textual Semantic Similarity Measure in Bengali},
journal = {Procedia Computer Science},
volume = {193},
pages = {92-101},
year = {2021},
note = {10th International Young Scientists Conference in Computational Science, YSC2021, 28 June – 2 July, 2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921020512},
author = {MD. Asif Iqbal and Omar Sharif and Mohammed Moshiul Hoque and Iqbal H. Sarker},
keywords = {Natural language processing, Textual semantic similarity, Word embedding, Cosine similarity, Part-of-speech weighting},
abstract = {Textual semantic similarity is a crucial constituent in many NLP tasks such as information retrieval, machine translation, information retrieval and textual forgery detection. It is a complicated task for rule-based techniques to address semantic similarity measures in low-resource languages due to the complex morphological structure and scarcity of linguistic resources. This paper investigates several word embedding techniques (Word2Vec, GloVe, FastText) to estimate the semantic similarity of Bengali sentences. Due to the unavailability of the standard dataset, this work developed a Bengali dataset containing 187031 text documents with 400824 unique words. Moreover, this work considers three semantic distance measures to compute the similarity between the word vectors using Cosine similarity with no weight, term frequency weighting and Part-of-Speech weighting. The performance of the proposed approach is evaluated on the developed dataset containing 50 pairs of Bengali sentences. The evaluation result shows that FastText with continuous bag-of-words with 100 vector size achieved the highest Pearson’s correlation (ρ) score of 77.28%.}
}
@article{VALPUT2023627,
title = {Assessment of the European mobility research landscape to support policy shaping through artificial intelligence models},
journal = {Transportation Research Procedia},
volume = {72},
pages = {627-634},
year = {2023},
note = {TRA Lisbon 2022 Conference Proceedings Transport Research Arena (TRA Lisbon 2022),14th-17th November 2022, Lisboa, Portugal},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2023.11.448},
url = {https://www.sciencedirect.com/science/article/pii/S2352146523007469},
author = {Damir Valput and Ulrike Schmalz and Pablo Hernández and Annika Paul},
keywords = {open science in mobility research, performance indicators, natural language processing, mobility policy shaping, data-centred decision making, green and digital mobility},
abstract = {This paper presents an approach for assessing EU-funded mobility research initiatives that relies on natural language processing (NLP) techniques. The developed prototype acts as a digital assistant that helps to analyze the mobility research landscape and delivers a bird-eye view of its status, gaps, and bottlenecks. We present data-based models that exploit common NLP techniques used for topic modeling and information retrieval to automatize the analysis of the textual data of over 40,000 H2020 and PF7 research projects and to deliver a series of metrics that support insight discovery. Further, we present an open-access dashboard that visually inspects the model results. Based on the developed models, we provide high-level strategic recommendations for future mobility development. A particular use case focuses on digitalization in mobility.}
}
@article{YE202493,
title = {Exploring a learning-to-rank approach to enhance the Retrieval Augmented Generation (RAG)-based electronic medical records search engines},
journal = {Informatics and Health},
volume = {1},
number = {2},
pages = {93-99},
year = {2024},
issn = {2949-9534},
doi = {https://doi.org/10.1016/j.infoh.2024.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S2949953424000146},
author = {Cheng Ye},
keywords = {Retrieval Augmented Generation, Electronic medical records, Information retrieval, Large Language Model, Learning to rank},
abstract = {Background
This study addresses the challenge of enhancing Retrieval Augmented Generation (RAG) search engines for electronic medical records (EMR) by learning users' distinct search semantics. The specific aim is to develop a learning-to-rank system that improves the accuracy and relevance of search results to support RAG-based search engines.
Methods
Given a prompt or search query, the system first asks the user to label a few randomly selected documents, which contain some keywords, as relevant to the prompt or not. The system then identifies relevant sentences and adjusts word similarities by updating a medical semantic embedding. New documents are ranked by the number of relevant sentences identified by the weighted embedding. Only the top-ranked documents and sentences are provided to a Large-Language-Model (LLM) to generate answers for further review.
Findings
To evaluate our approach, four medical researchers labeled documents based on their relevance to specific diseases. We measured the information retrieval performance of our approach and two baseline methods. Results show that our approach achieved at least a 0.60 Precision-at-10 (P @ 10) score with only ten positive labels, outperforming the baseline methods. In our pilot study, we demonstrate that the learned semantic preference can transfer to the analysis of unseen datasets, boosting the accuracy of an RAG model in extracting and explaining cancer progression diagnoses from 0.14 to 0.50.
Interpretation
This study demonstrates that a customized learning-to-rank method can enhance state-of-the-art natural language models, such as LLMs, by quickly adapting to users' semantics. This approach supports EMR document retrieval and helps RAG models generate clinically meaningful answers to specific questions, underscoring the potential of user-tailored learning-to-rank methods in clinical practice.}
}
@article{MARZOUK20224435,
title = {Natural Language Processing with Optimal Deep Learning-Enabled Intelligent Image Captioning System},
journal = {Computers, Materials and Continua},
volume = {74},
number = {2},
pages = {4435-4451},
year = {2022},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2023.033091},
url = {https://www.sciencedirect.com/science/article/pii/S1546221822004313},
author = {Radwa Marzouk and Eatedal Alabdulkreem and Mohamed K. Nour and Mesfer Al Duhayyim and Mahmoud Othman and Abu Sarwar Zamani and Ishfaq Yaseen and Abdelwahed Motwakel},
keywords = {Natural language processing, information retrieval, image captioning, deep learning, metaheuristics},
abstract = {The recent developments in Multimedia Internet of Things (MIoT) devices, empowered with Natural Language Processing (NLP) model, seem to be a promising future of smart devices. It plays an important role in industrial models such as speech understanding, emotion detection, home automation, and so on. If an image needs to be captioned, then the objects in that image, its actions and connections, and any silent feature that remains under-projected or missing from the images should be identified. The aim of the image captioning process is to generate a caption for image. In next step, the image should be provided with one of the most significant and detailed descriptions that is syntactically as well as semantically correct. In this scenario, computer vision model is used to identify the objects and NLP approaches are followed to describe the image. The current study develops a Natural Language Processing with Optimal Deep Learning Enabled Intelligent Image Captioning System (NLPODL-IICS). The aim of the presented NLPODL-IICS model is to produce a proper description for input image. To attain this, the proposed NLPODL-IICS follows two stages such as encoding and decoding processes. Initially, at the encoding side, the proposed NLPODL-IICS model makes use of Hunger Games Search (HGS) with Neural Search Architecture Network (NASNet) model. This model represents the input data appropriately by inserting it into a predefined length vector. Besides, during decoding phase, Chimp Optimization Algorithm (COA) with deeper Long Short Term Memory (LSTM) approach is followed to concatenate the description sentences produced by the method. The application of HGS and COA algorithms helps in accomplishing proper parameter tuning for NASNet and LSTM models respectively. The proposed NLPODL-IICS model was experimentally validated with the help of two benchmark datasets. A widespread comparative analysis confirmed the superior performance of NLPODL-IICS model over other models.}
}
@article{HARRIS2020101961,
title = {Construction and evaluation of gold standards for patent classification—A case study on quantum computing},
journal = {World Patent Information},
volume = {61},
pages = {101961},
year = {2020},
issn = {0172-2190},
doi = {https://doi.org/10.1016/j.wpi.2020.101961},
url = {https://www.sciencedirect.com/science/article/pii/S0172219019300791},
author = {Steve Harris and Anthony Trippe and David Challis and Nigel Swycher},
keywords = {Patent classification, Evaluation, Artificial intelligence, Information retrieval, Deep learning, Gold standard},
abstract = {This article discusses options for evaluation of patent and/or patent family classification algorithms by means of “gold standards”. It covers the creation criteria, and desirable attributes of evaluation mechanisms, then proposes an example gold standard, and discusses the results of applying the evaluation mechanism against the proposed gold standard and an existing commercial implementation.}
}
@article{RASOOL2024100083,
title = {Evaluating LLMs on document-based QA: Exact answer selection and numerical extraction using CogTale dataset},
journal = {Natural Language Processing Journal},
volume = {8},
pages = {100083},
year = {2024},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2024.100083},
url = {https://www.sciencedirect.com/science/article/pii/S2949719124000311},
author = {Zafaryab Rasool and Stefanus Kurniawan and Sherwin Balugo and Scott Barnett and Rajesh Vasa and Courtney Chesser and Benjamin M. Hampstead and Sylvie Belleville and Kon Mouzakis and Alex Bahar-Fuchs},
keywords = {Large language models, Document-based information retrieval, Question-answering, Retrieval augmented generation, CogTale dataset, Healthcare},
abstract = {Document-based Question-Answering (QA) tasks are crucial for precise information retrieval. While some existing work focus on evaluating large language model’s (LLMs) performance on retrieving and answering questions from documents, assessing the LLMs performance on QA types that require exact answer selection from predefined options and numerical extraction is yet to be fully assessed. In this paper, we specifically focus on this underexplored context and conduct empirical analysis of LLMs (GPT-4 and GPT-3.5) on question types, including single-choice, yes–no, multiple-choice, and number extraction questions from documents. We use the CogTale dataset for evaluation, which provide human expert-tagged responses, offering a robust benchmark for precision and factual grounding. We found that LLMs, particularly GPT-4, can precisely answer many single-choice and yes–no questions given relevant context, demonstrating their efficacy in information retrieval tasks. However, their performance diminishes when confronted with multiple-choice and number extraction formats, lowering the overall performance of the models on this task, indicating that these models may not yet be sufficiently reliable for the task. This limits the applications of LLMs on applications demanding precise information extraction and inference from documents, such as meta-analysis tasks. Our work offers a framework for ongoing dataset evaluation, ensuring that LLM applications for information retrieval and document analysis continue to meet evolving standards.}
}
@article{SICILIANI2023102284,
title = {AI-based decision support system for public procurement},
journal = {Information Systems},
volume = {119},
pages = {102284},
year = {2023},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2023.102284},
url = {https://www.sciencedirect.com/science/article/pii/S0306437923001205},
author = {Lucia Siciliani and Vincenzo Taccardi and Pierpaolo Basile and Marco {Di Ciano} and Pasquale Lops},
keywords = {E-procurement, Data analysis, Data visualisation, Natural language processing, Semantic search, Decision support systems},
abstract = {Tenders are powerful means of investment of public funds and represent a strategic development resource. Thus, improving the efficiency of procuring entities and developing evaluation models turn out to be essential to facilitate e-procurement procedures. With this contribution, we introduce our research to create a supporting system for the decision-making and monitoring process during the entire course of investments and contracts. This system employs artificial intelligence techniques based on natural language processing, focused on providing instruments for extracting useful information from both structured and unstructured (i.e., text) data. Therefore, we developed a framework based on a web app that provides integrated tools such as a semantic search engine, a summariser, an open information extraction engine in the form of triples (subject–predicate–object) for tender documents, and dashboards for analysing tender data.}
}
@article{ZHAO2023225,
title = {A Survey of Knowledge Graph Construction Using Machine Learning},
journal = {CMES - Computer Modeling in Engineering and Sciences},
volume = {139},
number = {1},
pages = {225-257},
year = {2023},
issn = {1526-1492},
doi = {https://doi.org/10.32604/cmes.2023.031513},
url = {https://www.sciencedirect.com/science/article/pii/S152614922300098X},
author = {Zhigang Zhao and Xiong Luo and Maojian Chen and Ling Ma},
keywords = {Knowledge graph (KG), semantic network, relation extraction, entity linking, knowledge reasoning},
abstract = {Knowledge graph (KG) serves as a specialized semantic network that encapsulates intricate relationships among real-world entities within a structured framework. This framework facilitates a transformation in information retrieval, transitioning it from mere string matching to far more sophisticated entity matching. In this transformative process, the advancement of artificial intelligence and intelligent information services is invigorated. Meanwhile, the role of machine learning method in the construction of KG is important, and these techniques have already achieved initial success. This article embarks on a comprehensive journey through the last strides in the field of KG via machine learning. With a profound amalgamation of cutting-edge research in machine learning, this article undertakes a systematical exploration of KG construction methods in three distinct phases: entity learning, ontology learning, and knowledge reasoning. Especially, a meticulous dissection of machine learning-driven algorithms is conducted, spotlighting their contributions to critical facets such as entity extraction, relation extraction, entity linking, and link prediction. Moreover, this article also provides an analysis of the unresolved challenges and emerging trajectories that beckon within the expansive application of machine learning-fueled, large-scale KG construction.}
}
@article{CHEN2022100001,
title = {Vision, status, and research topics of Natural Language Processing},
journal = {Natural Language Processing Journal},
volume = {1},
pages = {100001},
year = {2022},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2022.100001},
url = {https://www.sciencedirect.com/science/article/pii/S2949719122000012},
author = {Xieling Chen and Haoran Xie and Xiaohui Tao},
keywords = {Natural Language Processing, NLP, , Trustworthy Artificial Intelligence},
abstract = {The field of Natural Language Processing (NLP) has evolved with, and as well as influenced, recent advances in Artificial Intelligence (AI) and computing technologies, opening up new applications and novel interactions with humans. Modern NLP involves machines’ interaction with human languages for the study of patterns and obtaining meaningful insights. NLP is increasingly receiving attention across academia and industry and demonstrates extraordinary opportunities and across AI applications (e.g., question answering, information retrieval, sentiment analysis, and recommender systems) and helps to deal with new tasks such as machine translation and reading comprehension, with real world performance improving all the time. This editorial first provides an overview of the field of NLP in terms of research grants, publication venues, and research topics. We then introduce the mission of Natural Language Processing Journal, a new NLP-focused Elsevier journal intended as a forum for researchers and practitioners to publish theoretical, practical, and methodological achievements related to trustworthy AI development and applications for analyzing, processing, and modeling human languages.}
}
@article{RAZAVISOUSAN2022100093,
title = {Building Textual Fuzzy Interpretive Structural Modeling to Analyze Factors of Student Mobility Based on User Generated Content},
journal = {International Journal of Information Management Data Insights},
volume = {2},
number = {2},
pages = {100093},
year = {2022},
issn = {2667-0968},
doi = {https://doi.org/10.1016/j.jjimei.2022.100093},
url = {https://www.sciencedirect.com/science/article/pii/S2667096822000362},
author = {Ronak Razavisousan and Karuna Pande Joshi},
keywords = {Student Mobility, Fuzzy interpretive Structural modeling (FISM), Fuzzy ISM, Textual Fuzzy Interpretive Structural Modeling (TFISM), Complex decision-making problem},
abstract = {Many factors influence student mobility across regions and countries. The roles of these factors, along with their interrelationship and interaction, make student mobility a complex decision-making issue. Many textual data generated on social media can answer many open questions about factors affecting human behavior, particularly social mobility. We have developed a novel methodology, called Textual Fuzzy Interpretive Structural Modeling (TFISM), that automatically analyses large textual datasets to identify the internal and external relationships between management or decision-making problems. This computational social science methodology enhances Interpretive Structural Modeling (ISM) approaches to allow the input to be textual data. It is multi-disciplinary and integrates ISM with Artificial Intelligence, Text extraction, and information retrieval techniques. TFISM is a domain-free method, while we have validated this methodology on two different datasets from social media and academic articles. In this paper, we present the results of our study to identify the critical factors and most influential factors for global student mobility.}
}
@article{CHAI2023574,
title = {The Process and Algorithm Analysis of Text Mining System Based on Artificial Intelligence},
journal = {Procedia Computer Science},
volume = {228},
pages = {574-581},
year = {2023},
note = {3rd International Conference on Machine Learning and Big Data Analytics for IoT Security and Privacy},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.11.066},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923018902},
author = {Xiaoliang Chai and Songxiao Xu and Shilin Li and Junyu Zhao},
keywords = {Artificial Intelligence, Text Mining, Mining System, Implementation Process},
abstract = {The rapid development of the Internet leads to the rapid growth of network information, we call it information explosion. The Internet is full of information, and it is difficult for users to find this information and useful knowledge of the ocean. The Web has become the world's largest information repository, and there is an urgent need for efficient access to the valuable knowledge of vast amounts of web information. The purpose of this paper is to study the process and algorithm analysis of text mining system based on artificial intelligence. This paper presents an algorithm of document feature acquisition based on genetic algorithm. Selecting suitable features is an important task in specific text classification and information retrieval. Finding appropriate feature vectors to represent the text will undoubtedly help with subsequent sorting and grouping. Based on the genetic algorithm of variable length chromosome, this paper improves the crossover, mutation and selection operations, and proposes an algorithm to obtain text feature vectors. This method has a wide range of applications and good results.}
}
@article{VALDEZVALENZUELA2024101888,
title = {text2graphAPI: A library to transform text documents into different graph representations},
journal = {SoftwareX},
volume = {28},
pages = {101888},
year = {2024},
issn = {2352-7110},
doi = {https://doi.org/10.1016/j.softx.2024.101888},
url = {https://www.sciencedirect.com/science/article/pii/S2352711024002589},
author = {Andric Valdez-Valenzuela and Helena Gómez-Adorno},
keywords = {Python API, Text graph representation, Natural language processing, Graph neural networks},
abstract = {This paper introduces a new Python API called text2graphAPI. It is an easy-to-use library for transforming text documents into different graph representations, such as Word-Cooccurrence, Heterogeneous, and Integrated Syntactic Graphs. In addition, it contains a text pre-processing module that supports input text in different languages: English, Spanish, and French. These generated graph structures can be used to solve tasks in various areas, such as Authorship Analysis, Information Retrieval, and Topic Classification, to name a few.}
}
@article{LEI2022118,
title = {A Domain Specific Multi-Document Reading Comprehension Method for Artificial Intelligence Application},
journal = {Procedia Computer Science},
volume = {208},
pages = {118-127},
year = {2022},
note = {7th International Conference on Intelligent, Interactive Systems and Applications},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.10.019},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922014600},
author = {Chen Lei and Zhao Baojin and Dong Xinran and Cui Zaixing},
keywords = {Information extract, BERT, Multi-paragraph, QA, Multi-task training, artificial intelligence},
abstract = {With the development of artificial intelligence, information retrieval and information extraction and knowledge services from large-scale texts are currently one of the most urgent needs of people. Machine reading comprehension technology is one of the key technologies that can be applied to knowledge mining. At present, multi-document reading comprehension has received a lot of attention, and its application scenarios are also very extensive. The main goal of this article is to find the answer to the question from a large number of smartphone manuals based on the questions raised by the user about the operation of the smartphone. This paper designs a pipeline structure with three modules: retrieval, extraction, and sorting. At the same time, it designs auxiliary tasks for the extraction model to improve the extraction ability, and uses a new answer scoring method to select answers. The final experiment proves that our method can effectively improve the answer's quality.}
}
@article{SHUKLA20213557,
title = {Deep Neural Network and Pseudo Relevance Feedback Based Query Expansion},
journal = {Computers, Materials and Continua},
volume = {71},
number = {2},
pages = {3557-3570},
year = {2021},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2022.022411},
url = {https://www.sciencedirect.com/science/article/pii/S1546221821001983},
author = {Abhishek Kumar Shukla and Sujoy Das},
keywords = {Information retrieval, query expansion, word embedding, neural network, deep neural network},
abstract = {The neural network has attracted researchers immensely in the last couple of years due to its wide applications in various areas such as Data mining, Natural language processing, Image processing, and Information retrieval etc. Word embedding has been applied by many researchers for Information retrieval tasks. In this paper word embedding-based skip-gram model has been developed for the query expansion task. Vocabulary terms are obtained from the top “k” initially retrieved documents using the Pseudo relevance feedback model and then they are trained using the skip-gram model to find the expansion terms for the user query. The performance of the model based on mean average precision is 0.3176. The proposed model compares with other existing models. An improvement of 6.61%, 6.93%, and 9.07% on MAP value is observed compare to the Original query, BM25 model, and query expansion with the Chi-Square model respectively. The proposed model also retrieves 84, 25, and 81 additional relevant documents compare to the original query, query expansion with Chi-Square model, and BM25 model respectively and thus improves the recall value also. The per query analysis reveals that the proposed model performs well in 30, 36, and 30 queries compare to the original query, query expansion with Chi-square model, and BM25 model respectively.}
}
@article{OMAR2024e595,
title = {ChatGPT for digital pathology research},
journal = {The Lancet Digital Health},
volume = {6},
number = {8},
pages = {e595-e600},
year = {2024},
issn = {2589-7500},
doi = {https://doi.org/10.1016/S2589-7500(24)00114-6},
url = {https://www.sciencedirect.com/science/article/pii/S2589750024001146},
author = {Mohamed Omar and Varun Ullanat and Massimo Loda and Luigi Marchionni and Renato Umeton},
abstract = {Summary
The rapid evolution of generative artificial intelligence (AI) models including OpenAI's ChatGPT signals a promising era for medical research. In this Viewpoint, we explore the integration and challenges of large language models (LLMs) in digital pathology, a rapidly evolving domain demanding intricate contextual understanding. The restricted domain-specific efficiency of LLMs necessitates the advent of tailored AI tools, as illustrated by advancements seen in the last few years including FrugalGPT and BioBERT. Our initiative in digital pathology emphasises the potential of domain-specific AI tools, where a curated literature database coupled with a user-interactive web application facilitates precise, referenced information retrieval. Motivated by the success of this initiative, we discuss how domain-specific approaches substantially minimise the risk of inaccurate responses, enhancing the reliability and accuracy of information extraction. We also highlight the broader implications of such tools, particularly in streamlining access to scientific research and democratising access to computational pathology techniques for scientists with little coding experience. This Viewpoint calls for an enhanced integration of domain-specific text-generation AI tools in academic settings to facilitate continuous learning and adaptation to the dynamically evolving landscape of medical research.}
}
@article{SPARCKJONES1999257,
title = {Information retrieval and artificial intelligence},
journal = {Artificial Intelligence},
volume = {114},
number = {1},
pages = {257-281},
year = {1999},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(99)00075-2},
url = {https://www.sciencedirect.com/science/article/pii/S0004370299000752},
author = {Karen {Sparck Jones}},
keywords = {Information retrieval, Probabilistic model, Artificial Intelligence},
abstract = {This paper addresses the relations between information retrieval (IR) and AI. It examines document retrieval, summarising its essential features and illustrating the state of its art by presenting one probabilistic model in detail, with some test results showing its value. The paper then analyses this model and related successful approaches, concentrating on and justifying their use of weak, redundant representation and reasoning. It goes on to other information management tasks and considers how the concepts and methods developed for retrieval may be applied to these, concluding by arguing that such ways of dealing with information may also have wider relevance to AI.}
}
@article{CHEN2024108680,
title = {Characterising global antimicrobial resistance research explains why One Health solutions are slow in development: An application of AI-based gap analysis},
journal = {Environment International},
volume = {187},
pages = {108680},
year = {2024},
issn = {0160-4120},
doi = {https://doi.org/10.1016/j.envint.2024.108680},
url = {https://www.sciencedirect.com/science/article/pii/S0160412024002666},
author = {Cai Chen and Shu-Le Li and Yao-Yang Xu and Jue Liu and David W. Graham and Yong-Guan Zhu},
keywords = {Antimicrobial resistance, One Health, Natural language processing, Artificial intelligence, Methods harmonization},
abstract = {The global health crisis posed by increasing antimicrobial resistance (AMR) implicitly requires solutions based a One Health approach, yet multisectoral, multidisciplinary research on AMR is rare and huge knowledge gaps exist to guide integrated action. This is partly because a comprehensive survey of past research activity has never performed due to the massive scale and diversity of published information. Here we compiled 254,738 articles on AMR using Artificial Intelligence (AI; i.e., Natural Language Processing, NLP) methods to create a database and information retrieval system for knowledge extraction on research perfomed over the last 20 years. Global maps were created that describe regional, methodological, and sectoral AMR research activities that confirm limited intersectoral research has been performed, which is key to guiding science-informed policy solutions to AMR, especially in low-income countries (LICs). Further, we show greater harmonisation in research methods across sectors and regions is urgently needed. For example, differences in analytical methods used among sectors in AMR research, such as employing culture-based versus genomic methods, results in poor communication between sectors and partially explains why One Health-based solutions are not ensuing. Therefore, our analysis suggest that performing culture-based and genomic AMR analysis in tandem in all sectors is crucial for data integration and holistic One Health solutions. Finally, increased investment in capacity development in LICs should be prioritised as they are places where the AMR burden is often greatest. Our open-access database and AI methodology can be used to further develop, disseminate, and create new tools and practices for AMR knowledge and information sharing.}
}
@article{KIDWAI202075,
title = {Design and Development of Diagnostic Chabot for supporting Primary Health Care Systems},
journal = {Procedia Computer Science},
volume = {167},
pages = {75-84},
year = {2020},
note = {International Conference on Computational Intelligence and Data Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.03.184},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920306499},
author = {Bushra Kidwai and Nadesh RK},
keywords = {Artificial Intelligence, Chatbots, decision tree, natural language processing},
abstract = {Technology is increasingly becoming a massive part of today’s healthcare scenario. Technology has changed the way how patients communicate with doctors and not only that, but also how healthcare is administered. Artificial intelligence and Chabots are two groundbreaking technologies that have changed how patients and doctors perceive healthcare. To make healthcare system more interactive a diagnostic Chabot is designed and developed using latest algorithms in machine learning, decision tree algorithm to help the user to form a diagnosis of their condition based on their symptoms. The system will be fed with information pertaining to various diseases and using NLP, it will be able to understand the user query and give a suitable response. The system can be used for effective information retrieval in a similar manner like siri, alexa etc but the scope will be limited to disease diagnosis.}
}
@article{PARAMASIVAM20229644,
title = {A survey on textual entailment based question answering},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {10, Part B},
pages = {9644-9653},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2021.11.017},
url = {https://www.sciencedirect.com/science/article/pii/S1319157821003311},
author = {Aarthi Paramasivam and S. Jaya Nirmala},
keywords = {Natural Language Processing, Question Answering, Textual Entailment},
abstract = {Question answering, an information retrieval system that seeks knowledge, is one of the classic applications in Natural Language Processing. A question answering system comprises numerous sets of subtasks. Some of the subtasks are Passage Retrieval, Answer Ranking, Question Similarity, Question Generation, Question Classification, Answer Selection, and Answer Validation. Numerous approaches have been experimented on in the question answering system to achieve accurate results. One such approach for the question answering system is Textual Entailment. Textual Entailment is a framework that captures significant semantic inference. Textual Entailment of two text fragments can be defined as the task of deciding whether the meaning of one text fragment can be inferred from another text fragment. This survey discusses how and why Textual Entailment is applied to various subtasks in question answering.}
}
@article{HEGAZI2021e06191,
title = {Preprocessing Arabic text on social media},
journal = {Heliyon},
volume = {7},
number = {2},
pages = {e06191},
year = {2021},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2021.e06191},
url = {https://www.sciencedirect.com/science/article/pii/S2405844021002966},
author = {Mohamed Osman Hegazi and Yasser Al-Dossari and Abdullah Al-Yahy and Abdulaziz Al-Sumari and Anwer Hilal},
keywords = {Natural language processing, Information extraction, Information retrieval, Database, Data analysis, Knowledge discovery, Sentiment analysis, Document and text processing, Arabic text},
abstract = {Currently, social media plays an important role in daily life and routine. Millions of people use social media for different purposes. Large amounts of data flow through online networks every second, and these data contain valuable information that can be extracted if the data are properly processed and analyzed. However, most of the processing results are affected by preprocessing difficulties. This paper presents an approach to extract information from social media Arabic text. It provides an integrated solution for the challenges in preprocessing Arabic text on social media in four stages: data collection, cleaning, enrichment, and availability. The preprocessed Arabic text is stored in structured database tables to provide a useful corpus to which, information extraction and data analysis algorithms can be applied. The experiment in this study reveals that the implementation of the proposed approach yields a useful and full-featured dataset and valuable information. The resultant dataset presented the Arabic text in three structured levels with more than 20 features. Additionally, the experiment provides valuable information and processed results such as topic classification and sentiment analysis.}
}
@article{HARRER2023104512,
title = {Attention is not all you need: the complicated case of ethically using large language models in healthcare and medicine},
journal = {eBioMedicine},
volume = {90},
pages = {104512},
year = {2023},
issn = {2352-3964},
doi = {https://doi.org/10.1016/j.ebiom.2023.104512},
url = {https://www.sciencedirect.com/science/article/pii/S2352396423000774},
author = {Stefan Harrer},
keywords = {Generative artificial intelligence, Large language models, Foundation models, AI ethics, Augmented human intelligence, Information management, AI trustworthiness},
abstract = {Summary
Large Language Models (LLMs) are a key component of generative artificial intelligence (AI) applications for creating new content including text, imagery, audio, code, and videos in response to textual instructions. Without human oversight, guidance and responsible design and operation, such generative AI applications will remain a party trick with substantial potential for creating and spreading misinformation or harmful and inaccurate content at unprecedented scale. However, if positioned and developed responsibly as companions to humans augmenting but not replacing their role in decision making, knowledge retrieval and other cognitive processes, they could evolve into highly efficient, trustworthy, assistive tools for information management. This perspective describes how such tools could transform data management workflows in healthcare and medicine, explains how the underlying technology works, provides an assessment of risks and limitations, and proposes an ethical, technical, and cultural framework for responsible design, development, and deployment. It seeks to incentivise users, developers, providers, and regulators of generative AI that utilises LLMs to collectively prepare for the transformational role this technology could play in evidence-based sectors.}
}
@article{SANTOSARTEAGA2024108610,
title = {On the capacity of artificial intelligence techniques and statistical methods to deal with low-quality data in medical supply chain environments},
journal = {Engineering Applications of Artificial Intelligence},
volume = {133},
pages = {108610},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.108610},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624007681},
author = {Francisco Javier {Santos Arteaga} and Debora {Di Caprio} and Madjid Tavana and David Cucchiari and Josep M. Campistol and Federico Oppenheimer and Fritz Diekmann and Ignacio Revuelta},
keywords = {Information retrieval, Data quality, Artificial neural networks, Regression analysis, Supply chains, Kidney transplantation},
abstract = {We illustrate the capacity of Artificial Intelligence (AI) and Machine Learning (ML) techniques to preserve consistent categorization abilities whenever the quality of the data decreases, displaying mistakes or mismatches across matrix entries, while standard statistical methods exhibit significant modifications in the value of the corresponding coefficients. We design algorithms of different complexity to generate a series of comparable profiles. These profiles are compared within environments that allow for an immediate identification of the generating algorithms and within increasingly complex settings involving almost identical profiles derived from different algorithms. AI and ML techniques outperform standard statistical methods when distinguishing the algorithms generating the profiles. Building on these results, we perform a retrospective analysis where AI and ML techniques are applied to two empirical scenarios defined by different data series of patients transplanted through the period 2006–2019. The first scenario contains the variables describing the evolution of patients inputted correctly. In the second, we modify the content of the vectors of characteristics defining the evolution of patients by exchanging the values of a subset of realizations from two categorical variables. AI and ML techniques are consistently accurate when categorizing patients correctly within both scenarios, a feature particularly relevant when the quality of the information sources composing the medical chain varies. This latter problem is exacerbated among hospitals located in developing countries, where the quality of the data gathered limits their identification and extrapolation capacities.}
}
@article{STOJANOV2024100243,
title = {University students’ self-reported reliance on ChatGPT for learning: A latent profile analysis},
journal = {Computers and Education: Artificial Intelligence},
volume = {6},
pages = {100243},
year = {2024},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2024.100243},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X24000468},
author = {Ana Stojanov and Qian Liu and Joyce Hwee Ling Koh},
keywords = {ChatGPT, Artificial intelligence (AI), University students, Higher education, Latent profile analysis (LPA), Achievement goal orientation, Educational technology use},
abstract = {Although ChatGPT, a state-of-the-art, large language model, seems to be a disruptive technology in higher education, it is unclear to what extent students rely on this tool for completing different tasks. To address this gap, we asked university students (N = 490) recruited via CloudResearch to rate the extent to which they rely on ChatGPT for completing 13 tasks identified in a previous pilot study. Five distinct profiles emerged: ‘Versatile low reliers’ (38.2%) were characterised by low overall self-reported reliance across the tasks, while ‘all-rounders’ (10.4%) had high overall self-reported reliance. The ‘knowledge seekers’ (16.5%) scored particularly high on tasks such as content acquisition, information retrieval and summarising of texts, while the ‘proactive learners’ (11.8%) on tasks such as obtaining feedback, planning and quizzing. Finally, the ‘assignment delegators’ (23.1%) relied on ChatGPT for drafting assignments, writing homework and having ChatGPT write their assignment for them. The findings provide a nuanced understanding of how students rely on ChatGPT for learning.}
}
@article{HOSSAIN2024107987,
title = {AraCovTexFinder: Leveraging the transformer-based language model for Arabic COVID-19 text identification},
journal = {Engineering Applications of Artificial Intelligence},
volume = {133},
pages = {107987},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.107987},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624001453},
author = {Md. Rajib Hossain and Mohammed Moshiul Hoque and Nazmul Siddique and M. Ali Akber Dewan},
keywords = {Natural language processing, Low-resource text identification, Text processing, Language model, Arabic covid text, Ablation study, Late-fusion},
abstract = {In light of the pandemic, the identification and processing of COVID-19-related text have emerged as critical research areas within the field of Natural Language Processing (NLP). With a growing reliance on online portals and social media for information exchange and interaction, a surge in online textual content, comprising disinformation, misinformation, fake news, and rumors has led to the phenomenon of an infodemic on the World Wide Web. Arabic, spoken by over 420 million people worldwide, stands as a significant low-resource language, lacking efficient tools or applications for the detection of COVID-19-related text. Additionally, the identification of COVID-19 text is an essential prerequisite task for detecting fake and toxic content associated with COVID-19. This gap hampers crucial COVID information retrieval and processing necessary for policymakers and health authorities. Addressing this issue, this paper introduces an intelligent Arabic COVID-19 text identification system named ‘AraCovTexFinder,’ leveraging a fine-tuned fusion-based transformer model. Recognizing the challenges posed by a scarcity of related text corpora, substantial morphological variations in the language, and a deficiency of well-tuned hyperparameters, the proposed system aims to mitigate these hurdles. To support the proposed method, two corpora are developed: an Arabic embedding corpus (AraEC) and an Arabic COVID-19 text identification corpus (AraCoV). The study evaluates the performance of six transformer-based language models (mBERT, XML-RoBERTa, mDeBERTa-V3, mDistilBERT, BERT-Arabic, and AraBERT), 12 deep learning models (combining Word2Vec, GloVe, and FastText embedding with CNN, LSTM, VDCNN, and BiLSTM), and the newly introduced model AraCovTexFinder. Through extensive evaluation, AraCovTexFinder achieves a high accuracy of 98.89 ± 0.001%, outperforming other baseline models, including transformer-based language and deep learning models. This research highlights the importance of specialized tools in low-resource languages to combat the infodemic relating to COVID-19, which can assist policymakers and health authorities in making informed decisions.}
}
@article{ZAMIRALOV202132,
title = {Knowledge graph mining for realty domain using dependency parsing and QAT models},
journal = {Procedia Computer Science},
volume = {193},
pages = {32-41},
year = {2021},
note = {10th International Young Scientists Conference in Computational Science, YSC2021, 28 June – 2 July, 2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921020457},
author = {Alexander Zamiralov and Timur Sohin and Nikolay Butakov},
keywords = {ontology, knowledge-graph, QAT, neural network, dependency parsing, real estates},
abstract = {The real estate business has a lot of risks, and in order to minimize them, you need a lot of information from different sources. Systems based on natural language processing can help customers find this information more easily: question answering, information retrieval, etc. The existing method of question answering requires data aligned with possible questions, which are not easy to obtain, in contrast, the knowledge-graph provides structured information. In this paper, we propose semi-automated ontology generation for the realty domain and a subsequent method for information retrieval related to the knowledge-graph of this ontology. The first contribution is the method for relation extraction method based on dependency-parsing and semantic similarity evaluation, which allows us to form ontology for a particular domain. The second contribution is knowledge-graph completion method based on question answering over text neural network. Our experimental analysis shows the efficiency of the proposed approaches.}
}
@article{PAUZI2023111616,
title = {Applications of natural language processing in software traceability: A systematic mapping study},
journal = {Journal of Systems and Software},
volume = {198},
pages = {111616},
year = {2023},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2023.111616},
url = {https://www.sciencedirect.com/science/article/pii/S0164121223000110},
author = {Zaki Pauzi and Andrea Capiluppi},
keywords = {Software traceability, Information retrieval, Natural language processing},
abstract = {A key part of software evolution and maintenance is the continuous integration from collaborative efforts, often resulting in complex traceability challenges between software artifacts: features and modules remain scattered in the source code, and traceability links become harder to recover. In this paper, we perform a systematic mapping study dealing with recent research recovering these links through information retrieval, with a particular focus on natural language processing (NLP). Our search strategy gathered a total of 96 papers in focus of our study, covering a period from 2013 to 2021. We conducted trend analysis on NLP techniques and tools involved, and traceability efforts (applying NLP) across the software development life cycle (SDLC). Based on our study, we have identified the following key issues, barriers, and setbacks: syntax convention, configuration, translation, explainability, properties representation, tacit knowledge dependency, scalability, and data availability. Based on these, we consolidated the following open challenges: representation similarity across artifacts, the effectiveness of NLP for traceability, and achieving scalable, adaptive, and explainable models. To address these challenges, we recommend a holistic framework for NLP solutions to achieve effective traceability and efforts in achieving interoperability and explainability in NLP models for traceability.}
}
@article{IBRIHICH2022777,
title = {A Review on recent research in information retrieval},
journal = {Procedia Computer Science},
volume = {201},
pages = {777-782},
year = {2022},
note = {The 13th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 5th International Conference on Emerging Data and Industry 4.0 (EDI40)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.03.106},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922005191},
author = {S. Ibrihich and A. Oussous and O. Ibrihich and M. Esghir},
keywords = {Information Retrieval, Intelligent Search, IR models, Data Mining, Natural Language Processing},
abstract = {In this paper, we present a survey of modeling and simulation approaches to describe information retrieval basics. We investigate its methods, its challenges, its models, its components and its applications. Our contribution is twofold: on the one hand, reviewing the literature on discovery some search techniques that help to get pertinent results and reach an effective search, and on the other hand, discussing the different research perspectives for study and compare more techniques used in information retrieval. This paper will also shedding the light on some of the famous AI applications in the legal field.}
}
@article{EZZIKOURI20191261,
title = {A New Approach for Calculating Semantic Similarity between Words Using WordNet and Set Theory},
journal = {Procedia Computer Science},
volume = {151},
pages = {1261-1265},
year = {2019},
note = {The 10th International Conference on Ambient Systems, Networks and Technologies (ANT 2019) / The 2nd International Conference on Emerging Data and Industry 4.0 (EDI40 2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.04.182},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919306490},
author = {Hanane EZZIKOURI and Youness MADANI and Mohammed ERRITALI and Mohamed OUKESSOU},
keywords = {Semantic Similarity, Natural Language Processing, WordNet, Set Theory},
abstract = {Calculating semantic similarity between words is a challenging task of a lot of domains such as Natural language processing (NLP), information retrieval and plagiarism detection. WordNet is a lexical dictionary conceptually organized, where each concept has several characteristics: Synsets and Glosses. Synset represent sets of synonyms of a given word and Glosses are a short description. In this paper, we propose a new approach for calculating semantic similarity between two concepts. The proposed method is based on set theory’s concepts and WordNet properties, by calculating the relatedness between the synsets’ and glosses’s of the two concepts.}
}
@article{AVOGADRO2024112447,
title = {Feature/vector entity retrieval and disambiguation techniques to create a supervised and unsupervised semantic table interpretation approach},
journal = {Knowledge-Based Systems},
volume = {304},
pages = {112447},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112447},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124010815},
author = {Roberto Avogadro and Fabio D’Adda and Marco Cremaschi},
keywords = {Semantic web, Knowledge base, Knowledge base construction, Knowledge base extension, Knowledge graph, Semantic table interpretation, Table annotation, Data enrichment, Tabular data},
abstract = {Recently, there has been an increasing interest in extracting and annotating tables on the Web. This activity allows the transformation of textual data into machine-readable formats to enable the execution of various artificial intelligence tasks, e.g., semantic search and dataset extension. Semantic Table Interpretation (STI) is the process of annotating elements in a table. The paper explores Semantic Table Interpretation, addressing the challenges of Entity Retrieval and Entity Disambiguation in the context of Knowledge Graphs (KGs). It introduces LamAPI, an Information Retrieval system with string/type-based filtering and s-elBat, an Entity Disambiguation technique that combines heuristic and ML-based approaches. By applying the acquired know-how in the field and extracting algorithms, techniques and components from our previous STI approaches and the state of the art, we have created a new platform capable of annotating any tabular data, ensuring a high level of quality.}
}
@article{BADAWI2023100043,
title = {KurdSum: A new benchmark dataset for the Kurdish text summarization},
journal = {Natural Language Processing Journal},
volume = {5},
pages = {100043},
year = {2023},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2023.100043},
url = {https://www.sciencedirect.com/science/article/pii/S2949719123000407},
author = {Soran Badawi},
keywords = {Dataset annotation, Kurdish text summarization, Data collection, Evaluation},
abstract = {Summarizing a text is the process of condensing its content while still maintaining its essential information. With the abundance of digital information available, summarization has become a significant task in various fields, including information retrieval, NLP (Natural Language Processing), and machine learning. This task has been extensively studied in languages such as English and Chinese, but research on Kurdish language summarization is lacking. Therefore, we present the first-ever Kurdish summarization news dataset, KurdSum, which includes over 40,000 texts. We collected news articles from Kurdish websites, preprocessed the data, and manually created a summary for each article. We further assessed the performance of our benchmark dataset on four extractive systems (LEXRANK, TEXTRANK, ORACLE, and LEAD0-3) and three abstractive methods (Pointer-Generator, Sequence-to-Sequence and transformer-abstractive). Our experiments showed that the Pointer-Generator approach yielded superior ROUGE (Recall-Oriented Understudy for Gisting Evaluation) scores compared to other techniques and ORACLE outperformed other extractive methods. Our findings offer a promising direction for the summarization of Kurdish text and can contribute to developing NLP tools for processing the Kurdish language. Likewise, the dataset can serve as a benchmark dataset for Kurdish language summarization and a valuable resource for researchers interested in developing Kurdish summarization models.}
}
@article{CASTELLANO2022108859,
title = {Leveraging Knowledge Graphs and Deep Learning for automatic art analysis},
journal = {Knowledge-Based Systems},
volume = {248},
pages = {108859},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.108859},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122004105},
author = {Giovanna Castellano and Vincenzo Digeno and Giovanni Sansaro and Gennaro Vessio},
keywords = {Knowledge graphs, Artificial intelligence, Deep learning, Computer vision, Graph neural networks, Digital humanities, Fine arts},
abstract = {The growing availability of large collections of digitized artworks has disclosed new opportunities to develop intelligent systems for the automatic analysis of fine arts. Among other benefits, these tools can foster a deeper understanding of fine arts, ultimately supporting the spread of culture. However, most of the systems proposed in the literature are only based on visual features of digitized artwork images, which are sometimes only integrated with some metadata and textual comments. A Knowledge Graph (KG) that integrates a rich body of information about artworks, artists, painting schools, etc., in a unified structured framework, can provide a valuable resource for more powerful information retrieval and knowledge discovery tools in the artistic domain. To this end, in this paper we present ArtGraph:11ArtGraph and associated code are publicly available on https://doi.org/10.5281/zenodo.6337958.. an artistic KG based on WikiArt and DBpedia. The graph already provides knowledge discovery capabilities without having to train a learning system. In addition, we propose a novel KG-enabled fine art classification method based on ArtGraph, which is used to perform artwork attribute prediction tasks. The method extracts embeddings from ArtGraph and injects them as “contextual” knowledge into a Deep Learning model. Compared to the state-of-the-art, the proposed model provides encouraging results, suggesting that the exploitation of KGs in combination with Deep Learning can pave the way for bridging the gap between the Humanities and Computer Science communities.}
}
@article{KUMAR20231768,
title = {A Natural Language Processing System using CWS Pipeline for Extraction of Linguistic Features},
journal = {Procedia Computer Science},
volume = {218},
pages = {1768-1777},
year = {2023},
note = {International Conference on Machine Learning and Data Engineering},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.01.155},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923001552},
author = {Sandeep Kumar and Arun Solanki},
keywords = {Coreference Resolution, CWS Pipeline, Natural-Language-Processing, CoNLL-2012, Neuralcoref, Text Summarization, linguistic features},
abstract = {Understanding the rules of grammar and linguistic features is essential to understanding the context of a language, which helps to understand that language. Similarly, for Natural Language processing, the linguistic feature allows understanding of the language. This paper introduced how Coreference, Word-sense, and Semantic knowledge (CWS) of linguistic features work. It would improve the Natural Language Understanding (NLU) and Natural Language Processing (NLP) tasks of any NLP model and NLP applications (either existing or new). This paper proposed a CWS pipeline method to enhance the efficiency and performance of NLP applications like text summarization, information retrieval, question-answer, machine reading comprehension, etc. The proposed CWS pipeline model used a pre-trained CoNLL-2012 coreference dataset extracted from the famous Ontonotes-5.0 dataset for the English language. The model implementation is done in Python language. The performance evaluation is done using the standard CoNLL-2012 coreference dataset for the English language. The coreference marked output is evaluated against the manually tagged gold standard dataset. The proposed CWS pipeline model gives 78.98% of the average F1 score on the MUC metric, 1.78% higher than the previous models' top result. CWS pipeline model performs better than existing models.}
}
@article{BELETE2024100169,
title = {Contextual word disambiguates of Ge'ez language with homophonic using machine learning},
journal = {Ampersand},
volume = {12},
pages = {100169},
year = {2024},
issn = {2215-0390},
doi = {https://doi.org/10.1016/j.amper.2024.100169},
url = {https://www.sciencedirect.com/science/article/pii/S2215039024000079},
author = {Mequanent Degu Belete and Ayodeji Olalekan Salau and Girma Kassa Alitasb and Tigist Bezabh},
keywords = {Ge'ez language, WSD, Text vectorization, Machine learning},
abstract = {According to natural language processing experts, there are numerous ambiguous words in languages. Without automated word meaning disambiguation for any language, the development of natural language processing technologies such as information extraction, information retrieval, machine translation, and others are still challenging task. Therfore, this paper presents the development of a word sense disambiguation model for duplicate alphabet words for the Ge'ez language using corpus-based methods. Because there is no wordNet or public dataset for the Ge'ez language, 1010 samples of ambiguous words were gathered. Afterwards, the words were preprocessed and the text was vectorized using bag of words, Term Frequency-Inverse Document Frequency, and word embeddings such as word2vec and fastText. The vectorized texts are then analysed using the supervised machine learning algorithms such Naive Bayes, decision trees, random forests, K-nearest neighbor, linear support vector machine, and logistic regression. Bag of words paired with random forests outperformed all other combinations, with an accuracy of 99.52%. However, when Deep learning algorithms such as Deep neural network and Long Short-Term memory were used for the same dataset, a 100% accuracy was achieved.}
}
@article{ALNAIED2020209,
title = {An intelligent use of stemmer and morphology analysis for Arabic information retrieval},
journal = {Egyptian Informatics Journal},
volume = {21},
number = {4},
pages = {209-217},
year = {2020},
issn = {1110-8665},
doi = {https://doi.org/10.1016/j.eij.2020.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S1110866519303469},
author = {Ali Alnaied and Mosa Elbendak and Abdullah Bulbul},
keywords = {Natural language processing, Arabic morphological analysis, Information retrieval systems, Arabic stemmer},
abstract = {Arabic Information Retrieval has gained significant attention due to an increasing usage of Arabic text on the web and social media networks. This paper discusses a new approach for Arabic stem, called Arabic Morphology Information Retrieval (AMIR), to generate/extract stems by applying a set of rules regarding the relationship among Arabic letters to find the root/stem of the respective words used as indexing terms for the text search in Arabic retrieval systems. To demonstrate the usefulness of the proposed algorithm, we highlight the benefits of the proposed rules for different Arabic information retrieval systems. Finally, we have evaluated AMIR system by comparing its performance with LUCENE, FARASA, and no-stemmer counterpart system in terms of mean average precisions. The results obtained demonstrate that AMIR has achieved a mean average precision of 0.34% while LUCENE, FARASA and no stemmer giving 0.27%, 0.28% and 0.21, respectively. This demonstrates that AMIR is able to improve Arabic stemmer and increases retrieval as well as being strong against any type of stem.}
}
@article{SULEMAN2021114130,
title = {Extending latent semantic analysis to manage its syntactic blindness},
journal = {Expert Systems with Applications},
volume = {165},
pages = {114130},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.114130},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420308782},
author = {Raja Muhammad Suleman and Ioannis Korkontzelos},
keywords = {Natural Language Processing, Natural Language Understanding, Latent Semantic Analysis, Semantic Similarity},
abstract = {Natural Language Processing (NLP) is the sub-field of Artificial Intelligence that represents and analyses human language automatically. NLP has been employed in many applications, such as information retrieval, information processing and automated answer ranking. Semantic analysis focuses on understanding the meaning of text. Among other proposed approaches, Latent Semantic Analysis (LSA) is a widely used corpus-based approach that evaluates similarity of text based on the semantic relations among words. LSA has been applied successfully in diverse language systems for calculating the semantic similarity of texts. LSA ignores the structure of sentences, i.e., it suffers from a syntactic blindness problem. LSA fails to distinguish between sentences that contain semantically similar words but have opposite meanings. Disregarding sentence structure, LSA cannot differentiate between a sentence and a list of keywords. If the list and the sentence contain similar words, comparing them using LSA would lead to a high similarity score. In this paper, we propose xLSA, an extension of LSA that focuses on the syntactic structure of sentences to overcome the syntactic blindness problem of the original LSA approach. xLSA was tested on sentence pairs that contain similar words but have significantly different meaning. Our results showed that xLSA alleviates the syntactic blindness problem, providing more realistic semantic similarity scores.}
}
@article{GARCIADIAZ2024102307,
title = {Evaluating Transformers and Linguistic Features integration for Author Profiling tasks in Spanish},
journal = {Data & Knowledge Engineering},
volume = {151},
pages = {102307},
year = {2024},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2024.102307},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X24000314},
author = {José Antonio García-Díaz and Ghassan Beydoun and Rafel Valencia-García},
keywords = {Author profiling, Authorship analysis, Feature engineering, Natural language processing},
abstract = {Author profiling consists of extracting their demographic and psychographic information by examining their writings. This information can then be used to improve the reader experience and to detect bots or propagators of hoaxes and/or hate speech. Therefore, author profiling can be applied to build more robust and efficient Knowledge-Based Systems for tasks such as content moderation, user profiling, and information retrieval. Author profiling is typically performed automatically as a document classification task. Recently, language models based on transformers have also proven to be quite effective in this task. However, the size and heterogeneity of novel language models, makes it necessary to evaluate them in context. The contributions we make in this paper are four-fold: First, we evaluate which language models are best suited to perform author profiling in Spanish. These experiments include basic, distilled, and multilingual models. Second, we evaluate how feature integration can improve performance for this task. We evaluate two distinct strategies: knowledge integration and ensemble learning. Third, we evaluate the ability of linguistic features to improve the interpretability of the results. Fourth, we evaluate the performance of each language model in terms of memory, training, and inference times. Our results indicate that the use of lightweight models can indeed achieve similar performance to heavy models and that multilingual models are actually less effective than models trained with one language. Finally, we confirm that the best models and strategies for integrating features ultimately depend on the context of the task.}
}
@article{BOOTA2024102070,
title = {Integrating social media and deep learning for real-time urban waterlogging monitoring},
journal = {Journal of Hydrology: Regional Studies},
volume = {56},
pages = {102070},
year = {2024},
issn = {2214-5818},
doi = {https://doi.org/10.1016/j.ejrh.2024.102070},
url = {https://www.sciencedirect.com/science/article/pii/S2214581824004191},
author = {Muhammad Waseem Boota and Shan-e-hyder Soomro and Muhammad Irshad Ahmad and Sheheryar Khan and Haoming Xia and Yaochen Qin and Chaode Yan and Jikun Xu and Ayesha Yousaf and Muhammad Azeem Boota and Bilal Ahmed},
keywords = {Urban waterlogging, Social-media, Water depth information, Spatiotemporal evolution, Deep learning algorithms},
abstract = {Study region
Swat district, Khyber Pakhtunkhwa (KPK) Province, Pakistan.
Study focus
With the rise of social-media data, there is an increasing need to promptly and precisely identify content related to disasters, such as urban waterlogging. Social-media data, being cost-effective and abundant, can offer valuable insights into geographic phenomena by analyzing human behavioral patterns, making it a powerful resource for detailed waterlogging (WLG) analysis in urban settings.
Innovative insights
This research introduces a novel framework for precise information retrieval and real-time extraction of WLG points and fine-grained information in disaster-affected areas using the Facebook platform. First, topic modeling and transfer learning techniques were developed to examine the spatiotemporal dynamics of WLG locations. Second, water depth data from textual content and visual representations were extracted using various deep learning frameworks and integrated through decision-making processes. Third, a unique fine-grained location corpus tailored to urban flooding scenarios was created using the named entity recognition (NER) model. Finally, the BERT-BiLSTM-CRF model was employed to extract WLG points accurately. Using the Swat district as a case study, the extracted WLG points covered at least 79 % of the officially documented WLG points and were primarily located near roadways, especially in low-elevation areas. This framework provides a viable approach for enhancing situational awareness and conducting spatiotemporal analysis of urban floods and WLG disasters at the municipal level in real-time.}
}
@article{YUAN2024100030,
title = {Large language models illuminate a progressive pathway to artificial intelligent healthcare assistant},
journal = {Medicine Plus},
volume = {1},
number = {2},
pages = {100030},
year = {2024},
issn = {2950-3477},
doi = {https://doi.org/10.1016/j.medp.2024.100030},
url = {https://www.sciencedirect.com/science/article/pii/S2950347724000264},
author = {Mingze Yuan and Peng Bao and Jiajia Yuan and Yunhao Shen and Zifan Chen and Yi Xie and Jie Zhao and Quanzheng Li and Yang Chen and Li Zhang and Lin Shen and Bin Dong},
keywords = {Large language models, Artificial intelligence, Medicine, Healthcare assistant, Prompt engineering, In-context learning},
abstract = {With the rapid development of artificial intelligence, large language models (LLMs) have shown promising capabilities in mimicking human-level language comprehension and reasoning. This has sparked significant interest in applying LLMs to enhance various aspects of healthcare, ranging from medical education to clinical decision support. However, medicine involves multifaceted data modalities and nuanced reasoning skills, presenting challenges for integrating LLMs. This review introduces the fundamental applications of general-purpose and specialized LLMs, demonstrating their utilities in knowledge retrieval, research support, clinical workflow automation, and diagnostic assistance. Recognizing the inherent multimodality of medicine, the review emphasizes the multimodal LLMs and discusses their ability to process diverse data types like medical imaging and electronic health records to augment diagnostic accuracy. To address LLMs’ limitations regarding personalization and complex clinical reasoning, the review further explores the emerging development of LLM-powered autonomous agents for healthcare. Moreover, it summarizes the evaluation methodologies for assessing LLMs’ reliability and safety in medical contexts. LLMs have transformative potential in medicine; however, there is a pivotal need for continuous optimizations and ethical oversight before these models can be effectively integrated into clinical practice.}
}
@article{ARABZADEH2023104486,
title = {A self-supervised language model selection strategy for biomedical question answering},
journal = {Journal of Biomedical Informatics},
volume = {146},
pages = {104486},
year = {2023},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2023.104486},
url = {https://www.sciencedirect.com/science/article/pii/S1532046423002071},
author = {Negar Arabzadeh and Ebrahim Bagheri},
keywords = {Biomedical question answering, Domain-specific language model, General-purpose language model, Self-supervised learning},
abstract = {Large neural-based Pre-trained Language Models (PLM) have recently gained much attention due to their noteworthy performance in many downstream Information Retrieval (IR) and Natural Language Processing (NLP) tasks. PLMs can be categorized as either general-purpose, which are trained on resources such as large-scale Web corpora, and domain-specific which are trained on in-domain or mixed-domain corpora. While domain-specific PLMs have shown promising performance on domain-specific tasks, they are significantly more computationally expensive compared to general-purpose PLMs as they have to be either retrained or trained from scratch. The objective of our work in this paper is to explore whether it would be possible to leverage general-purpose PLMs to show competitive performance to domain-specific PLMs without the need for expensive retraining of the PLMs for domain-specific tasks. By focusing specifically on the recent BioASQ Biomedical Question Answering task, we show how different general-purpose PLMs show synergistic behaviour in terms of performance, which can lead to overall notable performance improvement when used in tandem with each other. More concretely, given a set of general-purpose PLMs, we propose a self-supervised method for training a classifier that systematically selects the PLM that is most likely to answer the question correctly on a per-input basis. We show that through such a selection strategy, the performance of general-purpose PLMs can become competitive with domain-specific PLMs while remaining computationally light since there is no need to retrain the large language model itself. We run experiments on the BioASQ dataset, which is a large-scale biomedical question-answering benchmark. We show that utilizing our proposed selection strategy can show statistically significant performance improvements on general-purpose language models with an average of 16.7% when using only lighter models such as DistilBERT and DistilRoBERTa, as well as 14.2% improvement when using relatively larger models such as BERT and RoBERTa and so, their performance become competitive with domain-specific large language models such as PubMedBERT.}
}
@article{LASTRADIAZ2019645,
title = {A reproducible survey on word embeddings and ontology-based methods for word similarity: Linear combinations outperform the state of the art},
journal = {Engineering Applications of Artificial Intelligence},
volume = {85},
pages = {645-665},
year = {2019},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2019.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S0952197619301745},
author = {Juan J. Lastra-Díaz and Josu Goikoetxea and Mohamed Ali {Hadj Taieb} and Ana García-Serrano and Mohamed {Ben Aouicha} and Eneko Agirre},
keywords = {Ontology-based semantic similarity measures, Word embedding models, Information Content models, WordNet, Experimental survey, HESML},
abstract = {Human similarity and relatedness judgements between concepts underlie most of cognitive capabilities, such as categorisation, memory, decision-making and reasoning. For this reason, the proposal of methods for the estimation of the degree of similarity and relatedness between words and concepts has been a very active line of research in the fields of artificial intelligence, information retrieval and natural language processing among others. Main approaches proposed in the literature can be categorised in two large families as follows: (1) Ontology-based semantic similarity Measures (OM) and (2) distributional measures whose most recent and successful methods are based on Word Embedding (WE) models. However, the lack of a deep analysis of both families of methods slows down the advance of this line of research and its applications. This work introduces the largest, reproducible and detailed experimental survey of OM measures and WE models reported in the literature which is based on the evaluation of both families of methods on a same software platform, with the aim of elucidating what is the state of the problem. We show that WE models which combine distributional and ontology-based information get the best results, and in addition, we show for the first time that a simple average of two best performing WE models with other ontology-based measures or WE models is able to improve the state of the art by a large margin. In addition, we provide a very detailed reproducibility protocol together with a collection of software tools and datasets as supplementary material to allow the exact replication of our results.}
}
@article{CARROLL2024102899,
title = {Integrating large language models and generative artificial intelligence tools into information literacy instruction},
journal = {The Journal of Academic Librarianship},
volume = {50},
number = {4},
pages = {102899},
year = {2024},
issn = {0099-1333},
doi = {https://doi.org/10.1016/j.acalib.2024.102899},
url = {https://www.sciencedirect.com/science/article/pii/S0099133324000600},
author = {Alexander J. Carroll and Joshua Borycz},
keywords = {Generative artificial intelligence, Large language models, Information literacy, STEM education, Information retrieval, Critical thinking},
abstract = {Generative artificial intelligence (AI) and large language models (LLMs) have induced a mixture of excitement and panic among educators. However, there is a lack of consensus over how much experience science and engineering students have with using these tools for research-related tasks. Likewise, it is not yet known how educators and information professionals can leverage these tools to teach students strategies for information retrieval and knowledge synthesis. This study assesses the extent of students' use of AI tools in research-related tasks and if information literacy instruction could impact their perception of these tools. Responses to Likert-scale questions indicate that many students did not have extensive experience using LLMs for research-related purposes prior to the information literacy sessions. However, after participating in a didactic lecture and discussion with an engineering librarian that explored how to use these tools effectively and responsibly, many students reported viewing these tools as potentially useful for future assignments. Student responses to open-response questions suggest that librarian-led information literacy training can assist students in developing more sophisticated understandings of the limitations and use cases for artificial intelligence in inquiry-based coursework.}
}
@article{KUMAR2024100308,
title = {AOPWIKI-EXPLORER: An interactive graph-based query engine leveraging large language models},
journal = {Computational Toxicology},
volume = {30},
pages = {100308},
year = {2024},
issn = {2468-1113},
doi = {https://doi.org/10.1016/j.comtox.2024.100308},
url = {https://www.sciencedirect.com/science/article/pii/S2468111324000100},
author = {Saurav Kumar and Deepika Deepika and Karin Slater and Vikas Kumar},
keywords = {Adverse outcome pathway, Large language model, Graph database, Risk assessment, Artificial intelligence, Data integration, Information retrieval, Information extraction},
abstract = {Adverse Outcome Pathways (AOPs) provide a basis for non-animal testing, by outlining the cascade of molecular and cellular events initiated upon stressor exposure, leading to adverse effects. In recent years, the scientific community has shown interest in developing AOPs through crowdsourcing, with the results archived in the AOP-Wiki: a centralized repository coordinated by the OECD, hosting nearly 512 AOPs (April, 2023). However, the AOP-Wiki platform currently lacks a versatile querying system, which hinders developers' exploration of the AOP network and impedes its practical use in risk assessment. This work proposes to unleash the full potential of the AOP-Wiki archive by adapting its data into a Labelled Property Graph (LPG) schema. Additionally, the tool offers a visual network query interface for both database-specific and natural language queries, facilitating the retrieval and analysis of graph data. The multi-query interface allows non-technical users to construct flexible queries, thereby enhancing the potential for AOP exploration. By reducing the time and technical requirements, the present query engine enhances the practical utilization of the valuable data within AOP-Wiki. To evaluate the platform, a case study is presented with three levels of use-case scenarios (simple, moderate, and complex queries). AOPWIKI-EXPLORER is freely available on GitHub (https://github.com/Crispae/AOPWiki_Explorer) for wider community reach and further enhancement.}
}
@article{CALIJORNESOARES2020635,
title = {A literature review on question answering techniques, paradigms and systems},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {32},
number = {6},
pages = {635-646},
year = {2020},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2018.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S131915781830082X},
author = {Marco Antonio {Calijorne Soares} and Fernando Silva Parreiras},
keywords = {Question answering systems, Natural language processing, Information retrieval},
abstract = {Background
Question Answering (QA) systems enable users to retrieve exact answers for questions posed in natural language.
Objective
This study aims at identifying QA techniques, tools and systems, as well as the metrics and indicators used to measure these approaches for QA systems and also to determine how the relationship between Question Answering and natural language processing is built.
Method
The method adopted was a Systematic Literature Review of studies published from 2000 to 2017.
Results
130 out of 1842 papers have been identified as describing a QA approach developed and evaluated with different techniques.
Conclusion
Question Answering researchers have concentrated their efforts in natural language processing, knowledge base and information retrieval paradigms. Most of the researches focused on open domain. Regarding the metrics used to evaluate the approaches, Precision and Recall are the most addressed.}
}
@article{BENALI2022733,
title = {Arabic Named Entity Recognition in Arabic Tweets Using BERT-based Models},
journal = {Procedia Computer Science},
volume = {203},
pages = {733-738},
year = {2022},
note = {17th International Conference on Future Networks and Communications / 19th International Conference on Mobile Systems and Pervasive Computing / 12th International Conference on Sustainable Energy Information Technology (FNC/MobiSPC/SEIT 2022), August 9-11, 2022, Niagara Falls, Ontario, Canada},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.07.109},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922007141},
author = {Brahim Ait Benali and Soukaina Mihi and Nabil Laachfoubi and Addi Ait Mlouk},
keywords = {Named entity recognition, BERT, Transfer learning, Conditional random field, BiLSTM, Dialect arabic, Natural language processing},
abstract = {With the large amount of unstructured data being broadcasted every day, building powerful methods enabling information retrieval and extraction becomes necessary. Unfortunately, named entity recognition is a difficult classification task to classify data into predefined labels, which is further challenged by the Arabic language's particular characteristics and complex nature. This work trains six BERT-based models (Bidirectional Encoder Representations from Transformers) and uses a BiLSTM-CRF architecture for the NER task on dialectal Arabic. Our fine-tuning approach yields new state-of-the-art results on publicly available dialectal Arabic social media datasets.}
}
@article{SCIANNAMEO2024108326,
title = {Information extraction from medical case reports using OpenAI InstructGPT},
journal = {Computer Methods and Programs in Biomedicine},
volume = {255},
pages = {108326},
year = {2024},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2024.108326},
url = {https://www.sciencedirect.com/science/article/pii/S0169260724003195},
author = {Veronica Sciannameo and Daniele Jahier Pagliari and Sara Urru and Piercesare Grimaldi and Honoria Ocagli and Sara Ahsani-Nasab and Rosanna Irene Comoretto and Dario Gregori and Paola Berchialla},
keywords = {Large language model, Natural language processing, Information retrieval, Case reports},
abstract = {Background and objective
Researchers commonly use automated solutions such as Natural Language Processing (NLP) systems to extract clinical information from large volumes of unstructured data. However, clinical text's poor semantic structure and domain-specific vocabulary can make it challenging to develop a one-size-fits-all solution. Large Language Models (LLMs), such as OpenAI's Generative Pre-Trained Transformer 3 (GPT-3), offer a promising solution for capturing and standardizing unstructured clinical information. This study evaluated the performance of InstructGPT, a family of models derived from LLM GPT-3, to extract relevant patient information from medical case reports and discussed the advantages and disadvantages of LLMs versus dedicated NLP methods.
Methods
In this paper, 208 articles related to case reports of foreign body injuries in children were identified by searching PubMed, Scopus, and Web of Science. A reviewer manually extracted information on sex, age, the object that caused the injury, and the injured body part for each patient to build a gold standard to compare the performance of InstructGPT.
Results
InstructGPT achieved high accuracy in classifying the sex, age, object and body part involved in the injury, with 94%, 82%, 94% and 89%, respectively. When excluding articles for which InstructGPT could not retrieve any information, the accuracy for determining the child's sex and age improved to 97%, and the accuracy for identifying the injured body part improved to 93%. InstructGPT was also able to extract information from non-English language articles.
Conclusions
The study highlights that LLMs have the potential to eliminate the necessity for task-specific training (zero-shot extraction), allowing the retrieval of clinical information from unstructured natural language text, particularly from published scientific literature like case reports, by directly utilizing the PDF file of the article without any pre-processing and without requiring any technical expertise in NLP or Machine Learning. The diverse nature of the corpus, which includes articles written in languages other than English, some of which contain a wide range of clinical details while others lack information, adds to the strength of the study.}
}
@article{PAN20242849,
title = {Comparing Fine-Tuning, Zero and Few-Shot Strategies with Large Language Models in Hate Speech Detection in English},
journal = {CMES - Computer Modeling in Engineering and Sciences},
volume = {140},
number = {3},
pages = {2849-2868},
year = {2024},
issn = {1526-1492},
doi = {https://doi.org/10.32604/cmes.2024.049631},
url = {https://www.sciencedirect.com/science/article/pii/S1526149224000493},
author = {Ronghao Pan and José {Antonio García-Díaz} and Rafael Valencia-García},
keywords = {Hate speech detection, zero-shot, few-shot, fine-tuning, natural language processing},
abstract = {Large Language Models (LLMs) are increasingly demonstrating their ability to understand natural language and solve complex tasks, especially through text generation. One of the relevant capabilities is contextual learning, which involves the ability to receive instructions in natural language or task demonstrations to generate expected outputs for test instances without the need for additional training or gradient updates. In recent years, the popularity of social networking has provided a medium through which some users can engage in offensive and harmful online behavior. In this study, we investigate the ability of different LLMs, ranging from zero-shot and few-shot learning to fine-tuning. Our experiments show that LLMs can identify sexist and hateful online texts using zero-shot and few-shot approaches through information retrieval. Furthermore, it is found that the encoder-decoder model called Zephyr achieves the best results with the fine-tuning approach, scoring 86.811% on the Explainable Detection of Online Sexism (EDOS) test-set and 57.453% on the Multilingual Detection of Hate Speech Against Immigrants and Women in Twitter (HatEval) test-set. Finally, it is confirmed that the evaluated models perform well in hate text detection, as they beat the best result in the HatEval task leaderboard. The error analysis shows that contextual learning had difficulty distinguishing between types of hate speech and figurative language. However, the fine-tuned approach tends to produce many false positives.}
}
@article{UPADHYAY2024100088,
title = {A comprehensive survey on answer generation methods using NLP},
journal = {Natural Language Processing Journal},
volume = {8},
pages = {100088},
year = {2024},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2024.100088},
url = {https://www.sciencedirect.com/science/article/pii/S2949719124000360},
author = {Prashant Upadhyay and Rishabh Agarwal and Sumeet Dhiman and Abhinav Sarkar and Saumya Chaturvedi},
keywords = {Question-answering systems, Natural language processing, Question analysis, Answer extraction, Information retrieval},
abstract = {Recent advancements in question-answering systems have significantly enhanced the capability of computers to understand and respond to queries in natural language. This paper presents a comprehensive review of the evolution of question answering systems, with a focus on the developments over the last few years. We examine the foundational aspects of a question answering framework, including question analysis, answer extraction, and passage retrieval. Additionally, we delve into the challenges that question answering systems encounter, such as the intricacies of question processing, the necessity of contextual data sources, and the complexities involved in real-time question answering. Our study categorizes existing question answering systems based on the types of questions they address, the nature of the answers they produce, and the various approaches employed to generate these answers. We also explore the distinctions between opinion-based, extraction-based, retrieval-based, and generative answer generation. The classification provides insight into the strengths and limitations of each method, paving the way for future innovations in the field. This review aims to offer a clear understanding of the current state of question answering systems and to identify the scaling needed to meet the rising expectations and demands of users for coherent and accurate automated responses in natural language.}
}
@article{RYBINSKI2024104734,
title = {Learning to match patients to clinical trials using large language models},
journal = {Journal of Biomedical Informatics},
volume = {159},
pages = {104734},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104734},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424001527},
author = {Maciej Rybinski and Wojciech Kusa and Sarvnaz Karimi and Allan Hanbury},
keywords = {Clinical trials, Patient to trials matching, TCRR, TREC CT, Large language models, Information retrieval, Learning-to-rank},
abstract = {Objective:
This study investigates the use of Large Language Models (LLMs) for matching patients to clinical trials (CTs) within an information retrieval pipeline. Our objective is to enhance the process of patient-trial matching by leveraging the semantic processing capabilities of LLMs, thereby improving the effectiveness of patient recruitment for clinical trials.
Methods:
We employed a multi-stage retrieval pipeline integrating various methodologies, including BM25 and Transformer-based rankers, along with LLM-based methods. Our primary datasets were the TREC Clinical Trials 2021–23 track collections. We compared LLM-based approaches, focusing on methods that leverage LLMs in query formulation, filtering, relevance ranking, and re-ranking of CTs.
Results:
Our results indicate that LLM-based systems, particularly those involving re-ranking with a fine-tuned LLM, outperform traditional methods in terms of nDCG and Precision measures. The study demonstrates that fine-tuning LLMs enhances their ability to find eligible trials. Moreover, our LLM-based approach is competitive with state-of-the-art systems in the TREC challenges. The study shows the effectiveness of LLMs in CT matching, highlighting their potential in handling complex semantic analysis and improving patient-trial matching. However, the use of LLMs increases the computational cost and reduces efficiency. We provide a detailed analysis of effectiveness-efficiency trade-offs.
Conclusion:
This research demonstrates the promising role of LLMs in enhancing the patient-to-clinical trial matching process, offering a significant advancement in the automation of patient recruitment. Future work should explore optimising the balance between computational cost and retrieval effectiveness in practical applications.}
}