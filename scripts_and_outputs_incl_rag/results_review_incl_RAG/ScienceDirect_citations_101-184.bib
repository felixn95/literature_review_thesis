@article{TAN2021347,
title = {Research on Knowledge Driven Intelligent Question Answering System for Electric Power Customer Service},
journal = {Procedia Computer Science},
volume = {187},
pages = {347-352},
year = {2021},
note = {2020 International Conference on Identification, Information and Knowledge in the Internet of Things, IIKI2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.04.072},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921008668},
author = {Yuanpeng Tan and Huifang Xu and Yaguang Wu and Zhonghao Zhang and Yeteng An and Yongping Xiong and Fang Wang},
keywords = {knowledge graph, electric power customer service, knowledge engineering, intelligent question answering system},
abstract = {The electric power customer service domain-specific knowledge graph aims to describe the concepts, entities, events and their relationships in the electric power customer service business field, with a structured manner, and provide a more effective data organization, management, and cognitive ability for the customer service business. This paper proposes a method for constructing a knowledge graph in the field of electric customer service, and constructs a knowledge graph with over 15,000 entities and 20,000 relationships. Based on the knowledge graph, an intelligent question answering application architecture is designed, which consists of multiple functional modules such as dialogue process configuration, natural language processing, and business process processing. It provides more efficient and open knowledge retrieval services for the electric customer service business, and improves the intelligence level of customer service question answering.}
}
@article{SELAMAT2016457,
title = {Word-length algorithm for language identification of under-resourced languages},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {28},
number = {4},
pages = {457-469},
year = {2016},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2014.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S1319157815000609},
author = {Ali Selamat and Nicholas Akosu},
keywords = {Language identification, Under-resourced languages, Resource-scarce, Digital divide, Spellchecker model},
abstract = {Language identification is widely used in machine learning, text mining, information retrieval, and speech processing. Available techniques for solving the problem of language identification do require large amount of training text that are not available for under-resourced languages which form the bulk of the World’s languages. The primary objective of this study is to propose a lexicon based algorithm which is able to perform language identification using minimal training data. Because language identification is often the first step in many natural language processing tasks, it is necessary to explore techniques that will perform language identification in the shortest possible time. Hence, the second objective of this research is to study the effect of the proposed algorithm on the run-time performance of language identification. Precision, recall, and F1 measures were used to determine the effectiveness of the proposed word length algorithm using datasets drawn from the Universal Declaration of Human Rights Act in 15 languages. The experimental results show good accuracy on language identification at the document level and at the sentence level based on the available dataset. The improved algorithm also showed significant improvement in run time performance compared with the spelling checker approach.}
}
@article{SALEEM2019381,
title = {Deep Learning for Internet of Things Data Analytics},
journal = {Procedia Computer Science},
volume = {163},
pages = {381-390},
year = {2019},
note = {16th Learning and Technology Conference 2019Artificial Intelligence and Machine Learning: Embedding the Intelligence},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.12.120},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919321593},
author = {Tausifa Jan Saleem and Mohammad Ahsan Chishti},
keywords = {Internet of Things, Deep Learning, Data Analytics},
abstract = {The recent technological innovations and brisk amalgamation of domains such as sensing and actuating technologies, embedded systems, wireless communication, and data analytics are accelerating the growth of Internet of Things (IoT). The massive number of sensors deployed in IoT generate humongous volumes of data for a broad range of applications such as smart home, smart healthcare, smart manufacturing, smart transportation, smart grid, smart agriculture etc. Analyzing such data in order to facilitate enhanced decision making, increase productivity and accuracy, ameliorate revenue is a critical process that makes IoT a precious idea for businesses and a standard of life improving paradigm. Although deriving concealed information and inferences out of IoT data is promising to improve the standard of our lives, it is a complicated task that cannot be accomplished by conventional paradigms. Deep Learning would play a vital role in creating smarter IoT as it has shown remarkable results in different fields including image recognition, information retrieval, speech recognition, natural language processing, indoor localization, physiological and psychological state detection etc. and these form the foundation services for IoT applications. In this regard, investigating the potential of Deep Learning for IoT data analytics becomes indispensable. Motivated to address this concern, this paper explores the flair of Deep Learning for analyzing data generated from IoT environments. A detailed discussion on various Deep Learning architectures, their role in IoT data analytics and potential use cases is also presented. Finally, open research challenges and future research directions are discussed in order to promote future research in this domain.}
}
@article{JIANG2023100456,
title = {MoreThanSentiments: A text analysis package},
journal = {Software Impacts},
volume = {15},
pages = {100456},
year = {2023},
issn = {2665-9638},
doi = {https://doi.org/10.1016/j.simpa.2022.100456},
url = {https://www.sciencedirect.com/science/article/pii/S2665963822001403},
author = {Jinhang Jiang and Karthik Srinivasan},
keywords = {Text mining, Natural language processing, Information extraction, Text complexity measures, Business analytics, Accounting},
abstract = {Text mining on a large corpus of data has gained utility and popularity over recent years owing to advancements in information retrieval and machine learning methods. However, popular text mining software packages mainly focus on either sentiment analysis or semantic meaning extraction, requiring pretraining on a large corpus of text data. In comparison, MoreThanSentiments provides computation of newer text attribution measures, including boiler score, specificity, redundancy, and hard info, which have been proposed in accounting analytics literature. Our software package, available in Python, is flexible in terms of parameter setting and is adaptable to different applications. Through this package, we seek to simplify the process of deploying nontrivial information extraction techniques published in domain-specific text analysis research into domain-agnostic analytics applications.}
}
@article{ANDREASEN2024102246,
title = {The power and potentials of Flexible Query Answering Systems: A critical and comprehensive analysis},
journal = {Data & Knowledge Engineering},
volume = {149},
pages = {102246},
year = {2024},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2023.102246},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X23001064},
author = {Troels Andreasen and Gloria Bordogna and Guy De Tré and Janusz Kacprzyk and Henrik Legind Larsen and Sławomir Zadrożny},
keywords = {Flexible query answering, Model-based query answering, Data-driven query answering},
abstract = {The popularity of chatbots, such as ChatGPT, has brought research attention to question answering systems, capable to generate natural language answers to user’s natural language queries. However, also in other kinds of systems, flexibility of querying, including but also going beyond the use of natural language, is an important feature. With this consideration in mind the paper presents a critical and comprehensive analysis of recent developments, trends and challenges of Flexible Query Answering Systems (FQASs). Flexible query answering is a multidisciplinary research field that is not limited to question answering in natural language, but comprises other query forms and interaction modalities, which aim to provide powerful means and techniques for better reflecting human preferences and intentions to retrieve relevant information. It adopts methods at the crossroad of several disciplines among which Information Retrieval (IR), databases, knowledge based systems, knowledge and data engineering, Natural Language Processing (NLP) and the semantic web may be mentioned. The analysis principles are inspired by the Preferred Reporting Items for Systematic Reviews and Meta-Analysis (PRISMA) framework, characterized by a top-down process, starting with relevant keywords for the topic of interest to retrieve relevant articles from meta-sources And complementing these articles with other relevant articles from seed sources Identified by a bottom-up process. to mine the retrieved publication data a network analysis is performed Which allows to present in a synthetic way intrinsic topics of the selected publications. issues dealt with are related to query answering methods Both model-based and data-driven (the latter based on either machine learning or deep learning) And to their needs for explainability and fairness to deal with big data Notably by taking into account data veracity. conclusions point out trends and challenges to help better shaping the future of the FQAS field.}
}
@article{GOLDBERG2022100270,
title = {Fumeus: A family of Python tools for text mining with smoke terms},
journal = {Software Impacts},
volume = {12},
pages = {100270},
year = {2022},
issn = {2665-9638},
doi = {https://doi.org/10.1016/j.simpa.2022.100270},
url = {https://www.sciencedirect.com/science/article/pii/S2665963822000276},
author = {David M. Goldberg and Richard J. Gruss and Alan S. Abrahams},
keywords = {Text mining, Natural language processing, Information retrieval, Machine learning, Decision support, Product safety},
abstract = {Synthesizing meaningful insights from voluminous textual datasets is complex and challenging. The task is especially difficult for sparse target classes. Recent works have proposed “smoke terms,” or machine-learned words and phrases prevalent in a target class. Smoke terms may be utilized to rank or sort text, or they may serve as features in follow-on machine learning models. This paper introduces Fumeus, a family of Python-based smoke term analysis tools. We provide functionality to generate new smoke terms and to use existing smoke term dictionaries to rank or sort datasets. These analyses have numerous academic, regulatory, and industry applications.}
}
@article{YU2009817,
title = {Psychiatric document retrieval using a discourse-aware model},
journal = {Artificial Intelligence},
volume = {173},
number = {7},
pages = {817-829},
year = {2009},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2008.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370208002130},
author = {Liang-Chih Yu and Chung-Hsien Wu and Fong-Lin Jang},
keywords = {Natural language processing, Information retrieval, Discourse structure, Discourse-aware model, Sequence kernel function, Discounted cumulative gain},
abstract = {With the increased incidence of depression-related disorders, many psychiatric websites have been developed to provide huge amounts of educational documents along with rich self-help information. Psychiatric document retrieval aims to assist individuals to locate documents relevant to their depressive problems efficiently and effectively. By referring to relevant documents, individuals can understand how to alleviate their depression-related symptoms according to recommendations from health professionals. This work proposes the use of high-level discourse information extracted from queries and documents to improve the precision of retrieval results. The discourse information adopted herein includes negative life events, depressive symptoms and semantic relations between symptoms, which are beneficial for better understanding of users' queries. Experimental results show that the discourse-aware retrieval model achieves higher precision than the word-based retrieval models, namely the vector space model (VSM) and Okapi model, adopting word-level information alone.}
}
@article{WANG2020105030,
title = {Word Sense Disambiguation: A comprehensive knowledge exploitation framework},
journal = {Knowledge-Based Systems},
volume = {190},
pages = {105030},
year = {2020},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2019.105030},
url = {https://www.sciencedirect.com/science/article/pii/S0950705119304344},
author = {Yinglin Wang and Ming Wang and Hamido Fujita},
keywords = {Word sense disambiguation, Background knowledge, Information retrieval, Relation exploitation, Semantic path},
abstract = {Word Sense Disambiguation (WSD) has been a basic and on-going issue since its introduction in natural language processing (NLP) community. Its application lies in many different areas including sentiment analysis, Information Retrieval (IR), machine translation and knowledge graph construction. Solutions to WSD are mostly categorized into supervised and knowledge-based approaches. In this paper, a knowledge-based method is proposed, modeling the problem with semantic space and semantic path hidden behind a given sentence. The approach relies on the well-known Knowledge Base (KB) named WordNet and models the semantic space and semantic path by Latent Semantic Analysis (LSA) and PageRank respectively. Experiments has proven the method’s effectiveness, achieving state-of-the-art performance in several WSD datasets.}
}
@article{KARIM2015488,
title = {Graph-based Methods for Significant Concept Selection},
journal = {Procedia Computer Science},
volume = {60},
pages = {488-497},
year = {2015},
note = {Knowledge-Based and Intelligent Information & Engineering Systems 19th Annual Conference, KES-2015, Singapore, September 2015 Proceedings},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.08.170},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915022978},
author = {Gasmi Karim and Torjmen-Khemakhem Mouna and Tamine Lynda and Ben Jemaa Maher},
keywords = {information retrieval, Natural Language Processing (NLP), semantic similarity, concept selection ;},
abstract = {It is well known in information retrieval area that one important issue is the gap between the query and document vocabularies. Concept-based representation of both the document and the query is one of the most effective approaches that lowers the effect of text mismatch and allows the selection of relevant documents that deal with the shared semantics hidden behind both. However, identifying the best representative concepts from texts is still challenging. In this paper, we propose a graph-based method to select the most significant concepts to be integrated into a conceptual indexing system. More specifically, we build the graph whose nodes represented concepts and weighted edges represent semantic distances. The importance of concepts are computed using centrality algorithms that levrage between structural and contextual importance. We experimentally evaluated our method of concept selection using the standard ImageClef2009 medical data set. Results showed that our approach significantly improves the retrieval effectiveness in comparison to state-of-the-art retrieval models.}
}
@article{HARNOUNE2021100042,
title = {BERT based clinical knowledge extraction for biomedical knowledge graph construction and analysis},
journal = {Computer Methods and Programs in Biomedicine Update},
volume = {1},
pages = {100042},
year = {2021},
issn = {2666-9900},
doi = {https://doi.org/10.1016/j.cmpbup.2021.100042},
url = {https://www.sciencedirect.com/science/article/pii/S2666990021000410},
author = {Ayoub Harnoune and Maryem Rhanoui and Mounia Mikram and Siham Yousfi and Zineb Elkaimbillah and Bouchra {El Asri}},
keywords = {Knowledge graph, Biomedical informatics, Clinical data, Natural language processing, BERT},
abstract = {Background: Knowledge is evolving over time, often as a result of new discoveries or changes in the adopted methods of reasoning. Also, new facts or evidence may become available, leading to new understandings of complex phenomena. This is particularly true in the biomedical field, where scientists and physicians are constantly striving to find new methods of diagnosis, treatment and eventually cure. Knowledge Graphs (KGs) offer a real way of organizing and retrieving the massive and growing amount of biomedical knowledge. Objective: We propose an end-to-end approach for knowledge extraction and analysis from biomedical clinical notes using the Bidirectional Encoder Representations from Transformers (BERT) model and Conditional Random Field (CRF) layer. Methods: The approach is based on knowledge graphs, which can effectively process abstract biomedical concepts such as relationships and interactions between medical entities. Besides offering an intuitive way to visualize these concepts, KGs can solve more complex knowledge retrieval problems by simplifying them into simpler representations or by transforming the problems into representations from different perspectives. We created a biomedical Knowledge Graph using using Natural Language Processing models for named entity recognition and relation extraction. The generated biomedical knowledge graphs (KGs) are then used for question answering. Results: The proposed framework can successfully extract relevant structured information with high accuracy (90.7% for Named-entity recognition (NER), 88% for relation extraction (RE)), according to experimental findings based on real-world 505 patient biomedical unstructured clinical notes. Conclusions:In this paper, we propose a novel end-to-end system for the construction of a biomedical knowledge graph from clinical textual using a variation of BERT models.}
}
@article{MOHAMMED2023102460,
title = {Building lexicon-based sentiment analysis model for low-resource languages},
journal = {MethodsX},
volume = {11},
pages = {102460},
year = {2023},
issn = {2215-0161},
doi = {https://doi.org/10.1016/j.mex.2023.102460},
url = {https://www.sciencedirect.com/science/article/pii/S2215016123004569},
author = {Idi Mohammed and Rajesh Prasad},
keywords = {Lexicon dictionary, Low-resource languages, Sentiment analysis, Fine-tuning, Hausa language},
abstract = {Natural Language Processing (NLP) has transformed machine translation, sentiment analysis, information retrieval, and conversation systems. NLP applications rely on complete linguistic resources, which might be difficult for low-resource languages. NLP solutions for every language require a language-specific dataset. Dataset in a language is essential for NLP solution creation. Over 7000 languages are spoken worldwide. Only around 20 languages have text corpora for NLP applications. English has the most datasets, then Chinese and Spanish. Japanese has several Western European language datasets. For an accurate NLP system, most Asian and African languages lack training datasets. To address this challenge, we propose a methodology for building a lexicon-based sentiment analysis model for languages with limited resources. The Hausa language was used as training and evaluation language. The methodology combines lexicon creation; augmentation, annotation, and fine-tuning model, and has been tested on a corpus of Hausa tweets achieving an accuracy of 98 %. The results suggest that our proposed model is a promising tool for sentiment analysis in a variety of applications, such as social media monitoring, customer service, and market research. Our methodology can be used for any low-resource language. The outline of the work done in this paper can be shown as follows:•We propose a methodology for building a lexicon-based sentiment analysis model for languages with limited resources, using the Hausa language as a case study.•The methodology combines lexicon creation, augmentation, annotation, and fine-tuning model, and achieves an accuracy of 98 % on a corpus of Hausa tweets.•The results suggest that the proposed model is a promising tool for sentiment analysis in a variety of applications for low-resource languages.}
}
@article{ALKABI201594,
title = {A novel root based Arabic stemmer},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {27},
number = {2},
pages = {94-103},
year = {2015},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2014.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S1319157815000166},
author = {Mohammed N. Al-Kabi and Saif A. Kazakzeh and Belal M. {Abu Ata} and Saif A. Al-Rababah and Izzat M. Alsmadi},
keywords = {Natural Language Processing (NLP), Computational intelligence, Stemming, Information retrieval},
abstract = {Stemming algorithms are used in information retrieval systems, indexers, text mining, text classifiers etc., to extract stems or roots of different words, so that words derived from the same stem or root are grouped together. Many stemming algorithms were built in different natural languages. Khoja stemmer is one of the known and widely used Arabic stemmers. In this paper, we introduced a new light and heavy Arabic stemmer. This new stemmer is presented in this study and compared with two well-known Arabic stemmers. Results showed that accuracy of our stemmer is slightly better than the accuracy yielded by each one of those two well-known Arabic stemmers used for evaluation and comparison. Evaluation tests on our novel stemmer yield 75.03% accuracy, while the other two Arabic stemmers yield slightly lower accuracy.}
}
@article{RYBINSKI2020103530,
title = {Clinical trial search: Using biomedical language understanding models for re-ranking},
journal = {Journal of Biomedical Informatics},
volume = {109},
pages = {103530},
year = {2020},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2020.103530},
url = {https://www.sciencedirect.com/science/article/pii/S1532046420301581},
author = {Maciej Rybinski and Jerry Xu and Sarvnaz Karimi},
keywords = {Clinical decision making, Document search, Information retrieval, Ranking functions, Learning-to-rank, Bidirectional transformer encoder, Natural language processing, Complex search, Precision medicine},
abstract = {Bidirectional Encoder Representations from Transformers (BERT) have achieved state-of-the-art effectiveness in some of the biomedical information processing applications. We investigate the effectiveness of these techniques for clinical trial search systems. In precision medicine, matching patients to relevant experimental evidence or prospective treatments is a complex task which requires both clinical and biological knowledge. To assist in this complex decision making, we investigate the effectiveness of different ranking models based on the BERT models under the same retrieval platform to ensure fair comparisons. An evaluation on the TREC Precision Medicine benchmarks indicates that our approach using the BERT model pre-trained on scientific abstracts and clinical notes achieves state-of-the-art results, on par with highly specialised, manually optimised heuristic models. We also report the best results to date on the TREC Precision Medicine 2017 ad hoc retrieval task for clinical trial search.}
}
@article{ABRAMOVICI2018265,
title = {Semantic Quality Assurance of Heterogeneous Unstructured Repair Reports},
journal = {Procedia CIRP},
volume = {73},
pages = {265-270},
year = {2018},
note = {10th CIRP Conference on Industrial Product-Service Systems, IPS2 2018, 29-31 May 2018, Linköping, Sweden},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.03.334},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118305158},
author = {Michael Abramovici and Philip Gebus and Jens Christian Göbel and Philipp Savarino},
keywords = {Quality Assurance, Product-Service Systems, Semantic Technologies, Knowledge Management},
abstract = {Service technicians spend a considerable amount of their working hours in order to search for information regarding a current work order. In case of an IPS² malfunction for example, they can either search for potential failure causes within previously documented repair reports that describe a similar malfunction or manually inspect the IPS². On the one hand the former IT-related information procurement is caused by a large amount of irrelevant search results rendered by current state of the art information retrieval approaches implemented in maintenance-related IT systems. On the other hand many repair reports suffer from missing or ambiguous information and a holistically low data quality, which makes it difficult for the service technicians to derive task-related information from a particular repair report, although this report basically describes the same malfunction. These current difficulties will be amplified once service partners get access to maintenance-related information documented by other service partners (companies) within the IPS² network as the amount of available data will increase drastically and new problems will raise concerning the heterogeneity of the data. The paper on hand presents a semantic quality assurance concept for heterogeneous unstructured repair reports that addresses the low data quality problem by utilizing natural language processing and machine learning methods to automatically analyze the service technician’s inputs during the repair report creation process and by notifying him of potential losses in data quality. The concept’s feasibility has been shown by performing a case study with a prototype that utilizes the developed methods.}
}
@article{WANG201812,
title = {A comparison of word embeddings for the biomedical natural language processing},
journal = {Journal of Biomedical Informatics},
volume = {87},
pages = {12-20},
year = {2018},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2018.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S1532046418301825},
author = {Yanshan Wang and Sijia Liu and Naveed Afzal and Majid Rastegar-Mojarad and Liwei Wang and Feichen Shen and Paul Kingsbury and Hongfang Liu},
keywords = {Word embeddings, Natural language processing, Information extraction, Information retrieval, Machine learning},
abstract = {Background
Word embeddings have been prevalently used in biomedical Natural Language Processing (NLP) applications due to the ability of the vector representations being able to capture useful semantic properties and linguistic relationships between words. Different textual resources (e.g., Wikipedia and biomedical literature corpus) have been utilized in biomedical NLP to train word embeddings and these word embeddings have been commonly leveraged as feature input to downstream machine learning models. However, there has been little work on evaluating the word embeddings trained from different textual resources.
Methods
In this study, we empirically evaluated word embeddings trained from four different corpora, namely clinical notes, biomedical publications, Wikipedia, and news. For the former two resources, we trained word embeddings using unstructured electronic health record (EHR) data available at Mayo Clinic and articles (MedLit) from PubMed Central, respectively. For the latter two resources, we used publicly available pre-trained word embeddings, GloVe and Google News. The evaluation was done qualitatively and quantitatively. For the qualitative evaluation, we randomly selected medical terms from three categories (i.e., disorder, symptom, and drug), and manually inspected the five most similar words computed by embeddings for each term. We also analyzed the word embeddings through a 2-dimensional visualization plot of 377 medical terms. For the quantitative evaluation, we conducted both intrinsic and extrinsic evaluation. For the intrinsic evaluation, we evaluated the word embeddings’ ability to capture medical semantics by measruing the semantic similarity between medical terms using four published datasets: Pedersen’s dataset, Hliaoutakis’s dataset, MayoSRS, and UMNSRS. For the extrinsic evaluation, we applied word embeddings to multiple downstream biomedical NLP applications, including clinical information extraction (IE), biomedical information retrieval (IR), and relation extraction (RE), with data from shared tasks.
Results
The qualitative evaluation shows that the word embeddings trained from EHR and MedLit can find more similar medical terms than those trained from GloVe and Google News. The intrinsic quantitative evaluation verifies that the semantic similarity captured by the word embeddings trained from EHR is closer to human experts’ judgments on all four tested datasets. The extrinsic quantitative evaluation shows that the word embeddings trained on EHR achieved the best F1 score of 0.900 for the clinical IE task; no word embeddings improved the performance for the biomedical IR task; and the word embeddings trained on Google News had the best overall F1 score of 0.790 for the RE task.
Conclusion
Based on the evaluation results, we can draw the following conclusions. First, the word embeddings trained from EHR and MedLit can capture the semantics of medical terms better, and find semantically relevant medical terms closer to human experts’ judgments than those trained from GloVe and Google News. Second, there does not exist a consistent global ranking of word embeddings for all downstream biomedical NLP applications. However, adding word embeddings as extra features will improve results on most downstream tasks. Finally, the word embeddings trained from the biomedical domain corpora do not necessarily have better performance than those trained from the general domain corpora for any downstream biomedical NLP task.}
}
@article{NGUYEN2022104005,
title = {Search like an expert: Reducing expertise disparity using a hybrid neural index for COVID-19 queries},
journal = {Journal of Biomedical Informatics},
volume = {127},
pages = {104005},
year = {2022},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2022.104005},
url = {https://www.sciencedirect.com/science/article/pii/S1532046422000211},
author = {Vincent Nguyen and Maciej Rybinski and Sarvnaz Karimi and Zhenchang Xing},
keywords = {COVID-19, Universal sentence embeddings, Information retrieval, Natural language processing, Neural index, Dense retrieval, Medical misinformation, Biomedical search},
abstract = {Consumers from non-medical backgrounds often look for information regarding a specific medical information need; however, they are limited by their lack of medical knowledge and may not be able to find reputable resources. As a case study, we investigate reducing this knowledge barrier to allow consumers to achieve search effectiveness comparable to that of an expert, or a medical professional, for COVID-19 related questions. We introduce and evaluate a hybrid index model that allows a consumer to formulate queries using consumer language to find relevant answers to COVID-19 questions. Our aim is to reduce performance degradation between medical professional queries and those of a consumer. We use a universal sentence embedding model to project consumer queries into the same semantic space as professional queries. We then incorporate sentence embeddings into a search framework alongside an inverted index. Documents from this index are retrieved using a novel scoring function that considers sentence embeddings and BM25 scoring. We find that our framework alleviates the expertise disparity, which we validate using an additional set of crowdsourced—consumer—queries even in an unsupervised setting. We also propose an extension of our method, where the sentence encoder is optimised in a supervised setup. Our framework allows for a consumer to search using consumer queries to match the search performance with that of a professional.}
}
@article{KUSA2023104444,
title = {Effective matching of patients to clinical trials using entity extraction and neural re-ranking},
journal = {Journal of Biomedical Informatics},
volume = {144},
pages = {104444},
year = {2023},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2023.104444},
url = {https://www.sciencedirect.com/science/article/pii/S153204642300165X},
author = {Wojciech Kusa and Óscar E. Mendoza and Petr Knoth and Gabriella Pasi and Allan Hanbury},
keywords = {Clinical trials matching, Query reformulation, TREC clinical trials, Clinical natural language processing, Neural re-ranking, Eligibility criteria, Information retrieval},
abstract = {Introduction:
Clinical trials (CTs) often fail due to inadequate patient recruitment. Finding eligible patients involves comparing the patient’s information with the CT eligibility criteria. Automated patient matching offers the promise of improving the process, yet the main difficulties of CT retrieval lie in the semantic complexity of matching unstructured patient descriptions with semi-structured, multi-field CT documents and in capturing the meaning of negation coming from the eligibility criteria.
Objectives:
This paper tackles the challenges of CT retrieval by presenting an approach that addresses the patient-to-trials paradigm. Our approach involves two key components in a pipeline-based model: (i) a data enrichment technique for enhancing both queries and documents during the first retrieval stage, and (ii) a novel re-ranking schema that uses a Transformer network in a setup adapted to this task by leveraging the structure of the CT documents.
Methods:
We use named entity recognition and negation detection in both patient description and the eligibility section of CTs. We further classify patient descriptions and CT eligibility criteria into current, past, and family medical conditions. This extracted information is used to boost the importance of disease and drug mentions in both query and index for lexical retrieval. Furthermore, we propose a two-step training schema for the Transformer network used to re-rank the results from the lexical retrieval. The first step focuses on matching patient information with the descriptive sections of trials, while the second step aims to determine eligibility by matching patient information with the criteria section.
Results
Our findings indicate that the inclusion criteria section of the CT has a great influence on the relevance score in lexical models, and that the enrichment techniques for queries and documents improve the retrieval of relevant trials. The re-ranking strategy, based on our training schema, consistently enhances CT retrieval and shows improved performance by 15% in terms of precision at retrieving eligible trials.
Conclusion
The results of our experiments suggest the benefit of making use of extracted entities. Moreover, our proposed re-ranking schema shows promising effectiveness compared to larger neural models, even with limited training data. These findings offer valuable insights for improving methods for retrieval of clinical documents.}
}
@article{MARTINEZ2014100,
title = {Improving search over Electronic Health Records using UMLS-based query expansion through random walks},
journal = {Journal of Biomedical Informatics},
volume = {51},
pages = {100-106},
year = {2014},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2014.04.013},
url = {https://www.sciencedirect.com/science/article/pii/S1532046414000987},
author = {David Martinez and Arantxa Otegi and Aitor Soroa and Eneko Agirre},
keywords = {Information storage and retrieval, Algorithms, Data mining, Semantics, Natural language processing},
abstract = {Objective
Most of the information in Electronic Health Records (EHRs) is represented in free textual form. Practitioners searching EHRs need to phrase their queries carefully, as the record might use synonyms or other related words. In this paper we show that an automatic query expansion method based on the Unified Medicine Language System (UMLS) Metathesaurus improves the results of a robust baseline when searching EHRs.
Materials and methods
The method uses a graph representation of the lexical units, concepts and relations in the UMLS Metathesaurus. It is based on random walks over the graph, which start on the query terms. Random walks are a well-studied discipline in both Web and Knowledge Base datasets.
Results
Our experiments over the TREC Medical Record track show improvements in both the 2011 and 2012 datasets over a strong baseline.
Discussion
Our analysis shows that the success of our method is due to the automatic expansion of the query with extra terms, even when they are not directly related in the UMLS Metathesaurus. The terms added in the expansion go beyond simple synonyms, and also add other kinds of topically related terms.
Conclusions
Expansion of queries using related terms in the UMLS Metathesaurus beyond synonymy is an effective way to overcome the gap between query and document vocabularies when searching for patient cohorts.}
}
@article{CABOT2019103176,
title = {Cimind: A phonetic-based tool for multilingual named entity recognition in biomedical texts},
journal = {Journal of Biomedical Informatics},
volume = {94},
pages = {103176},
year = {2019},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2019.103176},
url = {https://www.sciencedirect.com/science/article/pii/S1532046419300942},
author = {Chloé Cabot and Stéfan Darmoni and Lina F. Soualmia},
keywords = {Named Entity Recognition, Natural Language Processing, Vocabulary, Controlled},
abstract = {Background
Extracting concepts from biomedical texts is a key to support many advanced applications such as biomedical information retrieval. However, in clinical notes Named Entity Recognition (NER) has to deal with various types of errors such as spelling errors, grammatical errors, truncated sentences, and non-standard abbreviations. Moreover, in numerous countries, NER is challenged by the availability of many resources originally developed and only suitable for English texts. This paper presents the Cimind system, a multilingual system dedicated to named entity recognition in medical texts based on a phonetic similarity measure.
Methods
Cimind performs entity recognition by combining phonetic recognition using the DM phonetic algorithm to deal with spelling errors and string similarity measures. Three main steps are processed to identify terms in a controlled vocabulary: normalization, candidate selection by phonetic similarity and candidate ranking.
Results
Cimind was evaluated in the 2016 and 2017 editions of the CLEF eHealth challenge in the CépiDC/CDC tasks. In 2017, it obtained on each corpus the following results: English dataset: 83.9% P, 78.3% R, 81.0% F1; French raw dataset: 85.7% P, 68.9% R, 76.4% F1; French aligned dataset: 83.5% P, 77.5% R, 80.4% F1. It ranked first in French and fourth in English in officials runs.}
}
@article{SEONG2023421,
title = {Retrieval methodology for similar NPP LCO cases based on domain specific NLP},
journal = {Nuclear Engineering and Technology},
volume = {55},
number = {2},
pages = {421-431},
year = {2023},
issn = {1738-5733},
doi = {https://doi.org/10.1016/j.net.2022.09.028},
url = {https://www.sciencedirect.com/science/article/pii/S1738573322004600},
author = {No Kyu Seong and Jae Hee Lee and Jong Beom Lee and Poong Hyun Seong},
keywords = {Technical Specifications, Limiting Conditions for Operation, TF-IDF, Similarity, Information Retrieval, NLP},
abstract = {Nuclear power plants (NPPs) have technical specifications (Tech Specs) to ensure that the equipment and key operating parameters necessary for the safe operation of the power plant are maintained within limiting conditions for operation (LCO) determined by a safety analysis. The LCO of Tech Specs that identify the lowest functional capability of equipment required for safe operation for a facility must be complied for the safe operation of NPP. There have been previous studies to aid in compliance with LCO relevant to rule-based expert systems; however, there is an obvious limit to expert systems for implementing the rules for many situations related to LCO. Therefore, in this study, we present a retrieval methodology for similar LCO cases in determining whether LCO is met or not met. To reflect the natural language processing of NPP features, a domain dictionary was built, and the optimal term frequency-inverse document frequency variant was selected. The retrieval performance was improved by adding a Boolean retrieval model based on terms related to the LCO in addition to the vector space model. The developed domain dictionary and retrieval methodology are expected to be exceedingly useful in determining whether LCO is met.}
}
@article{NOVAK2022108545,
title = {Why is a document relevant? Understanding the relevance scores in cross-lingual document retrieval},
journal = {Knowledge-Based Systems},
volume = {244},
pages = {108545},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.108545},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122002416},
author = {Erik Novak and Luka Bizjak and Dunja Mladenić and Marko Grobelnik},
keywords = {Cross-lingual information retrieval, Language model, Optimal transport, Result interpretability, Natural language processing},
abstract = {Modern cross-lingual document retrieval models are capable of finding documents relevant to the query. However, they do not have the capabilities for explaining why the document is relevant. This paper proposes a novel learning-to-rank model named LM-EMD that uses the multilingual BERT language model and Earth Mover’s Distance (EMD) to measure the document’s relevancy to the input query and provide interpretable insights into why a document is relevant. The model uses the query and document token’s contextual embeddings generated with multilingual BERT to measure their distances in the embedding space, which are then used by EMD to calculate the document’s relevance score and identify which document tokens contribute the most to its relevancy. We evaluate the model on five language pairs of varying degrees of similarity and analyze its performance. We find that the model (1) performs similar as the best performing comparing model on high-resource languages, (2) is less effective on low-resource languages, and (3) provides insight into why a document is relevant to the query.}
}
@article{YEONG2019406,
title = {A Hybrid of Sentence-Level Approach and Fragment-Level Approach of Parallel Text Extraction from Comparable Text},
journal = {Procedia Computer Science},
volume = {161},
pages = {406-414},
year = {2019},
note = {The Fifth Information Systems International Conference, 23-24 July 2019, Surabaya, Indonesia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.11.139},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919318496},
author = {Yin-Lai Yeong and Tien-Ping Tan and Keng Hoon Gan},
keywords = {sentence-level, fragment-level, parallel text extracton, machine translation},
abstract = {Parallel texts are essential resources in linguistics, natural language processing, and multilingual information retrieval. Many studies attempt to extract parallel text from existing resources, particularly from comparable texts. The approaches to extract parallel text from comparable text can be divided into sentence-level approach and fragment-level approach. In this paper, an approach that combines sentence-level approach and fragment-level approach is proposed. The study was evaluated using statistical machine translation (SMT) and neural machine translation (NMT). The experiment results show a very significant improvement in the BLEU scores of SMT and NMT. The BLEU scores for SMT for the test in computer science domain and news domain increase from 17.45 and 41.45 to 18.56 and 48.65 respectively. On the other hand, the BLEU scores for NMT in the computer science domain and news domain increase from 14.42 and 19.39 to 21.17 and 41.75 respectively.}
}
@article{PAUL20161528,
title = {Automatic AMR Generation for Simple Sentences Using Dependency Parser},
journal = {Procedia Technology},
volume = {24},
pages = {1528-1533},
year = {2016},
note = {International Conference on Emerging Trends in Engineering, Science and Technology (ICETEST - 2015)},
issn = {2212-0173},
doi = {https://doi.org/10.1016/j.protcy.2016.05.119},
url = {https://www.sciencedirect.com/science/article/pii/S2212017316302080},
author = {N. Pelja Paul and P. Revathy and G.M. Sini and R. Binu},
keywords = {AMR, Dependency Parser, PropBank Lexicon},
abstract = {Information retrieval is an important task in the field of natural language processing. Retrieving information requires a basic representation at the sentence level. Representation for sentences with same meaning should be same so that we can claim that the representation seems to be good enough to use in various natural language tasks. AMR is such a semantic representation aimed at large-scale human annotation inorder to built a giant semantic bank. In this paper, we present an automatic AMR tool for simple sentences with the help of dependency parser.}
}
@article{SINGH2021508,
title = {Morphological evaluation and sentiment analysis of Punjabi text using deep learning classification},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {33},
number = {5},
pages = {508-517},
year = {2021},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2018.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S1319157818300612},
author = {Jaspreet Singh and Gurvinder Singh and Rajinder Singh and Prithvipal Singh},
abstract = {Morphological processing of Indian languages is one of the most escalating fields in the era of Natural Language Processing (NLP) since the last decade. The evaluation of Asian languages is a highly relevant field in the times of text mining and information retrieval. The morphological evaluation of a text can be employed for extraction and classification of knowledge. This paper amalgamates morphological evaluation and sentiment prediction of Punjabi language text. The textual data for Punjabi language is concerned with farmer suicide cases reported for Punjab state of India. The pre-processing phase of this study involves morphological evaluation and normalization of Punjabi words to their respective canonical forms. The next phase carries out training and testing of deep neural network model on refined Punjabi tokens obtained from the earlier phase. The proposed model classifies Punjabi tokens into four negatively oriented classes tailored for farmer suicide cases. The average accuracies of sentiment prediction obtained after 10-fold cross validation are 93.85%, 88.53%, 83.3%, and 95.45% for the four respective classes. The proposed framework yields satisfactory results on 275 Punjabi text documents with the overall accuracy of 90.29% for sentiment classification.}
}
@article{NAILI2019794,
title = {Comparative Study of Arabic Stemming Algorithms for Topic Identification},
journal = {Procedia Computer Science},
volume = {159},
pages = {794-802},
year = {2019},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 23rd International Conference KES2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.238},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919314255},
author = {Marwa Naili and Anja Habacha Chaibi and Henda Hajjami {Ben Ghezala}},
keywords = {Arabic Stemming algorithms, LDA, Topic identification},
abstract = {Stemming process is one of the important pre-processing steps in different natural language process tasks such as text mining and information retrieval. Yet, stemming process can be considered as a difficult step to realize according to the used language. In fact, due to the complex morphology of Arabic language, stemming results can be influenced. Thus, several algorithms have been proposed in order to overcome stemming problems. In this paper, we investigate different stemming algorithms by presenting a comparative study in the field of Arabic topic identification.}
}
@article{MISHRA2016345,
title = {A survey on question answering systems with classification},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {28},
number = {3},
pages = {345-361},
year = {2016},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2014.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S1319157815000890},
author = {Amit Mishra and Sanjay Kumar Jain},
keywords = {Question answering system, Information retrieval, Natural language processing, Natural language understanding, Search engine},
abstract = {Question answering systems (QASs) generate answers of questions asked in natural languages. Early QASs were developed for restricted domains and have limited capabilities. Current QASs focus on types of questions generally asked by users, characteristics of data sources consulted, and forms of correct answers generated. Research in the area of QASs began in 1960s and since then, a large number of QASs have been developed. To identify the future scope of research in this area, the need of a comprehensive survey on QASs arises naturally. This paper surveys QASs and classifies them based on different criteria. We identify the current status of the research in the each category of QASs, and suggest future scope of the research.}
}
@article{NAIR20241870,
title = {A Knowledge-Based Deep Learning Approach for Automatic Fake News Detection using BERT on Twitter},
journal = {Procedia Computer Science},
volume = {235},
pages = {1870-1882},
year = {2024},
note = {International Conference on Machine Learning and Data Engineering (ICMLDE 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.04.178},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924008548},
author = {Vinita Nair and Dr. Jyoti Pareek and Sanskruti Bhatt},
keywords = {Fake news, Knowledge-based, Twitter, SPO triple, sentiment polarity, topic modeling, Deep learning algorithms, BERT Transformer},
abstract = {Fake news generation and propagation is a major challenge of the digital age, resulting in various social impacts namely bandwagon, validity, echo chamber effects, deceiving the public with spams, misinformation, malicious content and many more. The widespread proliferation of fake news not only fosters misinformation but also undermines the credibility of news sources. The veracity of the information is a major concern at all the stages of generation, publication, and propagation. To comprehend the critical need for addressing this pervasive problem, this research paper presents a framework for automatic detection of fake news using a knowledge-based approach. An automatic fact checking mechanism is applied using concepts of Information Retrieval (IR), Natural Language Processing (NLP) and Graph theory. The knowledge base is generated using Twitter dataset, which basically contains four attributes: Subject-Predicate-Object (SPO) triplet, SPO sentiment polarity, SPO occurrence, and topic modeling. These attributes serve as pivotal indicators for the development of a knowledge base, subsequently employed to detect prevalent patterns and traits linked to deceptive or false information. We have employed Named Entity Recognition (NER) model to extract SPO triples and Latent Dirichlet Allocation (LDA) for topic modeling, thereby contributing to knowledge base generation. To evaluate the efficacy and efficiency of our proposed model, we utilize deep learning algorithms like RNN, GRU, LSTM, GPT-3 and BERT Transformer providing an acceptable level of accuracy. This research paper delivers valuable insights into addressing the proliferation of fake news on Twitter, employing data-driven approaches and advanced deep learning algorithms.}
}
@article{MOHAMED2022934,
title = {QSST: A Quranic Semantic Search Tool based on word embedding},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {3},
pages = {934-945},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2020.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S1319157819311012},
author = {Ensaf Hussein Mohamed and Eyad Mohamed Shokry},
keywords = {Information Retrieval, Word Embedding, Concept-based Search, Ontology, Semantic Search, Arabic Natural Language Processing, Holy Quran},
abstract = {Retrieving information from the Quran is an important field for Quran scholars and Arabic researchers. There are two types of Quran searching techniques: semantic or concept-based and keyword-based. Concept-based search is a challenging task, especially in a complex corpus such as Quran. This paper presents a concept-based searching tool (QSST) for the Holy Quran. It consists of four phases. In the first phase, the Quran dataset is built by manually annotating Quran verses based on the ontology of Mushaf Al-Tajweed. The second phase is word Embedding, this phase generates features’ vectors for words by training a Continuous Bag of Words (CBOW) architecture on large Quranic and Classic Arabic corpus. The third phase includes calculating the features’ vectors of both input query and Quranic topics. Finally, retrieving the most relevant verses by computing the cosine similarity between both topic and query vectors. The performance of the proposed QSST is measured by comparing results against Mushaf Al-Tajweed. Then, precision, recall, and F-score are computed and their percentages were 76.91%, 72.23% 69.28% respectively. In addition, the results are evaluated by three Islamic experts and the average precision was 91.95%. Finally, QSST results are compared with the recent existing tools; QSST outperformed them.}
}
@article{QIU2023105294,
title = {A question answering system based on mineral exploration ontology generation: A deep learning methodology},
journal = {Ore Geology Reviews},
volume = {153},
pages = {105294},
year = {2023},
issn = {0169-1368},
doi = {https://doi.org/10.1016/j.oregeorev.2023.105294},
url = {https://www.sciencedirect.com/science/article/pii/S0169136823000094},
author = {Qinjun Qiu and Miao Tian and Kai Ma and Yong Jian Tan and Liufeng Tao and Zhong Xie},
keywords = {Geological ontology, Question answering, Natural language processing, BERT model, Corpus construction},
abstract = {Mineral exploration reports and documents are a rich data source that contains a large amount of geological environments in which mineral deposits form. Among them, it is difficult to extract the required answers from the large amount of geological data. Despite the availability of search engines and digital databases that can be used to store geological data, users are unable to retrieve the information needed for a specific field in a timely manner. As a result, users usually have to contend with the burden of browsing and filtering information, which can be a time-consuming process. To address this issue, we propose a robust end-to-end approach that can improve the efficiency and effectiveness of retrieving queries related to mineral exploration terms. First, we present an automated workflow for constructing automatic question-and-answer datasets based on the names and definitions in the mineral exploration ontology. The Bidirectional Encoder Representation from Transformers (BERT) model is trained to test the answers generated from the user input question. Finally, a prototype chatbot system based on the WeChat platform and constructed experiments for evaluation is presented. Our proposed method has powerful feature representation and learning capabilities and thus has the potential to be adopted by other specialized fields (especially where a large number of mineral exploration ontologies already exist).}
}
@article{MASUDA2011281,
title = {Semantic Search based on the Online Integration of NLP Techniques},
journal = {Procedia - Social and Behavioral Sciences},
volume = {27},
pages = {281-290},
year = {2011},
note = {Computational Linguistics and Related Fields},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2011.10.609},
url = {https://www.sciencedirect.com/science/article/pii/S1877042811024360},
author = {Katsuya Masuda and Takuya Matsuzaki and Jun’ichi Tsujii},
keywords = {Information Retrieval, Semantic Search, Tag Annotations},
abstract = {This paper introduces a framework for semantic information retrieval based on the integration of various natural language processing (NLP) techniques, each of which annotates a base text with different kinds of information extracted from the text. Instead of running the NLP modules on the fly for individual search requests, the NLP modules are applied to the text in advance and the results are indexed in a way that enables flexible and efficient integration of them. The query language is based on a variant of the region algebra, in which we can specify a sub- structure in the annotated text that may involve different kinds of annotations. Given a query, the retrieval engine searches for the sub-structure by aggregating the different kinds of annotations through a search algorithm for the extended region algebra. We demonstrate the effectiveness and flexibility of the proposed framework through experiments with TREC Genomics Track data.}
}
@article{MABOTUWANA2013857,
title = {An ontology-based similarity measure for biomedical data – Application to radiology reports},
journal = {Journal of Biomedical Informatics},
volume = {46},
number = {5},
pages = {857-868},
year = {2013},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2013.06.013},
url = {https://www.sciencedirect.com/science/article/pii/S1532046413000889},
author = {Thusitha Mabotuwana and Michael C. Lee and Eric V. Cohen-Solal},
keywords = {Natural Language Processing, Radiology information systems, Semantics, Systematized Nomenclature of Medicine, Document similarity comparison, Radiology informatics, Semantic similarity},
abstract = {Background
Determining similarity between two individual concepts or two sets of concepts extracted from a free text document is important for various aspects of biomedicine, for instance, to find prior clinical reports for a patient that are relevant to the current clinical context. Using simple concept matching techniques, such as lexicon based comparisons, is typically not sufficient to determine an accurate measure of similarity.
Methods
In this study, we tested an enhancement to the standard document vector cosine similarity model in which ontological parent–child (is-a) relationships are exploited. For a given concept, we define a semantic vector consisting of all parent concepts and their corresponding weights as determined by the shortest distance between the concept and parent after accounting for all possible paths. Similarity between the two concepts is then determined by taking the cosine angle between the two corresponding vectors. To test the improvement over the non-semantic document vector cosine similarity model, we measured the similarity between groups of reports arising from similar clinical contexts, including anatomy and imaging procedure. We further applied the similarity metrics within a k-nearest-neighbor (k-NN) algorithm to classify reports based on their anatomical and procedure based groups. 2150 production CT radiology reports (952 abdomen reports and 1128 neuro reports) were used in testing with SNOMED CT, restricted to Body structure, Clinical finding and Procedure branches, as the reference ontology.
Results
The semantic algorithm preferentially increased the intra-class similarity over the inter-class similarity, with a 0.07 and 0.08 mean increase in the neuro–neuro and abdomen–abdomen pairs versus a 0.04 mean increase in the neuro–abdomen pairs. Using leave-one-out cross-validation in which each document was iteratively used as a test sample while excluding it from the training data, the k-NN based classification accuracy was shown in all cases to be consistently higher with the semantics based measure compared with the non-semantic case. Moreover, the accuracy remained steady even as k value was increased – for the two anatomy related classes accuracy for k=41 was 93.1% with semantics compared to 86.7% without semantics. Similarly, for the eight imaging procedures related classes, accuracy (for k=41) with semantics was 63.8% compared to 60.2% without semantics. At the same k, accuracy improved significantly to 82.8% and 77.4% respectively when procedures were logically grouped together into four classes (such as ignoring contrast information in the imaging procedure description). Similar results were seen at other k-values.
Conclusions
The addition of semantic context into the document vector space model improves the ability of the cosine similarity to differentiate between radiology reports of different anatomical and image procedure-based classes. This effect can be leveraged for document classification tasks, which suggests its potential applicability for biomedical information retrieval.}
}
@article{SAWANT2024100090,
title = {NLP-based smart decision making for business and academics},
journal = {Natural Language Processing Journal},
volume = {8},
pages = {100090},
year = {2024},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2024.100090},
url = {https://www.sciencedirect.com/science/article/pii/S2949719124000384},
author = {Pradnya Sawant and Kavita Sonawane},
keywords = {Correlation analysis, Enhanced Longest Common Subsequence (ELCS), Feature extraction, Semantic analysis},
abstract = {Natural Language Processing (NLP) systems enable machines to understand, interpret, and generate human-like language, bridging the gap between human communication and computer understanding. Natural Language Interface to Databases (NLIDB) and Natural Language Interface to Visualization (NLIV) systems are designed to enable non-technical users to retrieve and visualize data through natural language queries. However, these systems often face challenges in handling complex correlation and analytical questions, limiting their effectiveness for comprehensive data analysis. Additionally, current Business Intelligence (BI) tools also struggle with understanding the context and semantics of complex questions, further hindering their usability for strategic decision-making. Also, when building these models for generating the queries from natural language, the system handles only the semantic parsing issues as each column header is being changed manually to their normal names by all existing models which is time-consuming, tedious, and subjective. Recent studies reflect the need for attention to context, semantics, and especially ambiguities in dealing with natural language questions. To address this problem, the proposed architecture focuses on understanding the context, correlation-based semantic analysis, and removal of ambiguities using a novel approach. An Enhanced Longest Common Subsequence (ELCS) is suggested where existing LCS is modified with a memorization component for mapping the natural language question tokens with ambiguous table column headers. This can speed up the overall process as human intervention is not required to manually change the column headers. The same is evidenced by carrying out thorough experimentation and comparative study in terms of precision, recall, and F1 score. By synthesizing the latest advancements and addressing challenges, this paper has proved how NLP can significantly enhance the accuracy and efficiency of information retrieval and visualization, broadening the inclusivity and usability of NLIDB, NLIV, and BI systems.}
}
@article{DJELLALI2013977,
title = {A New Digital Conceptual Model Oriented Corporate Memory Constructing: Taking Data Mining Models as a Case},
journal = {Procedia Computer Science},
volume = {19},
pages = {977-983},
year = {2013},
note = {The 4th International Conference on Ambient Systems, Networks and Technologies (ANT 2013), the 3rd International Conference on Sustainable Energy Information Technology (SEIT-2013)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2013.06.136},
url = {https://www.sciencedirect.com/science/article/pii/S1877050913007448},
author = {Choukri Djellali},
keywords = {preprocessing, integration, corporate memory, data mining, ontology, information retrieval, machine learning},
abstract = {The integration of knowledge can be considered as a guideline for managing problems that occur in the task of knowledge management, and more particularly, in the collaborative decision-making. Integration is necessary because it allows communication between different sources. Most of the proposed approaches provide limited support for all activities of the engineering process, in particular, the phase of integration. We propose a new approach to treat the integration of the corporate knowledge. This model exploits indexation techniques and natural language processing to increase productivity of knowledge engineering task during the integration of conceptual model. Our integration system offers several advantages, these include speed search due to the structure and integrity of indexing.}
}
@article{FORSATI20143193,
title = {Hybrid PoS-tagging: A cooperation of evolutionary and statistical approaches},
journal = {Applied Mathematical Modelling},
volume = {38},
number = {13},
pages = {3193-3211},
year = {2014},
issn = {0307-904X},
doi = {https://doi.org/10.1016/j.apm.2013.11.047},
url = {https://www.sciencedirect.com/science/article/pii/S0307904X1300783X},
author = {Rana Forsati and Mehrnoush Shamsfard},
keywords = {Part-of-Speech tagging, Bee colony optimization, Natural language processing, Evolutionary algorithms, Optimization},
abstract = {The assigning of syntactic categories to words in a sentence, which is referred to as part-of-speech (PoS) tagging problem, plays an essential role in many natural language processing and information retrieval applications. Despite the vast scope of methods, PoS-tagging brings an array of challenges that require novel solutions. To address these challenges in a principled way, one solution would be to formulate the tagging problem as an optimization problem with well-specified objectives and then apply the evolutionary methods to solve the optimization problem. This paper discusses the relative advantages of different evolutionary approaches to handle Part-of-Speech tagging problem and aims at presenting novel language-independent evolutionary algorithms to solve the PoS tagging problem. We show that by exploiting statistical measures to evaluate the solutions in tagging process, the proposed algorithms are able to generate more accurate solution in a reasonable amount of time. The experiments we have conducted on few well known corpus reveal that the proposed algorithms achieve better average accuracy in comparison to other evolutionary-based and classical Part-of-Speech tagging methods.}
}
@article{ARIAS20185,
title = {A Framework for Managing Requirements of Software Product Lines},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {339},
pages = {5-20},
year = {2018},
note = {The XLII Latin American Computing Conference},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2018.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S157106611830046X},
author = {Maximiliano Arias and Agustina Buccella and Alejandra Cechich},
keywords = {Software Product Lines, Query Expansion, Requirements Engineering, Software Reuse},
abstract = {An emerging problem in the Software Product Line Engineering (SPLE) is the need for integral management of planned reuse. In SPLE there are two instances where managing requirements gains relevance. The first one arises during the construction of SPLs based on legacy software or previously developed SPLs. The second one appears when instantiating products from the SPLplatform, where instantiating variability meets the custom requirements of each product. The objective of this paper is to define a framework that allows management of requirements using Natural Language Processing and Information Retrieval techniques, to structure, clean, index and find reusable functionalities according to those requirements. This framework is built in a way that allows the combination of such techniques to evaluate the best combinations for finding the correct functionalities in each SPL domain.}
}
@article{BAKARI2016275,
title = {AQA-WebCorp: Web-based Factual Questions for Arabic},
journal = {Procedia Computer Science},
volume = {96},
pages = {275-284},
year = {2016},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 20th International Conference KES-2016},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.08.140},
url = {https://www.sciencedirect.com/science/article/pii/S187705091631941X},
author = {Wided Bakari and Patrice Bellot and Mahmoud Neji},
keywords = {Arabic, Corpus, Question analysis, Passage, Google, Corpus construction.},
abstract = {Working with corpus construction becomes an interesting alternative to different applications of natural language processing, such as, question-answering, machine translation, information retrieval, etc. Similarly, with the heterogeneous data and the user demands for the accurate information, many studies have accentuated the need of the Web to highlight the corpus construction. As well as, Arabic doesn’t have an equivalent number of linguistic corpuses as compared to other languages like English. In this paper, we focus on building our corpus of Arab questions-texts. We present a method for recovering text passages. This method is based on a real automatic interrogation of Google, in order to generate passages of texts and answer the factual questions. The first part of this paper describes the formal details about this method; the second part presents some experiments and results that validate our method.}
}
@article{FARO201461,
title = {Fast and flexible packed string matching},
journal = {Journal of Discrete Algorithms},
volume = {28},
pages = {61-72},
year = {2014},
note = {StringMasters 2012 & 2013 Special Issue (Volume 1)},
issn = {1570-8667},
doi = {https://doi.org/10.1016/j.jda.2014.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S1570866714000471},
author = {Simone Faro and M. Oğuzhan Külekci},
keywords = {Exact string matching, Text algorithms, Experimental algorithms, Online searching, Information retrieval},
abstract = {Searching for all occurrences of a pattern in a text is a fundamental problem in computer science with applications in many other fields, like natural language processing, information retrieval and computational biology. In the last two decades a general trend has appeared trying to exploit the power of the word RAM model to speed-up the performances of classical string matching algorithms. In this model an algorithm operates on words of length w, grouping blocks of characters, and arithmetic and logic operations on the words take one unit of time. In this paper we use specialized word-size packed string matching instructions, based on the Intel streaming SIMD extensions (SSE) technology, to design a very fast string matching algorithm. We evaluate our solution in terms of efficiency, stability and flexibility, where we propose to use the deviation in running time of an algorithm on distinct equal length patterns as a measure of stability. From our experimental results it turns out that, despite their quadratic worst case time complexity, the new presented algorithm becomes the clear winner on the average in many cases, when compared against the most recent and effective algorithms known in literature.}
}
@article{BUI2015436,
title = {Automatically finding relevant citations for clinical guideline development},
journal = {Journal of Biomedical Informatics},
volume = {57},
pages = {436-445},
year = {2015},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2015.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S1532046415001951},
author = {Duy Duc An Bui and Siddhartha Jonnalagadda and Guilherme {Del Fiol}},
keywords = {Information retrieval, PubMed, Practice guideline, Medical subject headings, Natural language processing},
abstract = {Objective
Literature database search is a crucial step in the development of clinical practice guidelines and systematic reviews. In the age of information technology, the process of literature search is still conducted manually, therefore it is costly, slow and subject to human errors. In this research, we sought to improve the traditional search approach using innovative query expansion and citation ranking approaches.
Methods
We developed a citation retrieval system composed of query expansion and citation ranking methods. The methods are unsupervised and easily integrated over the PubMed search engine. To validate the system, we developed a gold standard consisting of citations that were systematically searched and screened to support the development of cardiovascular clinical practice guidelines. The expansion and ranking methods were evaluated separately and compared with baseline approaches.
Results
Compared with the baseline PubMed expansion, the query expansion algorithm improved recall (80.2% vs. 51.5%) with small loss on precision (0.4% vs. 0.6%). The algorithm could find all citations used to support a larger number of guideline recommendations than the baseline approach (64.5% vs. 37.2%, p<0.001). In addition, the citation ranking approach performed better than PubMed’s “most recent” ranking (average precision +6.5%, recall@k +21.1%, p<0.001), PubMed’s rank by “relevance” (average precision +6.1%, recall@k +14.8%, p<0.001), and the machine learning classifier that identifies scientifically sound studies from MEDLINE citations (average precision +4.9%, recall@k +4.2%, p<0.001).
Conclusions
Our unsupervised query expansion and ranking techniques are more flexible and effective than PubMed’s default search engine behavior and the machine learning classifier. Automated citation finding is promising to augment the traditional literature search.}
}
@article{SILBERZTEIN200033,
title = {INTEX: an FST toolbox},
journal = {Theoretical Computer Science},
volume = {231},
number = {1},
pages = {33-46},
year = {2000},
issn = {0304-3975},
doi = {https://doi.org/10.1016/S0304-3975(99)00015-8},
url = {https://www.sciencedirect.com/science/article/pii/S0304397599000158},
author = {Max Silberztein},
keywords = {Finite state transducers, Natural language processing, Corpus processing, Information retrieval},
abstract = {INTEX is an integrated Natural Language Processing toolbox based on finite state transducers (FSTs). It parses texts of several million words, and includes large-coverage dictionaries and grammars. Texts, Dictionaries and Grammars are represented internally by FSTs. The user may add his/her own dictionaries and grammars; these tools are applied to texts in order to locate lexical and syntactic patterns, remove ambiguities, and tag simple words as well as complex utterances. INTEX builds lemmatized concordances and indices of texts with respect to all types of finite state patterns; it is used as a lexical parser to produce the input of a syntactic parser, but can also be viewed as an information retrieval system.}
}
@article{KONONOVA2021102155,
title = {Opportunities and challenges of text mining in materials research},
journal = {iScience},
volume = {24},
number = {3},
pages = {102155},
year = {2021},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2021.102155},
url = {https://www.sciencedirect.com/science/article/pii/S2589004221001231},
author = {Olga Kononova and Tanjin He and Haoyan Huo and Amalie Trewartha and Elsa A. Olivetti and Gerbrand Ceder},
keywords = {Data Analysis, Computing Methodology, Computational Materials Science, Materials Design},
abstract = {Summary
Research publications are the major repository of scientific knowledge. However, their unstructured and highly heterogenous format creates a significant obstacle to large-scale analysis of the information contained within. Recent progress in natural language processing (NLP) has provided a variety of tools for high-quality information extraction from unstructured text. These tools are primarily trained on non-technical text and struggle to produce accurate results when applied to scientific text, involving specific technical terminology. During the last years, significant efforts in information retrieval have been made for biomedical and biochemical publications. For materials science, text mining (TM) methodology is still at the dawn of its development. In this review, we survey the recent progress in creating and applying TM and NLP approaches to materials science field. This review is directed at the broad class of researchers aiming to learn the fundamentals of TM as applied to the materials science publications.}
}
@article{YAZDANI2013176,
title = {Computing text semantic relatedness using the contents and links of a hypertext encyclopedia},
journal = {Artificial Intelligence},
volume = {194},
pages = {176-202},
year = {2013},
note = {Artificial Intelligence, Wikipedia and Semi-Structured Resources},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2012.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370212000744},
author = {Majid Yazdani and Andrei Popescu-Belis},
keywords = {Text semantic relatedness, Distance metric learning, Learning to rank, Random walk, Text classification, Text similarity, Document clustering, Information retrieval, Word similarity},
abstract = {We propose a method for computing semantic relatedness between words or texts by using knowledge from hypertext encyclopedias such as Wikipedia. A network of concepts is built by filtering the encyclopediaʼs articles, each concept corresponding to an article. Two types of weighted links between concepts are considered: one based on hyperlinks between the texts of the articles, and another one based on the lexical similarity between them. We propose and implement an efficient random walk algorithm that computes the distance between nodes, and then between sets of nodes, using the visiting probability from one (set of) node(s) to another. Moreover, to make the algorithm tractable, we propose and validate empirically two truncation methods, and then use an embedding space to learn an approximation of visiting probability. To evaluate the proposed distance, we apply our method to four important tasks in natural language processing: word similarity, document similarity, document clustering and classification, and ranking in information retrieval. The performance of the method is state-of-the-art or close to it for each task, thus demonstrating the generality of the knowledge resource. Moreover, using both hyperlinks and lexical similarity links improves the scores with respect to a method using only one of them, because hyperlinks bring additional real-world knowledge not captured by lexical similarity.}
}
@article{BOTSIS2016354,
title = {Decision support environment for medical product safety surveillance},
journal = {Journal of Biomedical Informatics},
volume = {64},
pages = {354-362},
year = {2016},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2016.07.023},
url = {https://www.sciencedirect.com/science/article/pii/S1532046416300776},
author = {Taxiarchis Botsis and Christopher Jankosky and Deepa Arya and Kory Kreimeyer and Matthew Foster and Abhishek Pandey and Wei Wang and Guangfan Zhang and Richard Forshee and Ravi Goud and David Menschik and Mark Walderhaug and Emily Jane Woo and John Scott},
keywords = {Natural language processing, Text mining, Network analysis, Post-marketing surveillance, Information retrieval},
abstract = {We have developed a Decision Support Environment (DSE) for medical experts at the US Food and Drug Administration (FDA). The DSE contains two integrated systems: The Event-based Text-mining of Health Electronic Records (ETHER) and the Pattern-based and Advanced Network Analyzer for Clinical Evaluation and Assessment (PANACEA). These systems assist medical experts in reviewing reports submitted to the Vaccine Adverse Event Reporting System (VAERS) and the FDA Adverse Event Reporting System (FAERS). In this manuscript, we describe the DSE architecture and key functionalities, and examine its potential contributions to the signal management process by focusing on four use cases: the identification of missing cases from a case series, the identification of duplicate case reports, retrieving cases for a case series analysis, and community detection for signal identification and characterization.}
}
@article{GUO2024100287,
title = {Chinese named entity recognition with multi-network fusion of multi-scale lexical information},
journal = {Journal of Electronic Science and Technology},
volume = {22},
number = {4},
pages = {100287},
year = {2024},
issn = {1674-862X},
doi = {https://doi.org/10.1016/j.jnlest.2024.100287},
url = {https://www.sciencedirect.com/science/article/pii/S1674862X24000557},
author = {Yan Guo and Hong-Chen Liu and Fu-Jiang Liu and Wei-Hua Lin and Quan-Sen Shao and Jun-Shun Su},
keywords = {Bi-directional long short-term memory (BiLSTM), Chinese named entity recognition (CNER), Iterated dilated convolutional neural network (IDCNN), Multi-network integration, Multi-scale lexical features},
abstract = {Named entity recognition (NER) is an important part in knowledge extraction and one of the main tasks in constructing knowledge graphs. In today's Chinese named entity recognition (CNER) task, the BERT-BiLSTM-CRF model is widely used and often yields notable results. However, recognizing each entity with high accuracy remains challenging. Many entities do not appear as single words but as part of complex phrases, making it difficult to achieve accurate recognition using word embedding information alone because the intricate lexical structure often impacts the performance. To address this issue, we propose an improved Bidirectional Encoder Representations from Transformers (BERT) character word conditional random field (CRF) (BCWC) model. It incorporates a pre-trained word embedding model using the skip-gram with negative sampling (SGNS) method, alongside traditional BERT embeddings. By comparing datasets with different word segmentation tools, we obtain enhanced word embedding features for segmented data. These features are then processed using the multi-scale convolution and iterated dilated convolutional neural networks (IDCNNs) with varying expansion rates to capture features at multiple scales and extract diverse contextual information. Additionally, a multi-attention mechanism is employed to fuse word and character embeddings. Finally, CRFs are applied to learn sequence constraints and optimize entity label annotations. A series of experiments are conducted on three public datasets, demonstrating that the proposed method outperforms the recent advanced baselines. BCWC is capable to address the challenge of recognizing complex entities by combining character-level and word-level embedding information, thereby improving the accuracy of CNER. Such a model is potential to the applications of more precise knowledge extraction such as knowledge graph construction and information retrieval, particularly in domain-specific natural language processing tasks that require high entity recognition precision.}
}
@article{KWAIK20182,
title = {A Lexical Distance Study of Arabic Dialects},
journal = {Procedia Computer Science},
volume = {142},
pages = {2-13},
year = {2018},
note = {Arabic Computational Linguistics},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.456},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918321562},
author = {Kathrein Abu Kwaik and Motaz Saad and Stergios Chatzikyriakidis and Simon Dobnik},
keywords = {Diglossia, Lexical Distance, Vector Space Model, Latent Semantic Indexing, Hellinger Distance},
abstract = {Diglossia is a very common phenomenon in Arabic-speaking communities, where the spoken language is different from both Classical Arabic (CA) and Modern Standard Arabic (MSA). The spoken language is characterised as a number of dialects used in everyday communication as well as informal writing. In this paper, we highlight the lexical relation between the MSA and Dialectal Arabic (DA) in more than one Arabic region. We conduct a computational cross dialectal lexical distance study to measure the similarities and differences between dialects and the MSA. We exploit several methods from Natural Language Processing (NLP) and Information Retrieval (IR) like Vector Space Model (VSM), Latent Semantic Indexing (LSI) and Hellinger Distance (HD), and apply them on different Arabic dialectal corpora. We measure the overlap among all the dialects and compute the frequencies of the most frequent words in every dialect. The results are informative and indicate that Levantine dialects are very similar to each other and furthermore, that Palestinian appears to be the closest to MSA.}
}
@article{BOUZIANE2015366,
title = {Question Answering Systems: Survey and Trends},
journal = {Procedia Computer Science},
volume = {73},
pages = {366-375},
year = {2015},
note = {International Conference on Advanced Wireless Information and Communication Technologies (AWICT 2015)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915034663},
author = {Abdelghani Bouziane and Djelloul Bouchiha and Noureddine Doumi and Mimoun Malki},
keywords = {Question Answering System (QAS), Natural Language Processing (NLP), Information Retrieval, SPARQL, Semantic Web},
abstract = {The need to query information content available in various formats including structured and unstructured data (text in natural language, semi-structured Web documents, structured RDF data in the semantic Web, etc.) has become increasingly important. Thus, Question Answering Systems (QAS) are essential to satisfy this need. QAS aim at satisfying users who are looking to answer a specific question in natural language. In this paper we survey various QAS. We give also statistics and analysis. This can clear the way and help researchers to choose the appropriate solution to their issue. They can see the insufficiency, so that they can propose new systems for complex queries. They can also adapt or reuse QAS techniques for specific research issues.}
}
@article{GUO2014168,
title = {Enhancing a Rule-based Event Coder with Semantic Vectors},
journal = {Procedia Computer Science},
volume = {36},
pages = {168-174},
year = {2014},
note = {Complex Adaptive Systems Philadelphia, PA November 3-5, 2014},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.09.074},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914013234},
author = {Jinhong K. Guo and David {Van Brackle} and Martin O. Hofmann},
keywords = {NLP, semantic vectors, Random Indexing, event coding, rule-based system, machine learning},
abstract = {Rule based systems have achieved success in applications such as information retrieval and Natural Language Processing. However, due to the rigidity of pattern matching, these systems typically require a large number of rules to adequately cover the variations of expression in unstructured text. Consequently, knowledge engineering for a new domain and knowledge maintenance for a fielded system are labor intensive and expensive. In this paper, we present our research on enhancing a rule-based event coding system by relaxing the rigidity of pattern matching with a technique that formulates and matches patterns of the semantics of words instead of literal words. Our technique pairs literal words with semantic vectors that accumulate word meaning from the context of use of the word found in dictionaries, ontologies, and domain corpora. Our method improves the speed, accuracy, and coverage of the event coding algorithm without additional knowledge engineering effort. Operating on semantics instead of syntax, the improved system eases the workload of human analysts who screen input text for critical events. Our algorithms are based on high-dimensional distributed representations, and their effectiveness and versatility derive from the unintuitive properties of such representations---from the mathematical properties of high-dimensional spaces. Our current implementation encodes words, phrases, and rule patterns as semantic vectors using WordNet, We have started experimental evaluation using a large newswire dataset.}
}
@article{BHATTACHARYA2013805,
title = {Analysis of eligibility criteria representation in industry-standard clinical trial protocols},
journal = {Journal of Biomedical Informatics},
volume = {46},
number = {5},
pages = {805-813},
year = {2013},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2013.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S1532046413000762},
author = {Sanmitra Bhattacharya and Michael N. Cantor},
keywords = {Clinical trials, Information retrieval, Natural language processing, Controlled vocabulary, Eligibility determination},
abstract = {Previous research on standardization of eligibility criteria and its feasibility has traditionally been conducted on clinical trial protocols from ClinicalTrials.gov (CT). The portability and use of such standardization for full-text industry-standard protocols has not been studied in-depth. Towards this end, in this study we first compare the representation characteristics and textual complexity of a set of Pfizer’s internal full-text protocols to their corresponding entries in CT. Next, we identify clusters of similar criteria sentences from both full-text and CT protocols and outline methods for standardized representation of eligibility criteria. We also study the distribution of eligibility criteria in full-text and CT protocols with respect to pre-defined semantic classes used for eligibility criteria classification. We find that in comparison to full-text protocols, CT protocols are not only more condensed but also convey less information. We also find no correlation between the variations in word-counts of the ClinicalTrials.gov and full-text protocols. While we identify 65 and 103 clusters of inclusion and exclusion criteria from full text protocols, our methods found only 36 and 63 corresponding clusters from CT protocols. For both the full-text and CT protocols we are able to identify ‘templates’ for standardized representations with full-text standardization being more challenging of the two. In our exploration of the semantic class distributions we find that the majority of the inclusion criteria from both full-text and CT protocols belong to the semantic class “Diagnostic and Lab Results” while “Disease, Sign or Symptom” forms the majority for exclusion criteria. Overall, we show that developing a template set of eligibility criteria for clinical trials, specifically in their full-text form, is feasible and could lead to more efficient clinical trial protocol design.}
}
@article{RINDFLESCH2003462,
title = {The interaction of domain knowledge and linguistic structure in natural language processing: interpreting hypernymic propositions in biomedical text},
journal = {Journal of Biomedical Informatics},
volume = {36},
number = {6},
pages = {462-477},
year = {2003},
note = {Unified Medical Language System},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2003.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S1532046403001175},
author = {Thomas C Rindflesch and Marcelo Fiszman},
keywords = {Natural language processing, Semantic processing, Knowledge representation, Information extraction},
abstract = {Interpretation of semantic propositions in free-text documents such as MEDLINE citations would provide valuable support for biomedical applications, and several approaches to semantic interpretation are being pursued in the biomedical informatics community. In this paper, we describe a methodology for interpreting linguistic structures that encode hypernymic propositions, in which a more specific concept is in a taxonomic relationship with a more general concept. In order to effectively process these constructions, we exploit underspecified syntactic analysis and structured domain knowledge from the Unified Medical Language System (UMLS). After introducing the syntactic processing on which our system depends, we focus on the UMLS knowledge that supports interpretation of hypernymic propositions. We first use semantic groups from the Semantic Network to ensure that the two concepts involved are compatible; hierarchical information in the Metathesaurus then determines which concept is more general and which more specific. A preliminary evaluation of a sample based on the semantic group Chemicals and Drugs provides 83% precision. An error analysis was conducted and potential solutions to the problems encountered are presented. The research discussed here serves as a paradigm for investigating the interaction between domain knowledge and linguistic structure in natural language processing, and could also make a contribution to research on automatic processing of discourse structure. Additional implications of the system we present include its integration in advanced semantic interpretation processors for biomedical text and its use for information extraction in specific domains. The approach has the potential to support a range of applications, including information retrieval and ontology engineering.}
}
@article{ABUATA2015104,
title = {A rule-based stemmer for Arabic Gulf dialect},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {27},
number = {2},
pages = {104-112},
year = {2015},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2014.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S1319157815000191},
author = {Belal Abuata and Asma Al-Omari},
keywords = {Arabic dialect stemmer, Gulf dialect, Rule base stemming, Arabic NLP},
abstract = {Arabic dialects arewidely used from many years ago instead of Modern Standard Arabic language in many fields. The presence of dialects in any language is a big challenge. Dialects add a new set of variational dimensions in some fields like natural language processing, information retrieval and even in Arabic chatting between different Arab nationals. Spoken dialects have no standard morphological, phonological and lexical like Modern Standard Arabic. Hence, the objective of this paper is to describe a procedure or algorithm by which a stem for the Arabian Gulf dialect can be defined. The algorithm is rule based. Special rules are created to remove the suffixes and prefixes of the dialect words. Also, the algorithm applies rules related to the word size and the relation between adjacent letters. The algorithm was tested for a number of words and given a good correct stem ratio. The algorithm is also compared with two Modern Standard Arabic algorithms. The results showed that Modern Standard Arabic stemmers performed poorly with Arabic Gulf dialect and our algorithm performed poorly when applied for Modern Standard Arabic words.}
}
@article{ALKHATIB20211255,
title = {A New Enhanced Arabic Light Stemmer for IR in Medical Documents},
journal = {Computers, Materials and Continua},
volume = {68},
number = {1},
pages = {1255-1269},
year = {2021},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2021.016155},
url = {https://www.sciencedirect.com/science/article/pii/S1546221821009541},
author = {Ra’ed M. Al-Khatib and Taha Zerrouki and Mohammed M. Abu Shquier and Amar Balla and Asef Al-Khateeb},
keywords = {Machine learning, information retrieval systems, medical documents, stemming algorithms, arabic light stemmer, natural language processing},
abstract = {This paper introduces a new enhanced Arabic stemming algorithm for solving the information retrieval problem, especially in medical documents. Our proposed algorithm is a light stemming algorithm for extracting stems and roots from the input data. One of the main challenges facing the light stemming algorithm is cutting off the input word, to extract the initial segments. When initiating the light stemmer with strong initial segments, the final extracting stems and roots will be more accurate. Therefore, a new enhanced segmentation based on deploying the Direct Acyclic Graph (DAG) model is utilized. In addition to extracting the powerful initial segments, the main two procedures (i.e., stems and roots extraction), should be also reinforced with more efficient operators to improve the final outputs. To validate the proposed enhanced stemmer, four data sets are used. The achieved stems and roots resulted from our proposed light stemmer are compared with the results obtained from five other well-known Arabic light stemmers using the same data sets. This evaluation process proved that the proposed enhanced stemmer outperformed other comparative stemmers.}
}
@article{KOTSAKIS2023e16084,
title = {A web framework for information aggregation and management of multilingual hate speech},
journal = {Heliyon},
volume = {9},
number = {5},
pages = {e16084},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e16084},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023032917},
author = {Rigas Kotsakis and Lazaros Vrysis and Nikolaos Vryzas and Theodora Saridou and Maria Matsiola and Andreas Veglis and Charalampos Dimoulas},
keywords = {Hate speech, Sentiment analysis, Information retrieval, Content management and moderation, Social media, Multilingual, Natural language processing, Machine learning},
abstract = {Social media platforms have led to the creation of a vast amount of information produced by users and published publicly, facilitating participation in the public sphere, but also giving the opportunity for certain users to publish hateful content. This content mainly involves offensive/discriminative speech towards social groups or individuals (based on racial, religious, gender or other characteristics) and could possibly lead into subsequent hate actions/crimes due to persistent escalation. Content management and moderation in big data volumes can no longer be supported manually. In the current research, a web framework is presented and evaluated for the collection, analysis, and aggregation of multilingual textual content from various online sources. The framework is designed to address the needs of human users, journalists, academics, and the public to collect and analyze content from social media and the web in Spanish, Italian, Greek, and English, without prior training or a background in Computer Science. The backend functionality provides content collection and monitoring, semantic analysis including hate speech detection and sentiment analysis using machine learning models and rule-based algorithms, storing, querying, and retrieving such content along with the relevant metadata in a database. This functionality is assessed through a graphic user interface that is accessed using a web browser. An evaluation procedure was held through online questionnaires, including journalists and students, proving the feasibility of the use of the proposed framework by non-experts for the defined use-case scenarios.}
}
@article{ANUAR2013450,
title = {A Conceptual Model of Trademark Retrieval based on Conceptual Similarity},
journal = {Procedia Computer Science},
volume = {22},
pages = {450-459},
year = {2013},
note = {17th International Conference in Knowledge Based and Intelligent Information and Engineering Systems - KES2013},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2013.09.123},
url = {https://www.sciencedirect.com/science/article/pii/S1877050913009162},
author = {Fatahiyah Mohd Anuar and Rossitza Setchi and Yu-Kun Lai},
keywords = {trademark infringement, trademark retrieval, information retrieval, semantic technology, semantic similarity},
abstract = {The rapid expansion of e-commerce at the beginning of 21st century has had a significant impact on intellectual property management. A particular area of concern is the misuse of trademarks and trademark protection. Trademarks are proprietary words and images with high reputational value; they are important assets, often used as a marketing tool, which require infringement protection. One of the issues considered during infringement litigation is the visual, conceptual and phonetic similarity of different trademarks. In particular, the conceptual similarity of trademarks is an area never previously studied in information retrieval. This paper focuses on this important aspect by proposing a conceptual model of the comparison process, aimed at retrieving conceptually similar trademarks. The proposed model employs natural language processing and semantic technology to compute the conceptual similarity between trademarks.}
}
@article{CLARK20112,
title = {Text Normalization in Social Media: Progress, Problems and Applications for a Pre-Processing System of Casual English},
journal = {Procedia - Social and Behavioral Sciences},
volume = {27},
pages = {2-11},
year = {2011},
note = {Computational Linguistics and Related Fields},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2011.10.577},
url = {https://www.sciencedirect.com/science/article/pii/S1877042811024049},
author = {Eleanor Clark and Kenji Araki},
keywords = {Natural Language Processing, Machine Translation, Social Media, Twitter, Text Normalization},
abstract = {The rapid expansion in user-generated content on the Web of the 2000s, characterized by social media, has led to Web content featuring somewhat less standardized language than the Web of the 1990s. User creativity and individuality of language creates problems on two levels. The first is that social media text is often unsuitable as data for Natural Language Processing tasks such as Machine Translation, Information Retrieval and Opinion Mining, due to the irregularity of the language featured. The second is that non-native speakers of English, older Internet users and non-members of the “in-group” often find such texts difficult to understand. This paper discusses problems involved in automatically normalizing social media English, various applications for its use, and our progress thus far in a rule-based approach to the issue. Particularly, we evaluate the performance of two leading open source spell checkers on data taken from the microblogging service Twitter, and measure the extent to which their accuracy is improved by pre-processing with our system. We also present our database rules and classification system, results of evaluation experiments, and plans for expansion of the project.}
}
@article{RAJPUT2014662,
title = {Ontology based Semantic Annotation of Urdu Language Web Documents},
journal = {Procedia Computer Science},
volume = {35},
pages = {662-670},
year = {2014},
note = {Knowledge-Based and Intelligent Information & Engineering Systems 18th Annual Conference, KES-2014 Gdynia, Poland, September 2014 Proceedings},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.08.148},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914011132},
author = {Quratulain Rajput},
keywords = {semantic annotation, Urdu ads, ontology, information extraction;},
abstract = {Proliferation of multilingual text on the Internet has increased the demand for efficient information retrieval independent of language. Among variety of languages, the Urdu language is one of the most commonly spoken and written language in South Asia. However, due to unstructured format the access of relevant information is still a big challenge. The semantic web technologies enable the advancement in information retrieval systems by assigning semantics to information. This paper presents a semantic annotation framework that can annotate documents written in Urdu language. The framework uses domain specific ontology and context keywords instead of NLP (Natural Language processing) techniques. The experiment has been conducted to evaluate the presented annotation framework. The set of corpora used in the experiment belong to the online classified ads posted on the online Urdu newspapers. The purpose of this research is to find the challenges involved in semantic annotation of Urdu language web documents.}
}
@article{SARROUTI201796,
title = {A passage retrieval method based on probabilistic information retrieval model and UMLS concepts in biomedical question answering},
journal = {Journal of Biomedical Informatics},
volume = {68},
pages = {96-103},
year = {2017},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2017.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S1532046417300503},
author = {Mourad Sarrouti and Said {Ouatik El Alaoui}},
keywords = {Biomedical question answering system, Biomedical passage retieval, Probabilistic information retrieval model, Unified medical language system, Natural language processing, Biomedical informatics},
abstract = {Background and Objective
Passage retrieval, the identification of top-ranked passages that may contain the answer for a given biomedical question, is a crucial component for any biomedical question answering (QA) system. Passage retrieval in open-domain QA is a longstanding challenge widely studied over the last decades. However, it still requires further efforts in biomedical QA. In this paper, we present a new biomedical passage retrieval method based on Stanford CoreNLP sentence/passage length, probabilistic information retrieval (IR) model and UMLS concepts.
Methods
In the proposed method, we first use our document retrieval system based on PubMed search engine and UMLS similarity to retrieve relevant documents to a given biomedical question. We then take the abstracts from the retrieved documents and use Stanford CoreNLP for sentence splitter to make a set of sentences, i.e., candidate passages. Using stemmed words and UMLS concepts as features for the BM25 model, we finally compute the similarity scores between the biomedical question and each of the candidate passages and keep the N top-ranked ones.
Results
Experimental evaluations performed on large standard datasets, provided by the BioASQ challenge, show that the proposed method achieves good performances compared with the current state-of-the-art methods. The proposed method significantly outperforms the current state-of-the-art methods by an average of 6.84% in terms of mean average precision (MAP).
Conclusion
We have proposed an efficient passage retrieval method which can be used to retrieve relevant passages in biomedical QA systems with high mean average precision.}
}
@article{PRADEEPA20123215,
title = {Analyzing Distillation Process of Hidden Terms in Web Documents for IR},
journal = {Procedia Engineering},
volume = {38},
pages = {3215-3221},
year = {2012},
note = {INTERNATIONAL CONFERENCE ON MODELLING OPTIMIZATION AND COMPUTING},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2012.06.372},
url = {https://www.sciencedirect.com/science/article/pii/S1877705812022850},
author = {M. Pradeepa and C. Deisy},
keywords = {Web mining, hidden terms, sparse data, Latent Dirichlet Allocation (LDA), Gibbs sampler and clustering},
abstract = {The previous work in web based applications such as mining web content, pattern recognition and similarity measures between the web documents. This paper is about, analyzing web documents in an enhanced way and delve the distillation web document will be the next pace in hypertext mining. The sparse document is a very little data on the web, which may face problems like different words with almost identical or similar meanings and sparseness. Natural language processing (NLP) and information retrieval (IR) are the main obstacles of the above problem. The mining of hidden terms discovers the search queries from large external datasets (universal datasets). It helps to handle unseen data in a better way. The goal of this web document mining consists of an efficient information finding, filtering information based on user query, and discovers more topic focused keywords based on the rich source of global information datasets. The proposed method we use the Distillation model, it is the integration of probabilistic generative model, Gibbs sampling algorithm and deployment method. This model can be applied for different natural languages and data domains for achieving the goal.}
}
@article{COHEN2009390,
title = {Empirical distributional semantics: Methods and biomedical applications},
journal = {Journal of Biomedical Informatics},
volume = {42},
number = {2},
pages = {390-405},
year = {2009},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2009.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S1532046409000227},
author = {Trevor Cohen and Dominic Widdows},
keywords = {Distributional semantics, Methodological review, Latent semantic analysis, Natural language processing, Semantic similarity, Random indexing, Context vectors},
abstract = {Over the past 15 years, a range of methods have been developed that are able to learn human-like estimates of the semantic relatedness between terms from the way in which these terms are distributed in a corpus of unannotated natural language text. These methods have also been evaluated in a number of applications in the cognitive science, computational linguistics and the information retrieval literatures. In this paper, we review the available methodologies for derivation of semantic relatedness from free text, as well as their evaluation in a variety of biomedical and other applications. Recent methodological developments, and their applicability to several existing applications are also discussed.}
}
@article{ZHAO2022103028,
title = {Reviewer assignment algorithms for peer review automation: A survey},
journal = {Information Processing & Management},
volume = {59},
number = {5},
pages = {103028},
year = {2022},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2022.103028},
url = {https://www.sciencedirect.com/science/article/pii/S0306457322001388},
author = {Xiquan Zhao and Yangsen Zhang},
keywords = {Matching degree, Information retrieval, Reviewer assignment problem, Optimization algorithm, Natural language processing, Peer review},
abstract = {Assigning paper to suitable reviewers is of great significance to ensure the accuracy and fairness of peer review results. In the past three decades, many researchers have made a wealth of achievements on the reviewer assignment problem (RAP). In this survey, we provide a comprehensive review of the primary research achievements on reviewer assignment algorithm from 1992 to 2022. Specially, this survey first discusses the background and necessity of automatic reviewer assignment, and then systematically summarize the existing research work from three aspects, i.e., construction of candidate reviewer database, computation of matching degree between reviewers and papers, and reviewer assignment optimization algorithm, with objective comments on the advantages and disadvantages of the current algorithms. Afterwards, the evaluation metrics and datasets of reviewer assignment algorithm are summarized. To conclude, we prospect the potential research directions of RAP. Since there are few comprehensive survey papers on reviewer assignment algorithm in the past ten years, this survey can serve as a valuable reference for the related researchers and peer review organizers.}
}
@article{KONYS20191614,
title = {Knowledge Repository of Ontology Learning Tools from Text},
journal = {Procedia Computer Science},
volume = {159},
pages = {1614-1628},
year = {2019},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 23rd International Conference KES2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.332},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919315339},
author = {Agnieszka Konys},
keywords = {Ontology learning tools, Learning techniques, Ontology learninng from text, Knowledge repository},
abstract = {Ontologies are one of the fundamental elements of the Semantic Web, and they have gained a lot of popularity and recognition because they are viewed as the answer to the need for interoperable semantics in modern information systems. The intermingling of techniques in areas such as natural language processing, information retrieval, machine learning, data mining, and knowledge representation provide a lot of possibilities for development of ontology learning approaches. A rise in focus on the ability to cope with the scale of Web data required for ontology learning forces the potential growth of cross-language research, emphasizing the automatic or semi-automatic generation of the tools dedicated to text mining and information extraction. This paper presents the integration of ontology learning tools from text in the knowledge repository to incorporate the applied techniques and outputs of an ontology learning algorithm into the one complex multifunctional solution. The proposed knowledge repository covers various applicability of existing techniques of learning ontologies from text, and offers competency question-based reasoning mechanism for individuals to specify their profiles of ontology learning tools. The validation stage is also provided in the form of applied reasoning.}
}
@article{CODEN2009937,
title = {Automatically extracting cancer disease characteristics from pathology reports into a Disease Knowledge Representation Model},
journal = {Journal of Biomedical Informatics},
volume = {42},
number = {5},
pages = {937-949},
year = {2009},
note = {Biomedical Natural Language Processing},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2008.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S1532046408001585},
author = {Anni Coden and Guergana Savova and Igor Sominsky and Michael Tanenblatt and James Masanz and Karin Schuler and James Cooper and Wei Guan and Piet C. {de Groen}},
keywords = {Cancer Disease Knowledge Representation Model, Analysis system, Natural language processing, Concept formation, Information retrieval, Medical records},
abstract = {We introduce an extensible and modifiable knowledge representation model to represent cancer disease characteristics in a comparable and consistent fashion. We describe a system, MedTAS/P which automatically instantiates the knowledge representation model from free-text pathology reports. MedTAS/P is based on an open-source framework and its components use natural language processing principles, machine learning and rules to discover and populate elements of the model. To validate the model and measure the accuracy of MedTAS/P, we developed a gold-standard corpus of manually annotated colon cancer pathology reports. MedTAS/P achieves F1-scores of 0.97–1.0 for instantiating classes in the knowledge representation model such as histologies or anatomical sites, and F1-scores of 0.82–0.93 for primary tumors or lymph nodes, which require the extractions of relations. An F1-score of 0.65 is reported for metastatic tumors, a lower score predominantly due to a very small number of instances in the training and test sets.}
}
@article{ALKHATIB2017101,
title = {A Rich Arabic WordNet Resource for Al-Hadith Al-Shareef},
journal = {Procedia Computer Science},
volume = {117},
pages = {101-110},
year = {2017},
note = {Arabic Computational Linguistics},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.10.098},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917321555},
author = {Manar Alkhatib and Azza Abdel Monem and Khaled Shaalan},
keywords = {Arabic, WordNet, Al-Hadith Al-Shareef, Text Classification},
abstract = {Most Arabic computational linguistics researchers have focused on Modern Standard Arabic. Linguistic resources and tools for Classical Arabic, especially Al-Hadith Al-Shareef (i.e. the Prophet Muhammad’s saying), remain relatively unexplored, despite its importance as a reference, in addition to its use in the holy Qur’an, used by all Muslims worldwide. Computational linguistics research tools for Al-Hadith Al-Shareef would be useful for both Islamic scholars and learners. Arabic WordNet is a remarkable language resource on its own, allowing a user to determine the relationships among words. It has proven its importance in many of language processing tasks needing an understanding of the meaning of language, including Information Retrieval, Word Sense Disambiguation, Machine Translation, Question Answering, Text Classification, and Text Summarization. In this paper, we propose an approach for developing a WordNet linguistic resource for Al-Hadith Al-Shareef that serves its purposes for various Arabic natural language processing tasks. In particular, we establish semantic connections between words in order to achieve a good understanding of the meanings of the Al-Hadith words. Our approach employs Classical Arabic dictionaries and Al-Hadith ontology. Al-Hadith WordNet has demonstrated its capability in a text classification task that we developed for evaluation proposes. The classifier has been applied on around 8500 synsets that include 6126 nominal, 1990 verbal, 310 adjectival, and 71 adverbial expressions.}
}
@article{GABSI2017564,
title = {MeSH-based disambiguation method using an intrinsic information content measure of semantic similarity},
journal = {Procedia Computer Science},
volume = {112},
pages = {564-573},
year = {2017},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 21st International Conference, KES-20176-8 September 2017, Marseille, France},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.08.169},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917315260},
author = {Imen Gabsi and Hager Kammoun and Sarra brahmi and Ikram Amous},
keywords = {Word Sense Disambiguation, Semantic Similarity, Intrinsic Information Content, MeSH, Information Retrieval},
abstract = {Word Sense Disambiguation represents a crucial task in many natural language processing applications such as information extraction, information retrieval and text summarization. It intends to identify the correct sense of an ambiguous term (target word) according to its context. In this paper, we present a biomedical knowledge-based disambiguation method for determining the adequate domain or sub-domain (sense) of an ambiguous biomedical term. This method uses the MeSH thesaurus as a knowledge resource and an intrinsic information content-based semantic similarity to measure the likeness between their different senses of the ambiguous term MeSH and its context. We define this latter as the set of unambiguous MeSH terms characterizing a document. We aim on one hand to minimize the time and computational complexity and on the other hand to increase the accuracy of the disambiguation by using such context. To evaluate this method, we involved it into a document indexing and retrieval system. The results of experiments, carried out on a sub-set of the OHSUMED collection, in terms of Mean Average Precision show that our method performs well.}
}
@article{BHATTACHARYA20232723,
title = {Improving biomedical named entity recognition through transfer learning and asymmetric tri-training},
journal = {Procedia Computer Science},
volume = {218},
pages = {2723-2733},
year = {2023},
note = {International Conference on Machine Learning and Data Engineering},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.01.244},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923002442},
author = {Medha Bhattacharya and Swati Bhat and Sirshasree Tripathy and Anvita Bansal and Monika Choudhary},
keywords = {Electronic Health Records, Biomedical Named Entity Recognition, Machine Learning, BiLSTM-CRF, Transfer Learning, Asymmetric tri-training},
abstract = {Today, electronic health records have turned into prime sources of information for physicians looking after their patients. EHRs and computerized patient data resources have expedited the accelerated discovery of formerly obscure biomedical and clinical information. Because of the lengthy, error-prone, non-scalable, and expensive manual abstraction process, natural language processing (NLP) procedures are being wielded more and more in biomedical and clinical fields. One of the building blocks of all NLP systems, Named Entity Recognition (NER) is considered a sub-activity of information retrieval. To extract biomedical knowledge from electronic health records, a prerequisite is the efficient recognition of biomedical entities. Deep-learning techniques have gained more and more consideration recently for the above-mentioned task. Notwithstanding, these methods are based on high-caliber, high-cost, labeled data. In this work, a biomedical-named entity recognition model based on transfer learning and asymmetric tri-training is proposed to diminish the limited annotated data problem in the biomedical-named entity recognition domain. The proposed model showed a significant improvement of more than 9% over the baseline BiLSTM-CRF model in the exact F1 scores on four different datasets considered in this work.}
}
@article{DAGA2020123,
title = {Prediction of Likes and Retweets Using Text Information Retrieval},
journal = {Procedia Computer Science},
volume = {168},
pages = {123-128},
year = {2020},
note = {“Complex Adaptive Systems”Malvern, PennsylvaniaNovember 13-15, 2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.02.273},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920304129},
author = {Ishita Daga and Anchal Gupta and Raj Vardhan and Partha Mukherjee},
keywords = {TF-IDF, Doc2Vec, Text Mining, Twitter, Predictive Modeling of Words},
abstract = {Twitter is one of the major social media platforms today to study human behaviours by analysing their interactions. To ensure popularity of the tweet, the focus should be on the content of the tweet that results in numerous followings of that message with sufficient number of likes and retweets. The high quality of tweets, increases the online reputation of the users who post it. If a user can get the prediction of likes and retweets on his text before posting it on the internet, it would improve the popularity of the tweet from information sharing perspective. In this paper we employed different machine learning classifiers like SVM, Naïve Bayes, Logistic Regression, Random Forest, and Neural Network, on top of two different text processing approaches used in NLP (natural language processing), namely bag-of-words (TFIDF) and word embeddings (Doc2Vec), to check how many likes and retweets can a tweet generate. The results obtained indicate that all the models performed 10-15% better with the bag-of-word technique.}
}
@article{MARIEFRANCINE20187,
title = {Keynote II},
journal = {Procedia Computer Science},
volume = {141},
pages = {7},
year = {2018},
note = {The 9th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2018) / The 8th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2018) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.121},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918317691},
author = { Marie-Francine},
abstract = {About the Speaker Prof. Marie-Francine Moens heads the Language Intelligence and Information Retrieval research group at the Department of Computer Science of KU Leuven, Belgium. Her research expertise includes: content recognition in text, information extraction, discourse understanding, text mining, knowledge acquisition, machine reading of text, processing of noisy text such as user-generated content and speech transcripts, information retrieval, search models, and machine learning for natural language processing. She is author of more than 300 international peer-reviewed publications and of several books. She is involved in the organization or program committee (as program chair, area chair or reviewer) of major conferences on computational linguistics, information retrieval and machine learning. In 2011 and 2012 she was appointed as chair of the European Chapter of the Association for Computational Linguistics (EACL) and was a member of the executive board of the Association for Computational Linguistics (ACL). From 2010 until 2014 she was a member of the Research Council of KU Leuven and is currently a member of the Council of the Industrial Research Fund of KU Leuven. She is the scientific manager of the EU COST action iV&L (The European Network on Integrating Vision and Language). She was appointed as Scottish Informatics and Computer Science Alliance (SICSA) Distinguished Visiting Fellow in 2014.}
}
@article{GAO2021100,
title = {Advances and challenges in conversational recommender systems: A survey},
journal = {AI Open},
volume = {2},
pages = {100-126},
year = {2021},
issn = {2666-6510},
doi = {https://doi.org/10.1016/j.aiopen.2021.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S2666651021000164},
author = {Chongming Gao and Wenqiang Lei and Xiangnan He and Maarten {de Rijke} and Tat-Seng Chua},
keywords = {Conversational recommendation system, Interactive recommendation, Preference elicitation, Multi-turn conversation strategy, Exploration-exploitation},
abstract = {Recommender systems exploit interaction history to estimate user preference, having been heavily used in a wide range of industry applications. However, static recommendation models are difficult to answer two important questions well due to inherent shortcomings: (a) What exactly does a user like? (b) Why does a user like an item? The shortcomings are due to the way that static models learn user preference, i.e., without explicit instructions and active feedback from users. The recent rise of conversational recommender systems (CRSs) changes this situation fundamentally. In a CRS, users and the system can dynamically communicate through natural language interactions, which provide unprecedented opportunities to explicitly obtain the exact preference of users. Considerable efforts, spread across disparate settings and applications, have been put into developing CRSs. Existing models, technologies, and evaluation methods for CRSs are far from mature. In this paper, we provide a systematic review of the techniques used in current CRSs. We summarize the key challenges of developing CRSs in five directions: (1) Question-based user preference elicitation. (2) Multi-turn conversational recommendation strategies. (3) Dialogue understanding and generation. (4) Exploitation-exploration trade-offs. (5) Evaluation and user simulation. These research directions involve multiple research fields like information retrieval (IR), natural language processing (NLP), and human-computer interaction (HCI). Based on these research directions, we discuss some future challenges and opportunities. We provide a road map for researchers from multiple communities to get started in this area. We hope this survey can help to identify and address challenges in CRSs and inspire future research.}
}
@article{PANTAZI2010844,
title = {Unsupervised grammar induction and similarity retrieval in medical language processing using the Deterministic Dynamic Associative Memory (DDAM) model},
journal = {Journal of Biomedical Informatics},
volume = {43},
number = {5},
pages = {844-857},
year = {2010},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2010.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S1532046410001024},
author = {Stefan V. Pantazi},
keywords = {Associative memory, Natural language processing, Unsupervised grammar induction, Case-based reasoning, Information retrieval},
abstract = {This paper is an overview of unsupervised grammar induction and similarity retrieval, two fundamental information processing functions of importance to medical language processing applications and to the construction of intelligent medical information systems. Existing literature with a focus on text segmentation tasks is reviewed. The review includes a comparison of existing approaches and reveals the longstanding interest in these traditionally distinct topics despite the significant computational challenges that characterizes them. Further, a unifying approach to unsupervised representation and processing of sequential data, the Deterministic Dynamic Associative Memory (DDAM) model, is introduced and described theoretically from both structural and functional perspectives. The theoretical descriptions of the model are complemented by a selection and discussion of interesting experimental results in the tasks of unsupervised grammar induction and similarity retrieval with applications to medical language processing. Notwithstanding the challenges associated with the evaluation of unsupervised information-processing models, it is concluded that the DDAM model demonstrates interesting properties that encourage further investigations in both theoretical and applied contexts.}
}
@article{CODEN2005422,
title = {Domain-specific language models and lexicons for tagging},
journal = {Journal of Biomedical Informatics},
volume = {38},
number = {6},
pages = {422-430},
year = {2005},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2005.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S1532046405000213},
author = {Anni R. Coden and Serguei V. Pakhomov and Rie K. Ando and Patrick H. Duffy and Christopher G. Chute},
keywords = {Clinical report analysis, Part-of-speech tagging accuracy, Domain adaptation, Clinical information systems, Biomedical domain, Corpus linguistics, Statistical part-of-speech tagging, Hidden Markov Model},
abstract = {Accurate and reliable part-of-speech tagging is useful for many Natural Language Processing (NLP) tasks that form the foundation of NLP-based approaches to information retrieval and data mining. In general, large annotated corpora are necessary to achieve desired part-of-speech tagger accuracy. We show that a large annotated general-English corpus is not sufficient for building a part-of-speech tagger model adequate for tagging documents from the medical domain. However, adding a quite small domain-specific corpus to a large general-English one boosts performance to over 92% accuracy from 87% in our studies. We also suggest a number of characteristics to quantify the similarities between a training corpus and the test data. These results give guidance for creating an appropriate corpus for building a part-of-speech tagger model that gives satisfactory accuracy results on a new domain at a relatively small cost.}
}
@article{MASSAI201970,
title = {PAVAL: A location-aware virtual personal assistant for retrieving geolocated points of interest and location-based services},
journal = {Engineering Applications of Artificial Intelligence},
volume = {77},
pages = {70-85},
year = {2019},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2018.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S0952197618301994},
author = {Lorenzo Massai and Paolo Nesi and Gianni Pantaleo},
keywords = {Virtual personal assistants, Location-aware recommender systems, Natural language processing, User-intent detection, Semantic web technologies, Geographic information retrieval, Geoparsing, Geocoding},
abstract = {Today most of the users on the move require contextualized local and georeferenced information. Several solutions aim to meet these trends, thus assisting users and satisfying their needs and preferences, such as virtual assistants and Location-Aware Recommender Systems (LARS), both in commercial and research literature. However, general purpose virtual assistants usually have to manage large domains, dealing with big amounts of data and online resources, losingfocus on more specific requirements and local information. On the other hand, traditional recommender systems are based on filtering techniques and contextual knowledge, and they usually do not rely on Natural Language Processing (NLP) features on users’ queries, which are useful to understand and contextualize users’ necessities on the spot. Therefore, comprehending the actual users’ information needs and other key information that can be included in the user query, such as geographical references, is a challenging task which is not yet fully accomplished by current state-of-the-art solutions. In this paper, we propose Paval (Location-Aware Virtual Personal Assistant 2 2The name Paval is chosen as a permutation of the initials of “Location-aware virtual personal assistant”.), a semantic assisting engine for suggesting local points of interest (POIs) and services by analyzing users’ natural language queries, in order to estimate the information need and potential geographic references expressed by the users. The system exploits NLP and semantic techniques providing as output recommendations on local geolocated POIs and services which best match the users’ requests, retrieved by querying our semantic Km4City Knowledge Base. The proposed system is validated against the most popular virtual assistants, such as Google Assistant, Apple Siri and Microsoft Cortana, focusing the assessment on the request of geolocated POIs and services, showing very promising capabilities in successfully estimating the users’ information needs and multiple geographic references.}
}
@article{ZHANG2022103397,
title = {Necessary conditions for convergence of CNNs and initialization of convolution kernels},
journal = {Digital Signal Processing},
volume = {123},
pages = {103397},
year = {2022},
issn = {1051-2004},
doi = {https://doi.org/10.1016/j.dsp.2022.103397},
url = {https://www.sciencedirect.com/science/article/pii/S1051200422000148},
author = {Huibin Zhang and Liping Feng and Xiaohua Zhang and Yuchi Yang and Jing Li},
keywords = {Convolutional Neural Networks (CNNs), Convergence, Batch normalization, Initialization of the convolution kernel, Initial learning rate, Gaussian distribution},
abstract = {Despite the great success of deep learning in many fields such as computer vision, natural language processing, and information retrieval, there are relatively few studies on the convergence of deep convolutional neural networks (CNNs), and there is a lack of theoretical studies on the necessary conditions for the convergence of CNNs. The initialization of the convolution kernel of CNNs is an important factor in whether the network can converge. However, the existing initialization methods do not analyze the influence of their methods on the convergence performance of CNNs and did not analyze the conditions of their application, and thus the performance is not the best in different network models. In this work, the computational process of both forward and backward propagation of CNNs is considered as a mapping in linear normed space, and thus a necessary condition for CNNs stable converge is proposed. According to this necessary condition of convergence, we first derive initialization formulas for plain networks applicable to any activation function, and derive the initialization method of plain networks whose activation function is ReLU and PReLU. Secondly, the necessary conditions for convergence of CNNs proposed in this work can explain the mathematical reasons why the BN contribute to the training of CNNs, and this problem has always been an active research topic. Finally, we find that the learning rate and the initialization of the convolutional kernel jointly affect the convergence performance of the network. Based on the plain networks convolution kernel initialization method, we derive the convolution kernel initialization method of network with BN layer related to the learning rate. In order to verify the effectiveness of the proposed initialization method, we test it on the CIFAR-10 and CIFAR-100 datasets, and performed image classification with four network models (VGG-19, ResNet-110, DenseNet-100 and WideResNet28-10), and the experimental results showed that the initialization method proposed in this work improved the accuracy of image classification.}
}
@article{DOOTIO2021468,
title = {Development of Sindhi text corpus},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {33},
number = {4},
pages = {468-475},
year = {2021},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2019.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S1319157818311649},
author = {Mazhar Ali Dootio and Asim Imdad Wagan},
keywords = {Text corpus, NLP, Sindhi, DTM, TF-IDF},
abstract = {Sindhi language is a rich language with plenty of literary and general texts. There are number of books, newspapers, magazines and internet material available to develop Sindhi text corpus but yet proper and useful text corpus could not be developed and presented online for research, language features analysis, linguistics analysis and information retrieval systems. The lack of resources for research on computational linguistics and NLP applications for Sindhi language are challenging tasks at this stage. However, we have developed Sindhi text corpora in order to provide text resources to computational linguists, Natural Languages process (NLP) experts and researchers. Online books, newspapers, magazines, blogs and social websites are utilized to build Sindhi text corpus. Sindhi sentiment based text corpus is developed and analyzed with Document Term Matrix and TF-IDF models using 2-gram technique of n-gram model. The corpus may be useful for research on language variation analysis, sentiment analysis, aspect based sentiment analysis, semantic analysis, machine translation, information retrieval, Word2Vec, topic modeling and cluster analysis.}
}
@article{FILANNINO201519,
title = {Temporal expression extraction with extensive feature type selection and a posteriori label adjustment},
journal = {Data & Knowledge Engineering},
volume = {100},
pages = {19-33},
year = {2015},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2015.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X15000725},
author = {Michele Filannino and Goran Nenadic},
keywords = {Text mining, Mining methods and algorithms, Data mining},
abstract = {The automatic extraction of temporal information from written texts is pivotal for many Natural Language Processing applications such as question answering, text summarisation and information retrieval. It allows to filter information and infer temporal flows of events. This paper presents ManTIME, a general domain temporal expression identification and normalisation system, and systematically explores the impact of different features and training corpora on the performance. The identification phase combines the use of conditional random fields along with a post-processing pipeline, whereas the normalisation phase is carried out using NorMA, an open-source rule-based temporal normaliser. We investigate the performance variation with respect to different feature types. Specifically, we show that the use of WordNet-based features in the identification task negatively affects the overall performance, and that there is no statistically significant difference in the results based on gazetteers, shallow parsing and propositional noun phrases labels on top of the morpho-lexical features. We also show that the use of silver data (alone or in addition to the human-annotated ones) does not improve the performance. We evaluate six combinations of training data and post-processing pipeline with respect to the TempEval-3 benchmark test set. The best run achieved 0.95 (precision), 0.85 (recall) and 0.90 (Fβ=1) in the identification phase. Normalisation accuracies are 0.86 (for type attribute) and 0.77 (for value attribute). The proposed approach ranked 3rd in the TempEval-3 challenge (task A) as the best performing machine learning-based system among 21 participants.}
}
@article{IDREES20241115,
title = {A Weighted Multi-Layer Analytics Based Model for Emoji Recommendation},
journal = {Computers, Materials and Continua},
volume = {78},
number = {1},
pages = {1115-1133},
year = {2024},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2023.046457},
url = {https://www.sciencedirect.com/science/article/pii/S1546221824001723},
author = {Amira M. Idrees and Abdul Lateef Marzouq Al-Solami},
keywords = {Social networks, text analytics, emoji prediction, features extraction, information retrieval},
abstract = {The developed system for eye and face detection using Convolutional Neural Networks (CNN) models, followed by eye classification and voice-based assistance, has shown promising potential in enhancing accessibility for individuals with visual impairments. The modular approach implemented in this research allows for a seamless flow of information and assistance between the different components of the system. This research significantly contributes to the field of accessibility technology by integrating computer vision, natural language processing, and voice technologies. By leveraging these advancements, the developed system offers a practical and efficient solution for assisting blind individuals. The modular design ensures flexibility, scalability, and ease of integration with existing assistive technologies. However, it is important to acknowledge that further research and improvements are necessary to enhance the system’s accuracy and usability. Fine-tuning the CNN models and expanding the training dataset can improve eye and face detection as well as eye classification capabilities. Additionally, incorporating real-time responses through sophisticated natural language understanding techniques and expanding the knowledge base of ChatGPT can enhance the system’s ability to provide comprehensive and accurate responses. Overall, this research paves the way for the development of more advanced and robust systems for assisting visually impaired individuals. By leveraging cutting-edge technologies and integrating them into a modular framework, this research contributes to creating a more inclusive and accessible society for individuals with visual impairments. Future work can focus on refining the system, addressing its limitations, and conducting user studies to evaluate its effectiveness and impact in real-world scenarios.}
}
@article{BHUYAN2023100028,
title = {Textual entailment as an evaluation metric for abstractive text summarization},
journal = {Natural Language Processing Journal},
volume = {4},
pages = {100028},
year = {2023},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2023.100028},
url = {https://www.sciencedirect.com/science/article/pii/S2949719123000250},
author = {Swagat Shubham Bhuyan and Saranga Kingkor Mahanta and Partha Pakray and Benoit Favre},
keywords = {Automatic evaluation, Abstractive text summarization, Text entailment, Natural language processing, Deep learning},
abstract = {Automated text summarization systems require to be heedful of the reader and the communication goals since it may be the determining component of whether the original textual content is actually worth reading in full. The summary can also assist enhance document indexing for information retrieval, and it is generally much less biased than a human-written summary. A crucial part while building intelligent systems is evaluating them. Consequently, the choice of evaluation metric(s) is of utmost importance. Current standard evaluation metrics like BLEU and ROUGE, although fairly effective for evaluation of extractive text summarization systems, become futile when it comes to comparing semantic information between two texts, i.e in abstractive summarization. We propose textual entailment as a potential metric to evaluate abstractive summaries. The results show the contribution of text entailment as a strong automated evaluation model for such summaries. The textual entailment scores between the text and generated summaries, and between the reference and predicted summaries were calculated, and an overall summarizer score was generated to give a fair idea of how efficient the generated summaries are. We put forward some novel methods that use the entailment scores and the final summarizer scores for a reasonable evaluation of the same across various scenarios. A Final Entailment Metric Score (FEMS) was generated to get an insightful idea in order to compare both the generated summaries.}
}
@article{CHAIBI2014437,
title = {Topic Segmentation for Textual Document Written in Arabic Language},
journal = {Procedia Computer Science},
volume = {35},
pages = {437-446},
year = {2014},
note = {Knowledge-Based and Intelligent Information & Engineering Systems 18th Annual Conference, KES-2014 Gdynia, Poland, September 2014 Proceedings},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.08.124},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914010898},
author = {Anja Habacha Chaibi and Marwa Naili and Samia Sammoud},
keywords = {Topic segmentation, Arabic language processing, ArabC99, ArabTextTiling.},
abstract = {Topic segmentation is important for many natural language processing applications such as information retrieval, text summarization. In our work, we are interested in the topic segmentation of textual document. We present a survey of related works particularly C99 and TextTiling. Then, we propose an adaptation of these topic segmenters for textual document written in Arabic language named as ArabC99 and ArabTextTiling. For experimental results, we construct an Arabic corpus based on newspapers of different Arab countries. Finally, we evaluate the performance of these new segmenters by comparing them together and to related works using the metrics WindowDiff and F-measure.}
}
@article{YOUNES2018238,
title = {A Sequence-to-Sequence based Approach For the double Transliteration of Tunisian Dialect},
journal = {Procedia Computer Science},
volume = {142},
pages = {238-245},
year = {2018},
note = {Arabic Computational Linguistics},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.481},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918321859},
author = {Jihene Younes and Emna Souissi and Hadhemi Achour and Ahmed Ferchichi},
keywords = {Tunsian dialect, transliteration, Latin transcription, Arabic transcription, Sequence-to-Sequence, deep learning, natural language processing},
abstract = {Transliteration consists of automatically transforming a grapheme’s transcription from one writing system to another, while preserving its pronunciation. It is usually used in the context of machine translation and cross language information retrieval, mainly to deal with the issue of named entities and technical terms. In the case of some Arabic dialects, which are used on the social web in both Latin and Arabic scripts and which are still low-resource languages, transliteration is of great benefit for the automatic generation of various linguistic resources (parallel corpora and lexica), useful for their automatic processing. In this work, we focus on the Tunisian dialect transliteration. We propose a deep learning based Sequence-to-Sequence approach to perform a word-level transliteration of the user generated Tunisian dialect on the social web, in both Latin to Arabic and Arabic to Latin senses.}
}
@article{ZHANG2011830,
title = {Degree centrality for semantic abstraction summarization of therapeutic studies},
journal = {Journal of Biomedical Informatics},
volume = {44},
number = {5},
pages = {830-838},
year = {2011},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2011.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S1532046411000773},
author = {Han Zhang and Marcelo Fiszman and Dongwook Shin and Christopher M. Miller and Graciela Rosemblat and Thomas C. Rindflesch},
keywords = {Automatic summarization, Natural language processing, Graph theory, Degree centrality, Semantic processing, Disease treatment},
abstract = {Automatic summarization has been proposed to help manage the results of biomedical information retrieval systems. Semantic MEDLINE, for example, summarizes semantic predications representing assertions in MEDLINE citations. Results are presented as a graph which maintains links to the original citations. Graphs summarizing more than 500 citations are hard to read and navigate, however. We exploit graph theory for focusing these large graphs. The method is based on degree centrality, which measures connectedness in a graph. Four categories of clinical concepts related to treatment of disease were identified and presented as a summary of input text. A baseline was created using term frequency of occurrence. The system was evaluated on summaries for treatment of five diseases compared to a reference standard produced manually by two physicians. The results showed that recall for system results was 72%, precision was 73%, and F-score was 0.72. The system F-score was considerably higher than that for the baseline (0.47).}
}
@article{AMBALAVANAN2020103578,
title = {Using the contextual language model BERT for multi-criteria classification of scientific articles},
journal = {Journal of Biomedical Informatics},
volume = {112},
pages = {103578},
year = {2020},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2020.103578},
url = {https://www.sciencedirect.com/science/article/pii/S1532046420302069},
author = {Ashwin Karthik Ambalavanan and Murthy V. Devarakonda},
keywords = {Biomedical natural language processing, Neural networks, Screening scientific articles, Text classification, Machine learning, BERT, SciBERT},
abstract = {Background
Finding specific scientific articles in a large collection is an important natural language processing challenge in the biomedical domain. Systematic reviews and interactive article search are the type of downstream applications that benefit from addressing this problem. The task often involves screening articles for a combination of selection criteria. While machine learning was previously used for this purpose, it is not known if different criteria should be modeled together or separately in an ensemble model. The performance impact of the modern contextual language models on the task is also not known.
Methods
We framed the problem as text classification and conducted experiments to compare ensemble architectures, where the selection criteria were mapped to the components of the ensemble. We proposed a novel cascade ensemble analogous to the step-wise screening process employed in developing the gold standard. We compared performance of the ensembles with a single integrated model, which we refer to as the individual task learner (ITL). We used SciBERT, a variant of BERT pre-trained on scientific articles, and conducted experiments using a manually annotated dataset of ~49 K MEDLINE abstracts, known as Clinical Hedges.
Results
The cascade ensemble had significantly higher precision (0.663 vs. 0.388 vs. 0.478 vs. 0.320) and F measure (0.753 vs. 0.553 vs. 0.628 vs. 0.477) than ITL and ensembles using Boolean logic and a feed-forward network. However, ITL had significantly higher recall than the other classifiers (0.965 vs. 0.872 vs. 0.917 vs. 0.944). In fixed high recall studies, ITL achieved 0.509 precision @ 0.970 recall and 0.381 precision @ 0.985 recall on a subset that was studied earlier, and 0.295 precision @ 0.985 recall on the full dataset, all of which were improvements over the previous studies.
Conclusion
Pre-trained neural contextual language models (e.g. SciBERT) performed well for screening scientific articles. Performance at high fixed recall makes the single integrated model (ITL) more suitable among the architectures considered here, for systematic reviews. However, high F measure of the cascade ensemble makes it a better approach for interactive search applications. The effectiveness of the cascade ensemble architecture suggests broader applicability beyond this task and the dataset, and the approach is analogous to query optimization in Information Retrieval and query optimization in databases.}
}
@article{HUANG2013940,
title = {PICO element detection in medical text without metadata: Are first sentences enough?},
journal = {Journal of Biomedical Informatics},
volume = {46},
number = {5},
pages = {940-946},
year = {2013},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2013.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S153204641300110X},
author = {Ke-Chun Huang and I-Jen Chiang and Furen Xiao and Chun-Chih Liao and Charles Chih-Ho Liu and Jau-Min Wong},
keywords = {Text mining, Information retrieval, Natural language processing, Question answering, Information extraction, Evidence-based medicine},
abstract = {Efficient identification of patient, intervention, comparison, and outcome (PICO) components in medical articles is helpful in evidence-based medicine. The purpose of this study is to clarify whether first sentences of these components are good enough to train naive Bayes classifiers for sentence-level PICO element detection. We extracted 19,854 structured abstracts of randomized controlled trials with any P/I/O label from PubMed for naive Bayes classifiers training. Performances of classifiers trained by first sentences of each section (CF) and those trained by all sentences (CA) were compared using all sentences by ten-fold cross-validation. The results measured by recall, precision, and F-measures show that there are no significant differences in performance between CF and CA for detection of O-element (F-measure=0.731±0.009 vs. 0.738±0.010, p=0.123). However, CA perform better for I-elements, in terms of recall (0.752±0.012 vs. 0.620±0.007, p<0.001) and F-measures (0.728±0.006 vs. 0.662±0.007, p<0.001). For P-elements, CF have higher precision (0.714±0.009 vs. 0.665±0.010, p<0.001), but lower recall (0.766±0.013 vs. 0.811±0.012, p<0.001). CF are not always better than CA in sentence-level PICO element detection. Their performance varies in detecting different elements.}
}
@article{LIN201549,
title = {A multi-technique approach to bridge electronic case report form design and data standard adoption},
journal = {Journal of Biomedical Informatics},
volume = {53},
pages = {49-57},
year = {2015},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2014.08.013},
url = {https://www.sciencedirect.com/science/article/pii/S1532046414001944},
author = {Ching-Heng Lin and Nai-Yuan Wu and Der-Ming Liou},
keywords = {Data standard, Common data elements, Case report form, Natural language processing, Ontology-based knowledgebase},
abstract = {Background and objective
The importance of data standards when integrating clinical research data has been recognized. The common data element (CDE) is a consensus-based data element for data harmonization and sharing between clinical researchers, it can support data standards adoption and mapping. However, the lack of a suitable methodology has become a barrier to data standard adoption. Our aim was to demonstrate an approach that allowed clinical researchers to design electronic case report forms (eCRFs) that complied with the data standard.
Methods
We used a multi-technique approach, including information retrieval, natural language processing and an ontology-based knowledgebase to facilitate data standard adoption using the eCRF design. The approach took research questions as query texts with the aim of retrieving and associating relevant CDEs with the research questions.
Results
The approach was implemented using a CDE-based eCRF builder, which was evaluated using CDE- related questions from CRFs used in the Parkinson Disease Biomarker Program, as well as CDE-unrelated questions from a technique support website. Our approach had a precision of 0.84, a recall of 0.80, a F-measure of 0.82 and an error of 0.31. Using the 303 testing CDE-related questions, our approach responded and provided suggested CDEs for 88.8% (269/303) of the study questions with a 90.3% accuracy (243/269). The reason for any missed and failed responses was also analyzed.
Conclusion
This study demonstrates an approach that helps to cross the barrier that inhibits data standard adoption in eCRF building and our evaluation reveals the approach has satisfactory performance. Our CDE-based form builder provides an alternative perspective regarding data standard compliant eCRF design.}
}
@article{FOX1987151,
title = {Architecture of an expert system for composite document analysis, representation, and retrieval},
journal = {International Journal of Approximate Reasoning},
volume = {1},
number = {2},
pages = {151-175},
year = {1987},
issn = {0888-613X},
doi = {https://doi.org/10.1016/0888-613X(87)90012-0},
url = {https://www.sciencedirect.com/science/article/pii/0888613X87900120},
author = {Edward A Fox and Robert K France},
keywords = {information retrieval, artificial intelligence, distributed expert system, knowledge bases, blackboard architecture, lexicon construction},
abstract = {The CODER (COmposite Document Expert/extended/effective Retrieval) project is a multi-yeare effort to investigate how best to apply artificial intelligence methods to increase the effectiveness of information retrieval systems handling collections of composite documents. To ensure system adaptability and to allow controlled experimentation, CODER has been designed as a distributed expert system. The use of individually tailored specialist experts, coupled with standardized blackboard modules for communication and control and external knowledge bases for maintenance of factual world knowledge, allows for quick prototyping, incremental development, and flexibility under change. The system as a whole is being implemented under UNIX as a set of MU-Prolog and C modules communicating through pipes and TCP/IP sockets.}
}
@article{ALHASAN2018158,
title = {POS Tagging for Arabic Text Using Bee Colony Algorithm},
journal = {Procedia Computer Science},
volume = {142},
pages = {158-165},
year = {2018},
note = {Arabic Computational Linguistics},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.471},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918321732},
author = {Ahmad Alhasan and Ahmad T. Al-Taani},
keywords = {Text Summarization, POS Tagging, Question Answering, Bee Colony Algorithm, Meta-heuristics Optimization Algorithms},
abstract = {Part-of-Speech (POS) Tagging is the process of automatically determining the proper grammatical tag or syntactic category of a word depending on a its context. POS Tagging is an essential step in most Natural Language Processing (NLP) applications such as text summarization, question answering, information extraction and information retrieval. In this study, we propose an efficient tagging approach for the Arabic language using Bee Colony Optimization algorithm. The problem is represented as a graph and a novel technique is proposed to assign scores to possible tags of a sentence, then the bees find the best solution path. The proposed approach is evaluated using KALIMAT corpus which consists of 18M words. Experimental results showed that the proposed approach achieved 98.2% of accuracy compared to 98%, 97.4% and 94.6% for Hybrid, Hidden Markov Model and Rule-Based methods respectively. Furthermore, the proposed approach determined all the tags presented in the corpus while the mentioned approaches can identify only three tags.}
}
@article{WANG2016379,
title = {A Part-Of-Speech term weighting scheme for biomedical information retrieval},
journal = {Journal of Biomedical Informatics},
volume = {63},
pages = {379-389},
year = {2016},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2016.08.026},
url = {https://www.sciencedirect.com/science/article/pii/S1532046416301125},
author = {Yanshan Wang and Stephen Wu and Dingcheng Li and Saeed Mehrabi and Hongfang Liu},
keywords = {Biomedical information retrieval, Natural language processing, Part-Of-Speech, Bag-of-word, Markov random field},
abstract = {In the era of digitalization, information retrieval (IR), which retrieves and ranks documents from large collections according to users’ search queries, has been popularly applied in the biomedical domain. Building patient cohorts using electronic health records (EHRs) and searching literature for topics of interest are some IR use cases. Meanwhile, natural language processing (NLP), such as tokenization or Part-Of-Speech (POS) tagging, has been developed for processing clinical documents or biomedical literature. We hypothesize that NLP can be incorporated into IR to strengthen the conventional IR models. In this study, we propose two NLP-empowered IR models, POS-BoW and POS-MRF, which incorporate automatic POS-based term weighting schemes into bag-of-word (BoW) and Markov Random Field (MRF) IR models, respectively. In the proposed models, the POS-based term weights are iteratively calculated by utilizing a cyclic coordinate method where golden section line search algorithm is applied along each coordinate to optimize the objective function defined by mean average precision (MAP). In the empirical experiments, we used the data sets from the Medical Records track in Text REtrieval Conference (TREC) 2011 and 2012 and the Genomics track in TREC 2004. The evaluation on TREC 2011 and 2012 Medical Records tracks shows that, for the POS-BoW models, the mean improvement rates for IR evaluation metrics, MAP, bpref, and P@10, are 10.88%, 4.54%, and 3.82%, compared to the BoW models; and for the POS-MRF models, these rates are 13.59%, 8.20%, and 8.78%, compared to the MRF models. Additionally, we experimentally verify that the proposed weighting approach is superior to the simple heuristic and frequency based weighting approaches, and validate our POS category selection. Using the optimal weights calculated in this experiment, we tested the proposed models on the TREC 2004 Genomics track and obtained average of 8.63% and 10.04% improvement rates for POS-BoW and POS-MRF, respectively. These significant improvements verify the effectiveness of leveraging POS tagging for biomedical IR tasks.}
}
@article{ALSHALABI20226635,
title = {Arabic light-based stemmer using new rules},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {9},
pages = {6635-6642},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2021.08.017},
url = {https://www.sciencedirect.com/science/article/pii/S1319157821002202},
author = {Hamood Alshalabi and Sabrina Tiun and Nazlia Omar and Fatima N. AL-Aswadi and Kamal {Ali Alezabi}},
keywords = {Arabic stemmer, Arabic light stemmer, Arabic information retrieval, Suffix and prefix stripping, Arabic corpus},
abstract = {Superior stemming algorithms aid significantly in many natural language processing (NLP) applications such as information retrieval. Arabic light-based stemmer is one of the most important stemming algorithms. However, partially due to the highly inflected and complexity of Arabic language morphological structure, most of the existing Arabic light-based stemmer algorithms eliminate a few numbers of suffixes and prefixes or both in the process of recognising the infix patterns to determine roots. The elimination of suffixes and prefixes leads to many inefficient results. Hence, this study aims to develop an improved light-based algorithm of the Arabic stemmer by proposing an appropriate suffixes and prefixes list, which is supported by rules according to word length (without using a morpheme or patterns on a stem). Our improved Dlight Arabic stemmer focuses on determining and removing the infix patterns under many rules on length-words and according to a specific order of the stages of the stemming to extract the double, triple and quadruple roots from long and short Arabic words. To evaluate our proposed light-based Arabic stemmer, we compared our stemmer against existing Arabic stemmers, namely Light10, Condlight and ARLST. The experimental results showed the proposed Develop Arabic Light-Based Stemmer (Dlight) obtained the best performance with 68% of F-measure, while the other three Arabic stemmers yield slightly lower F-measure. Finally, establishing an appropriate list of suffixes and prefixes with word length rules to stem Arabic words can improve the performance of a light-based Arabic stemmer.}
}