@article{KADRIU2022108436,
title = {Human-annotated dataset for social media sentiment analysis for Albanian language},
journal = {Data in Brief},
volume = {43},
pages = {108436},
year = {2022},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2022.108436},
url = {https://www.sciencedirect.com/science/article/pii/S2352340922006333},
author = {Fatbardh Kadriu and Doruntina Murtezaj and Fatbardh Gashi and Lule Ahmedi and Arianit Kurti and Zenun Kastrati},
keywords = {Sentiment analysis, Machine/deep learning, Affective computing, NLP, Text classification},
abstract = {Social media was a heavily used platform by people in different countries to express their opinions about different crises, especially during the Covid-19 pandemics. This dataset is created through collecting people's comments in the news items on the official Facebook site of the National Institute of Public Health of Kosovo. The dataset contains a total of 10,132 comments that are human-annotated in the Albanian language as a low-resource language. The dataset was collected from March 12, 2020, and this coincides with the emergence of the first confirmed Covid-19 case in Kosovo until August 31, 2020, when the second wave started. Due to the scarcity of labeled data for low-resource languages, the dataset can be used by the research community in the field of machine learning, information retrieval, affective computing, as well as by the public agencies and decision makers.}
}
@article{YIANNAKOULIAS2024103392,
title = {Spatial intelligence and contextual relevance in AI-driven health information retrieval},
journal = {Applied Geography},
volume = {171},
pages = {103392},
year = {2024},
issn = {0143-6228},
doi = {https://doi.org/10.1016/j.apgeog.2024.103392},
url = {https://www.sciencedirect.com/science/article/pii/S0143622824001978},
author = {Niko Yiannakoulias},
keywords = {Large language models, Spatial artificial intelligence, Health information},
abstract = {The evolution of large language models (LLMs) has already significantly influenced online health information retrieval. As these models gain more widespread use, it is important to understand their ability to contextualize responses based on spatial and geographic information. This study investigates whether LLMs can vary responses based on geographic and spatial context. Using a structured set of prompts submitted to ChatGPT, responses were analyzed to discern patterns based on prompt question and geographic identifiers included in queries. The analysis used word frequency analysis and bidirectional encoder representations from transformers (BERT) embeddings to evaluate the variation in responses concerning geographic specificity. The results provide some evidence that LLMs can generate geographically tailored responses when the query specifies such a need, thereby supporting localized information retrieval. Moreover, prompt responses exhibit an association between spatial distance and word frequency/sentence embedding differences between texts. This result suggests a nuanced representation of spatial information, which could impact user experience by providing more relevant health information based on the user's location. This study lays the groundwork for further exploration into the spatial intelligence of LLMs and their impact on the accessibility of health information online.}
}
@article{MARWAN2023101742,
title = {Leveraging artificial intelligence and mutual authentication to optimize content caching in edge data centers},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {35},
number = {9},
pages = {101742},
year = {2023},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2023.101742},
url = {https://www.sciencedirect.com/science/article/pii/S1319157823002963},
author = {Mbarek Marwan and Feda AlShahwan and Yassine Afoudi and Abdelkarim {Ait Temghart} and Mohamed Lazaar},
keywords = {Reactive caching, Information retrieval, Mutual authentication, Recommendation systems, Feature selection, Machine learning},
abstract = {Available online Edge data centers are designed to meet the stringent QoE requirements of delay-sensitive and computationally intensive services in Content Delivery Network (CDN) and 5G networks. The primary purpose of this paper was to formulate and solve the problem of optimizing many control variables jointly: (i) what contents to store by taking into consideration edge capacity, and (ii) what contents to recommend to each Internet of Everything (IoE) item, based on identity and access management (IAM). In reactive caching policy, we proposed a new Two-Factor Authentication (2FA) scheme founded upon the Elliptic Curve Cryptography (ECC) and one-way hash function for access control. More interestingly, we use Non-negative Matrix Factorization (NMF), Fuzzy C-Means (FCM), Random Forest (RF) and Pearson Correlation (PC) to improve the accuracy and latency of traditional data filtering models. The intelligent recommendation engine we propose is designed to be implemented by cloud for caching and prefetching contents at the edge. The experimental results validate the theoretical guarantees of the proposed solution and its ability to achieve significant performance gains compared to common baseline models.}
}
@article{WANG2020105030,
title = {Word Sense Disambiguation: A comprehensive knowledge exploitation framework},
journal = {Knowledge-Based Systems},
volume = {190},
pages = {105030},
year = {2020},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2019.105030},
url = {https://www.sciencedirect.com/science/article/pii/S0950705119304344},
author = {Yinglin Wang and Ming Wang and Hamido Fujita},
keywords = {Word sense disambiguation, Background knowledge, Information retrieval, Relation exploitation, Semantic path},
abstract = {Word Sense Disambiguation (WSD) has been a basic and on-going issue since its introduction in natural language processing (NLP) community. Its application lies in many different areas including sentiment analysis, Information Retrieval (IR), machine translation and knowledge graph construction. Solutions to WSD are mostly categorized into supervised and knowledge-based approaches. In this paper, a knowledge-based method is proposed, modeling the problem with semantic space and semantic path hidden behind a given sentence. The approach relies on the well-known Knowledge Base (KB) named WordNet and models the semantic space and semantic path by Latent Semantic Analysis (LSA) and PageRank respectively. Experiments has proven the method’s effectiveness, achieving state-of-the-art performance in several WSD datasets.}
}
@article{KRUIPER2024102653,
title = {A platform-based Natural Language processing-driven strategy for digitalising regulatory compliance processes for the built environment},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102653},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102653},
url = {https://www.sciencedirect.com/science/article/pii/S147403462400301X},
author = {Ruben Kruiper and Bimal Kumar and Richard Watson and Farhad Sadeghineko and Alasdair Gray and Ioannis Konstas},
keywords = {Digital Regulatory Compliance, Natural Language Processing, Semantic Web, Machine Learning, Knowledge Graph, Automated Compliance Checking},
abstract = {The digitalisation of the regulatory compliance process has been an active area of research for several decades. However, more recently the level of activities in this area has increased considerably. In the UK, the tragic incident of Grenfell fire in 2017 has been a major catalyst for this as a result of the Hackitt report’s recommendations pointing a lot of the blame on the broken regulatory regime in the country. The Hackitt report emphasises the need to overhaul the building regulations, but the approach to do so remains an open research question. Existing work in this space tends to overlook the processing of actual regulatory documents, or limits their scope to solving a relatively small subtask. This paper presents a new comprehensive platform approach to the digitalisation of the regulatory compliance processing. We present i-ReC (intelligent Regulatory Compliance), a platform approach to digitalisation of regulatory compliance that takes into consideration the enormous diversity of all the stakeholders’ activities. A historical perspective on research in this area is first presented to put things in perspective which identifies the challenges in such an endeavour and identifies the gaps in state-of-the-art. After enumerating all the challenges in implementing a platform-based approach to digitalising the regulatory compliance process, the implementation of some parts of the platform is described. Our research demonstrates that the identification and extraction of all relevant requirements from the corpus of several hundred regulatory documents is a key part of the whole process which underlies the entire process from authoring to eventually compliance checking of designs. Some of the issues that need addressing in this endeavour include ambiguous language, inconsistent use of terms, contradicting requirements and handling multi-word expressions. The implementation of these tools is driven by NLP, ML and Semantic Web technologies. A semantic search engine was developed and validated against other popular and comparable engines with a corpus of 420 (out of about 800) documents used in the UK for compliance checking of building designs. In every search scenario, our search engine performed better on all objective criteria. Limitations of the approach are discussed which includes the challenges around licensing for all the documents in the corpus. Further work includes improving the performance of SPaR.txt (the tool created to identify multi-word expressions) as well as the information retrieval engine by increasing the dataset and providing the model with examples from more diverse formats of regulations. There is also a need to develop and align strategies to collect a comprehensive set of domain vocabularies to be combined in a Knowledge Graph.}
}
@article{VASSILIADES2024100816,
title = {Extraction of object-action and object-state associations from Knowledge Graphs},
journal = {Journal of Web Semantics},
volume = {81},
pages = {100816},
year = {2024},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100816},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000027},
author = {Alexandros Vassiliades and Theodore Patkos and Vasilis Efthymiou and Antonis Bikakis and Nick Bassiliades and Dimitris Plexousakis},
keywords = {Association extraction, Knowledge Graphs, Semantics-based extraction, Topology-based extraction, Linking entities},
abstract = {Infusing autonomous artificial systems with knowledge about the physical world they inhabit is a critical and long-held aim for the Artificial Intelligence community. Training systems with relevant data is a typical approach; however, finding the data required is not always possible, especially when much of this knowledge is commonsense. In this paper, we present a comparison of topology-based and semantics-based methods for extracting information about object-action and object-state association relations from knowledge graphs, such as ConceptNet, WordNet, ATOMIC, YAGO, WebChild and DBpedia. Moreover, we propose a novel method for extracting information about object-action and object-state associations from knowledge graphs. Our method is composed of a set of techniques for locating, enriching, evaluating, cleaning and exposing knowledge from such resources, relying on semantic similarity methods. Some important aspects of our method are the flexibility in deciding how to deal with the noise that exists in the data, and the capability to determine the importance of a path through training, rather than through manual annotation.}
}
@article{ALKABI201594,
title = {A novel root based Arabic stemmer},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {27},
number = {2},
pages = {94-103},
year = {2015},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2014.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S1319157815000166},
author = {Mohammed N. Al-Kabi and Saif A. Kazakzeh and Belal M. {Abu Ata} and Saif A. Al-Rababah and Izzat M. Alsmadi},
keywords = {Natural Language Processing (NLP), Computational intelligence, Stemming, Information retrieval},
abstract = {Stemming algorithms are used in information retrieval systems, indexers, text mining, text classifiers etc., to extract stems or roots of different words, so that words derived from the same stem or root are grouped together. Many stemming algorithms were built in different natural languages. Khoja stemmer is one of the known and widely used Arabic stemmers. In this paper, we introduced a new light and heavy Arabic stemmer. This new stemmer is presented in this study and compared with two well-known Arabic stemmers. Results showed that accuracy of our stemmer is slightly better than the accuracy yielded by each one of those two well-known Arabic stemmers used for evaluation and comparison. Evaluation tests on our novel stemmer yield 75.03% accuracy, while the other two Arabic stemmers yield slightly lower accuracy.}
}
@article{KEJRIWAL2021100052,
title = {A meta-engine for building domain-specific search engines},
journal = {Software Impacts},
volume = {7},
pages = {100052},
year = {2021},
issn = {2665-9638},
doi = {https://doi.org/10.1016/j.simpa.2020.100052},
url = {https://www.sciencedirect.com/science/article/pii/S2665963820300439},
author = {Mayank Kejriwal},
keywords = {Domain-specific search, Knowledge graphs, Information extraction, Complex domains, Knowledge discovery, Information retrieval},
abstract = {In recent years, domain-specific search (DSS) has emerged as a growing and important area of applied research in artificial intelligence (AI) and information retrieval (IR). Over the last 6 years of research, our group has developed a ‘meta-engine’ called myDIG (my Domain-specific Insight Graphs) that provides a relatively easy and customizable workflow for building DSSs without advanced technical training in crawling, information retrieval or user-interfaces. The myDIG system has been applied to some important and difficult use cases (most notably, fighting human trafficking), in addition to being used in classrooms by graduate students for building complex DSSs from scratch.}
}
@article{SHEN202099,
title = {Visual exploration of latent space for traditional Chinese music},
journal = {Visual Informatics},
volume = {4},
number = {2},
pages = {99-108},
year = {2020},
note = {PacificVis 2020 Workshop on Visualization Meets AI},
issn = {2468-502X},
doi = {https://doi.org/10.1016/j.visinf.2020.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S2468502X20300152},
author = {Jingyi Shen and Runqi Wang and Han-Wei Shen},
keywords = {Music information retrieval, Latent space analysis, Long Short-Term Memory, Autoencoder, Traditional Chinese music},
abstract = {Generating compact and effective numerical representations of data is a fundamental step for many machine learning tasks. Traditionally, handcrafted features are used but as deep learning starts to show its potential, using deep learning models to extract compact representations becomes a new trend. Among them, adopting vectors from the model’s latent space is the most popular. There are several studies focused on visual analysis of latent space in NLP and computer vision. However, relatively little work has been done for music information retrieval (MIR) especially incorporating visualization. To bridge this gap, we propose a visual analysis system utilizing Autoencoders to facilitate analysis and exploration of traditional Chinese music. Due to the lack of proper traditional Chinese music data, we construct a labeled dataset from a collection of pre-recorded audios and then convert them into spectrograms. Our system takes music features learned from two deep learning models (a fully-connected Autoencoder and a Long Short-Term Memory (LSTM) Autoencoder) as input. Through interactive selection, similarity calculation, clustering and listening, we show that the latent representations of the encoded data allow our system to identify essential music elements, which lay the foundation for further analysis and retrieval of Chinese music in the future.}
}
@article{VENUGOPAL2021100290,
title = {Looking through glass: Knowledge discovery from materials science literature using natural language processing},
journal = {Patterns},
volume = {2},
number = {7},
pages = {100290},
year = {2021},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2021.100290},
url = {https://www.sciencedirect.com/science/article/pii/S2666389921001239},
author = {Vineeth Venugopal and Sourav Sahoo and Mohd Zaki and Manish Agarwal and Nitya Nand Gosvami and N. M. Anoop Krishnan},
keywords = {natural language processing, artificial intelligence, glass science, materials science, knowledge discovery},
abstract = {Summary
Most of the knowledge in materials science literature is in the form of unstructured data such as text and images. Here, we present a framework employing natural language processing, which automates text and image comprehension and precision knowledge extraction from inorganic glasses’ literature. The abstracts are automatically categorized using latent Dirichlet allocation (LDA) to classify and search semantically linked publications. Similarly, a comprehensive summary of images and plots is presented using the caption cluster plot (CCP), providing direct access to images buried in the papers. Finally, we combine the LDA and CCP with chemical elements to present an elemental map, a topical and image-wise distribution of elements occurring in the literature. Overall, the framework presented here can be a generic and powerful tool to extract and disseminate material-specific information on composition–structure–processing–property dataspaces, allowing insights into fundamental problems relevant to the materials science community and accelerated materials discovery.}
}
@article{WANG201812,
title = {A comparison of word embeddings for the biomedical natural language processing},
journal = {Journal of Biomedical Informatics},
volume = {87},
pages = {12-20},
year = {2018},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2018.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S1532046418301825},
author = {Yanshan Wang and Sijia Liu and Naveed Afzal and Majid Rastegar-Mojarad and Liwei Wang and Feichen Shen and Paul Kingsbury and Hongfang Liu},
keywords = {Word embeddings, Natural language processing, Information extraction, Information retrieval, Machine learning},
abstract = {Background
Word embeddings have been prevalently used in biomedical Natural Language Processing (NLP) applications due to the ability of the vector representations being able to capture useful semantic properties and linguistic relationships between words. Different textual resources (e.g., Wikipedia and biomedical literature corpus) have been utilized in biomedical NLP to train word embeddings and these word embeddings have been commonly leveraged as feature input to downstream machine learning models. However, there has been little work on evaluating the word embeddings trained from different textual resources.
Methods
In this study, we empirically evaluated word embeddings trained from four different corpora, namely clinical notes, biomedical publications, Wikipedia, and news. For the former two resources, we trained word embeddings using unstructured electronic health record (EHR) data available at Mayo Clinic and articles (MedLit) from PubMed Central, respectively. For the latter two resources, we used publicly available pre-trained word embeddings, GloVe and Google News. The evaluation was done qualitatively and quantitatively. For the qualitative evaluation, we randomly selected medical terms from three categories (i.e., disorder, symptom, and drug), and manually inspected the five most similar words computed by embeddings for each term. We also analyzed the word embeddings through a 2-dimensional visualization plot of 377 medical terms. For the quantitative evaluation, we conducted both intrinsic and extrinsic evaluation. For the intrinsic evaluation, we evaluated the word embeddings’ ability to capture medical semantics by measruing the semantic similarity between medical terms using four published datasets: Pedersen’s dataset, Hliaoutakis’s dataset, MayoSRS, and UMNSRS. For the extrinsic evaluation, we applied word embeddings to multiple downstream biomedical NLP applications, including clinical information extraction (IE), biomedical information retrieval (IR), and relation extraction (RE), with data from shared tasks.
Results
The qualitative evaluation shows that the word embeddings trained from EHR and MedLit can find more similar medical terms than those trained from GloVe and Google News. The intrinsic quantitative evaluation verifies that the semantic similarity captured by the word embeddings trained from EHR is closer to human experts’ judgments on all four tested datasets. The extrinsic quantitative evaluation shows that the word embeddings trained on EHR achieved the best F1 score of 0.900 for the clinical IE task; no word embeddings improved the performance for the biomedical IR task; and the word embeddings trained on Google News had the best overall F1 score of 0.790 for the RE task.
Conclusion
Based on the evaluation results, we can draw the following conclusions. First, the word embeddings trained from EHR and MedLit can capture the semantics of medical terms better, and find semantically relevant medical terms closer to human experts’ judgments than those trained from GloVe and Google News. Second, there does not exist a consistent global ranking of word embeddings for all downstream biomedical NLP applications. However, adding word embeddings as extra features will improve results on most downstream tasks. Finally, the word embeddings trained from the biomedical domain corpora do not necessarily have better performance than those trained from the general domain corpora for any downstream biomedical NLP task.}
}
@article{ALSHALABI2022363,
title = {BPR algorithm: New broken plural rules for an Arabic stemmer},
journal = {Egyptian Informatics Journal},
volume = {23},
number = {3},
pages = {363-371},
year = {2022},
issn = {1110-8665},
doi = {https://doi.org/10.1016/j.eij.2022.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S1110866522000184},
author = {Hamood Alshalabi and Sabrina Tiun and Nazlia Omar and Elham {abdulwahab Anaam} and Yazid Saif},
keywords = {Arabic broken plural, Arabic root-based, Artificial intelligence, Arabic Root Extraction, Arabic morphological analyzer, Arabic corpus, Arabic text processing},
abstract = {One of the most important phases in text processing is stemming, whose aim is to aggregate all variations in a word into one group to aid natural language processing. The morphological structure of the Arabic language is more challenging than that of the English language; thus, it requires superior stemming algorithms for Arabic stemmers to be effective. One of the challenges is the irregular broken plural, which has been a problematic issue in Arabic natural language processing that affects the performance of Arabic information retrieval and other Arabic language engineering applications. Several studies have attempted to develop solutions to irregular plural problems, but the challenge remains, especially in extracting correct Arabic root words. In this paper, the broken plural rule (BPR) algorithm introduces new solutions to solve the problem in which an existing root-based method cannot extract correct roots by using their proposed rules. The BPR algorithm introduces several rules (main rules and subrules) to extract the correct roots of the Arabic irregular broken plural words. To evaluate the effectiveness of the BPR algorithm, we extracted roots from an Arabic standard dataset and applied the BPR algorithm as an enhancement to a root-based Arabic stemmer, ISRI. The obtained results from both evaluations showed encouraging results: (i) Only a few numbers of incorrect roots were stemmed on the large-sized Arabic word dataset. (ii) The enhanced root-based Arabic stemmer, ISRI + BPR, exhibited the best performance compared with the original ISRI stemmer and a well-known Arabic stemmer, ARLS 2. Thus, the proposed BPR algorithm has solved some of the irregular broken plural problems that eventually increase the performance of a root-based Arabic stemmer.}
}
@article{SEONG2023421,
title = {Retrieval methodology for similar NPP LCO cases based on domain specific NLP},
journal = {Nuclear Engineering and Technology},
volume = {55},
number = {2},
pages = {421-431},
year = {2023},
issn = {1738-5733},
doi = {https://doi.org/10.1016/j.net.2022.09.028},
url = {https://www.sciencedirect.com/science/article/pii/S1738573322004600},
author = {No Kyu Seong and Jae Hee Lee and Jong Beom Lee and Poong Hyun Seong},
keywords = {Technical Specifications, Limiting Conditions for Operation, TF-IDF, Similarity, Information Retrieval, NLP},
abstract = {Nuclear power plants (NPPs) have technical specifications (Tech Specs) to ensure that the equipment and key operating parameters necessary for the safe operation of the power plant are maintained within limiting conditions for operation (LCO) determined by a safety analysis. The LCO of Tech Specs that identify the lowest functional capability of equipment required for safe operation for a facility must be complied for the safe operation of NPP. There have been previous studies to aid in compliance with LCO relevant to rule-based expert systems; however, there is an obvious limit to expert systems for implementing the rules for many situations related to LCO. Therefore, in this study, we present a retrieval methodology for similar LCO cases in determining whether LCO is met or not met. To reflect the natural language processing of NPP features, a domain dictionary was built, and the optimal term frequency-inverse document frequency variant was selected. The retrieval performance was improved by adding a Boolean retrieval model based on terms related to the LCO in addition to the vector space model. The developed domain dictionary and retrieval methodology are expected to be exceedingly useful in determining whether LCO is met.}
}
@article{GIMENOBALLESTER2024246,
title = {El rol de la inteligencia artificial en la publicación científica: perspectivas desde la farmacia hospitalaria},
journal = {Farmacia Hospitalaria},
volume = {48},
number = {5},
pages = {246-251},
year = {2024},
issn = {1130-6343},
doi = {https://doi.org/10.1016/j.farma.2024.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S1130634324000965},
author = {Vicente Gimeno-Ballester and Cristina Trigo-Vicente},
keywords = {Inteligencia artificial, Publicación científica, Farmacia Hospitalaria, Escritura científica, Chatbots, Herramientas inteligencia artificial, Ética, Investigación, Artificial Intelligence, Scientific Publications, Hospital Pharmacy, Scientific Writing, Chatbots, AI Tools, Ethics, Research},
abstract = {Resumen
El artículo explora el impacto de la inteligencia artificial en la escritura científica, con especial atención a su aplicación en la farmacia hospitalaria. Se analizan herramientas de inteligencia artificial que optimizan la búsqueda de información, el análisis de la literatura, la calidad de la escritura y la redacción de manuscritos. Chatbots como Consensus, junto con plataformas como Scite y SciSpace, facilitan la búsqueda precisa en bases de datos científicas, ofreciendo respuestas con evidencia y referencias. SciSpace permite la generación de tablas comparativas y la formulación de preguntas sobre estudios, mientras que ResearchRabbit mapea la literatura científica para identificar tendencias. DeepL y ProWritingAid mejoran la calidad de la escritura al corregir errores gramaticales, de estilo y plagio. A.R.I.A. optimiza la gestión de referencias, mientras que Jenny AI ayuda a superar el bloqueo del escritor. Librerías de Python como LangChain permiten realizar búsquedas semánticas avanzadas y la creación de agentes. A pesar de sus beneficios, la inteligencia artificial plantea preocupaciones éticas como sesgos, desinformación y plagio. Se destaca la importancia de un uso responsable y la revisión crítica por expertos. En la farmacia hospitalaria, la inteligencia artificial puede mejorar la eficiencia y la precisión en la investigación y la comunicación científica. Los farmacéuticos pueden utilizar estas herramientas para mantenerse actualizados, mejorar la calidad de sus publicaciones, optimizar la gestión de la información y facilitar la toma de decisiones clínicas. En conclusión, la inteligencia artificial es una herramienta poderosa para la farmacia hospitalaria, siempre que se utilice de manera responsable y ética.
The article examines the impact of artificial intelligence on scientific writing, with a particular focus on its application in hospital pharmacy. It analyzes artificial intelligence tools that enhance information retrieval, literature analysis, writing quality, and manuscript drafting. Chatbots like Consensus, along with platforms such as Scite and SciSpace, enable precise searches in scientific databases, providing evidence-based responses and references. SciSpace facilitates the generation of comparative tables and the formulation of queries regarding studies, while ResearchRabbit maps the scientific literature to identify trends. Tools like DeepL and ProWritingAid improve writing quality by correcting grammatical, stylistic, and plagiarism errors. A.R.I.A. enhances reference management, and Jenny AI assists in overcoming writer's block. Python libraries such as LangChain enable advanced semantic searches and the creation of agents. Despite their benefits, artificial intelligence raises ethical concerns including biases, misinformation, and plagiarism. The importance of responsible use and critical review by experts is emphasized. In hospital pharmacy, artificial intelligence can enhance efficiency and precision in research and scientific communication. Pharmacists can use these tools to stay updated, enhance the quality of their publications, optimize information management, and facilitate clinical decision-making. In conclusion, artificial intelligence is a powerful tool for hospital pharmacy, provided it is used responsibly and ethically.}
}
@article{WANG2024100181,
title = {Comparing ChatGPT and clinical nurses’ performances on tracheostomy care: A cross-sectional study},
journal = {International Journal of Nursing Studies Advances},
volume = {6},
pages = {100181},
year = {2024},
issn = {2666-142X},
doi = {https://doi.org/10.1016/j.ijnsa.2024.100181},
url = {https://www.sciencedirect.com/science/article/pii/S2666142X24000080},
author = {Tongyao Wang and Juan Mu and Jialing Chen and Chia-Chin Lin},
keywords = {Generative artificial intelligence, ChatGPT, Education, Tracheostomy, Nursing},
abstract = {Background
The release of ChatGPT for general use in 2023 by OpenAI has significantly expanded the possible applications of generative artificial intelligence in the healthcare sector, particularly in terms of information retrieval by patients, medical and nursing students, and healthcare personnel.
Objective
To compare the performance of ChatGPT-3.5 and ChatGPT-4.0 to clinical nurses on answering questions about tracheostomy care, as well as to determine whether using different prompts to pre-define the scope of the ChatGPT affects the accuracy of their responses.
Design
Cross-sectional study.
Setting
The data collected from the ChatGPT was collected using the ChatGPT-3.5 and 4.0 using access provided by the University of Hong Kong. The data from the clinical nurses working in mainland China was collected using the Qualtrics survey program.
Participants
No participants were needed for collecting the ChatGPT responses. A total of 272 clinical nurses, with 98.5 % of them working in tertiary care hospitals in mainland China, were recruited using a snowball sampling approach.
Method
We used 43 tracheostomy care-related questions in a multiple-choice format to evaluate the performance of ChatGPT-3.5, ChatGPT-4.0, and clinical nurses. ChatGPT-3.5 and GPT-4.0 were both queried three times with the same questions by different prompts: no prompt, patient-friendly prompt, and act-as-nurse prompt. All responses were independently graded by two qualified otorhinolaryngology nurses on a 3-point accuracy scale (correct, partially correct, and incorrect). The Chi-squared test and Fisher exact test with post-hoc Bonferroni adjustment were used to assess the differences in performance between the three groups, as well as the differences in accuracy between different prompts.
Results
ChatGPT-4.0 showed significantly higher accuracy, with 64.3 % of responses rated as ‘correct’, compared to 60.5 % in ChatGPT-3.5 and 36.7 % in clinical nurses (X 2 = 74.192, p < .001). Except for the ‘care for the tracheostomy stoma and surrounding skin’ domain (X2 = 6.227, p = .156), scores from ChatGPT-3.5 and -4.0 were significantly better than nurses’ on domains related to airway humidification, cuff management, tracheostomy tube care, suction techniques, and management of complications. Overall, ChatGPT-4.0 consistently performed well in all domains, achieving over 50 % accuracy in each domain. Alterations to the prompt had no impact on the performance of ChatGPT-3.5 or -4.0.
Conclusion
ChatGPT may serve as a complementary medical information tool for patients and physicians to improve knowledge in tracheostomy care.
Tweetable abstract
ChatGPT-4.0 can answer tracheostomy care questions better than most clinical nurses. There is no reason nurses should not be using it.}
}
@article{ABBAS202255,
title = {Retrieval of behavior trees using map-and-reduce technique},
journal = {Egyptian Informatics Journal},
volume = {23},
number = {1},
pages = {55-64},
year = {2022},
issn = {1110-8665},
doi = {https://doi.org/10.1016/j.eij.2021.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S1110866521000402},
author = {Safia Abbas and Rania Hodhod and Mohamed El-Sheikh},
keywords = {Information retrieval, Behavior trees, Cognitive scripts, Map-and-reduce, Pharaoh},
abstract = {There has been an increased interest in the creation of AI social agents who possess complex behaviors that allow them to perform social interactions. Behavior trees provide a plan model execution that has been widely used to build complex behaviors for AI social agents. Behavior trees can be represented in the form of a memory structure known as cognitive scripts, which would allow them to evolve through further development over multiple exposure to repeated enactment of a particular behavior or similar ones. Behavior trees that share the same context will then be able to learn from each other resulting in new behavior trees with richer experience. The main challenge appears in the expensive cost of retrieving contextually similar behavior trees (scripts) from a repertoire of scripts to allow for that learning process to occur. This paper introduces a novel application of map-and-reduce technique to retrieve cognitive with low computational time and memory allocation. The paper focuses on the design of a corpus of cognitive scripts, as a knowledge engineering key challenge, and the application of map-and-reduce with semantic information to retrieve contextually similar cognitive scripts. The results are compared to other techniques used to retrieve cognitive scripts in the literature, such as Pharaoh which uses the least common parent (LCP) technique in its core. The results show that the map-and-reduce technique can be successfully used to retrieve cognitive scripts with high retrieval accuracy of 92.6%, in addition to being cost effective.}
}
@article{ABDULLAH2024764,
title = {Design of automated model for inspecting and evaluating handwritten answer scripts: A pedagogical approach with NLP and deep learning},
journal = {Alexandria Engineering Journal},
volume = {108},
pages = {764-788},
year = {2024},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2024.08.067},
url = {https://www.sciencedirect.com/science/article/pii/S1110016824009530},
author = {A.Sheik Abdullah and S. Geetha and A.B. {Abdul Aziz} and Utkarsh Mishra},
keywords = {Automated answer script evaluation, Natural language processing, Information retrieval models, Machine learning, Deep learning, Image processing, Artificial intelligence, Educational technology, pedagogical data modeling},
abstract = {We address common challenges examiners face, such as accidental question skipping, marking omissions, and potential bias in assessment. These issues often arise due to the necessity of examining scripts in separate sessions, driven by the high volume of examination materials. In response, we propose the implementation of a self-regulating examiner, harnessing contemporary technology to reduce examiner workload and mitigate the possibility of errors. This automated approach aims to ensure fairness and accuracy in evaluating response scripts, offering a promising solution to the challenges encountered by examiners in the field Our study introduces an innovative approach that seamlessly integrates technologies, including Optical Character Recognition (OCR) for text ex- traction, Natural Language Processing (NLP) for keyword analysis, and ma- chine learning for grading. The results of our method are efficiently presented through a user-friendly web application, providing a streamlined and understandable means for examiners to evaluate response scripts.}
}
@article{GIMENOBALLESTER2024T246,
title = {[Translated article] The role of artificial intelligence in scientific publishing: perspectives from hospital pharmacy},
journal = {Farmacia Hospitalaria},
volume = {48},
number = {5},
pages = {T246-T251},
year = {2024},
issn = {1130-6343},
doi = {https://doi.org/10.1016/j.farma.2024.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S1130634324001259},
author = {Vicente Gimeno-Ballester and Cristina Trigo-Vicente},
keywords = {Artificial intelligence, Scientific publications, Hospital pharmacy, Scientific writing, Chatbots, AI tools, Ethics, Research, Inteligencia Artificial, Publicación científica, Farmacia Hospitalaria, Escritura científica, Chatbots, Herramientas inteligencia Artificial, Ética, Investigación},
abstract = {The article examines the impact of artificial intelligence on scientific writing, with a particular focus on its application in hospital pharmacy. It analyses artificial intelligence tools that enhance information retrieval, literature analysis, writing quality, and manuscript drafting. Chatbots like Consensus, along with platforms such as Scite and SciSpace, enable precise searches in scientific databases, providing evidence-based responses and references. SciSpace facilitates the generation of comparative tables and the formulation of queries regarding studies, while ResearchRabbit maps the scientific literature to identify trends. Tools like DeepL and ProWritingAid improve writing quality by correcting grammatical, stylistic, and plagiarism errors. A.R.I.A. enhances reference management, and Jenny AI assists in overcoming writer's block. Python libraries such as langchain enable advanced semantic searches and the creation of agents. Despite their benefits, artificial intelligence raises ethical concerns including biases, misinformation, and plagiarism. The importance of responsible use and critical review by experts is emphasised. In hospital pharmacy, artificial intelligence can enhance efficiency and precision in research and scientific communication. Pharmacists can use these tools to stay updated, enhance the quality of their publications, optimise information management, and facilitate clinical decision-making. In conclusion, artificial intelligence is a powerful tool for hospital pharmacy, provided it is used responsibly and ethically.
Resumen
El artículo explora el impacto de la Inteligencia artificial en la escritura científica, con especial atención a su aplicación en la farmacia hospitalaria. Se analizan herramientas de inteligencia artificial que optimizan la búsqueda de información, el análisis de la literatura, la calidad de la escritura y la redacción de manuscritos. Chatbots como Consensus, junto con plataformas como Scite y SciSpace, facilitan la búsqueda precisa en bases de datos científicas, ofreciendo respuestas con evidencia y referencias. SciSpace permite la generación de tablas comparativas y la formulación de preguntas sobre estudios, mientras que ResearchRabbit mapea la literatura científica para identificar tendencias. DeepL y ProWritingAid mejoran la calidad de la escritura al corregir errores gramaticales, de estilo y plagio. A.R.I.A. optimiza la gestión de referencias, mientras que Jenny AI ayuda a superar el bloqueo del escritor. Librerías de Python como langchain permiten realizar búsquedas semánticas avanzadas y la creación de agentes. A pesar de sus beneficios, la inteligencia artificial plantea preocupaciones éticas como sesgos, desinformación y plagio. Se destaca la importancia de un uso responsable y la revisión crítica por expertos. En la farmacia hospitalaria, la inteligencia artificial puede mejorar la eficiencia y la precisión en la investigación y la comunicación científica. Los farmacéuticos pueden utilizar estas herramientas para mantenerse actualizados, mejorar la calidad de sus publicaciones, optimizar la gestión de la información y facilitar la toma de decisiones clínicas. En conclusión, la inteligencia artificial es una herramienta poderosa para la farmacia hospitalaria, siempre que se utilice de manera responsable y ética.}
}
@article{GERO2019100047,
title = {PMCVec: Distributed phrase representation for biomedical text processing},
journal = {Journal of Biomedical Informatics},
volume = {100},
pages = {100047},
year = {2019},
note = {Articles initially published in Journal of Biomedical Informatics: X 1-4, 2019},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.yjbinx.2019.100047},
url = {https://www.sciencedirect.com/science/article/pii/S2590177X19300460},
author = {Zelalem Gero and Joyce Ho},
keywords = {Phrase embeddings, Biomedical NLP, PubMed abstracts},
abstract = {Distributed semantic representation of biomedical text can be beneficial for text classification, named entity recognition, query expansion, human comprehension, and information retrieval. Despite the success of high-quality vector space models such as Word2Vec and GloVe, they only provide unigram word representations and the semantics for multi-word phrases can only be approximated by composition. This is problematic in biomedical text processing where technical phrases for diseases, symptoms, and drugs should be represented as single entities to capture the correct meaning. In this paper, we introduce PMCVec, an unsupervised technique that generates important phrases from PubMed abstracts and learns embeddings for single words and multi-word phrases simultaneously. Evaluations performed on benchmark datasets produce significant performance gains both qualitatively and quantitatively.}
}
@article{SINGH2021508,
title = {Morphological evaluation and sentiment analysis of Punjabi text using deep learning classification},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {33},
number = {5},
pages = {508-517},
year = {2021},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2018.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S1319157818300612},
author = {Jaspreet Singh and Gurvinder Singh and Rajinder Singh and Prithvipal Singh},
abstract = {Morphological processing of Indian languages is one of the most escalating fields in the era of Natural Language Processing (NLP) since the last decade. The evaluation of Asian languages is a highly relevant field in the times of text mining and information retrieval. The morphological evaluation of a text can be employed for extraction and classification of knowledge. This paper amalgamates morphological evaluation and sentiment prediction of Punjabi language text. The textual data for Punjabi language is concerned with farmer suicide cases reported for Punjab state of India. The pre-processing phase of this study involves morphological evaluation and normalization of Punjabi words to their respective canonical forms. The next phase carries out training and testing of deep neural network model on refined Punjabi tokens obtained from the earlier phase. The proposed model classifies Punjabi tokens into four negatively oriented classes tailored for farmer suicide cases. The average accuracies of sentiment prediction obtained after 10-fold cross validation are 93.85%, 88.53%, 83.3%, and 95.45% for the four respective classes. The proposed framework yields satisfactory results on 275 Punjabi text documents with the overall accuracy of 90.29% for sentiment classification.}
}
@article{FISZMAN2009801,
title = {Automatic summarization of MEDLINE citations for evidence-based medical treatment: A topic-oriented evaluation},
journal = {Journal of Biomedical Informatics},
volume = {42},
number = {5},
pages = {801-813},
year = {2009},
note = {Biomedical Natural Language Processing},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2008.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S1532046408001263},
author = {Marcelo Fiszman and Dina Demner-Fushman and Halil Kilicoglu and Thomas C. Rindflesch},
keywords = {Natural language processing, Semantic processing, Automatic summarization, Evidence-based medicine, Knowledge representation, Artificial intelligence, Evaluation},
abstract = {As the number of electronic biomedical textual resources increases, it becomes harder for physicians to find useful answers at the point of care. Information retrieval applications provide access to databases; however, little research has been done on using automatic summarization to help navigate the documents returned by these systems. After presenting a semantic abstraction automatic summarization system for MEDLINE citations, we concentrate on evaluating its ability to identify useful drug interventions for 53 diseases. The evaluation methodology uses existing sources of evidence-based medicine as surrogates for a physician-annotated reference standard. Mean average precision (MAP) and a clinical usefulness score developed for this study were computed as performance metrics. The automatic summarization system significantly outperformed the baseline in both metrics. The MAP gain was 0.17 (p<0.01) and the increase in the overall score of clinical usefulness was 0.39 (p<0.05).}
}
@article{PETHANI2023104282,
title = {Natural language processing for clinical notes in dentistry: A systematic review},
journal = {Journal of Biomedical Informatics},
volume = {138},
pages = {104282},
year = {2023},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2023.104282},
url = {https://www.sciencedirect.com/science/article/pii/S1532046423000035},
author = {Farhana Pethani and Adam G. Dunn},
keywords = {Dental informatics, Dental records, Dentistry, Information sciences, Natural language processing},
abstract = {Objective
To identify and synthesise research on applications of natural language processing (NLP) for information extraction and retrieval from clinical notes in dentistry.
Materials and methods
A predefined search strategy was applied in EMBASE, CINAHL and Medline. Studies eligible for inclusion were those that that described, evaluated, or applied NLP to clinical notes containing either human or simulated patient information. Quality of the study design and reporting was independently assessed based on a set of questions derived from relevant tools including CHecklist for critical Appraisal and data extraction for systematic Reviews of prediction Modelling Studies (CHARMS). A narrative synthesis was conducted to present the results.
Results
Of the 17 included studies, 10 developed and evaluated NLP methods and 7 described applications of NLP-based information retrieval methods in dental records. Studies were published between 2015 and 2021, most were missing key details needed for reproducibility, and there was no consistency in design or reporting. The 10 studies developing or evaluating NLP methods used document classification or entity extraction, and 4 compared NLP methods to non-NLP methods. The quality of reporting on NLP studies in dentistry has modestly improved over time.
Conclusions
Study design heterogeneity and incomplete reporting of studies currently limits our ability to synthesise NLP applications in dental records. Standardisation of reporting and improved connections between NLP methods and applied NLP in dentistry may improve how we can make use of clinical notes from dentistry in population health or decision support systems. Protocol Registration. PROSPERO CRD42021227823.}
}
@article{KIM20241,
title = {Assessing the performance of ChatGPT's responses to questions related to epilepsy: A cross-sectional study on natural language processing and medical information retrieval},
journal = {Seizure: European Journal of Epilepsy},
volume = {114},
pages = {1-8},
year = {2024},
issn = {1059-1311},
doi = {https://doi.org/10.1016/j.seizure.2023.11.013},
url = {https://www.sciencedirect.com/science/article/pii/S105913112300300X},
author = {Hyun-Woo Kim and Dong-Hyeon Shin and Jiyoung Kim and Gha-Hyun Lee and Jae Wook Cho},
keywords = {Epilepsy, Artificial intelligence, ChatGPT, Natural language processing},
abstract = {Background
Epilepsy is a neurological condition marked by frequent seizures and various cognitive and psychological effects. Reliable information is essential for effective treatment. Natural language processing models like ChatGPT are increasingly used in healthcare for information access and data analysis, making it crucial to assess their accuracy.
Objective
This study aimed to investigate the accuracy of ChatGPT in providing educational information related to epilepsy.
Methods
We compared the answers from ChatGPT-4 and ChatGPT-3.5 to 57 common epilepsy questions based on the Korean Epilepsy Society's "Epilepsy Patient and Caregiver Guide." Two epileptologists reviewed the responses, with a third serving as an arbiter in cases of disagreement.
Results
Out of 57 questions, 40 responses from ChatGPT-4 had "sufficient educational value," 16 were "correct but inadequate," and one was "mixed with correct and incorrect" information. No answers were entirely incorrect. GPT-4 generally outperformed GPT-3.5 and was often on par with or better than the official guide.
Conclusions
ChatGPT-4 shows promise as a tool for delivering reliable epilepsy-related information and could help alleviate the educational burden on healthcare professionals. Further research is needed to explore the benefits and limitations of using such models in medical contexts.}
}
@article{NAIR20241870,
title = {A Knowledge-Based Deep Learning Approach for Automatic Fake News Detection using BERT on Twitter},
journal = {Procedia Computer Science},
volume = {235},
pages = {1870-1882},
year = {2024},
note = {International Conference on Machine Learning and Data Engineering (ICMLDE 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.04.178},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924008548},
author = {Vinita Nair and Dr. Jyoti Pareek and Sanskruti Bhatt},
keywords = {Fake news, Knowledge-based, Twitter, SPO triple, sentiment polarity, topic modeling, Deep learning algorithms, BERT Transformer},
abstract = {Fake news generation and propagation is a major challenge of the digital age, resulting in various social impacts namely bandwagon, validity, echo chamber effects, deceiving the public with spams, misinformation, malicious content and many more. The widespread proliferation of fake news not only fosters misinformation but also undermines the credibility of news sources. The veracity of the information is a major concern at all the stages of generation, publication, and propagation. To comprehend the critical need for addressing this pervasive problem, this research paper presents a framework for automatic detection of fake news using a knowledge-based approach. An automatic fact checking mechanism is applied using concepts of Information Retrieval (IR), Natural Language Processing (NLP) and Graph theory. The knowledge base is generated using Twitter dataset, which basically contains four attributes: Subject-Predicate-Object (SPO) triplet, SPO sentiment polarity, SPO occurrence, and topic modeling. These attributes serve as pivotal indicators for the development of a knowledge base, subsequently employed to detect prevalent patterns and traits linked to deceptive or false information. We have employed Named Entity Recognition (NER) model to extract SPO triples and Latent Dirichlet Allocation (LDA) for topic modeling, thereby contributing to knowledge base generation. To evaluate the efficacy and efficiency of our proposed model, we utilize deep learning algorithms like RNN, GRU, LSTM, GPT-3 and BERT Transformer providing an acceptable level of accuracy. This research paper delivers valuable insights into addressing the proliferation of fake news on Twitter, employing data-driven approaches and advanced deep learning algorithms.}
}
@article{MASUDA2011281,
title = {Semantic Search based on the Online Integration of NLP Techniques},
journal = {Procedia - Social and Behavioral Sciences},
volume = {27},
pages = {281-290},
year = {2011},
note = {Computational Linguistics and Related Fields},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2011.10.609},
url = {https://www.sciencedirect.com/science/article/pii/S1877042811024360},
author = {Katsuya Masuda and Takuya Matsuzaki and Jun’ichi Tsujii},
keywords = {Information Retrieval, Semantic Search, Tag Annotations},
abstract = {This paper introduces a framework for semantic information retrieval based on the integration of various natural language processing (NLP) techniques, each of which annotates a base text with different kinds of information extracted from the text. Instead of running the NLP modules on the fly for individual search requests, the NLP modules are applied to the text in advance and the results are indexed in a way that enables flexible and efficient integration of them. The query language is based on a variant of the region algebra, in which we can specify a sub- structure in the annotated text that may involve different kinds of annotations. Given a query, the retrieval engine searches for the sub-structure by aggregating the different kinds of annotations through a search algorithm for the extended region algebra. We demonstrate the effectiveness and flexibility of the proposed framework through experiments with TREC Genomics Track data.}
}
@article{ZHAN2021100289,
title = {Structuring clinical text with AI: Old versus new natural language processing techniques evaluated on eight common cardiovascular diseases},
journal = {Patterns},
volume = {2},
number = {7},
pages = {100289},
year = {2021},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2021.100289},
url = {https://www.sciencedirect.com/science/article/pii/S2666389921001227},
author = {Xianghao Zhan and Marie Humbert-Droz and Pritam Mukherjee and Olivier Gevaert},
keywords = {clinical notes, cardiovascular disease, ICD-10 codes, natural language processing, interpretability},
abstract = {Summary
Free-text clinical notes in electronic health records are more difficult for data mining while the structured diagnostic codes can be missing or erroneous. To improve the quality of diagnostic codes, this work extracts diagnostic codes from free-text notes: five old and new word vectorization methods were used to vectorize Stanford progress notes and predict eight ICD-10 codes of common cardiovascular diseases with logistic regression. The models showed good performance, with TF-IDF as the best vectorization model showing the highest AUROC (0.9499–0.9915) and AUPRC (0.2956–0.8072). The models also showed transferability when tested on MIMIC-III data with AUROC from 0.7952 to 0.9790 and AUPRC from 0.2353 to 0.8084. Model interpretability was shown by the important words with clinical meanings matching each disease. This study shows the feasibility of accurately extracting structured diagnostic codes, imputing missing codes, and correcting erroneous codes from free-text clinical notes for information retrieval and downstream machine-learning applications.}
}
@article{GHAFAROLLAHI20241389,
title = {ProtAgents: protein discovery via large language model multi-agent collaborations combining physics and machine learning††Electronic supplementary information (ESI) available: The full records of different conversation experiments along with additional materials are provided as supplementary materials. See DOI: https://doi.org/10.1039/d4dd00013g},
journal = {Digital Discovery},
volume = {3},
number = {7},
pages = {1389-1409},
year = {2024},
issn = {2635-098X},
doi = {https://doi.org/10.1039/d4dd00013g},
url = {https://www.sciencedirect.com/science/article/pii/S2635098X24001153},
author = {Alireza Ghafarollahi and Markus J. Buehler},
abstract = {Designing de novo proteins beyond those found in nature holds significant promise for advancements in both scientific and engineering applications. Current methodologies for protein design often rely on AI-based models, such as surrogate models that address end-to-end problems by linking protein structure to material properties or vice versa. However, these models frequently focus on specific material objectives or structural properties, limiting their flexibility when incorporating out-of-domain knowledge into the design process or comprehensive data analysis is required. In this study, we introduce ProtAgents, a platform for de novo protein design based on Large Language Models (LLMs), where multiple AI agents with distinct capabilities collaboratively address complex tasks within a dynamic environment. The versatility in agent development allows for expertise in diverse domains, including knowledge retrieval, protein structure analysis, physics-based simulations, and results analysis. The dynamic collaboration between agents, empowered by LLMs, provides a versatile approach to tackling protein design and analysis problems, as demonstrated through diverse examples in this study. The problems of interest encompass designing new proteins, analyzing protein structures and obtaining new first-principles data – natural vibrational frequencies – via physics simulations. The concerted effort of the system allows for powerful automated and synergistic design of de novo proteins with targeted mechanical properties. The flexibility in designing the agents, on one hand, and their capacity in autonomous collaboration through the dynamic LLM-based multi-agent environment on the other hand, unleashes great potentials of LLMs in addressing multi-objective materials problems and opens up new avenues for autonomous materials discovery and design.}
}
@article{PS20202541,
title = {Towards an efficient Malayalam Named Entity Recognizer Analysis on the Challenges},
journal = {Procedia Computer Science},
volume = {171},
pages = {2541-2546},
year = {2020},
note = {Third International Conference on Computing and Network Communications (CoCoNet'19)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.04.275},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920312679},
author = {Sreeja {P S} and Anitha S Pillai},
keywords = {Named Entity Recognition, Natural Language Processing, Machine Learning, Recurrent Neural Network, Long short term memory},
abstract = {Named Entity Recognition (NER) also known as entity extraction plays an important role in identifying and classifying the named entities into different categories like name of person, place, organization, things, quantity, monetary value etc. that appear in a document. NER has applications in various Natural Language Processing (NLP) tasks such as information retrieval, question answering system, Machine Translation, Sentiment Analysis. NER systems can be developed using rule based, machine learning or hybrid approach. Now Deep learning is being used to develop efficient NER as these models are capable of learning patterns easily and efficiently. Quite a large number of work has been done in English compared to the work done for Indian Languages. The focus of this paper is to highlight the challenges in building an efficient NER for one of the south Indian language namely Malayalam. The different issues that we need to address in order to develop an efficient NER for Malayalam is presented.}
}
@article{BEELEN2024100668,
title = {Does conversation lead to better searches? Investigating single-shot and multi-turn spoken searches with children},
journal = {International Journal of Child-Computer Interaction},
volume = {41},
pages = {100668},
year = {2024},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2024.100668},
url = {https://www.sciencedirect.com/science/article/pii/S2212868924000369},
author = {Thomas Beelen and Roeland Ordelman and Khiet P. Truong and Vanessa Evers and Theo Huibers},
keywords = {Information retrieval, Conversational agents, Voice assistants, Spoken conversational search, Children},
abstract = {Interactive speech technologies such as Siri and Amazon Echo allow people to search for information via speech. These systems generally take a single spoken statement or query as input, which may not be enough to fully capture the user’s information need. Therefore, multi-turn conversations have been proposed to establish more detail. A conversational approach can particularly benefit children who experience more difficulty formulating precise queries. We carried out a study with 32 children comparing multi-turn conversations with single-shot (or Query-Response) interactions. We compared the descriptions of the information needs, as well as children’s search experience. Findings indicate that more elaborate descriptions of children’s information needs (more keywords) were gathered through conversational search, leading to more focused search results compared with the Query-Response search. Further analysis of children’s responses yielded insights into their preferences for conversing with speech-based interfaces. This paper also offers design and methodological recommendations for conversational agents that support children’s information search.}
}
@article{KONONOVA2021102155,
title = {Opportunities and challenges of text mining in materials research},
journal = {iScience},
volume = {24},
number = {3},
pages = {102155},
year = {2021},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2021.102155},
url = {https://www.sciencedirect.com/science/article/pii/S2589004221001231},
author = {Olga Kononova and Tanjin He and Haoyan Huo and Amalie Trewartha and Elsa A. Olivetti and Gerbrand Ceder},
keywords = {Data Analysis, Computing Methodology, Computational Materials Science, Materials Design},
abstract = {Summary
Research publications are the major repository of scientific knowledge. However, their unstructured and highly heterogenous format creates a significant obstacle to large-scale analysis of the information contained within. Recent progress in natural language processing (NLP) has provided a variety of tools for high-quality information extraction from unstructured text. These tools are primarily trained on non-technical text and struggle to produce accurate results when applied to scientific text, involving specific technical terminology. During the last years, significant efforts in information retrieval have been made for biomedical and biochemical publications. For materials science, text mining (TM) methodology is still at the dawn of its development. In this review, we survey the recent progress in creating and applying TM and NLP approaches to materials science field. This review is directed at the broad class of researchers aiming to learn the fundamentals of TM as applied to the materials science publications.}
}
@article{FERIDOONI2024100137,
title = {Development of a vascular surgery-specific artificial intelligence chat interface using retrieval-augmented generation: VASC.AI, a specialized vascular surgery chatbot},
journal = {JVS-Vascular Insights},
volume = {2},
pages = {100137},
year = {2024},
issn = {2949-9127},
doi = {https://doi.org/10.1016/j.jvsvi.2024.100137},
url = {https://www.sciencedirect.com/science/article/pii/S2949912724000850},
author = {Tiam Feridooni and Arshia P. Javidan and Daniyal N. Mahmood and Zoya Gomes and Andrew Dueck and Mark Wheatcroft and David Szalay},
keywords = {Artificial intelligence, Large language model, Retrieval-augmented generation, Natural language processing, Medical education, Vascular surgery},
abstract = {Background
Large language models (LLMs) exhibit considerable potential in processing educational content for vascular surgery. However, commercially available LLMs are not optimized for medical education and can generate inaccurate information or “hallucinations.” Retrieval-augmented generation (RAG) is an advanced architecture that integrates specialized vascular surgery data into LLMs. This approach customizes LLMs, potentially decreasing the generation of incorrect information and enhancing their educational usefulness.
Methods
This study evaluated the proficiency of baseline Chat Generative Pre-trained Transformer (GPT)-3.5, ChatGPT-4, and ChatGPT-4o models using 244 text-based multiple-choice questions from six VESAP-5 modules, covering aortoiliac disease, cerebrovascular disease, lower extremity disease, renal and mesenteric disease, vascular medicine, and venous disease. The questions were input directly into each model between November 2023 and May 2024. Incorrect responses were categorized as either logical errors or information errors. Additionally, a vascular surgery-specific LLM, VASC.AI, was developed using OpenAI's application programming interface combined with RAG. This model used a database of >200,000 clinical abstracts, guidelines, and landmark trials, vectorized into embeddings for information retrieval. VASC.AI's proficiency was assessed using the same Vascular Education and Self-Assessment Program questions and compared against the baseline models.
Results
ChatGPT-4o and ChatGPT-4 demonstrated improved performance over ChatGPT-3.5, with ChatGPT-4o achieving an average correct response rate of 77.7% ± 7.6%, ChatGPT-4 at 69.0% ± 4.9%, and ChatGPT-3.5 at 55.3% ± 4.3%. VASC.AI significantly outperformed all baseline models, achieving a correct response rate of 93.8% ± 2.4%. Detailed analysis showed that ChatGPT-3.5 had 34.5% ± 13.9% logical errors and 65.5% ± 13.9% information errors, ChatGPT-4 had 22.3% ± 12.7% logical errors and 76.5% ± 12.7% information errors, and ChatGPT-4o had 25.5% ± 9.7% logical errors and 74.7% ± 9.5% information errors. VASC.AI's incorrect responses were solely due to logical errors, demonstrating the efficacy of RAG in providing accurate, specialized information.
Conclusions
The integration of RAG into LLMs significantly improves their performance in specialized medical fields. VASC.AI, tailored with up-to-date vascular surgery-specific data, outperforms general purpose LLMs in answering complex vascular surgery questions. This approach has the potential to enhance medical education, patient care, and clinical decision-making, representing a significant advancement in the application of AI in vascular surgery.}
}
@article{KWAIK20182,
title = {A Lexical Distance Study of Arabic Dialects},
journal = {Procedia Computer Science},
volume = {142},
pages = {2-13},
year = {2018},
note = {Arabic Computational Linguistics},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.456},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918321562},
author = {Kathrein Abu Kwaik and Motaz Saad and Stergios Chatzikyriakidis and Simon Dobnik},
keywords = {Diglossia, Lexical Distance, Vector Space Model, Latent Semantic Indexing, Hellinger Distance},
abstract = {Diglossia is a very common phenomenon in Arabic-speaking communities, where the spoken language is different from both Classical Arabic (CA) and Modern Standard Arabic (MSA). The spoken language is characterised as a number of dialects used in everyday communication as well as informal writing. In this paper, we highlight the lexical relation between the MSA and Dialectal Arabic (DA) in more than one Arabic region. We conduct a computational cross dialectal lexical distance study to measure the similarities and differences between dialects and the MSA. We exploit several methods from Natural Language Processing (NLP) and Information Retrieval (IR) like Vector Space Model (VSM), Latent Semantic Indexing (LSI) and Hellinger Distance (HD), and apply them on different Arabic dialectal corpora. We measure the overlap among all the dialects and compute the frequencies of the most frequent words in every dialect. The results are informative and indicate that Levantine dialects are very similar to each other and furthermore, that Palestinian appears to be the closest to MSA.}
}
@article{KAJIWARA2024100251,
title = {AI literacy for ethical use of chatbot: Will students accept AI ethics?},
journal = {Computers and Education: Artificial Intelligence},
volume = {6},
pages = {100251},
year = {2024},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2024.100251},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X24000547},
author = {Yusuke Kajiwara and Kouhei Kawabata},
keywords = {AI literacy, Ethics, Chatbot, Large language models, Technology acceptance, Secondary education},
abstract = {In AI literacy education, there are few examples of education based on AI ethical principles, and limited knowledge exists regarding curriculum design that incorporates AI ethical principles and its effects. Therefore, in this study, we propose a curriculum that teaches the ethical use of large language models (LLM) such as ChatGPT and verify its impact on educational effectiveness and technology acceptance among students aged 12 to 24. The validation results show that the proposed curriculum particularly contributes to the understanding of LLM concepts and their ethical use in decision support. We also demonstrate that experience using ChatGPT influences the level of understanding of ethical usage. Additionally, students aged 12 to 18 may actively adopt ChatGPT responses in decision support, and careful consideration is needed when using LLMs in the 12- to 18-year-old age group. Using technology acceptance model, AI ethical principles were also examined to determine technology acceptance, and it was found that usefulness, justice and fairness, privacy, and data protection directly impact attitudes toward ChatGPT. It has also become clear that students feel uneasy about using their personal information for learning ChatGPT, even if they have consented to the use of their personal information. This result suggests that AI developers and providers need to handle personal information carefully to foster a positive AI attitude.}
}
@article{BOUZIANE2015366,
title = {Question Answering Systems: Survey and Trends},
journal = {Procedia Computer Science},
volume = {73},
pages = {366-375},
year = {2015},
note = {International Conference on Advanced Wireless Information and Communication Technologies (AWICT 2015)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915034663},
author = {Abdelghani Bouziane and Djelloul Bouchiha and Noureddine Doumi and Mimoun Malki},
keywords = {Question Answering System (QAS), Natural Language Processing (NLP), Information Retrieval, SPARQL, Semantic Web},
abstract = {The need to query information content available in various formats including structured and unstructured data (text in natural language, semi-structured Web documents, structured RDF data in the semantic Web, etc.) has become increasingly important. Thus, Question Answering Systems (QAS) are essential to satisfy this need. QAS aim at satisfying users who are looking to answer a specific question in natural language. In this paper we survey various QAS. We give also statistics and analysis. This can clear the way and help researchers to choose the appropriate solution to their issue. They can see the insufficiency, so that they can propose new systems for complex queries. They can also adapt or reuse QAS techniques for specific research issues.}
}
@article{HARTMANN2024120,
title = {Leveraging AI for Current Research Information Systems: Opportunities and Challenges},
journal = {Procedia Computer Science},
volume = {249},
pages = {120-130},
year = {2024},
note = {16th International Conference on Current Research Information Systems (CRIS 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.11.056},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924032678},
author = {Simone Hartmann and Daniel Niederlechner},
keywords = {AI in Research Management, Ethics in AI, CRIS Enhancement},
abstract = {Integrating Artificial Intelligence (AI) into Current Research Information Systems (CRIS) offers significant opportunities to enhance research management. This paper explores AI's potential to automate data handling, improve analytical capabilities, and enhance user experiences within CRIS. Key areas of impact include data enrichment, advanced information retrieval, trend analysis, and predictive analytics. The paper also addresses the challenges and ethical considerations of AI integration, such as data privacy, security, and algorithmic bias. Insights from a Live Poll at the CRIS2024 conference reveal high familiarity with AI among participants, optimism about its potential, and recognition of implementation challenges. By overcoming these obstacles, AI can transform CRIS, making research management more efficient and effective. The paper concludes by advocating for collaboration and dialogue to guide the responsible integration of AI in CRIS, ensuring alignment with stakeholder interests.}
}
@article{GAKIS202379,
title = {Extraction and normalization of IR indexing terms and phrases in a highly inflectional language},
journal = {Journal of Greek Linguistics},
volume = {23},
number = {1},
pages = {79-96},
year = {2023},
issn = {1566-5844},
doi = {https://doi.org/10.1163/15699846-02301001},
url = {https://www.sciencedirect.com/science/article/pii/S156658442300003X},
author = {Panagiotis Gakis and Theodoros Kokkinos and Christos Tsalidis},
keywords = {natural language processing for Information Retrieval, stemming/morphological analysis, phrase detection and use},
abstract = {Term-based indexing of documents is conventionally implemented by stemmers or their corpus-based improvements, both of which encode implicit linguistic information. Terms are directly derived from document content such that a unique indexing approach is available at indexing run-time. For highly inflectional languages where term variation is high, such techniques are more error-prone. The main focus of the current study is the extraction and normalization of single terms and phrases and the proposal of authenticated control of indexing. The proposed approach relies on the use of explicit linguistic knowledge, appropriately encoded in large language resources. Such control guarantees the highest possible expansion factor for indexing terms as well as indexing consistency. Moreover, it offers a framework where different and eventually contradicting indexing criteria can be practiced, conventional and Natural Language Processing (NLP)-based Information Retrieval (IR) applications can be served, while adaptations can be made for tuning to a specific domain or corpus.}
}
@article{DINGIL2024e33645,
title = {Understanding state-of-the-art situation of transport planning strategies in earthquake-prone areas by using AI-supported literature review methodology},
journal = {Heliyon},
volume = {10},
number = {13},
pages = {e33645},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e33645},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024096762},
author = {Ali Enes Dingil and Ondrej Pribyl},
keywords = {artificial intelligence, Information retrieval, AI-Supported review, Earth-quake prone areas, Seismic risk, Transport planning, Transport system},
abstract = {Aim
This review aims to explore earthquake-based transport strategies in seismic areas, providing state-of-the-art insights into the components necessary to guide urban planners and policymakers in their decision-making processes.
Outputs
The review provides a variety of methodologies and approaches employed for the reinforcement planning and emergency demand management to analyze and evaluate the impact of seismic events on transportation systems, in turn to develop strategies for preparedness, mitigation, response, and recovery phases. The selection of the appropriate approach depends on factors such as the specific transport system, urbanization level and type, built environment, and critical components involved.
Originality and value
Besides providing a distinctive illustration of the integration of transportation and seismic literature as a valuable consolidated resource, this article introduces a novel methodology named ALARM for conducting state-of-the-art reviews on any topic, incorporating AI through the utilization of large language models (LLMs) built upon transformer deep neural networks, along with indexing data structures (in this study mainly OPEN-AI DAVINCI-003 model and vector-storing index). Hence, it is of paramount significance as the first instance of implementing LLMs within academic review standards. This paves the way for the potential integration of AI and human collaboration to become a standard practice under enhanced criteria for comprehending and analyzing specific information.}
}
@article{PASCAZIO2024105428,
title = {Question-answering system for combustion kinetics},
journal = {Proceedings of the Combustion Institute},
volume = {40},
number = {1},
pages = {105428},
year = {2024},
issn = {1540-7489},
doi = {https://doi.org/10.1016/j.proci.2024.105428},
url = {https://www.sciencedirect.com/science/article/pii/S1540748924002360},
author = {Laura Pascazio and Dan Tran and Simon D. Rihm and Jiaru Bai and Sebastian Mosbach and Jethro Akroyd and Markus Kraft},
keywords = {Automated kinetic modeling, Reaction mechanism, Cheminformatics, Artificial intelligence, Knowledge graph},
abstract = {In this paper, we introduce for the first time a natural language question-answering (QA) system specifically designed for the field of combustion kinetics. This system marks a significant step towards achieving the PrIMe vision as outlined by Frenklach in 2007, offering a user-friendly interface that allows researchers and practitioners to easily access and query information about chemical mechanisms. This QA system is a key component of “The World Avatar” (TWA), a dynamic framework built upon semantic web technologies. TWA is characterized by its layered structure, which includes a knowledge graph (KG), software agents, and real-world data integration. These layers collectively create a comprehensive unified system for managing and analyzing complex chemical data from various domains. We detail the enhancements made to TWA’s ontologies (OntoSpecies, OntoKin, and OntoCompChem) to meet specific challenges in chemical kinetics and improve their representation accuracy. By focusing on data provenance and interoperability, our approach ensures transparent and reliable data management that adheres to the FAIR principles, which is vital for precise information retrieval and analysis. The role of software agents in populating these ontologies is highlighted, showcasing how they transform raw data into meaningful structured knowledge and generate new insights within the TWA ecosystem. Additionally, the semantic web technologies’ interoperability feature facilitates data integration and exchange across different platforms and tools, making the data machine-actionable. We instantiated in the KG data on four H2/O2 and five CH4/O2 reaction mechanisms taken from the literature and we then demonstrate the QA system’s capabilities in answering questions related to these reaction mechanisms as a proof of concept. Lastly, we discuss the future directions of the TWA framework, which include not only future extensions of the QA system but also the integration of external tool to automate tasks such as generation of kinetic mechanism, further expanding TWA’s functionality and application in the field of chemical kinetics.}
}
@article{BHATTACHARYA20232723,
title = {Improving biomedical named entity recognition through transfer learning and asymmetric tri-training},
journal = {Procedia Computer Science},
volume = {218},
pages = {2723-2733},
year = {2023},
note = {International Conference on Machine Learning and Data Engineering},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.01.244},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923002442},
author = {Medha Bhattacharya and Swati Bhat and Sirshasree Tripathy and Anvita Bansal and Monika Choudhary},
keywords = {Electronic Health Records, Biomedical Named Entity Recognition, Machine Learning, BiLSTM-CRF, Transfer Learning, Asymmetric tri-training},
abstract = {Today, electronic health records have turned into prime sources of information for physicians looking after their patients. EHRs and computerized patient data resources have expedited the accelerated discovery of formerly obscure biomedical and clinical information. Because of the lengthy, error-prone, non-scalable, and expensive manual abstraction process, natural language processing (NLP) procedures are being wielded more and more in biomedical and clinical fields. One of the building blocks of all NLP systems, Named Entity Recognition (NER) is considered a sub-activity of information retrieval. To extract biomedical knowledge from electronic health records, a prerequisite is the efficient recognition of biomedical entities. Deep-learning techniques have gained more and more consideration recently for the above-mentioned task. Notwithstanding, these methods are based on high-caliber, high-cost, labeled data. In this work, a biomedical-named entity recognition model based on transfer learning and asymmetric tri-training is proposed to diminish the limited annotated data problem in the biomedical-named entity recognition domain. The proposed model showed a significant improvement of more than 9% over the baseline BiLSTM-CRF model in the exact F1 scores on four different datasets considered in this work.}
}
@article{DAGA2020123,
title = {Prediction of Likes and Retweets Using Text Information Retrieval},
journal = {Procedia Computer Science},
volume = {168},
pages = {123-128},
year = {2020},
note = {“Complex Adaptive Systems”Malvern, PennsylvaniaNovember 13-15, 2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.02.273},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920304129},
author = {Ishita Daga and Anchal Gupta and Raj Vardhan and Partha Mukherjee},
keywords = {TF-IDF, Doc2Vec, Text Mining, Twitter, Predictive Modeling of Words},
abstract = {Twitter is one of the major social media platforms today to study human behaviours by analysing their interactions. To ensure popularity of the tweet, the focus should be on the content of the tweet that results in numerous followings of that message with sufficient number of likes and retweets. The high quality of tweets, increases the online reputation of the users who post it. If a user can get the prediction of likes and retweets on his text before posting it on the internet, it would improve the popularity of the tweet from information sharing perspective. In this paper we employed different machine learning classifiers like SVM, Naïve Bayes, Logistic Regression, Random Forest, and Neural Network, on top of two different text processing approaches used in NLP (natural language processing), namely bag-of-words (TFIDF) and word embeddings (Doc2Vec), to check how many likes and retweets can a tweet generate. The results obtained indicate that all the models performed 10-15% better with the bag-of-word technique.}
}
@article{LANDSCHAFT2024105531,
title = {Implementation and evaluation of an additional GPT-4-based reviewer in PRISMA-based medical systematic literature reviews},
journal = {International Journal of Medical Informatics},
volume = {189},
pages = {105531},
year = {2024},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2024.105531},
url = {https://www.sciencedirect.com/science/article/pii/S1386505624001941},
author = {Assaf Landschaft and Dario Antweiler and Sina Mackay and Sabine Kugler and Stefan Rüping and Stefan Wrobel and Timm Höres and Hector Allende-Cid},
keywords = {Systematic literature review, PRISMA, GPT-4 API, AI-based reviewer},
abstract = {Background
PRISMA-based literature reviews require meticulous scrutiny of extensive textual data by multiple reviewers, which is associated with considerable human effort.
Objective
To evaluate feasibility and reliability of using GPT-4 API as a complementary reviewer in systematic literature reviews based on the PRISMA framework.
Methodology
A systematic literature review on the role of natural language processing and Large Language Models (LLMs) in automatic patient-trial matching was conducted using human reviewers and an AI-based reviewer (GPT-4 API). A RAG methodology with LangChain integration was used to process full-text articles. Agreement levels between two human reviewers and GPT-4 API for abstract screening and between a single reviewer and GPT-4 API for full-text parameter extraction were evaluated.
Results
An almost perfect GPT–human reviewer agreement in the abstract screening process (Cohen’s kappa > 0.9) and a lower agreement in the full-text parameter extraction were observed.
Conclusion
As GPT-4 has performed on a par with human reviewers in abstract screening, we conclude that GPT-4 has an exciting potential of being used as a main screening tool for systematic literature reviews, replacing at least one of the human reviewers.}
}
@article{DOOTIO2021468,
title = {Development of Sindhi text corpus},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {33},
number = {4},
pages = {468-475},
year = {2021},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2019.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S1319157818311649},
author = {Mazhar Ali Dootio and Asim Imdad Wagan},
keywords = {Text corpus, NLP, Sindhi, DTM, TF-IDF},
abstract = {Sindhi language is a rich language with plenty of literary and general texts. There are number of books, newspapers, magazines and internet material available to develop Sindhi text corpus but yet proper and useful text corpus could not be developed and presented online for research, language features analysis, linguistics analysis and information retrieval systems. The lack of resources for research on computational linguistics and NLP applications for Sindhi language are challenging tasks at this stage. However, we have developed Sindhi text corpora in order to provide text resources to computational linguists, Natural Languages process (NLP) experts and researchers. Online books, newspapers, magazines, blogs and social websites are utilized to build Sindhi text corpus. Sindhi sentiment based text corpus is developed and analyzed with Document Term Matrix and TF-IDF models using 2-gram technique of n-gram model. The corpus may be useful for research on language variation analysis, sentiment analysis, aspect based sentiment analysis, semantic analysis, machine translation, information retrieval, Word2Vec, topic modeling and cluster analysis.}
}
@article{TURCHET2024103340,
title = {Musician-AI partnership mediated by emotionally-aware smart musical instruments},
journal = {International Journal of Human-Computer Studies},
volume = {191},
pages = {103340},
year = {2024},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2024.103340},
url = {https://www.sciencedirect.com/science/article/pii/S107158192400123X},
author = {Luca Turchet and Domenico Stefani and Johan Pauwels},
keywords = {Music information retrieval, Music emotion recognition, Smart musical instruments, Transfer learning, Context-aware computing, Trustworthy AI},
abstract = {The integration of emotion recognition capabilities within musical instruments can spur the emergence of novel art formats and services for musicians. This paper proposes the concept of emotionally-aware smart musical instruments, a class of musical devices embedding an artificial intelligence agent able to recognize the emotion contained in the musical signal. This spurs the emergence of novel services for musicians. Two prototypes of emotionally-aware smart piano and smart electric guitar were created, which embedded a recognition method for happiness, sadness, relaxation, aggressiveness and combination thereof. A user study, conducted with eleven pianists and eleven electric guitarists, revealed the strengths and limitations of the developed technology. On average musicians appreciated the proposed concept, who found its value in various musical activities. Most of participants tended to justify the system with respect to erroneous or partially erroneous classifications of the emotions they expressed, reporting to understand the reasons why a given output was produced. Some participants even seemed to trust more the system than their own judgments. Conversely, other participants requested to improve the accuracy, reliability and explainability of the system in order to achieve a higher degree of partnership with it. Our results suggest that, while desirable, perfect prediction of the intended emotion is not an absolute requirement for music emotion recognition to be useful in the construction of smart musical instruments.}
}
@article{GRECHISHCHEVA2019142,
title = {Risk markers identification in EHR using natural language processing: hemorrhagic and ischemic stroke cases},
journal = {Procedia Computer Science},
volume = {156},
pages = {142-149},
year = {2019},
note = {8th International Young Scientists Conference on Computational Science, YSC2019, 24-28 June 2019, Heraklion, Greece},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.08.189},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919311081},
author = {Sofia Grechishcheva and Egor Efimov and Oleg Metsker},
keywords = {Natural language processing, data extraction, latent semantic analysis, electronic health records, hemorrhagic stroke, ischemic stroke},
abstract = {This article describes the study results in the development of the method of analysis of semi-structured data from electronic health records to improve the quality of data describing patients’ treatment processes. Improving the accuracy of information retrieval from electronic medical records was achieved by using developed problem-solving oriented library. Moreover, the latent-semantic analysis of the electronic health records of chronic patients with chronic heart failure, diabetes mellitus, hypertension was performed. The main tokens characterizing different groups of patients were revealed. The developed library and semantic analysis based on it can be used to accurately automatic extraction of information from semi-structured electronic medical records. Automated markup of medical texts on the Russian language is also possible for the development of artificial intelligence systems and new generation clinical decision support systems.}
}
@article{ALMUZAINI2023101695,
title = {TaSbeeb: A judicial decision support system based on deep learning framework},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {35},
number = {8},
pages = {101695},
year = {2023},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2023.101695},
url = {https://www.sciencedirect.com/science/article/pii/S1319157823002495},
author = {Huda A. Almuzaini and Aqil M. Azmi},
keywords = {Arabic NLP, Deep learning, Legal AI, Judicial decision support system},
abstract = {Since the early 1980s, the legal domain has shown a growing interest in Artificial Intelligence approaches to tackle the increasing number of cases worldwide. TaSbeeb is a deep learning (DL)-based judicial decision support system (JDSS) designed for legal professionals in Saudi courts by retrieving judicial reasoning, Qur’anic verses, and hadiths from a knowledge base. The proposed system consists of three phases: annotation, classification, and information retrieval. To annotate judicial text, we developed Ann-Judicial, a semi-automatic method. To handle the imbalanced corpus for classification, we devised homogeneous and heterogeneous stacking DL models. For information retrieval, we proposed Jud_RoBERTa, a judicial language model. TaSbeeb achieved high accuracy and F-scores in both the classification and information retrieval blocks, showing good accuracy despite complexities in the judicial field and interference between cases. Specifically, the classification phase achieved an accuracy and F-score of 95.8%, while the information retrieval phase achieved an accuracy of 79.8% and F-score of 79.3%. The proposed JDSS has potential for extension to other courts and can be used in judicial inspection. TaSbeeb represents a significant stride towards a more efficient and accurate judicial decision-making process in the Arabic legal system, which has been hindered by a lack of research on Arabic JDSS.}
}
@article{WAN2023100504,
title = {Implantable QR code subcutaneous microchip using photoacoustic and ultrasound microscopy for secure and convenient individual identification and authentication},
journal = {Photoacoustics},
volume = {31},
pages = {100504},
year = {2023},
issn = {2213-5979},
doi = {https://doi.org/10.1016/j.pacs.2023.100504},
url = {https://www.sciencedirect.com/science/article/pii/S2213597923000575},
author = {Nan Wan and Pengcheng Zhang and Zuheng Liu and Zhe Li and Wei Niu and Xiuye Rui and Shibo Wang and Myeongsu Seong and Pengbo He and Siqi Liang and Jiasheng Zhou and Rui Yang and Sung-Liang Chen},
keywords = {Individual identification and authentication, Implantable devices, Quick response code, Ultrasound microscopy, Acoustic-resolution photoacoustic microscopy},
abstract = {Individual identification and authentication techniques are merged into many aspects of human life with various applications, including access control, payment or banking transfer, and healthcare. Yet conventional identification and authentication methods such as passwords, biometrics, tokens, and smart cards suffer from inconvenience and/or insecurity. Here, inspired by quick response (QR) code and implantable microdevices, implantable and minimally-invasive QR code subcutaneous microchips (QRC-SMs) are proposed to be an effective approach to carry useful and private information, thus enabling individual identification and authentication. Two types of QRC-SMs, QRC-SMs with “hole” and “flat” elements and QRC-SMs with “titanium-coated” and “non-coated” elements, are designed and fabricated to store personal information. Corresponding ultrasound microscopy and photoacoustic microscopy are used for imaging the QR code pattern underneath skin, and open-source artificial intelligence algorithm is applied for QR code detection and recognition. Ex vivo experiments under tissue and in vivo experiments with QRC-SMs implanted in live mice have been performed, demonstrating successful information retrieval from implanted QRC-SMs. QRC-SMs are hidden subcutaneously and invisible to the eyes. They cannot be forgotten, misplaced or lost, and can always be ready for timely medical identification, access control, and payment or banking transfer. Hence, QRC-SMs provide promising routes towards private, secure, and convenient individual identification and authentication.}
}
@article{SNIEGULA2019260,
title = {Study of Named Entity Recognition methods in biomedical field},
journal = {Procedia Computer Science},
volume = {160},
pages = {260-265},
year = {2019},
note = {The 10th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2019) / The 9th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.466},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919316813},
author = {Anna Śniegula and Aneta Poniszewska-Marańda and Łukasz Chomątek},
keywords = {Machine learning, Natural Language Processing, recurrent neural networks, Named Entity Recognition, Conditional Random Fields, UMLS, Long-Short Term Memory},
abstract = {Natural Language Processing (NLP) is very important in modern data processing taking into consideration different sources, forms and purpose of data as well as information in different areas our industry, administration, public and private life. Our studies concern Natural Language Processing techniques in biomedical field. The increasing volume of information stored in medical health record databases both in natural language and in structured forms is creating increasing challenges for information retrieval (IR) technologies. The paper presents the comparison study of chosen Named Entity Recognition techniques for biomedical field.}
}
@article{KRAFT2003145,
title = {Rules and fuzzy rules in text: concept, extraction and usage},
journal = {International Journal of Approximate Reasoning},
volume = {34},
number = {2},
pages = {145-161},
year = {2003},
note = {Soft Computing Applications to Intelligent Information Retrieval on the Internet},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2003.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X03000896},
author = {D.H. Kraft and M.J. Martı́n-Bautista and J. Chen and D. Sánchez},
keywords = {Rules, Fuzzy logic, Information retrieval, Association rules},
abstract = {Several concepts and techniques have been imported from other disciplines such as Machine Learning and Artificial Intelligence to the field of textual data. In this paper, we focus on the concept of rule and the management of uncertainty in text applications. The different structures considered for the construction of the rules, the extraction of the knowledge base and the applications and usage of these rules are detailed. We include a review of the most relevant works of the different types of rules based on their representation and their application to most of the common tasks of Information Retrieval such as categorization, indexing and classification.}
}
@article{FOX1987151,
title = {Architecture of an expert system for composite document analysis, representation, and retrieval},
journal = {International Journal of Approximate Reasoning},
volume = {1},
number = {2},
pages = {151-175},
year = {1987},
issn = {0888-613X},
doi = {https://doi.org/10.1016/0888-613X(87)90012-0},
url = {https://www.sciencedirect.com/science/article/pii/0888613X87900120},
author = {Edward A Fox and Robert K France},
keywords = {information retrieval, artificial intelligence, distributed expert system, knowledge bases, blackboard architecture, lexicon construction},
abstract = {The CODER (COmposite Document Expert/extended/effective Retrieval) project is a multi-yeare effort to investigate how best to apply artificial intelligence methods to increase the effectiveness of information retrieval systems handling collections of composite documents. To ensure system adaptability and to allow controlled experimentation, CODER has been designed as a distributed expert system. The use of individually tailored specialist experts, coupled with standardized blackboard modules for communication and control and external knowledge bases for maintenance of factual world knowledge, allows for quick prototyping, incremental development, and flexibility under change. The system as a whole is being implemented under UNIX as a set of MU-Prolog and C modules communicating through pipes and TCP/IP sockets.}
}
@article{ALHASAN2018158,
title = {POS Tagging for Arabic Text Using Bee Colony Algorithm},
journal = {Procedia Computer Science},
volume = {142},
pages = {158-165},
year = {2018},
note = {Arabic Computational Linguistics},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.471},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918321732},
author = {Ahmad Alhasan and Ahmad T. Al-Taani},
keywords = {Text Summarization, POS Tagging, Question Answering, Bee Colony Algorithm, Meta-heuristics Optimization Algorithms},
abstract = {Part-of-Speech (POS) Tagging is the process of automatically determining the proper grammatical tag or syntactic category of a word depending on a its context. POS Tagging is an essential step in most Natural Language Processing (NLP) applications such as text summarization, question answering, information extraction and information retrieval. In this study, we propose an efficient tagging approach for the Arabic language using Bee Colony Optimization algorithm. The problem is represented as a graph and a novel technique is proposed to assign scores to possible tags of a sentence, then the bees find the best solution path. The proposed approach is evaluated using KALIMAT corpus which consists of 18M words. Experimental results showed that the proposed approach achieved 98.2% of accuracy compared to 98%, 97.4% and 94.6% for Hybrid, Hidden Markov Model and Rule-Based methods respectively. Furthermore, the proposed approach determined all the tags presented in the corpus while the mentioned approaches can identify only three tags.}
}
@article{MATHKOR2024559,
title = {Multirole of the internet of medical things (IoMT) in biomedical systems for managing smart healthcare systems: An overview of current and future innovative trends},
journal = {Journal of Infection and Public Health},
volume = {17},
number = {4},
pages = {559-572},
year = {2024},
issn = {1876-0341},
doi = {https://doi.org/10.1016/j.jiph.2024.01.013},
url = {https://www.sciencedirect.com/science/article/pii/S1876034124000194},
author = {Darin Mansor Mathkor and Noof Mathkor and Zaid Bassfar and Farkad Bantun and Petr Slama and Faraz Ahmad and Shafiul Haque},
keywords = {Internet of medical things (IoMT), Biomedical systems, Smart healthcare systems, IoT in healthcare, Real-time patient data},
abstract = {Internet of Medical Things (IoMT) is an emerging subset of Internet of Things (IoT), often called as IoT in healthcare, refers to medical devices and applications with internet connectivity, is exponentially gaining researchers’ attention due to its wide-ranging applicability in biomedical systems for Smart Healthcare systems. IoMT facilitates remote health biomedical system and plays a crucial role within the healthcare industry to enhance precision, reliability, consistency and productivity of electronic devices used for various healthcare purposes. It comprises a conceptualized architecture for providing information retrieval strategies to extract the data from patient records using sensors for biomedical analysis and diagnostics against manifold diseases to provide cost-effective medical solutions, quick hospital treatments, and personalized healthcare. This article provides a comprehensive overview of IoMT with special emphasis on its current and future trends used in biomedical systems, such as deep learning, machine learning, blockchains, artificial intelligence, radio frequency identification, and industry 5.0.}
}
@article{WANG2016379,
title = {A Part-Of-Speech term weighting scheme for biomedical information retrieval},
journal = {Journal of Biomedical Informatics},
volume = {63},
pages = {379-389},
year = {2016},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2016.08.026},
url = {https://www.sciencedirect.com/science/article/pii/S1532046416301125},
author = {Yanshan Wang and Stephen Wu and Dingcheng Li and Saeed Mehrabi and Hongfang Liu},
keywords = {Biomedical information retrieval, Natural language processing, Part-Of-Speech, Bag-of-word, Markov random field},
abstract = {In the era of digitalization, information retrieval (IR), which retrieves and ranks documents from large collections according to users’ search queries, has been popularly applied in the biomedical domain. Building patient cohorts using electronic health records (EHRs) and searching literature for topics of interest are some IR use cases. Meanwhile, natural language processing (NLP), such as tokenization or Part-Of-Speech (POS) tagging, has been developed for processing clinical documents or biomedical literature. We hypothesize that NLP can be incorporated into IR to strengthen the conventional IR models. In this study, we propose two NLP-empowered IR models, POS-BoW and POS-MRF, which incorporate automatic POS-based term weighting schemes into bag-of-word (BoW) and Markov Random Field (MRF) IR models, respectively. In the proposed models, the POS-based term weights are iteratively calculated by utilizing a cyclic coordinate method where golden section line search algorithm is applied along each coordinate to optimize the objective function defined by mean average precision (MAP). In the empirical experiments, we used the data sets from the Medical Records track in Text REtrieval Conference (TREC) 2011 and 2012 and the Genomics track in TREC 2004. The evaluation on TREC 2011 and 2012 Medical Records tracks shows that, for the POS-BoW models, the mean improvement rates for IR evaluation metrics, MAP, bpref, and P@10, are 10.88%, 4.54%, and 3.82%, compared to the BoW models; and for the POS-MRF models, these rates are 13.59%, 8.20%, and 8.78%, compared to the MRF models. Additionally, we experimentally verify that the proposed weighting approach is superior to the simple heuristic and frequency based weighting approaches, and validate our POS category selection. Using the optimal weights calculated in this experiment, we tested the proposed models on the TREC 2004 Genomics track and obtained average of 8.63% and 10.04% improvement rates for POS-BoW and POS-MRF, respectively. These significant improvements verify the effectiveness of leveraging POS tagging for biomedical IR tasks.}
}
@article{ALSHALABI20226635,
title = {Arabic light-based stemmer using new rules},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {9},
pages = {6635-6642},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2021.08.017},
url = {https://www.sciencedirect.com/science/article/pii/S1319157821002202},
author = {Hamood Alshalabi and Sabrina Tiun and Nazlia Omar and Fatima N. AL-Aswadi and Kamal {Ali Alezabi}},
keywords = {Arabic stemmer, Arabic light stemmer, Arabic information retrieval, Suffix and prefix stripping, Arabic corpus},
abstract = {Superior stemming algorithms aid significantly in many natural language processing (NLP) applications such as information retrieval. Arabic light-based stemmer is one of the most important stemming algorithms. However, partially due to the highly inflected and complexity of Arabic language morphological structure, most of the existing Arabic light-based stemmer algorithms eliminate a few numbers of suffixes and prefixes or both in the process of recognising the infix patterns to determine roots. The elimination of suffixes and prefixes leads to many inefficient results. Hence, this study aims to develop an improved light-based algorithm of the Arabic stemmer by proposing an appropriate suffixes and prefixes list, which is supported by rules according to word length (without using a morpheme or patterns on a stem). Our improved Dlight Arabic stemmer focuses on determining and removing the infix patterns under many rules on length-words and according to a specific order of the stages of the stemming to extract the double, triple and quadruple roots from long and short Arabic words. To evaluate our proposed light-based Arabic stemmer, we compared our stemmer against existing Arabic stemmers, namely Light10, Condlight and ARLST. The experimental results showed the proposed Develop Arabic Light-Based Stemmer (Dlight) obtained the best performance with 68% of F-measure, while the other three Arabic stemmers yield slightly lower F-measure. Finally, establishing an appropriate list of suffixes and prefixes with word length rules to stem Arabic words can improve the performance of a light-based Arabic stemmer.}
}
@article{GOPALAKRISHNAN2019103141,
title = {A survey on literature based discovery approaches in biomedical domain},
journal = {Journal of Biomedical Informatics},
volume = {93},
pages = {103141},
year = {2019},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2019.103141},
url = {https://www.sciencedirect.com/science/article/pii/S1532046419300590},
author = {Vishrawas Gopalakrishnan and Kishlay Jha and Wei Jin and Aidong Zhang},
keywords = {Literature based discovery, MEDLINE, Text-mining, Semantic knowledge, Hypothesis generation},
abstract = {Literature Based Discovery (LBD) refers to the problem of inferring new and interesting knowledge by logically connecting independent fragments of information units through explicit or implicit means. This area of research, which incorporates techniques from Natural Language Processing (NLP), Information Retrieval and Artificial Intelligence, has significant potential to reduce discovery time in biomedical research fields. Formally introduced in 1986, LBD has grown to be a significant and a core task for text mining practitioners in the biomedical domain. Together with its inter-disciplinary nature, this has led researchers across domains to contribute in advancing this field of study. This survey attempts to consolidate and present the evolution of techniques in this area. We cover a variety of techniques and provide a detailed description of the problem setting, the intuition, the advantages and limitations of various influential papers. We also list the current bottlenecks in this field and provide a general direction of research activities for the future. In an effort to be comprehensive and for ease of reference for off-the-shelf users, we also list many publicly available tools for LBD. We hope this survey will act as a guide to both academic and industry (bio)-informaticians, introduce the various methodologies currently employed and also the challenges yet to be tackled.}
}
@article{PREININGER2021104530,
title = {Differences in information accessed in a pharmacologic knowledge base using a conversational agent vs traditional search methods},
journal = {International Journal of Medical Informatics},
volume = {153},
pages = {104530},
year = {2021},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2021.104530},
url = {https://www.sciencedirect.com/science/article/pii/S1386505621001568},
author = {Anita M. Preininger and Bedda L. Rosario and Adam M. Buchold and Jeff Heiland and Nawshin Kutub and Bryan S. Bohanan and Brett South and Gretchen P. Jackson},
keywords = {Pharmacologic knowledge base, Conversational agent, Artificial intelligence, Natural language processing, Information retrieval},
abstract = {Introduction
Clinicians rely on pharmacologic knowledge bases to answer medication questions and avoid potential adverse drug events. In late 2018, an artificial intelligence-based conversational agent, Watson Assistant (WA), was made available to online subscribers to the pharmacologic knowledge base, Micromedex®. WA allows users to ask medication-related questions in natural language. This study evaluated search method-dependent differences in the frequency of information accessed by traditional methods (keyword search and heading navigation) vs conversational agent search.
Materials and methods
We compared the proportion of information types accessed through the conversational agent to the proportion of analogous information types accessed by traditional methods during the first 6 months of 2020.
Results
Addition of the conversational agent allowed early adopters to access 22 different information types contained in the ‘quick answers’ portion of the knowledge base. These information types were accessed 117,550 times with WA during the study period, compared to 33,649,651 times using traditional search methods. The distribution across information types differed by method employed (c2 test, P < .0001). Single drug/dosing, FDA/non-FDA uses, adverse effects, and drug administration emerged as 4 of the top 5 information types accessed by either method. Intravenous compatibility was accessed more frequently using the conversational agent (7.7% vs. 0.6% for traditional methods), whereas dose adjustments were accessed more frequently via traditional methods (4.8% vs. 1.4% for WA).
Conclusion
In a widely used pharmacologic knowledge base, information accessed through conversational agents versus traditional methods differed. User-centered studies are needed to understand these differences.}
}
@article{FRANCES2024102338,
title = {A comprehensive methodology to construct standardised datasets for Science and Technology Parks},
journal = {Data & Knowledge Engineering},
volume = {153},
pages = {102338},
year = {2024},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2024.102338},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X24000624},
author = {Olga Francés and Javi Fernández and José Abreu-Salas and Yoan Gutiérrez and Manuel Palomar},
keywords = {Standardised dataset, Science and Technology Park (STP), Data science, Information retrieval, Methodologies and tools},
abstract = {This work presents a standardised approach to create datasets for Science and Technology Parks (STPs), facilitating future analysis of STP characteristics, trends and performance. STPs are the most representative examples of innovation ecosystems. The ETL (extraction-transformation-load) structure was adapted to a global field study of STPs. A selection stage and quality check were incorporated, and the methodology was applied to Spanish STPs. This study applies diverse techniques such as expert labelling and information extraction which uses language technologies. A novel methodology for building quality and standardised STP datasets was designed and applied to a Spanish STP case study with 49 STPs. An updatable dataset and a list of the main features impacting STPs are presented. Twenty-one (n = 21) core features were refined and selected, with fifteen of them (71.4 %) being robust enough for developing further quality analysis. The methodology presented integrates different sources with heterogeneous information that is often decentralised, disaggregated and in different formats: excel files, and unstructured information in HTML or PDF format. The existence of this updatable dataset and the defined methodology will enable powerful AI tools to be applied that focus on more sophisticated analysis, such as taxonomy, monitoring, and predictive and prescriptive analytics in the innovation ecosystems field.}
}
@article{KARIM2015488,
title = {Graph-based Methods for Significant Concept Selection},
journal = {Procedia Computer Science},
volume = {60},
pages = {488-497},
year = {2015},
note = {Knowledge-Based and Intelligent Information & Engineering Systems 19th Annual Conference, KES-2015, Singapore, September 2015 Proceedings},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.08.170},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915022978},
author = {Gasmi Karim and Torjmen-Khemakhem Mouna and Tamine Lynda and Ben Jemaa Maher},
keywords = {information retrieval, Natural Language Processing (NLP), semantic similarity, concept selection ;},
abstract = {It is well known in information retrieval area that one important issue is the gap between the query and document vocabularies. Concept-based representation of both the document and the query is one of the most effective approaches that lowers the effect of text mismatch and allows the selection of relevant documents that deal with the shared semantics hidden behind both. However, identifying the best representative concepts from texts is still challenging. In this paper, we propose a graph-based method to select the most significant concepts to be integrated into a conceptual indexing system. More specifically, we build the graph whose nodes represented concepts and weighted edges represent semantic distances. The importance of concepts are computed using centrality algorithms that levrage between structural and contextual importance. We experimentally evaluated our method of concept selection using the standard ImageClef2009 medical data set. Results showed that our approach significantly improves the retrieval effectiveness in comparison to state-of-the-art retrieval models.}
}
@article{RICKY2015459,
title = {A Personal Agents in Ubiquitous Environment: A Survey},
journal = {Procedia Computer Science},
volume = {59},
pages = {459-467},
year = {2015},
note = {International Conference on Computer Science and Computational Intelligence (ICCSCI 2015)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.07.514},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915020438},
author = {Michael Yoseph Ricky and Robin Solala Gulo},
keywords = {Personal Agent, Collaborative System, Intelligence Agent, Adaptive Agent.},
abstract = {A personal agents can be implements in various areas. The previous work has been conducted in website and mobile in corresponding to information retrieval, mobile computing, and artificial intelligence. There are different methods and framework are proposed in previous research to obtain and enhance agent's performance for better recommendations. This research aims to present comparison previous research based on personal agent in different areas for understanding of proposed framework design, architecture and its implementations. Personal agent can be applied to analyse and assisting in completing task especially for solving one purpose, and multi agents system can be applied at education, industrial, commercial, governmental, military, and entertainment applications for solving multi purposes.}
}
@article{IQBAL202192,
title = {Word Embedding based Textual Semantic Similarity Measure in Bengali},
journal = {Procedia Computer Science},
volume = {193},
pages = {92-101},
year = {2021},
note = {10th International Young Scientists Conference in Computational Science, YSC2021, 28 June – 2 July, 2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921020512},
author = {MD. Asif Iqbal and Omar Sharif and Mohammed Moshiul Hoque and Iqbal H. Sarker},
keywords = {Natural language processing, Textual semantic similarity, Word embedding, Cosine similarity, Part-of-speech weighting},
abstract = {Textual semantic similarity is a crucial constituent in many NLP tasks such as information retrieval, machine translation, information retrieval and textual forgery detection. It is a complicated task for rule-based techniques to address semantic similarity measures in low-resource languages due to the complex morphological structure and scarcity of linguistic resources. This paper investigates several word embedding techniques (Word2Vec, GloVe, FastText) to estimate the semantic similarity of Bengali sentences. Due to the unavailability of the standard dataset, this work developed a Bengali dataset containing 187031 text documents with 400824 unique words. Moreover, this work considers three semantic distance measures to compute the similarity between the word vectors using Cosine similarity with no weight, term frequency weighting and Part-of-Speech weighting. The performance of the proposed approach is evaluated on the developed dataset containing 50 pairs of Bengali sentences. The evaluation result shows that FastText with continuous bag-of-words with 100 vector size achieved the highest Pearson’s correlation (ρ) score of 77.28%.}
}
@article{VALPUT2023627,
title = {Assessment of the European mobility research landscape to support policy shaping through artificial intelligence models},
journal = {Transportation Research Procedia},
volume = {72},
pages = {627-634},
year = {2023},
note = {TRA Lisbon 2022 Conference Proceedings Transport Research Arena (TRA Lisbon 2022),14th-17th November 2022, Lisboa, Portugal},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2023.11.448},
url = {https://www.sciencedirect.com/science/article/pii/S2352146523007469},
author = {Damir Valput and Ulrike Schmalz and Pablo Hernández and Annika Paul},
keywords = {open science in mobility research, performance indicators, natural language processing, mobility policy shaping, data-centred decision making, green and digital mobility},
abstract = {This paper presents an approach for assessing EU-funded mobility research initiatives that relies on natural language processing (NLP) techniques. The developed prototype acts as a digital assistant that helps to analyze the mobility research landscape and delivers a bird-eye view of its status, gaps, and bottlenecks. We present data-based models that exploit common NLP techniques used for topic modeling and information retrieval to automatize the analysis of the textual data of over 40,000 H2020 and PF7 research projects and to deliver a series of metrics that support insight discovery. Further, we present an open-access dashboard that visually inspects the model results. Based on the developed models, we provide high-level strategic recommendations for future mobility development. A particular use case focuses on digitalization in mobility.}
}
@article{MOHAMMED2023102460,
title = {Building lexicon-based sentiment analysis model for low-resource languages},
journal = {MethodsX},
volume = {11},
pages = {102460},
year = {2023},
issn = {2215-0161},
doi = {https://doi.org/10.1016/j.mex.2023.102460},
url = {https://www.sciencedirect.com/science/article/pii/S2215016123004569},
author = {Idi Mohammed and Rajesh Prasad},
keywords = {Lexicon dictionary, Low-resource languages, Sentiment analysis, Fine-tuning, Hausa language},
abstract = {Natural Language Processing (NLP) has transformed machine translation, sentiment analysis, information retrieval, and conversation systems. NLP applications rely on complete linguistic resources, which might be difficult for low-resource languages. NLP solutions for every language require a language-specific dataset. Dataset in a language is essential for NLP solution creation. Over 7000 languages are spoken worldwide. Only around 20 languages have text corpora for NLP applications. English has the most datasets, then Chinese and Spanish. Japanese has several Western European language datasets. For an accurate NLP system, most Asian and African languages lack training datasets. To address this challenge, we propose a methodology for building a lexicon-based sentiment analysis model for languages with limited resources. The Hausa language was used as training and evaluation language. The methodology combines lexicon creation; augmentation, annotation, and fine-tuning model, and has been tested on a corpus of Hausa tweets achieving an accuracy of 98 %. The results suggest that our proposed model is a promising tool for sentiment analysis in a variety of applications, such as social media monitoring, customer service, and market research. Our methodology can be used for any low-resource language. The outline of the work done in this paper can be shown as follows:•We propose a methodology for building a lexicon-based sentiment analysis model for languages with limited resources, using the Hausa language as a case study.•The methodology combines lexicon creation, augmentation, annotation, and fine-tuning model, and achieves an accuracy of 98 % on a corpus of Hausa tweets.•The results suggest that the proposed model is a promising tool for sentiment analysis in a variety of applications for low-resource languages.}
}
@article{MARZOUK20224435,
title = {Natural Language Processing with Optimal Deep Learning-Enabled Intelligent Image Captioning System},
journal = {Computers, Materials and Continua},
volume = {74},
number = {2},
pages = {4435-4451},
year = {2022},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2023.033091},
url = {https://www.sciencedirect.com/science/article/pii/S1546221822004313},
author = {Radwa Marzouk and Eatedal Alabdulkreem and Mohamed K. Nour and Mesfer Al Duhayyim and Mahmoud Othman and Abu Sarwar Zamani and Ishfaq Yaseen and Abdelwahed Motwakel},
keywords = {Natural language processing, information retrieval, image captioning, deep learning, metaheuristics},
abstract = {The recent developments in Multimedia Internet of Things (MIoT) devices, empowered with Natural Language Processing (NLP) model, seem to be a promising future of smart devices. It plays an important role in industrial models such as speech understanding, emotion detection, home automation, and so on. If an image needs to be captioned, then the objects in that image, its actions and connections, and any silent feature that remains under-projected or missing from the images should be identified. The aim of the image captioning process is to generate a caption for image. In next step, the image should be provided with one of the most significant and detailed descriptions that is syntactically as well as semantically correct. In this scenario, computer vision model is used to identify the objects and NLP approaches are followed to describe the image. The current study develops a Natural Language Processing with Optimal Deep Learning Enabled Intelligent Image Captioning System (NLPODL-IICS). The aim of the presented NLPODL-IICS model is to produce a proper description for input image. To attain this, the proposed NLPODL-IICS follows two stages such as encoding and decoding processes. Initially, at the encoding side, the proposed NLPODL-IICS model makes use of Hunger Games Search (HGS) with Neural Search Architecture Network (NASNet) model. This model represents the input data appropriately by inserting it into a predefined length vector. Besides, during decoding phase, Chimp Optimization Algorithm (COA) with deeper Long Short Term Memory (LSTM) approach is followed to concatenate the description sentences produced by the method. The application of HGS and COA algorithms helps in accomplishing proper parameter tuning for NASNet and LSTM models respectively. The proposed NLPODL-IICS model was experimentally validated with the help of two benchmark datasets. A widespread comparative analysis confirmed the superior performance of NLPODL-IICS model over other models.}
}
@article{HARRIS2020101961,
title = {Construction and evaluation of gold standards for patent classification—A case study on quantum computing},
journal = {World Patent Information},
volume = {61},
pages = {101961},
year = {2020},
issn = {0172-2190},
doi = {https://doi.org/10.1016/j.wpi.2020.101961},
url = {https://www.sciencedirect.com/science/article/pii/S0172219019300791},
author = {Steve Harris and Anthony Trippe and David Challis and Nigel Swycher},
keywords = {Patent classification, Evaluation, Artificial intelligence, Information retrieval, Deep learning, Gold standard},
abstract = {This article discusses options for evaluation of patent and/or patent family classification algorithms by means of “gold standards”. It covers the creation criteria, and desirable attributes of evaluation mechanisms, then proposes an example gold standard, and discusses the results of applying the evaluation mechanism against the proposed gold standard and an existing commercial implementation.}
}
@article{SICILIANI2023102284,
title = {AI-based decision support system for public procurement},
journal = {Information Systems},
volume = {119},
pages = {102284},
year = {2023},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2023.102284},
url = {https://www.sciencedirect.com/science/article/pii/S0306437923001205},
author = {Lucia Siciliani and Vincenzo Taccardi and Pierpaolo Basile and Marco {Di Ciano} and Pasquale Lops},
keywords = {E-procurement, Data analysis, Data visualisation, Natural language processing, Semantic search, Decision support systems},
abstract = {Tenders are powerful means of investment of public funds and represent a strategic development resource. Thus, improving the efficiency of procuring entities and developing evaluation models turn out to be essential to facilitate e-procurement procedures. With this contribution, we introduce our research to create a supporting system for the decision-making and monitoring process during the entire course of investments and contracts. This system employs artificial intelligence techniques based on natural language processing, focused on providing instruments for extracting useful information from both structured and unstructured (i.e., text) data. Therefore, we developed a framework based on a web app that provides integrated tools such as a semantic search engine, a summariser, an open information extraction engine in the form of triples (subject–predicate–object) for tender documents, and dashboards for analysing tender data.}
}
@article{ZHAO2023225,
title = {A Survey of Knowledge Graph Construction Using Machine Learning},
journal = {CMES - Computer Modeling in Engineering and Sciences},
volume = {139},
number = {1},
pages = {225-257},
year = {2023},
issn = {1526-1492},
doi = {https://doi.org/10.32604/cmes.2023.031513},
url = {https://www.sciencedirect.com/science/article/pii/S152614922300098X},
author = {Zhigang Zhao and Xiong Luo and Maojian Chen and Ling Ma},
keywords = {Knowledge graph (KG), semantic network, relation extraction, entity linking, knowledge reasoning},
abstract = {Knowledge graph (KG) serves as a specialized semantic network that encapsulates intricate relationships among real-world entities within a structured framework. This framework facilitates a transformation in information retrieval, transitioning it from mere string matching to far more sophisticated entity matching. In this transformative process, the advancement of artificial intelligence and intelligent information services is invigorated. Meanwhile, the role of machine learning method in the construction of KG is important, and these techniques have already achieved initial success. This article embarks on a comprehensive journey through the last strides in the field of KG via machine learning. With a profound amalgamation of cutting-edge research in machine learning, this article undertakes a systematical exploration of KG construction methods in three distinct phases: entity learning, ontology learning, and knowledge reasoning. Especially, a meticulous dissection of machine learning-driven algorithms is conducted, spotlighting their contributions to critical facets such as entity extraction, relation extraction, entity linking, and link prediction. Moreover, this article also provides an analysis of the unresolved challenges and emerging trajectories that beckon within the expansive application of machine learning-fueled, large-scale KG construction.}
}
@article{CHEN2022100001,
title = {Vision, status, and research topics of Natural Language Processing},
journal = {Natural Language Processing Journal},
volume = {1},
pages = {100001},
year = {2022},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2022.100001},
url = {https://www.sciencedirect.com/science/article/pii/S2949719122000012},
author = {Xieling Chen and Haoran Xie and Xiaohui Tao},
keywords = {Natural Language Processing, NLP, , Trustworthy Artificial Intelligence},
abstract = {The field of Natural Language Processing (NLP) has evolved with, and as well as influenced, recent advances in Artificial Intelligence (AI) and computing technologies, opening up new applications and novel interactions with humans. Modern NLP involves machines’ interaction with human languages for the study of patterns and obtaining meaningful insights. NLP is increasingly receiving attention across academia and industry and demonstrates extraordinary opportunities and across AI applications (e.g., question answering, information retrieval, sentiment analysis, and recommender systems) and helps to deal with new tasks such as machine translation and reading comprehension, with real world performance improving all the time. This editorial first provides an overview of the field of NLP in terms of research grants, publication venues, and research topics. We then introduce the mission of Natural Language Processing Journal, a new NLP-focused Elsevier journal intended as a forum for researchers and practitioners to publish theoretical, practical, and methodological achievements related to trustworthy AI development and applications for analyzing, processing, and modeling human languages.}
}
@article{RAZAVISOUSAN2022100093,
title = {Building Textual Fuzzy Interpretive Structural Modeling to Analyze Factors of Student Mobility Based on User Generated Content},
journal = {International Journal of Information Management Data Insights},
volume = {2},
number = {2},
pages = {100093},
year = {2022},
issn = {2667-0968},
doi = {https://doi.org/10.1016/j.jjimei.2022.100093},
url = {https://www.sciencedirect.com/science/article/pii/S2667096822000362},
author = {Ronak Razavisousan and Karuna Pande Joshi},
keywords = {Student Mobility, Fuzzy interpretive Structural modeling (FISM), Fuzzy ISM, Textual Fuzzy Interpretive Structural Modeling (TFISM), Complex decision-making problem},
abstract = {Many factors influence student mobility across regions and countries. The roles of these factors, along with their interrelationship and interaction, make student mobility a complex decision-making issue. Many textual data generated on social media can answer many open questions about factors affecting human behavior, particularly social mobility. We have developed a novel methodology, called Textual Fuzzy Interpretive Structural Modeling (TFISM), that automatically analyses large textual datasets to identify the internal and external relationships between management or decision-making problems. This computational social science methodology enhances Interpretive Structural Modeling (ISM) approaches to allow the input to be textual data. It is multi-disciplinary and integrates ISM with Artificial Intelligence, Text extraction, and information retrieval techniques. TFISM is a domain-free method, while we have validated this methodology on two different datasets from social media and academic articles. In this paper, we present the results of our study to identify the critical factors and most influential factors for global student mobility.}
}
@article{CHAI2023574,
title = {The Process and Algorithm Analysis of Text Mining System Based on Artificial Intelligence},
journal = {Procedia Computer Science},
volume = {228},
pages = {574-581},
year = {2023},
note = {3rd International Conference on Machine Learning and Big Data Analytics for IoT Security and Privacy},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.11.066},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923018902},
author = {Xiaoliang Chai and Songxiao Xu and Shilin Li and Junyu Zhao},
keywords = {Artificial Intelligence, Text Mining, Mining System, Implementation Process},
abstract = {The rapid development of the Internet leads to the rapid growth of network information, we call it information explosion. The Internet is full of information, and it is difficult for users to find this information and useful knowledge of the ocean. The Web has become the world's largest information repository, and there is an urgent need for efficient access to the valuable knowledge of vast amounts of web information. The purpose of this paper is to study the process and algorithm analysis of text mining system based on artificial intelligence. This paper presents an algorithm of document feature acquisition based on genetic algorithm. Selecting suitable features is an important task in specific text classification and information retrieval. Finding appropriate feature vectors to represent the text will undoubtedly help with subsequent sorting and grouping. Based on the genetic algorithm of variable length chromosome, this paper improves the crossover, mutation and selection operations, and proposes an algorithm to obtain text feature vectors. This method has a wide range of applications and good results.}
}
@article{LEI2022118,
title = {A Domain Specific Multi-Document Reading Comprehension Method for Artificial Intelligence Application},
journal = {Procedia Computer Science},
volume = {208},
pages = {118-127},
year = {2022},
note = {7th International Conference on Intelligent, Interactive Systems and Applications},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.10.019},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922014600},
author = {Chen Lei and Zhao Baojin and Dong Xinran and Cui Zaixing},
keywords = {Information extract, BERT, Multi-paragraph, QA, Multi-task training, artificial intelligence},
abstract = {With the development of artificial intelligence, information retrieval and information extraction and knowledge services from large-scale texts are currently one of the most urgent needs of people. Machine reading comprehension technology is one of the key technologies that can be applied to knowledge mining. At present, multi-document reading comprehension has received a lot of attention, and its application scenarios are also very extensive. The main goal of this article is to find the answer to the question from a large number of smartphone manuals based on the questions raised by the user about the operation of the smartphone. This paper designs a pipeline structure with three modules: retrieval, extraction, and sorting. At the same time, it designs auxiliary tasks for the extraction model to improve the extraction ability, and uses a new answer scoring method to select answers. The final experiment proves that our method can effectively improve the answer's quality.}
}
@article{OMAR2024e595,
title = {ChatGPT for digital pathology research},
journal = {The Lancet Digital Health},
volume = {6},
number = {8},
pages = {e595-e600},
year = {2024},
issn = {2589-7500},
doi = {https://doi.org/10.1016/S2589-7500(24)00114-6},
url = {https://www.sciencedirect.com/science/article/pii/S2589750024001146},
author = {Mohamed Omar and Varun Ullanat and Massimo Loda and Luigi Marchionni and Renato Umeton},
abstract = {Summary
The rapid evolution of generative artificial intelligence (AI) models including OpenAI's ChatGPT signals a promising era for medical research. In this Viewpoint, we explore the integration and challenges of large language models (LLMs) in digital pathology, a rapidly evolving domain demanding intricate contextual understanding. The restricted domain-specific efficiency of LLMs necessitates the advent of tailored AI tools, as illustrated by advancements seen in the last few years including FrugalGPT and BioBERT. Our initiative in digital pathology emphasises the potential of domain-specific AI tools, where a curated literature database coupled with a user-interactive web application facilitates precise, referenced information retrieval. Motivated by the success of this initiative, we discuss how domain-specific approaches substantially minimise the risk of inaccurate responses, enhancing the reliability and accuracy of information extraction. We also highlight the broader implications of such tools, particularly in streamlining access to scientific research and democratising access to computational pathology techniques for scientists with little coding experience. This Viewpoint calls for an enhanced integration of domain-specific text-generation AI tools in academic settings to facilitate continuous learning and adaptation to the dynamically evolving landscape of medical research.}
}
@article{SPARCKJONES1999257,
title = {Information retrieval and artificial intelligence},
journal = {Artificial Intelligence},
volume = {114},
number = {1},
pages = {257-281},
year = {1999},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(99)00075-2},
url = {https://www.sciencedirect.com/science/article/pii/S0004370299000752},
author = {Karen {Sparck Jones}},
keywords = {Information retrieval, Probabilistic model, Artificial Intelligence},
abstract = {This paper addresses the relations between information retrieval (IR) and AI. It examines document retrieval, summarising its essential features and illustrating the state of its art by presenting one probabilistic model in detail, with some test results showing its value. The paper then analyses this model and related successful approaches, concentrating on and justifying their use of weak, redundant representation and reasoning. It goes on to other information management tasks and considers how the concepts and methods developed for retrieval may be applied to these, concluding by arguing that such ways of dealing with information may also have wider relevance to AI.}
}
@article{SAWANT2024100090,
title = {NLP-based smart decision making for business and academics},
journal = {Natural Language Processing Journal},
volume = {8},
pages = {100090},
year = {2024},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2024.100090},
url = {https://www.sciencedirect.com/science/article/pii/S2949719124000384},
author = {Pradnya Sawant and Kavita Sonawane},
keywords = {Correlation analysis, Enhanced Longest Common Subsequence (ELCS), Feature extraction, Semantic analysis},
abstract = {Natural Language Processing (NLP) systems enable machines to understand, interpret, and generate human-like language, bridging the gap between human communication and computer understanding. Natural Language Interface to Databases (NLIDB) and Natural Language Interface to Visualization (NLIV) systems are designed to enable non-technical users to retrieve and visualize data through natural language queries. However, these systems often face challenges in handling complex correlation and analytical questions, limiting their effectiveness for comprehensive data analysis. Additionally, current Business Intelligence (BI) tools also struggle with understanding the context and semantics of complex questions, further hindering their usability for strategic decision-making. Also, when building these models for generating the queries from natural language, the system handles only the semantic parsing issues as each column header is being changed manually to their normal names by all existing models which is time-consuming, tedious, and subjective. Recent studies reflect the need for attention to context, semantics, and especially ambiguities in dealing with natural language questions. To address this problem, the proposed architecture focuses on understanding the context, correlation-based semantic analysis, and removal of ambiguities using a novel approach. An Enhanced Longest Common Subsequence (ELCS) is suggested where existing LCS is modified with a memorization component for mapping the natural language question tokens with ambiguous table column headers. This can speed up the overall process as human intervention is not required to manually change the column headers. The same is evidenced by carrying out thorough experimentation and comparative study in terms of precision, recall, and F1 score. By synthesizing the latest advancements and addressing challenges, this paper has proved how NLP can significantly enhance the accuracy and efficiency of information retrieval and visualization, broadening the inclusivity and usability of NLIDB, NLIV, and BI systems.}
}
@article{WANG2021103918,
title = {Query bot for retrieving patients’ clinical history: A COVID-19 use-case},
journal = {Journal of Biomedical Informatics},
volume = {123},
pages = {103918},
year = {2021},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2021.103918},
url = {https://www.sciencedirect.com/science/article/pii/S1532046421002471},
author = {Yibo Wang and Amara Tariq and Fiza Khan and Judy Wawira Gichoya and Hari Trivedi and Imon Banerjee},
keywords = {Information retrieval, Clinical notes, BERT, k-means, Relevance feedback},
abstract = {Objective
With increasing patient complexity whose data are stored in fragmented health information systems, automated and time-efficient ways of gathering important information from the patients' medical history are needed for effective clinical decision making. Using COVID-19 as a case study, we developed a query-bot information retrieval system with user-feedback to allow clinicians to ask natural questions to retrieve data from patient notes.
Materials and methods
We applied clinicalBERT, a pre-trained contextual language model, to our dataset of patient notes to obtain sentence embeddings, using K-Means to reduce computation time for real-time interaction. Rocchio algorithm was then employed to incorporate user-feedback and improve retrieval performance.
Results
In an iterative feedback loop experiment, MAP for final iteration was 0.93/0.94 as compared to initial MAP of 0.66/0.52 for generic and 1./1. compared to 0.79/0.83 for COVID-19 specific queries confirming that contextual model handles the ambiguity in natural language queries and feedback helps to improve retrieval performance. User-in-loop experiment also outperformed the automated pseudo relevance feedback method. Moreover, the null hypothesis which assumes identical precision between initial retrieval and relevance feedback was rejected with high statistical significance (p ≪ 0.05). Compared to Word2Vec, TF-IDF and bioBERT models, clinicalBERT works optimally considering the balance between response precision and user-feedback.
Discussion
Our model works well for generic as well as COVID-19 specific queries. However, some generic queries are not answered as well as others because clustering reduces query performance and vague relations between queries and sentences are considered non-relevant. We also tested our model for queries with the same meaning but different expressions and demonstrated that these query variations yielded similar performance after incorporation of user-feedback.
Conclusion
In conclusion, we develop an NLP-based query-bot that handles synonyms and natural language ambiguity in order to retrieve relevant information from the patient chart. User-feedback is critical to improve model performance.}
}
@article{CHEN2024108680,
title = {Characterising global antimicrobial resistance research explains why One Health solutions are slow in development: An application of AI-based gap analysis},
journal = {Environment International},
volume = {187},
pages = {108680},
year = {2024},
issn = {0160-4120},
doi = {https://doi.org/10.1016/j.envint.2024.108680},
url = {https://www.sciencedirect.com/science/article/pii/S0160412024002666},
author = {Cai Chen and Shu-Le Li and Yao-Yang Xu and Jue Liu and David W. Graham and Yong-Guan Zhu},
keywords = {Antimicrobial resistance, One Health, Natural language processing, Artificial intelligence, Methods harmonization},
abstract = {The global health crisis posed by increasing antimicrobial resistance (AMR) implicitly requires solutions based a One Health approach, yet multisectoral, multidisciplinary research on AMR is rare and huge knowledge gaps exist to guide integrated action. This is partly because a comprehensive survey of past research activity has never performed due to the massive scale and diversity of published information. Here we compiled 254,738 articles on AMR using Artificial Intelligence (AI; i.e., Natural Language Processing, NLP) methods to create a database and information retrieval system for knowledge extraction on research perfomed over the last 20 years. Global maps were created that describe regional, methodological, and sectoral AMR research activities that confirm limited intersectoral research has been performed, which is key to guiding science-informed policy solutions to AMR, especially in low-income countries (LICs). Further, we show greater harmonisation in research methods across sectors and regions is urgently needed. For example, differences in analytical methods used among sectors in AMR research, such as employing culture-based versus genomic methods, results in poor communication between sectors and partially explains why One Health-based solutions are not ensuing. Therefore, our analysis suggest that performing culture-based and genomic AMR analysis in tandem in all sectors is crucial for data integration and holistic One Health solutions. Finally, increased investment in capacity development in LICs should be prioritised as they are places where the AMR burden is often greatest. Our open-access database and AI methodology can be used to further develop, disseminate, and create new tools and practices for AMR knowledge and information sharing.}
}
@article{KIDWAI202075,
title = {Design and Development of Diagnostic Chabot for supporting Primary Health Care Systems},
journal = {Procedia Computer Science},
volume = {167},
pages = {75-84},
year = {2020},
note = {International Conference on Computational Intelligence and Data Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.03.184},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920306499},
author = {Bushra Kidwai and Nadesh RK},
keywords = {Artificial Intelligence, Chatbots, decision tree, natural language processing},
abstract = {Technology is increasingly becoming a massive part of today’s healthcare scenario. Technology has changed the way how patients communicate with doctors and not only that, but also how healthcare is administered. Artificial intelligence and Chabots are two groundbreaking technologies that have changed how patients and doctors perceive healthcare. To make healthcare system more interactive a diagnostic Chabot is designed and developed using latest algorithms in machine learning, decision tree algorithm to help the user to form a diagnosis of their condition based on their symptoms. The system will be fed with information pertaining to various diseases and using NLP, it will be able to understand the user query and give a suitable response. The system can be used for effective information retrieval in a similar manner like siri, alexa etc but the scope will be limited to disease diagnosis.}
}
@article{HARRER2023104512,
title = {Attention is not all you need: the complicated case of ethically using large language models in healthcare and medicine},
journal = {eBioMedicine},
volume = {90},
pages = {104512},
year = {2023},
issn = {2352-3964},
doi = {https://doi.org/10.1016/j.ebiom.2023.104512},
url = {https://www.sciencedirect.com/science/article/pii/S2352396423000774},
author = {Stefan Harrer},
keywords = {Generative artificial intelligence, Large language models, Foundation models, AI ethics, Augmented human intelligence, Information management, AI trustworthiness},
abstract = {Summary
Large Language Models (LLMs) are a key component of generative artificial intelligence (AI) applications for creating new content including text, imagery, audio, code, and videos in response to textual instructions. Without human oversight, guidance and responsible design and operation, such generative AI applications will remain a party trick with substantial potential for creating and spreading misinformation or harmful and inaccurate content at unprecedented scale. However, if positioned and developed responsibly as companions to humans augmenting but not replacing their role in decision making, knowledge retrieval and other cognitive processes, they could evolve into highly efficient, trustworthy, assistive tools for information management. This perspective describes how such tools could transform data management workflows in healthcare and medicine, explains how the underlying technology works, provides an assessment of risks and limitations, and proposes an ethical, technical, and cultural framework for responsible design, development, and deployment. It seeks to incentivise users, developers, providers, and regulators of generative AI that utilises LLMs to collectively prepare for the transformational role this technology could play in evidence-based sectors.}
}
@article{SANTOSARTEAGA2024108610,
title = {On the capacity of artificial intelligence techniques and statistical methods to deal with low-quality data in medical supply chain environments},
journal = {Engineering Applications of Artificial Intelligence},
volume = {133},
pages = {108610},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.108610},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624007681},
author = {Francisco Javier {Santos Arteaga} and Debora {Di Caprio} and Madjid Tavana and David Cucchiari and Josep M. Campistol and Federico Oppenheimer and Fritz Diekmann and Ignacio Revuelta},
keywords = {Information retrieval, Data quality, Artificial neural networks, Regression analysis, Supply chains, Kidney transplantation},
abstract = {We illustrate the capacity of Artificial Intelligence (AI) and Machine Learning (ML) techniques to preserve consistent categorization abilities whenever the quality of the data decreases, displaying mistakes or mismatches across matrix entries, while standard statistical methods exhibit significant modifications in the value of the corresponding coefficients. We design algorithms of different complexity to generate a series of comparable profiles. These profiles are compared within environments that allow for an immediate identification of the generating algorithms and within increasingly complex settings involving almost identical profiles derived from different algorithms. AI and ML techniques outperform standard statistical methods when distinguishing the algorithms generating the profiles. Building on these results, we perform a retrospective analysis where AI and ML techniques are applied to two empirical scenarios defined by different data series of patients transplanted through the period 2006–2019. The first scenario contains the variables describing the evolution of patients inputted correctly. In the second, we modify the content of the vectors of characteristics defining the evolution of patients by exchanging the values of a subset of realizations from two categorical variables. AI and ML techniques are consistently accurate when categorizing patients correctly within both scenarios, a feature particularly relevant when the quality of the information sources composing the medical chain varies. This latter problem is exacerbated among hospitals located in developing countries, where the quality of the data gathered limits their identification and extrapolation capacities.}
}
@article{STOJANOV2024100243,
title = {University students’ self-reported reliance on ChatGPT for learning: A latent profile analysis},
journal = {Computers and Education: Artificial Intelligence},
volume = {6},
pages = {100243},
year = {2024},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2024.100243},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X24000468},
author = {Ana Stojanov and Qian Liu and Joyce Hwee Ling Koh},
keywords = {ChatGPT, Artificial intelligence (AI), University students, Higher education, Latent profile analysis (LPA), Achievement goal orientation, Educational technology use},
abstract = {Although ChatGPT, a state-of-the-art, large language model, seems to be a disruptive technology in higher education, it is unclear to what extent students rely on this tool for completing different tasks. To address this gap, we asked university students (N = 490) recruited via CloudResearch to rate the extent to which they rely on ChatGPT for completing 13 tasks identified in a previous pilot study. Five distinct profiles emerged: ‘Versatile low reliers’ (38.2%) were characterised by low overall self-reported reliance across the tasks, while ‘all-rounders’ (10.4%) had high overall self-reported reliance. The ‘knowledge seekers’ (16.5%) scored particularly high on tasks such as content acquisition, information retrieval and summarising of texts, while the ‘proactive learners’ (11.8%) on tasks such as obtaining feedback, planning and quizzing. Finally, the ‘assignment delegators’ (23.1%) relied on ChatGPT for drafting assignments, writing homework and having ChatGPT write their assignment for them. The findings provide a nuanced understanding of how students rely on ChatGPT for learning.}
}
@article{HOSSAIN2024107987,
title = {AraCovTexFinder: Leveraging the transformer-based language model for Arabic COVID-19 text identification},
journal = {Engineering Applications of Artificial Intelligence},
volume = {133},
pages = {107987},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.107987},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624001453},
author = {Md. Rajib Hossain and Mohammed Moshiul Hoque and Nazmul Siddique and M. Ali Akber Dewan},
keywords = {Natural language processing, Low-resource text identification, Text processing, Language model, Arabic covid text, Ablation study, Late-fusion},
abstract = {In light of the pandemic, the identification and processing of COVID-19-related text have emerged as critical research areas within the field of Natural Language Processing (NLP). With a growing reliance on online portals and social media for information exchange and interaction, a surge in online textual content, comprising disinformation, misinformation, fake news, and rumors has led to the phenomenon of an infodemic on the World Wide Web. Arabic, spoken by over 420 million people worldwide, stands as a significant low-resource language, lacking efficient tools or applications for the detection of COVID-19-related text. Additionally, the identification of COVID-19 text is an essential prerequisite task for detecting fake and toxic content associated with COVID-19. This gap hampers crucial COVID information retrieval and processing necessary for policymakers and health authorities. Addressing this issue, this paper introduces an intelligent Arabic COVID-19 text identification system named ‘AraCovTexFinder,’ leveraging a fine-tuned fusion-based transformer model. Recognizing the challenges posed by a scarcity of related text corpora, substantial morphological variations in the language, and a deficiency of well-tuned hyperparameters, the proposed system aims to mitigate these hurdles. To support the proposed method, two corpora are developed: an Arabic embedding corpus (AraEC) and an Arabic COVID-19 text identification corpus (AraCoV). The study evaluates the performance of six transformer-based language models (mBERT, XML-RoBERTa, mDeBERTa-V3, mDistilBERT, BERT-Arabic, and AraBERT), 12 deep learning models (combining Word2Vec, GloVe, and FastText embedding with CNN, LSTM, VDCNN, and BiLSTM), and the newly introduced model AraCovTexFinder. Through extensive evaluation, AraCovTexFinder achieves a high accuracy of 98.89 ± 0.001%, outperforming other baseline models, including transformer-based language and deep learning models. This research highlights the importance of specialized tools in low-resource languages to combat the infodemic relating to COVID-19, which can assist policymakers and health authorities in making informed decisions.}
}
@article{KABIR2023100011,
title = {ASPER: Attention-based approach to extract syntactic patterns denoting semantic relations in sentential context},
journal = {Natural Language Processing Journal},
volume = {3},
pages = {100011},
year = {2023},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2023.100011},
url = {https://www.sciencedirect.com/science/article/pii/S2949719123000080},
author = {Md. Ahsanul Kabir and Tyler Phillips and Xiao Luo and Mohammad {Al Hasan}},
keywords = {Syntactic pattern, Syntactic pattern extraction},
abstract = {Semantic relationships, such as hyponym–hypernym, cause–effect, meronym–holonym etc., between a pair of entities in a sentence are usually reflected through syntactic patterns. Automatic extraction of such patterns benefits several downstream tasks, including, entity extraction, ontology building, and question answering. Unfortunately, automatic extraction of such patterns has not yet received much attention from NLP and information retrieval researchers. In this work, we propose an attention-based supervised deep learning model, ASPER, which extracts syntactic patterns between entities exhibiting a given semantic relation in the sentential context. We validate the performance of ASPER on three distinct semantic relations—hyponym–hypernym, cause–effect, and meronym–holonym on six datasets. Experimental results show that for all these semantic relations, ASPER can automatically identify a collection of syntactic patterns reflecting the existence of such a relation between a pair of entities in a sentence. In comparison to the existing methodologies of syntactic pattern extraction, ASPER’s performance is substantially superior.}
}
@article{RAJPUT2014662,
title = {Ontology based Semantic Annotation of Urdu Language Web Documents},
journal = {Procedia Computer Science},
volume = {35},
pages = {662-670},
year = {2014},
note = {Knowledge-Based and Intelligent Information & Engineering Systems 18th Annual Conference, KES-2014 Gdynia, Poland, September 2014 Proceedings},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.08.148},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914011132},
author = {Quratulain Rajput},
keywords = {semantic annotation, Urdu ads, ontology, information extraction;},
abstract = {Proliferation of multilingual text on the Internet has increased the demand for efficient information retrieval independent of language. Among variety of languages, the Urdu language is one of the most commonly spoken and written language in South Asia. However, due to unstructured format the access of relevant information is still a big challenge. The semantic web technologies enable the advancement in information retrieval systems by assigning semantics to information. This paper presents a semantic annotation framework that can annotate documents written in Urdu language. The framework uses domain specific ontology and context keywords instead of NLP (Natural Language processing) techniques. The experiment has been conducted to evaluate the presented annotation framework. The set of corpora used in the experiment belong to the online classified ads posted on the online Urdu newspapers. The purpose of this research is to find the challenges involved in semantic annotation of Urdu language web documents.}
}
@article{PAUZI2023111616,
title = {Applications of natural language processing in software traceability: A systematic mapping study},
journal = {Journal of Systems and Software},
volume = {198},
pages = {111616},
year = {2023},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2023.111616},
url = {https://www.sciencedirect.com/science/article/pii/S0164121223000110},
author = {Zaki Pauzi and Andrea Capiluppi},
keywords = {Software traceability, Information retrieval, Natural language processing},
abstract = {A key part of software evolution and maintenance is the continuous integration from collaborative efforts, often resulting in complex traceability challenges between software artifacts: features and modules remain scattered in the source code, and traceability links become harder to recover. In this paper, we perform a systematic mapping study dealing with recent research recovering these links through information retrieval, with a particular focus on natural language processing (NLP). Our search strategy gathered a total of 96 papers in focus of our study, covering a period from 2013 to 2021. We conducted trend analysis on NLP techniques and tools involved, and traceability efforts (applying NLP) across the software development life cycle (SDLC). Based on our study, we have identified the following key issues, barriers, and setbacks: syntax convention, configuration, translation, explainability, properties representation, tacit knowledge dependency, scalability, and data availability. Based on these, we consolidated the following open challenges: representation similarity across artifacts, the effectiveness of NLP for traceability, and achieving scalable, adaptive, and explainable models. To address these challenges, we recommend a holistic framework for NLP solutions to achieve effective traceability and efforts in achieving interoperability and explainability in NLP models for traceability.}
}
@article{IBRIHICH2022777,
title = {A Review on recent research in information retrieval},
journal = {Procedia Computer Science},
volume = {201},
pages = {777-782},
year = {2022},
note = {The 13th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 5th International Conference on Emerging Data and Industry 4.0 (EDI40)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.03.106},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922005191},
author = {S. Ibrihich and A. Oussous and O. Ibrihich and M. Esghir},
keywords = {Information Retrieval, Intelligent Search, IR models, Data Mining, Natural Language Processing},
abstract = {In this paper, we present a survey of modeling and simulation approaches to describe information retrieval basics. We investigate its methods, its challenges, its models, its components and its applications. Our contribution is twofold: on the one hand, reviewing the literature on discovery some search techniques that help to get pertinent results and reach an effective search, and on the other hand, discussing the different research perspectives for study and compare more techniques used in information retrieval. This paper will also shedding the light on some of the famous AI applications in the legal field.}
}
@article{MEGHATRIA2020320,
title = {Event Nugget Detection using Pre-trained Language Models},
journal = {Procedia Computer Science},
volume = {176},
pages = {320-329},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.08.034},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920318585},
author = {Riadh Meghatria and Chiraz Latiri and Fahima Nader},
keywords = {Information retrieval, Event detection, Nugget, RoBERTa, BERT, Fine-tuning},
abstract = {This paper handles the task of event nugget detection. In fact, deep learning methods were able to manage the extraction of relevant learned features. However, these methods tend to rely on NLP-Toolkits, as they feed gradually handcrafted features into their initial model. To alleviate this dependency and offer a deeper semantic understanding of the information encompassed in data, we investigate the use of pre-trained language models. The proposed approach uses the RoBERTa model because it offers a robust context-sensitive and pertinent representation of trends in data. The results demonstrate that our approach significantly outperforms its BERT-based variants and state-of-the-art approaches.}
}
@article{EZZIKOURI20191261,
title = {A New Approach for Calculating Semantic Similarity between Words Using WordNet and Set Theory},
journal = {Procedia Computer Science},
volume = {151},
pages = {1261-1265},
year = {2019},
note = {The 10th International Conference on Ambient Systems, Networks and Technologies (ANT 2019) / The 2nd International Conference on Emerging Data and Industry 4.0 (EDI40 2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.04.182},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919306490},
author = {Hanane EZZIKOURI and Youness MADANI and Mohammed ERRITALI and Mohamed OUKESSOU},
keywords = {Semantic Similarity, Natural Language Processing, WordNet, Set Theory},
abstract = {Calculating semantic similarity between words is a challenging task of a lot of domains such as Natural language processing (NLP), information retrieval and plagiarism detection. WordNet is a lexical dictionary conceptually organized, where each concept has several characteristics: Synsets and Glosses. Synset represent sets of synonyms of a given word and Glosses are a short description. In this paper, we propose a new approach for calculating semantic similarity between two concepts. The proposed method is based on set theory’s concepts and WordNet properties, by calculating the relatedness between the synsets’ and glosses’s of the two concepts.}
}
@article{AVOGADRO2024112447,
title = {Feature/vector entity retrieval and disambiguation techniques to create a supervised and unsupervised semantic table interpretation approach},
journal = {Knowledge-Based Systems},
volume = {304},
pages = {112447},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112447},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124010815},
author = {Roberto Avogadro and Fabio D’Adda and Marco Cremaschi},
keywords = {Semantic web, Knowledge base, Knowledge base construction, Knowledge base extension, Knowledge graph, Semantic table interpretation, Table annotation, Data enrichment, Tabular data},
abstract = {Recently, there has been an increasing interest in extracting and annotating tables on the Web. This activity allows the transformation of textual data into machine-readable formats to enable the execution of various artificial intelligence tasks, e.g., semantic search and dataset extension. Semantic Table Interpretation (STI) is the process of annotating elements in a table. The paper explores Semantic Table Interpretation, addressing the challenges of Entity Retrieval and Entity Disambiguation in the context of Knowledge Graphs (KGs). It introduces LamAPI, an Information Retrieval system with string/type-based filtering and s-elBat, an Entity Disambiguation technique that combines heuristic and ML-based approaches. By applying the acquired know-how in the field and extracting algorithms, techniques and components from our previous STI approaches and the state of the art, we have created a new platform capable of annotating any tabular data, ensuring a high level of quality.}
}
@article{BADAWI2023100043,
title = {KurdSum: A new benchmark dataset for the Kurdish text summarization},
journal = {Natural Language Processing Journal},
volume = {5},
pages = {100043},
year = {2023},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2023.100043},
url = {https://www.sciencedirect.com/science/article/pii/S2949719123000407},
author = {Soran Badawi},
keywords = {Dataset annotation, Kurdish text summarization, Data collection, Evaluation},
abstract = {Summarizing a text is the process of condensing its content while still maintaining its essential information. With the abundance of digital information available, summarization has become a significant task in various fields, including information retrieval, NLP (Natural Language Processing), and machine learning. This task has been extensively studied in languages such as English and Chinese, but research on Kurdish language summarization is lacking. Therefore, we present the first-ever Kurdish summarization news dataset, KurdSum, which includes over 40,000 texts. We collected news articles from Kurdish websites, preprocessed the data, and manually created a summary for each article. We further assessed the performance of our benchmark dataset on four extractive systems (LEXRANK, TEXTRANK, ORACLE, and LEAD0-3) and three abstractive methods (Pointer-Generator, Sequence-to-Sequence and transformer-abstractive). Our experiments showed that the Pointer-Generator approach yielded superior ROUGE (Recall-Oriented Understudy for Gisting Evaluation) scores compared to other techniques and ORACLE outperformed other extractive methods. Our findings offer a promising direction for the summarization of Kurdish text and can contribute to developing NLP tools for processing the Kurdish language. Likewise, the dataset can serve as a benchmark dataset for Kurdish language summarization and a valuable resource for researchers interested in developing Kurdish summarization models.}
}
@article{CASTELLANO2022108859,
title = {Leveraging Knowledge Graphs and Deep Learning for automatic art analysis},
journal = {Knowledge-Based Systems},
volume = {248},
pages = {108859},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.108859},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122004105},
author = {Giovanna Castellano and Vincenzo Digeno and Giovanni Sansaro and Gennaro Vessio},
keywords = {Knowledge graphs, Artificial intelligence, Deep learning, Computer vision, Graph neural networks, Digital humanities, Fine arts},
abstract = {The growing availability of large collections of digitized artworks has disclosed new opportunities to develop intelligent systems for the automatic analysis of fine arts. Among other benefits, these tools can foster a deeper understanding of fine arts, ultimately supporting the spread of culture. However, most of the systems proposed in the literature are only based on visual features of digitized artwork images, which are sometimes only integrated with some metadata and textual comments. A Knowledge Graph (KG) that integrates a rich body of information about artworks, artists, painting schools, etc., in a unified structured framework, can provide a valuable resource for more powerful information retrieval and knowledge discovery tools in the artistic domain. To this end, in this paper we present ArtGraph:11ArtGraph and associated code are publicly available on https://doi.org/10.5281/zenodo.6337958.. an artistic KG based on WikiArt and DBpedia. The graph already provides knowledge discovery capabilities without having to train a learning system. In addition, we propose a novel KG-enabled fine art classification method based on ArtGraph, which is used to perform artwork attribute prediction tasks. The method extracts embeddings from ArtGraph and injects them as “contextual” knowledge into a Deep Learning model. Compared to the state-of-the-art, the proposed model provides encouraging results, suggesting that the exploitation of KGs in combination with Deep Learning can pave the way for bridging the gap between the Humanities and Computer Science communities.}
}
@article{KUMAR20231768,
title = {A Natural Language Processing System using CWS Pipeline for Extraction of Linguistic Features},
journal = {Procedia Computer Science},
volume = {218},
pages = {1768-1777},
year = {2023},
note = {International Conference on Machine Learning and Data Engineering},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.01.155},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923001552},
author = {Sandeep Kumar and Arun Solanki},
keywords = {Coreference Resolution, CWS Pipeline, Natural-Language-Processing, CoNLL-2012, Neuralcoref, Text Summarization, linguistic features},
abstract = {Understanding the rules of grammar and linguistic features is essential to understanding the context of a language, which helps to understand that language. Similarly, for Natural Language processing, the linguistic feature allows understanding of the language. This paper introduced how Coreference, Word-sense, and Semantic knowledge (CWS) of linguistic features work. It would improve the Natural Language Understanding (NLU) and Natural Language Processing (NLP) tasks of any NLP model and NLP applications (either existing or new). This paper proposed a CWS pipeline method to enhance the efficiency and performance of NLP applications like text summarization, information retrieval, question-answer, machine reading comprehension, etc. The proposed CWS pipeline model used a pre-trained CoNLL-2012 coreference dataset extracted from the famous Ontonotes-5.0 dataset for the English language. The model implementation is done in Python language. The performance evaluation is done using the standard CoNLL-2012 coreference dataset for the English language. The coreference marked output is evaluated against the manually tagged gold standard dataset. The proposed CWS pipeline model gives 78.98% of the average F1 score on the MUC metric, 1.78% higher than the previous models' top result. CWS pipeline model performs better than existing models.}
}
@article{CODEN2005422,
title = {Domain-specific language models and lexicons for tagging},
journal = {Journal of Biomedical Informatics},
volume = {38},
number = {6},
pages = {422-430},
year = {2005},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2005.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S1532046405000213},
author = {Anni R. Coden and Serguei V. Pakhomov and Rie K. Ando and Patrick H. Duffy and Christopher G. Chute},
keywords = {Clinical report analysis, Part-of-speech tagging accuracy, Domain adaptation, Clinical information systems, Biomedical domain, Corpus linguistics, Statistical part-of-speech tagging, Hidden Markov Model},
abstract = {Accurate and reliable part-of-speech tagging is useful for many Natural Language Processing (NLP) tasks that form the foundation of NLP-based approaches to information retrieval and data mining. In general, large annotated corpora are necessary to achieve desired part-of-speech tagger accuracy. We show that a large annotated general-English corpus is not sufficient for building a part-of-speech tagger model adequate for tagging documents from the medical domain. However, adding a quite small domain-specific corpus to a large general-English one boosts performance to over 92% accuracy from 87% in our studies. We also suggest a number of characteristics to quantify the similarities between a training corpus and the test data. These results give guidance for creating an appropriate corpus for building a part-of-speech tagger model that gives satisfactory accuracy results on a new domain at a relatively small cost.}
}
@article{SULEMAN2021114130,
title = {Extending latent semantic analysis to manage its syntactic blindness},
journal = {Expert Systems with Applications},
volume = {165},
pages = {114130},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.114130},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420308782},
author = {Raja Muhammad Suleman and Ioannis Korkontzelos},
keywords = {Natural Language Processing, Natural Language Understanding, Latent Semantic Analysis, Semantic Similarity},
abstract = {Natural Language Processing (NLP) is the sub-field of Artificial Intelligence that represents and analyses human language automatically. NLP has been employed in many applications, such as information retrieval, information processing and automated answer ranking. Semantic analysis focuses on understanding the meaning of text. Among other proposed approaches, Latent Semantic Analysis (LSA) is a widely used corpus-based approach that evaluates similarity of text based on the semantic relations among words. LSA has been applied successfully in diverse language systems for calculating the semantic similarity of texts. LSA ignores the structure of sentences, i.e., it suffers from a syntactic blindness problem. LSA fails to distinguish between sentences that contain semantically similar words but have opposite meanings. Disregarding sentence structure, LSA cannot differentiate between a sentence and a list of keywords. If the list and the sentence contain similar words, comparing them using LSA would lead to a high similarity score. In this paper, we propose xLSA, an extension of LSA that focuses on the syntactic structure of sentences to overcome the syntactic blindness problem of the original LSA approach. xLSA was tested on sentence pairs that contain similar words but have significantly different meaning. Our results showed that xLSA alleviates the syntactic blindness problem, providing more realistic semantic similarity scores.}
}
@article{BOOTA2024102070,
title = {Integrating social media and deep learning for real-time urban waterlogging monitoring},
journal = {Journal of Hydrology: Regional Studies},
volume = {56},
pages = {102070},
year = {2024},
issn = {2214-5818},
doi = {https://doi.org/10.1016/j.ejrh.2024.102070},
url = {https://www.sciencedirect.com/science/article/pii/S2214581824004191},
author = {Muhammad Waseem Boota and Shan-e-hyder Soomro and Muhammad Irshad Ahmad and Sheheryar Khan and Haoming Xia and Yaochen Qin and Chaode Yan and Jikun Xu and Ayesha Yousaf and Muhammad Azeem Boota and Bilal Ahmed},
keywords = {Urban waterlogging, Social-media, Water depth information, Spatiotemporal evolution, Deep learning algorithms},
abstract = {Study region
Swat district, Khyber Pakhtunkhwa (KPK) Province, Pakistan.
Study focus
With the rise of social-media data, there is an increasing need to promptly and precisely identify content related to disasters, such as urban waterlogging. Social-media data, being cost-effective and abundant, can offer valuable insights into geographic phenomena by analyzing human behavioral patterns, making it a powerful resource for detailed waterlogging (WLG) analysis in urban settings.
Innovative insights
This research introduces a novel framework for precise information retrieval and real-time extraction of WLG points and fine-grained information in disaster-affected areas using the Facebook platform. First, topic modeling and transfer learning techniques were developed to examine the spatiotemporal dynamics of WLG locations. Second, water depth data from textual content and visual representations were extracted using various deep learning frameworks and integrated through decision-making processes. Third, a unique fine-grained location corpus tailored to urban flooding scenarios was created using the named entity recognition (NER) model. Finally, the BERT-BiLSTM-CRF model was employed to extract WLG points accurately. Using the Swat district as a case study, the extracted WLG points covered at least 79 % of the officially documented WLG points and were primarily located near roadways, especially in low-elevation areas. This framework provides a viable approach for enhancing situational awareness and conducting spatiotemporal analysis of urban floods and WLG disasters at the municipal level in real-time.}
}
@article{YUAN2024100030,
title = {Large language models illuminate a progressive pathway to artificial intelligent healthcare assistant},
journal = {Medicine Plus},
volume = {1},
number = {2},
pages = {100030},
year = {2024},
issn = {2950-3477},
doi = {https://doi.org/10.1016/j.medp.2024.100030},
url = {https://www.sciencedirect.com/science/article/pii/S2950347724000264},
author = {Mingze Yuan and Peng Bao and Jiajia Yuan and Yunhao Shen and Zifan Chen and Yi Xie and Jie Zhao and Quanzheng Li and Yang Chen and Li Zhang and Lin Shen and Bin Dong},
keywords = {Large language models, Artificial intelligence, Medicine, Healthcare assistant, Prompt engineering, In-context learning},
abstract = {With the rapid development of artificial intelligence, large language models (LLMs) have shown promising capabilities in mimicking human-level language comprehension and reasoning. This has sparked significant interest in applying LLMs to enhance various aspects of healthcare, ranging from medical education to clinical decision support. However, medicine involves multifaceted data modalities and nuanced reasoning skills, presenting challenges for integrating LLMs. This review introduces the fundamental applications of general-purpose and specialized LLMs, demonstrating their utilities in knowledge retrieval, research support, clinical workflow automation, and diagnostic assistance. Recognizing the inherent multimodality of medicine, the review emphasizes the multimodal LLMs and discusses their ability to process diverse data types like medical imaging and electronic health records to augment diagnostic accuracy. To address LLMs’ limitations regarding personalization and complex clinical reasoning, the review further explores the emerging development of LLM-powered autonomous agents for healthcare. Moreover, it summarizes the evaluation methodologies for assessing LLMs’ reliability and safety in medical contexts. LLMs have transformative potential in medicine; however, there is a pivotal need for continuous optimizations and ethical oversight before these models can be effectively integrated into clinical practice.}
}
@article{DETROJA2023200244,
title = {A survey on Relation Extraction},
journal = {Intelligent Systems with Applications},
volume = {19},
pages = {200244},
year = {2023},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2023.200244},
url = {https://www.sciencedirect.com/science/article/pii/S2667305323000698},
author = {Kartik Detroja and C.K. Bhensdadia and Brijesh S. Bhatt},
keywords = {Information Extraction (IE), Relation Extraction (RE), Machine Learning (ML), Deep Learning (DL), Convolutional Neural Network (CNN), Recurrent Neural Network (RNN)},
abstract = {With the advent of the Internet, the daily production of digital text in the form of social media, emails, blogs, news items, books, research papers, and Q&A forums has increased significantly. This unstructured or semi-structured text contains a huge amount of information. Information Extraction (IE) can extract meaningful information from text sources and present it in a structured format. The sub-tasks of IE include Named Entity Recognition (NER), Event Extraction, Relation Extraction (RE), Sentiment Extraction, Opinion Extraction, Terminology Extraction, Reference Extraction, and so on. One way to represent information in the text is in the form of entities and relations representing links between entities. The Entity Extraction task identifies entities from the text, and the Relation Extraction (RE) task can identify relationships between those entities. Many NLP applications can benefit from relational information derived from natural language, including Structured Search, Knowledge Base (KB) population, Information Retrieval, Question-Answering, Language Understanding, Ontology Learning, etc. This survey covers (1) basic concepts of Relation Extraction; (2) various Relation Extraction methodologies; (3) Deep Learning techniques for Relation Extraction; and (4) different datasets that can be used to evaluate the RE system.}
}
@article{LASTRADIAZ2019645,
title = {A reproducible survey on word embeddings and ontology-based methods for word similarity: Linear combinations outperform the state of the art},
journal = {Engineering Applications of Artificial Intelligence},
volume = {85},
pages = {645-665},
year = {2019},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2019.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S0952197619301745},
author = {Juan J. Lastra-Díaz and Josu Goikoetxea and Mohamed Ali {Hadj Taieb} and Ana García-Serrano and Mohamed {Ben Aouicha} and Eneko Agirre},
keywords = {Ontology-based semantic similarity measures, Word embedding models, Information Content models, WordNet, Experimental survey, HESML},
abstract = {Human similarity and relatedness judgements between concepts underlie most of cognitive capabilities, such as categorisation, memory, decision-making and reasoning. For this reason, the proposal of methods for the estimation of the degree of similarity and relatedness between words and concepts has been a very active line of research in the fields of artificial intelligence, information retrieval and natural language processing among others. Main approaches proposed in the literature can be categorised in two large families as follows: (1) Ontology-based semantic similarity Measures (OM) and (2) distributional measures whose most recent and successful methods are based on Word Embedding (WE) models. However, the lack of a deep analysis of both families of methods slows down the advance of this line of research and its applications. This work introduces the largest, reproducible and detailed experimental survey of OM measures and WE models reported in the literature which is based on the evaluation of both families of methods on a same software platform, with the aim of elucidating what is the state of the problem. We show that WE models which combine distributional and ontology-based information get the best results, and in addition, we show for the first time that a simple average of two best performing WE models with other ontology-based measures or WE models is able to improve the state of the art by a large margin. In addition, we provide a very detailed reproducibility protocol together with a collection of software tools and datasets as supplementary material to allow the exact replication of our results.}
}
@article{ARABZADEH2023104486,
title = {A self-supervised language model selection strategy for biomedical question answering},
journal = {Journal of Biomedical Informatics},
volume = {146},
pages = {104486},
year = {2023},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2023.104486},
url = {https://www.sciencedirect.com/science/article/pii/S1532046423002071},
author = {Negar Arabzadeh and Ebrahim Bagheri},
keywords = {Biomedical question answering, Domain-specific language model, General-purpose language model, Self-supervised learning},
abstract = {Large neural-based Pre-trained Language Models (PLM) have recently gained much attention due to their noteworthy performance in many downstream Information Retrieval (IR) and Natural Language Processing (NLP) tasks. PLMs can be categorized as either general-purpose, which are trained on resources such as large-scale Web corpora, and domain-specific which are trained on in-domain or mixed-domain corpora. While domain-specific PLMs have shown promising performance on domain-specific tasks, they are significantly more computationally expensive compared to general-purpose PLMs as they have to be either retrained or trained from scratch. The objective of our work in this paper is to explore whether it would be possible to leverage general-purpose PLMs to show competitive performance to domain-specific PLMs without the need for expensive retraining of the PLMs for domain-specific tasks. By focusing specifically on the recent BioASQ Biomedical Question Answering task, we show how different general-purpose PLMs show synergistic behaviour in terms of performance, which can lead to overall notable performance improvement when used in tandem with each other. More concretely, given a set of general-purpose PLMs, we propose a self-supervised method for training a classifier that systematically selects the PLM that is most likely to answer the question correctly on a per-input basis. We show that through such a selection strategy, the performance of general-purpose PLMs can become competitive with domain-specific PLMs while remaining computationally light since there is no need to retrain the large language model itself. We run experiments on the BioASQ dataset, which is a large-scale biomedical question-answering benchmark. We show that utilizing our proposed selection strategy can show statistically significant performance improvements on general-purpose language models with an average of 16.7% when using only lighter models such as DistilBERT and DistilRoBERTa, as well as 14.2% improvement when using relatively larger models such as BERT and RoBERTa and so, their performance become competitive with domain-specific large language models such as PubMedBERT.}
}
@article{CARROLL2024102899,
title = {Integrating large language models and generative artificial intelligence tools into information literacy instruction},
journal = {The Journal of Academic Librarianship},
volume = {50},
number = {4},
pages = {102899},
year = {2024},
issn = {0099-1333},
doi = {https://doi.org/10.1016/j.acalib.2024.102899},
url = {https://www.sciencedirect.com/science/article/pii/S0099133324000600},
author = {Alexander J. Carroll and Joshua Borycz},
keywords = {Generative artificial intelligence, Large language models, Information literacy, STEM education, Information retrieval, Critical thinking},
abstract = {Generative artificial intelligence (AI) and large language models (LLMs) have induced a mixture of excitement and panic among educators. However, there is a lack of consensus over how much experience science and engineering students have with using these tools for research-related tasks. Likewise, it is not yet known how educators and information professionals can leverage these tools to teach students strategies for information retrieval and knowledge synthesis. This study assesses the extent of students' use of AI tools in research-related tasks and if information literacy instruction could impact their perception of these tools. Responses to Likert-scale questions indicate that many students did not have extensive experience using LLMs for research-related purposes prior to the information literacy sessions. However, after participating in a didactic lecture and discussion with an engineering librarian that explored how to use these tools effectively and responsibly, many students reported viewing these tools as potentially useful for future assignments. Student responses to open-response questions suggest that librarian-led information literacy training can assist students in developing more sophisticated understandings of the limitations and use cases for artificial intelligence in inquiry-based coursework.}
}
@article{KUMAR2024100308,
title = {AOPWIKI-EXPLORER: An interactive graph-based query engine leveraging large language models},
journal = {Computational Toxicology},
volume = {30},
pages = {100308},
year = {2024},
issn = {2468-1113},
doi = {https://doi.org/10.1016/j.comtox.2024.100308},
url = {https://www.sciencedirect.com/science/article/pii/S2468111324000100},
author = {Saurav Kumar and Deepika Deepika and Karin Slater and Vikas Kumar},
keywords = {Adverse outcome pathway, Large language model, Graph database, Risk assessment, Artificial intelligence, Data integration, Information retrieval, Information extraction},
abstract = {Adverse Outcome Pathways (AOPs) provide a basis for non-animal testing, by outlining the cascade of molecular and cellular events initiated upon stressor exposure, leading to adverse effects. In recent years, the scientific community has shown interest in developing AOPs through crowdsourcing, with the results archived in the AOP-Wiki: a centralized repository coordinated by the OECD, hosting nearly 512 AOPs (April, 2023). However, the AOP-Wiki platform currently lacks a versatile querying system, which hinders developers' exploration of the AOP network and impedes its practical use in risk assessment. This work proposes to unleash the full potential of the AOP-Wiki archive by adapting its data into a Labelled Property Graph (LPG) schema. Additionally, the tool offers a visual network query interface for both database-specific and natural language queries, facilitating the retrieval and analysis of graph data. The multi-query interface allows non-technical users to construct flexible queries, thereby enhancing the potential for AOP exploration. By reducing the time and technical requirements, the present query engine enhances the practical utilization of the valuable data within AOP-Wiki. To evaluate the platform, a case study is presented with three levels of use-case scenarios (simple, moderate, and complex queries). AOPWIKI-EXPLORER is freely available on GitHub (https://github.com/Crispae/AOPWiki_Explorer) for wider community reach and further enhancement.}
}
@article{SCIANNAMEO2024108326,
title = {Information extraction from medical case reports using OpenAI InstructGPT},
journal = {Computer Methods and Programs in Biomedicine},
volume = {255},
pages = {108326},
year = {2024},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2024.108326},
url = {https://www.sciencedirect.com/science/article/pii/S0169260724003195},
author = {Veronica Sciannameo and Daniele Jahier Pagliari and Sara Urru and Piercesare Grimaldi and Honoria Ocagli and Sara Ahsani-Nasab and Rosanna Irene Comoretto and Dario Gregori and Paola Berchialla},
keywords = {Large language model, Natural language processing, Information retrieval, Case reports},
abstract = {Background and objective
Researchers commonly use automated solutions such as Natural Language Processing (NLP) systems to extract clinical information from large volumes of unstructured data. However, clinical text's poor semantic structure and domain-specific vocabulary can make it challenging to develop a one-size-fits-all solution. Large Language Models (LLMs), such as OpenAI's Generative Pre-Trained Transformer 3 (GPT-3), offer a promising solution for capturing and standardizing unstructured clinical information. This study evaluated the performance of InstructGPT, a family of models derived from LLM GPT-3, to extract relevant patient information from medical case reports and discussed the advantages and disadvantages of LLMs versus dedicated NLP methods.
Methods
In this paper, 208 articles related to case reports of foreign body injuries in children were identified by searching PubMed, Scopus, and Web of Science. A reviewer manually extracted information on sex, age, the object that caused the injury, and the injured body part for each patient to build a gold standard to compare the performance of InstructGPT.
Results
InstructGPT achieved high accuracy in classifying the sex, age, object and body part involved in the injury, with 94%, 82%, 94% and 89%, respectively. When excluding articles for which InstructGPT could not retrieve any information, the accuracy for determining the child's sex and age improved to 97%, and the accuracy for identifying the injured body part improved to 93%. InstructGPT was also able to extract information from non-English language articles.
Conclusions
The study highlights that LLMs have the potential to eliminate the necessity for task-specific training (zero-shot extraction), allowing the retrieval of clinical information from unstructured natural language text, particularly from published scientific literature like case reports, by directly utilizing the PDF file of the article without any pre-processing and without requiring any technical expertise in NLP or Machine Learning. The diverse nature of the corpus, which includes articles written in languages other than English, some of which contain a wide range of clinical details while others lack information, adds to the strength of the study.}
}
@article{UPADHYAY2024100088,
title = {A comprehensive survey on answer generation methods using NLP},
journal = {Natural Language Processing Journal},
volume = {8},
pages = {100088},
year = {2024},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2024.100088},
url = {https://www.sciencedirect.com/science/article/pii/S2949719124000360},
author = {Prashant Upadhyay and Rishabh Agarwal and Sumeet Dhiman and Abhinav Sarkar and Saumya Chaturvedi},
keywords = {Question-answering systems, Natural language processing, Question analysis, Answer extraction, Information retrieval},
abstract = {Recent advancements in question-answering systems have significantly enhanced the capability of computers to understand and respond to queries in natural language. This paper presents a comprehensive review of the evolution of question answering systems, with a focus on the developments over the last few years. We examine the foundational aspects of a question answering framework, including question analysis, answer extraction, and passage retrieval. Additionally, we delve into the challenges that question answering systems encounter, such as the intricacies of question processing, the necessity of contextual data sources, and the complexities involved in real-time question answering. Our study categorizes existing question answering systems based on the types of questions they address, the nature of the answers they produce, and the various approaches employed to generate these answers. We also explore the distinctions between opinion-based, extraction-based, retrieval-based, and generative answer generation. The classification provides insight into the strengths and limitations of each method, paving the way for future innovations in the field. This review aims to offer a clear understanding of the current state of question answering systems and to identify the scaling needed to meet the rising expectations and demands of users for coherent and accurate automated responses in natural language.}
}