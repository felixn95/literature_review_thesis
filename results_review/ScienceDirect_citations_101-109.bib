@article{MASSAI201970,
title = {PAVAL: A location-aware virtual personal assistant for retrieving geolocated points of interest and location-based services},
journal = {Engineering Applications of Artificial Intelligence},
volume = {77},
pages = {70-85},
year = {2019},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2018.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S0952197618301994},
author = {Lorenzo Massai and Paolo Nesi and Gianni Pantaleo},
keywords = {Virtual personal assistants, Location-aware recommender systems, Natural language processing, User-intent detection, Semantic web technologies, Geographic information retrieval, Geoparsing, Geocoding},
abstract = {Today most of the users on the move require contextualized local and georeferenced information. Several solutions aim to meet these trends, thus assisting users and satisfying their needs and preferences, such as virtual assistants and Location-Aware Recommender Systems (LARS), both in commercial and research literature. However, general purpose virtual assistants usually have to manage large domains, dealing with big amounts of data and online resources, losingfocus on more specific requirements and local information. On the other hand, traditional recommender systems are based on filtering techniques and contextual knowledge, and they usually do not rely on Natural Language Processing (NLP) features on users’ queries, which are useful to understand and contextualize users’ necessities on the spot. Therefore, comprehending the actual users’ information needs and other key information that can be included in the user query, such as geographical references, is a challenging task which is not yet fully accomplished by current state-of-the-art solutions. In this paper, we propose Paval (Location-Aware Virtual Personal Assistant 2 2The name Paval is chosen as a permutation of the initials of “Location-aware virtual personal assistant”.), a semantic assisting engine for suggesting local points of interest (POIs) and services by analyzing users’ natural language queries, in order to estimate the information need and potential geographic references expressed by the users. The system exploits NLP and semantic techniques providing as output recommendations on local geolocated POIs and services which best match the users’ requests, retrieved by querying our semantic Km4City Knowledge Base. The proposed system is validated against the most popular virtual assistants, such as Google Assistant, Apple Siri and Microsoft Cortana, focusing the assessment on the request of geolocated POIs and services, showing very promising capabilities in successfully estimating the users’ information needs and multiple geographic references.}
}
@article{ANDREASEN2024102246,
title = {The power and potentials of Flexible Query Answering Systems: A critical and comprehensive analysis},
journal = {Data & Knowledge Engineering},
volume = {149},
pages = {102246},
year = {2024},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2023.102246},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X23001064},
author = {Troels Andreasen and Gloria Bordogna and Guy De Tré and Janusz Kacprzyk and Henrik Legind Larsen and Sławomir Zadrożny},
keywords = {Flexible query answering, Model-based query answering, Data-driven query answering},
abstract = {The popularity of chatbots, such as ChatGPT, has brought research attention to question answering systems, capable to generate natural language answers to user’s natural language queries. However, also in other kinds of systems, flexibility of querying, including but also going beyond the use of natural language, is an important feature. With this consideration in mind the paper presents a critical and comprehensive analysis of recent developments, trends and challenges of Flexible Query Answering Systems (FQASs). Flexible query answering is a multidisciplinary research field that is not limited to question answering in natural language, but comprises other query forms and interaction modalities, which aim to provide powerful means and techniques for better reflecting human preferences and intentions to retrieve relevant information. It adopts methods at the crossroad of several disciplines among which Information Retrieval (IR), databases, knowledge based systems, knowledge and data engineering, Natural Language Processing (NLP) and the semantic web may be mentioned. The analysis principles are inspired by the Preferred Reporting Items for Systematic Reviews and Meta-Analysis (PRISMA) framework, characterized by a top-down process, starting with relevant keywords for the topic of interest to retrieve relevant articles from meta-sources And complementing these articles with other relevant articles from seed sources Identified by a bottom-up process. to mine the retrieved publication data a network analysis is performed Which allows to present in a synthetic way intrinsic topics of the selected publications. issues dealt with are related to query answering methods Both model-based and data-driven (the latter based on either machine learning or deep learning) And to their needs for explainability and fairness to deal with big data Notably by taking into account data veracity. conclusions point out trends and challenges to help better shaping the future of the FQAS field.}
}
@article{REDEY199933,
title = {iCTRL: Intensional conformal text representation language},
journal = {Artificial Intelligence},
volume = {109},
number = {1},
pages = {33-70},
year = {1999},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(99)00016-8},
url = {https://www.sciencedirect.com/science/article/pii/S0004370299000168},
author = {Gábor Rédey},
keywords = {Intensional logic, Aristotelian term logic, Sentence–formula proximity, Knowledge base validation, Natural language syntax conform text modelling, Computer-aided knowledge acquisition, Content relevant textual knowledge base query handling, Information retrieval systems},
abstract = {A new compact and homogeneous symbolism is introduced to achieve a more general and exact representation of natural language texts. Traditional first-order and intensional logic cannot cope with numerous natural language phenomena such as the large variety of modalities, satisfactory interpretation of iterative application of modal operators or certain modelling problems like one-to-one sentence–formula mapping. The CTRL/iCTRL formalism can model them successfully and they are able to control many other different shades of meaning by applying only a minimal number of syntactic tools. The most profitable and beneficial AI application of the presented natural language syntax consistent knowledge representation technique is automated knowledge acquisition: computer-aided textual data base generation and logical inference based information retrieval. CTRL/iCTRL applicability is demonstrated by various illustrative examples including a transparent graphical interpretation analogous to Frege's graph language that help clarify new concepts and exemplify partial inappropriateness of traditional logical language. The CTRL/iCTRL paradigm is based on a novel and interesting synthesis of the two traditional logic schools, the Stoic and the Peripatetic school, refuting a century long scientific prejudice against the latter stated to be completely outworn. An interesting issue of this analysis points out that expressing subordination unconsciously and simply by co-ordination causes a typical restriction of meaning in classical logic.}
}
@article{AMBALAVANAN2020103578,
title = {Using the contextual language model BERT for multi-criteria classification of scientific articles},
journal = {Journal of Biomedical Informatics},
volume = {112},
pages = {103578},
year = {2020},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2020.103578},
url = {https://www.sciencedirect.com/science/article/pii/S1532046420302069},
author = {Ashwin Karthik Ambalavanan and Murthy V. Devarakonda},
keywords = {Biomedical natural language processing, Neural networks, Screening scientific articles, Text classification, Machine learning, BERT, SciBERT},
abstract = {Background
Finding specific scientific articles in a large collection is an important natural language processing challenge in the biomedical domain. Systematic reviews and interactive article search are the type of downstream applications that benefit from addressing this problem. The task often involves screening articles for a combination of selection criteria. While machine learning was previously used for this purpose, it is not known if different criteria should be modeled together or separately in an ensemble model. The performance impact of the modern contextual language models on the task is also not known.
Methods
We framed the problem as text classification and conducted experiments to compare ensemble architectures, where the selection criteria were mapped to the components of the ensemble. We proposed a novel cascade ensemble analogous to the step-wise screening process employed in developing the gold standard. We compared performance of the ensembles with a single integrated model, which we refer to as the individual task learner (ITL). We used SciBERT, a variant of BERT pre-trained on scientific articles, and conducted experiments using a manually annotated dataset of ~49 K MEDLINE abstracts, known as Clinical Hedges.
Results
The cascade ensemble had significantly higher precision (0.663 vs. 0.388 vs. 0.478 vs. 0.320) and F measure (0.753 vs. 0.553 vs. 0.628 vs. 0.477) than ITL and ensembles using Boolean logic and a feed-forward network. However, ITL had significantly higher recall than the other classifiers (0.965 vs. 0.872 vs. 0.917 vs. 0.944). In fixed high recall studies, ITL achieved 0.509 precision @ 0.970 recall and 0.381 precision @ 0.985 recall on a subset that was studied earlier, and 0.295 precision @ 0.985 recall on the full dataset, all of which were improvements over the previous studies.
Conclusion
Pre-trained neural contextual language models (e.g. SciBERT) performed well for screening scientific articles. Performance at high fixed recall makes the single integrated model (ITL) more suitable among the architectures considered here, for systematic reviews. However, high F measure of the cascade ensemble makes it a better approach for interactive search applications. The effectiveness of the cascade ensemble architecture suggests broader applicability beyond this task and the dataset, and the approach is analogous to query optimization in Information Retrieval and query optimization in databases.}
}
@article{GUO2014168,
title = {Enhancing a Rule-based Event Coder with Semantic Vectors},
journal = {Procedia Computer Science},
volume = {36},
pages = {168-174},
year = {2014},
note = {Complex Adaptive Systems Philadelphia, PA November 3-5, 2014},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.09.074},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914013234},
author = {Jinhong K. Guo and David {Van Brackle} and Martin O. Hofmann},
keywords = {NLP, semantic vectors, Random Indexing, event coding, rule-based system, machine learning},
abstract = {Rule based systems have achieved success in applications such as information retrieval and Natural Language Processing. However, due to the rigidity of pattern matching, these systems typically require a large number of rules to adequately cover the variations of expression in unstructured text. Consequently, knowledge engineering for a new domain and knowledge maintenance for a fielded system are labor intensive and expensive. In this paper, we present our research on enhancing a rule-based event coding system by relaxing the rigidity of pattern matching with a technique that formulates and matches patterns of the semantics of words instead of literal words. Our technique pairs literal words with semantic vectors that accumulate word meaning from the context of use of the word found in dictionaries, ontologies, and domain corpora. Our method improves the speed, accuracy, and coverage of the event coding algorithm without additional knowledge engineering effort. Operating on semantics instead of syntax, the improved system eases the workload of human analysts who screen input text for critical events. Our algorithms are based on high-dimensional distributed representations, and their effectiveness and versatility derive from the unintuitive properties of such representations---from the mathematical properties of high-dimensional spaces. Our current implementation encodes words, phrases, and rule patterns as semantic vectors using WordNet, We have started experimental evaluation using a large newswire dataset.}
}
@article{ABUATA2015104,
title = {A rule-based stemmer for Arabic Gulf dialect},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {27},
number = {2},
pages = {104-112},
year = {2015},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2014.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S1319157815000191},
author = {Belal Abuata and Asma Al-Omari},
keywords = {Arabic dialect stemmer, Gulf dialect, Rule base stemming, Arabic NLP},
abstract = {Arabic dialects arewidely used from many years ago instead of Modern Standard Arabic language in many fields. The presence of dialects in any language is a big challenge. Dialects add a new set of variational dimensions in some fields like natural language processing, information retrieval and even in Arabic chatting between different Arab nationals. Spoken dialects have no standard morphological, phonological and lexical like Modern Standard Arabic. Hence, the objective of this paper is to describe a procedure or algorithm by which a stem for the Arabian Gulf dialect can be defined. The algorithm is rule based. Special rules are created to remove the suffixes and prefixes of the dialect words. Also, the algorithm applies rules related to the word size and the relation between adjacent letters. The algorithm was tested for a number of words and given a good correct stem ratio. The algorithm is also compared with two Modern Standard Arabic algorithms. The results showed that Modern Standard Arabic stemmers performed poorly with Arabic Gulf dialect and our algorithm performed poorly when applied for Modern Standard Arabic words.}
}
@article{GAO2021100,
title = {Advances and challenges in conversational recommender systems: A survey},
journal = {AI Open},
volume = {2},
pages = {100-126},
year = {2021},
issn = {2666-6510},
doi = {https://doi.org/10.1016/j.aiopen.2021.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S2666651021000164},
author = {Chongming Gao and Wenqiang Lei and Xiangnan He and Maarten {de Rijke} and Tat-Seng Chua},
keywords = {Conversational recommendation system, Interactive recommendation, Preference elicitation, Multi-turn conversation strategy, Exploration-exploitation},
abstract = {Recommender systems exploit interaction history to estimate user preference, having been heavily used in a wide range of industry applications. However, static recommendation models are difficult to answer two important questions well due to inherent shortcomings: (a) What exactly does a user like? (b) Why does a user like an item? The shortcomings are due to the way that static models learn user preference, i.e., without explicit instructions and active feedback from users. The recent rise of conversational recommender systems (CRSs) changes this situation fundamentally. In a CRS, users and the system can dynamically communicate through natural language interactions, which provide unprecedented opportunities to explicitly obtain the exact preference of users. Considerable efforts, spread across disparate settings and applications, have been put into developing CRSs. Existing models, technologies, and evaluation methods for CRSs are far from mature. In this paper, we provide a systematic review of the techniques used in current CRSs. We summarize the key challenges of developing CRSs in five directions: (1) Question-based user preference elicitation. (2) Multi-turn conversational recommendation strategies. (3) Dialogue understanding and generation. (4) Exploitation-exploration trade-offs. (5) Evaluation and user simulation. These research directions involve multiple research fields like information retrieval (IR), natural language processing (NLP), and human-computer interaction (HCI). Based on these research directions, we discuss some future challenges and opportunities. We provide a road map for researchers from multiple communities to get started in this area. We hope this survey can help to identify and address challenges in CRSs and inspire future research.}
}
@article{ZHU2014275,
title = {Using large clinical corpora for query expansion in text-based cohort identification},
journal = {Journal of Biomedical Informatics},
volume = {49},
pages = {275-281},
year = {2014},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2014.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S1532046414000677},
author = {Dongqing Zhu and Stephen Wu and Ben Carterette and Hongfang Liu},
keywords = {Cohort identification, Information retrieval, Query expansion, Clinical text, Electronic medical records},
abstract = {In light of the heightened problems of polysemy, synonymy, and hyponymy in clinical text, we hypothesize that patient cohort identification can be improved by using a large, in-domain clinical corpus for query expansion. We evaluate the utility of four auxiliary collections for the Text REtrieval Conference task of IR-based cohort retrieval, considering the effects of collection size, the inherent difficulty of a query, and the interaction between the collections. Each collection was applied to aid in cohort retrieval from the Pittsburgh NLP Repository by using a mixture of relevance models. Measured by mean average precision, performance using any auxiliary resource (MAP=0.386 and above) is shown to improve over the baseline query likelihood model (MAP=0.373). Considering subsets of the Mayo Clinic collection, we found that after including 2.5 billion term instances, retrieval is not improved by adding more instances. However, adding the Mayo Clinic collection did improve performance significantly over any existing setup, with a system using all four auxiliary collections obtaining the best results (MAP=0.4223). Because optimal results in the mixture of relevance models would require selective sampling of the collections, the common sense approach of “use all available data” is inappropriate. However, we found that it was still beneficial to add the Mayo corpus to any mixture of relevance models. On the task of IR-based cohort identification, query expansion with the Mayo Clinic corpus resulted in consistent and significant improvements. As such, any IR query expansion with access to a large clinical corpus could benefit from the additional resource. Additionally, we have shown that more data is not necessarily better, implying that there is value in collection curation.}
}
@article{PRADEEPA20123215,
title = {Analyzing Distillation Process of Hidden Terms in Web Documents for IR},
journal = {Procedia Engineering},
volume = {38},
pages = {3215-3221},
year = {2012},
note = {INTERNATIONAL CONFERENCE ON MODELLING OPTIMIZATION AND COMPUTING},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2012.06.372},
url = {https://www.sciencedirect.com/science/article/pii/S1877705812022850},
author = {M. Pradeepa and C. Deisy},
keywords = {Web mining, hidden terms, sparse data, Latent Dirichlet Allocation (LDA), Gibbs sampler and clustering},
abstract = {The previous work in web based applications such as mining web content, pattern recognition and similarity measures between the web documents. This paper is about, analyzing web documents in an enhanced way and delve the distillation web document will be the next pace in hypertext mining. The sparse document is a very little data on the web, which may face problems like different words with almost identical or similar meanings and sparseness. Natural language processing (NLP) and information retrieval (IR) are the main obstacles of the above problem. The mining of hidden terms discovers the search queries from large external datasets (universal datasets). It helps to handle unseen data in a better way. The goal of this web document mining consists of an efficient information finding, filtering information based on user query, and discovers more topic focused keywords based on the rich source of global information datasets. The proposed method we use the Distillation model, it is the integration of probabilistic generative model, Gibbs sampling algorithm and deployment method. This model can be applied for different natural languages and data domains for achieving the goal.}
}