"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"The NLP Cookbook: Modern Recipes for Transformer Based Deep Learning Architectures","S. Singh; A. Mahmood","Department of Computer Science and Engineering, University of Bridgeport, Bridgeport, CT, USA; Department of Computer Science and Engineering, University of Bridgeport, Bridgeport, CT, USA",IEEE Access,"13 May 2021","2021","9","","68675","68702","In recent years, Natural Language Processing (NLP) models have achieved phenomenal success in linguistic and semantic tasks like text classification, machine translation, cognitive dialogue systems, information retrieval via Natural Language Understanding (NLU), and Natural Language Generation (NLG). This feat is primarily attributed due to the seminal Transformer architecture, leading to designs such as BERT, GPT (I, II, III), etc. Although these large-size models have achieved unprecedented performances, they come at high computational costs. Consequently, some of the recent NLP architectures have utilized concepts of transfer learning, pruning, quantization, and knowledge distillation to achieve moderate model sizes while keeping nearly similar performances as achieved by their predecessors. Additionally, to mitigate the data size challenge raised by language models from a knowledge extraction perspective, Knowledge Retrievers have been built to extricate explicit data documents from a large corpus of databases with greater efficiency and accuracy. Recent research has also focused on superior inference by providing efficient attention to longer input sequences. In this paper, we summarize and examine the current state-of-the-art (SOTA) NLP models that have been employed for numerous NLP tasks for optimal performance and efficiency. We provide a detailed understanding and functioning of the different architectures, a taxonomy of NLP designs, comparative evaluations, and future directions in NLP.","2169-3536","","10.1109/ACCESS.2021.3077350","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9422763","Deep learning;natural language processing (NLP);natural language understanding (NLU);natural language generation (NLG);information retrieval (IR);knowledge distillation (KD);pruning;quantization","Biological system modeling;Task analysis;Computer architecture;Computational modeling;Natural language processing;Recurrent neural networks;Quantization (signal)","","58","","98","CCBYNCND","4 May 2021","","","IEEE","IEEE Journals"
"On the Effectiveness of Pre-Trained Language Models for Legal Natural Language Processing: An Empirical Study","D. Song; S. Gao; B. He; F. Schilder","Thomson Reuters, Eagan, MN, USA; Thomson Reuters, New York, NY, USA; Meta Platforms Inc., Menlo Park, CA, USA; Thomson Reuters, Eagan, MN, USA",IEEE Access,"25 Jul 2022","2022","10","","75835","75858","We present the first comprehensive empirical evaluation of pre-trained language models (PLMs) for legal natural language processing (NLP) in order to examine their effectiveness in this domain. Our study covers eight representative and challenging legal datasets, ranging from 900 to 57K samples, across five NLP tasks: binary classification, multi-label classification, multiple choice question answering, summarization and information retrieval. We first run unsupervised, classical machine learning and/or non-PLM based deep learning methods on these datasets, and show that baseline systems’ performance can be 4%~35% lower than that of PLM-based methods. Next, we compare general-domain PLMs and those specifically pre-trained for the legal domain, and find that domain-specific PLMs demonstrate 1%~5% higher performance than general-domain models, but only when the datasets are extremely close to the pre-training corpora. Finally, we evaluate six general-domain state-of-the-art systems, and show that they have limited generalizability to legal data, with performance gains from 0.1% to 1.2% over other PLM-based methods. Our experiments suggest that both general-domain and domain-specific PLM-based methods generally achieve better results than simpler methods on most tasks, with the exception of the retrieval task, where the best-performing baseline outperformed all PLM-based methods by at least 5%. Our findings can help legal NLP practitioners choose the appropriate methods for different tasks, and also shed light on potential future directions for legal NLP research.","2169-3536","","10.1109/ACCESS.2022.3190408","Thomson Reuters; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9826728","Legal natural language processing;pre-trained language model;deep learning;machine learning","Law;Task analysis;Feature extraction;Deep learning;Bit error rate;Support vector machines;Convolutional neural networks","","7","","129","CCBY","12 Jul 2022","","","IEEE","IEEE Journals"
"A Conceptual Model Framework for XAI Requirement Elicitation of Application Domain System","M. Aslam; D. Segura-Velandia; Y. M. Goh","Wolfson School of Mechanical, Electrical and Manufacturing Engineering, Loughborough University, Loughborough, U.K.; Wolfson School of Mechanical, Electrical and Manufacturing Engineering, Loughborough University, Loughborough, U.K.; Wolfson School of Mechanical, Electrical and Manufacturing Engineering, Loughborough University, Loughborough, U.K.",IEEE Access,"9 Oct 2023","2023","11","","108080","108091","The use of data analytics and Machine Learning (ML) branches of AI for predictive and analytic knowledge retrieval has surged significantly in various industries (e.g., health, finance, business, and manufacturing). However, the acceptance of AI has been hindered by opaque models that lack transparency. Explainability in AI (XAI) has gained significant prominence owing to its focus on introducing avenues of accountability in AI. XAI acknowledges the importance of human factors and strives to incorporate them into the design process, recognising that the cognitive effort involved in understanding explanations is a key aspect. Mental Models play a crucial role in the XAI evaluative premise, but their current utility is limited. By intentionally designing explanations that align with users’ mental models, their experiences can be significantly enhanced, leading to improved understanding, satisfaction, trust, and performance. This study proposes using Mental Models to elicit explainability requirements and to develop an Ontology-Driven Conceptual Model to facilitate the learning process for a better understanding of explanations.","2169-3536","","10.1109/ACCESS.2023.3315605","Engineering and Physical Sciences Research Council (EPSRC)(grant numbers:EP/V062042/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10251525","Conceptual model;explainability in AI;mental models;requirements elicitation","Artificial intelligence;Cognitive science;Decision making;Requirements engineering;Ontologies;Context modeling;Cognitive processes","","2","","68","CCBY","14 Sep 2023","","","IEEE","IEEE Journals"
"Building a Multilevel Inflection Handling Stemmer to Improve Search Effectiveness for Urdu Language","A. Jabbar; S. Iqbal; A. A. Alaulamie; M. Ilahi","Department of Computer Science, COMSATS University Islamabad, Main Campus, Islamabad, Pakistan; Department of Information Systems, College of Computer Science and Information Technology, King Faisal University, Hofuf, Saudi Arabia; Department of Information Systems, College of Computer Science and Information Technology, King Faisal University, Hofuf, Saudi Arabia; Department of Computer Science, COMSATS University Islamabad, Main Campus, Islamabad, Pakistan",IEEE Access,"19 Mar 2024","2024","12","","39313","39329","Stemming is an essential step in various Natural Language Processing (NLP) applications and is used to reduce different variants of the query words to a standard form to avoid the vocabulary mismatch issue in Information Retrieval (IR) systems. Due to specific grammatical rules and complex morphological structures, finding an effective stemming algorithm in Urdu is a challenging task. Although, several stemming algorithms have been proposed for the Urdu text stemming; however, none of them extract the stem from multilevel inflected forms. In this context, according to the best of our knowledge, this is a first effort towards the proposition and evaluation of a novel Urdu Text Stemmer (UTS) that can deal with multi-level inflection forms in Urdu text. The experimental evaluation of the proposed scheme has been conducted on the text-based and word-based custom-developed corpus. The proposed stemming technique is rigorously evaluated and compared with state-of-the-art stemming algorithms. Experimental results demonstrate that UTS outperforms existing Urdu stemmers and achieves an accuracy of 94.92% and 91.8% on word corpus and text corpus, respectively. We also evaluated our proposed system in an Information Retrieval application for Urdu, using the Collection for Urdu Retrieval Evaluation (CURE) dataset. Our approach for information retrieval outperformed and improved both recall and precision metrics.","2169-3536","","10.1109/ACCESS.2024.3373714","Deanship of Scientific Research, Vice Presidency for Graduate Studies and Scientific Research, King Faisal University, Saudi Arabia(grant numbers:5992); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10460562","Stemmer;information retrieval;Urdu stemmer;lemmatizer;natural language processing;text mining","Information retrieval;Natural language processing;Text mining;Vocabulary;Measurement techniques;Data models;Information analysis","","","","72","CCBYNCND","8 Mar 2024","","","IEEE","IEEE Journals"
"Transparent, Low Resource, and Context-Aware Information Retrieval From a Closed Domain Knowledge Base","S. Rateria; S. Singh","Custiv Manufacturing, Bengaluru, India; Department of Information and Communication Technology, Manipal Institute of Technology, Manipal Academy of Higher Education, Manipal, India",IEEE Access,"28 Mar 2024","2024","12","","44233","44243","In large-scale enterprises, vast amounts of textual information are shared across corporate repositories and intranet websites. Traditional search techniques that lack context sensitivity, often fail to retrieve pertinent data efficiently. Modern techniques that use a distributed representation of words require a considerable training dataset and computation, thereby presenting financial and operational burdens. Generative models for information search suffer from problems of transparency and hallucination, which can be detrimental, especially for organizations and their stakeholders who rely on these results for critical business operations. This paper presents a non-goal oriented conversational agent based on a collection of finite state machines and an information search model for text search from an extensive collection of stored corporate documents and intranet websites. We used a distributed representation of words derived from the BERT model, which allows for contextual searching. We minimally fine-tuned a BERT model on a multi-label text classification task specific to a closed-domain knowledge base. Based on DCG metrics, our information retrieval model using distributed embeddings from the minimally trained BERT model and Word Movers Distance for calculating topic similarity is more relevant to user queries than BERT embeddings with cosine similarity and BM25. Our architecture promises to significantly expedite and improve the accuracy of information retrieval in closed-domain systems without the need for a massive training dataset or expensive computing while maintaining transparency.","2169-3536","","10.1109/ACCESS.2024.3380006","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10477429","Natural language processing;conversational agent;information retrieval;closed domain knowledge base;BERT word movers distance","Chatbots;Oral communication;Computational modeling;Information retrieval;Neural networks;Training;Data mining;Natural language processing;Knowledge based systems;Text analysis;Text categorization","","","","53","CCBYNCND","21 Mar 2024","","","IEEE","IEEE Journals"
"Consumer Document Analytical Accelerator Hardware","A. Radhakrishnan; D. Mahapatra; A. James","School of Electronic Systems and Automation, Kerala University of Digital Sciences Innovation and Technology (Digital University Kerala), Thiruvananthapuram, Kerala, India; School of Electronic Systems and Automation, Kerala University of Digital Sciences Innovation and Technology (Digital University Kerala), Thiruvananthapuram, Kerala, India; School of Electronic Systems and Automation, Kerala University of Digital Sciences Innovation and Technology (Digital University Kerala), Thiruvananthapuram, Kerala, India",IEEE Access,"20 Jan 2023","2023","11","","5161","5167","Document scanning devices are used for visual character recognition, followed by text analytics in the software. Often such character extraction is insecure, and any third party can manipulate the information. On the other hand, near-edge processing devices are restrained by limited resources and connectivity issues. The primary factors that lead to exploring independent hardware devices with natural language processing (NLP) capabilities are latency during cloud processing and computing costs. This paper introduces a hardware accelerator for information retrieval using memristive TF-IDF implementation. In this system, each sentence is represented using a memristive crossbar layer, with each column containing a single word. The number of matching scores for the TF and IDF values was implemented using operational amplifier-based comparator accumulator circuits. The circuit is designed with a 180nm CMOS process, Knowm Multi-Stable Switch memristor model, and WOx device parameters. We compared its performance with that of a standard benchmark dataset. Variability and device-to-device related issues were also taken into consideration in the analysis. This paper concludes with implementing TF-IDF score calculation for applications such as information retrieval and text summarization.","2169-3536","","10.1109/ACCESS.2023.3237463","Clootrack Pvt Ltd.; Kerala University of Digital Sciences Innovation and Technology (Digital University Kerala); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10018187","Natural language processing;TF-IDF;hardware accelerator;memristive systems;memristor;analog computation","Data mining;Information retrieval;Feature extraction;Data models;Analytical models;Memristors;Databases","","","","25","CCBYNCND","16 Jan 2023","","","IEEE","IEEE Journals"
"Graph-Based Text Representation and Matching: A Review of the State of the Art and Future Challenges","A. H. Osman; O. M. Barukub","Department of Information System, Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia; Department of Information System, Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia",IEEE Access,"20 May 2020","2020","8","","87562","87583","Graph-based text representation is one of the important preprocessing steps in data and text mining, Natural Language Processing (NLP), and information retrieval approaches. The graph-based methods focus on how to represent text documents in the shape of a graph to exploit the best features of their characteristics. This study reviews and lists the advantages and disadvantages of such methods employed or developed in graph-based text representations. The literature shows that some of the proposed graph-based methods suffer from a lack of representing texts in certain situations. Currently, several techniques are commonly used in graph-based text representation. However, there are still some weaknesses and shortages in these techniques and tools that significantly affect the success of graph representation and graph matching. In this review, we conduct an inclusive survey of the state of the art in graph-based text representation and learning. We provide a formal description of the problem of graph-based text representation and introduce some basic concepts. More significantly, this study proposes a new taxonomy of graph-based text representation, categorizing the existing studies based on representation characteristics and scheme techniques. In terms of the representation scheme taxonomy, we introduce four main types of conceptual graph schemes and summarize the challenges faced in each scheme. The main issues of graph representation, such as research topics and the sub-taxonomy of graph models for web documents, are introduced and categorized. This research also covers some tasks of understanding natural language processing (NLP) that depend on different types of graph structures. In addition, the graph matching taxonomy implements three main categories based on the matching approach, including structural-, semantic-, and similarity-based approaches. Moreover, a deep comparison of these approaches is discussed and reported in terms of methods and tools, the concepts of matching and locality, and the application domains that use these tools. Finally, the paper recommends seven promising future study directions in the graph-based text representation field. These recommendation points are summarized and highlighted as open problems and challenges of graph-based text representation and learning to facilitate and fill the research gaps for scientific researchers in this field.","2169-3536","","10.1109/ACCESS.2020.2993191","Deanship of Scientific Research (DSR), King Abdulaziz University, Jeddah(grant numbers:DF-681-830-1441); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9088989","Graph representation;NLP;graph;graph matching;representation scheme;text mining","Natural language processing;Numerical models;Text mining;Semantics;Labeling;Tools;Taxonomy","","28","","175","CCBY","7 May 2020","","","IEEE","IEEE Journals"
"Classical Arabic Named Entity Recognition Using Variant Deep Neural Network Architectures and BERT","N. Alsaaran; M. Alrabiah","Computer Science Department, Imam Muhammad Ibn Saud Islamic University, Riyadh, Saudi Arabia; Computer Science Department, Imam Muhammad Ibn Saud Islamic University, Riyadh, Saudi Arabia",IEEE Access,"30 Jun 2021","2021","9","","91537","91547","Recurrent Neural Networks (RNNs) and transformers are deep learning models that have achieved remarkable success in several Natural Language Processing (NLP) tasks since they do not rely on handcrafted features nor enormous knowledge resources. Named Entity Recognition (NER) is an essential NLP task that is used in many applications such as information retrieval, question answering, and machine translation. NER aims to locate, extract, and classify named entities into predefined categories such as person, organization and location. Arabic NER is considered a challenging task because of the complexity and the unique characteristics of Arabic. Most of the previous research on deep learning based-Arabic NER focused on Modern Standard Arabic and Dialectal Arabic, which are different variations from Classical Arabic. In this paper, we investigate deep learning-based Classical Arabic NER using different deep neural network architectures and a BERT based contextual language model that is trained on general domain Arabic text. We propose two RNN-based models by fine-tunning the pretrained BERT language model to recognize and classify named entities from Classical Arabic. The pre-trained BERT contextual language model representations were used as input features to a BGRU/BLSTM model and were fine-tuned using a Classical Arabic NER dataset. In addition, we explore variant architectures of the proposed BERT-BGRU/BLSTM-CRF models. Experimentations showed that the BERT-BGRU-CRF model outperformed the other models by achieving an F-measure of 94.76% on the CANERCorpus. To the best of our knowledge, this is the first work that aims to recognize named entities in Classical Arabic using deep learning.","2169-3536","","10.1109/ACCESS.2021.3092261","Deanship of Scientific Research at Imam Mohammad Ibn Saud Islamic University through the Graduate Students Research Support Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9464352","NER;named entity recognition;classical arabic;BGRU;BLSTM;BERT;CRF;deep learning;natural language processing","Task analysis;Bit error rate;Deep learning;Context modeling;Natural language processing;Transfer learning;Feature extraction","","22","","70","CCBY","24 Jun 2021","","","IEEE","IEEE Journals"
"An Analytical Analysis of Text Stemming Methodologies in Information Retrieval and Natural Language Processing Systems","A. Jabbar; S. Iqbal; M. I. Tamimy; A. Rehman; S. A. Bahaj; T. Saba","Department of Computer Science, COMSATS University Islamabad (CUI), Main Campus, Tarlai Kalan, Islamabad, Pakistan; Department of Information Systems, College of Computer Science and Information Technology, King Faisal University, Al Hofuf, Saudi Arabia; Department of Computer Science, COMSATS University Islamabad (CUI), Main Campus, Tarlai Kalan, Islamabad, Pakistan; Artificial Intelligence & Data Analytics Laboratory (AIDA), CCIS, Prince Sultan University, Riyadh, Saudi Arabia; MIS Department, College of Business Administration, Prince Sattam bin Abdulaziz University, Al-Kharj, Saudi Arabia; Artificial Intelligence & Data Analytics Laboratory (AIDA), CCIS, Prince Sultan University, Riyadh, Saudi Arabia",IEEE Access,"1 Dec 2023","2023","11","","133681","133702","The exponential increase in textual unstructured digital data creates significant demand for advanced and smart stemming systems. As a preprocessing stage, stemming is applied in various research fields such as information retrieval (IR), domain vocabulary analysis, and feature reduction in many natural language processing (NLP). Text stemming (TS), an important step, can significantly improve performance in such systems. Text-stemming methods developed till now could be better in their results and can produce errors of different types leading to degraded performance of the applications in which these are used. This work presents a systematic study with an in-depth review of selected stemming works published from 1968 to 2023. The work presents a multidimensional review of studied stemming algorithms i.e., methodology, data source, performance, and evaluation methods. For this study, we have chosen different stemmers, which can be categorized as 1) linguistic knowledge-based, 2) statistical, 3) corpus-based, 4) context-sensitive, and 5) hybrid stemmers. The study shows that linguistic knowledge-based stemming techniques were widely used for highly inflected languages (such as Arabic, Hindi, and Urdu) and have reported higher accuracy than other techniques. We compare and analyze the performance of various state-of-the-art TS approaches, including their issues and challenges, which are summarized as research gaps. This work also analyzes different NLP applications utilizing stemming methods. At the end, we list the future work directions for interested researchers.","2169-3536","","10.1109/ACCESS.2023.3332710","Deanship of Scientific Research, Vice Presidency for Graduate Studies and Scientific Research, King Faisal University, Saudi Arabia(grant numbers:5428); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10318122","Text stemming;information retrieval (IR) systems;text classification;stemmer evaluation;technological development;natural language processing (NLP)","Natural language processing;Vocabulary;Information retrieval;Linguistics;Text categorization;Sentiment analysis;Tokenization","","5","","121","CCBYNCND","14 Nov 2023","","","IEEE","IEEE Journals"
"A Generative AI-Driven Method-Level Semantic Clone Detection Based on the Structural and Semantical Comparison of Methods","A. Gupta; R. Goyal","University School of Information, Communication and Technology, Guru Gobind Singh (GGS) Indraprastha University, New Delhi, India; University School of Information, Communication and Technology, Guru Gobind Singh (GGS) Indraprastha University, New Delhi, India",IEEE Access,"23 May 2024","2024","12","","70773","70791","Code fragments with identical or similar functionality are called code clones. This study aims to detect semantic clones in Java-based programs, focusing on the method-level granularity. To accomplish this, our approach extracts and compares both semantical and structural features of Java methods and applies certain heuristics to obtain the final result set. The semantics of a method are often described by its documentation, while its structural details are characterized by its implementation within the code body. However, Java methods may not always be accompanied by descriptive documentation. To address this, we have employed a Generative AI tool named ChatGPT, which has the ability to understand the given code and generate its documentation. To compare the documentation of methods, we have used various corpus-based and knowledge-based information retrieval (IR) techniques. However, for comparing a method’s structural details, we tokenized the method’s body and applied an information retrieval technique called VSM (Vector Space Modelling) on the tokenized body. Additionally, we also extracted and compared certain method-level metrics for this purpose. Further, while doing the semantical comparison of methods, eight IR variants are formed based on the internal processing requirements of different IR techniques. The technique proposed in this paper relies on the textual and metrics-based analysis of Java programs with few parsing requirements, making it lightweight and less computation-intensive. To examine the efficiency of the proposed technique, we have validated it using a semantic clone benchmark. The results show that the proposed technique detects semantic clones with high recall values ranging from 67% to 81% and precision values ranging from 60% to 96% for different IR variants explored in our research.","2169-3536","","10.1109/ACCESS.2024.3401770","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10531717","Clone detection;semantic clones;information retrieval;metrics;documentation","Cloning;Semantics;Codes;Documentation;Large scale integration;Measurement;Source coding;Information retrieval;Generative AI;Java","","","","85","CCBYNCND","16 May 2024","","","IEEE","IEEE Journals"
"An Eclectic Approach for Enhancing Language Models Through Rich Embedding Features","E. Aldana-Bobadilla; V. J. Sosa-Sosa; A. Molina-Villegas; K. Gazca-Hernandez; J. A. Olivas","CONAHCYT, Mexico City, Mexico; Cinvestav, Unidad Tamaulipas, Ciudad Victoria, Tamaulipas, Mexico; CONAHCYT, Mexico City, Mexico; Cinvestav, Unidad Tamaulipas, Ciudad Victoria, Tamaulipas, Mexico; Grupo SMILe, Universidad de Castilla-La Mancha, Ciudad Real, Spain",IEEE Access,"24 Jul 2024","2024","12","","100921","100938","Text processing is a fundamental aspect of Natural Language Processing (NLP) and is crucial for various applications in fields such as artificial intelligence, data science, and information retrieval. It plays a core role in language models. Most text-processing approaches focus on describing and synthesizing, to a greater or lesser degree, lexical, syntactic, and semantic properties of text in the form of numerical vectors that induce a metric space, in which, it is possible to find underlying patterns and structures related to the original text. Since each approach has strengths and weaknesses, finding a single approach that perfectly extracts representative text properties for every task and application domain is hard. This paper proposes a novel approach capable of synthesizing information from heterogeneous state-of-the-art text processing approaches into a unified representation. Encouraging results demonstrate that using this representation in popular machine-learning tasks not only leads to superior performance but also offers notable advantages in memory efficiency and preservation of underlying information of the distinct sources involved in such a representation.","2169-3536","","10.1109/ACCESS.2024.3422971","Consejo Tamaulipeco de Ciencia y Tecnologia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10584535","Self-organizing map;word embeddings;feature extraction;natural language processing","Feature extraction;Task analysis;Semantics;Transformers;Neurons;Linguistics;Self-organizing feature maps;Natural language processing;Text analysis","","","","66","CCBYNCND","4 Jul 2024","","","IEEE","IEEE Journals"
"A Deep Learning Approach for Robust Detection of Bots in Twitter Using Transformers","D. Martín-Gutiérrez; G. Hernández-Peñaloza; A. B. Hernández; A. Lozano-Diez; F. Álvarez","Visual Telecommunication Applications Group, Signals, Systems and Radio Communications (SSR) Department, Universidad Politécnica de Madrid, Madrid, Spain; Visual Telecommunication Applications Group, Signals, Systems and Radio Communications (SSR) Department, Universidad Politécnica de Madrid, Madrid, Spain; Visual Telecommunication Applications Group, Signals, Systems and Radio Communications (SSR) Department, Universidad Politécnica de Madrid, Madrid, Spain; AUDIAS–Audio Data Intelligence and Speech, Universidad Autónoma de Madrid, Madrid, Spain; Visual Telecommunication Applications Group, Signals, Systems and Radio Communications (SSR) Department, Universidad Politécnica de Madrid, Madrid, Spain",IEEE Access,"13 Apr 2021","2021","9","","54591","54601","During the last decades, the volume of multimedia content posted in social networks has grown exponentially and such information is immediately propagated and consumed by a significant number of users. In this scenario, the disruption of fake news providers and bot accounts for spreading propaganda information as well as sensitive content throughout the network has fostered applied research to automatically measure the reliability of social networks accounts via Artificial Intelligence (AI). In this paper, we present a multilingual approach for addressing the bot identification task in Twitter via Deep learning (DL) approaches to support end-users when checking the credibility of a certain Twitter account. To do so, several experiments were conducted using state-of-the-art Multilingual Language Models to generate an encoding of the text-based features of the user account that are later on concatenated with the rest of the metadata to build a potential input vector on top of a Dense Network denoted as Bot-DenseNet. Consequently, this paper assesses the language constraint from previous studies where the encoding of the user account only considered either the metadata information or the metadata information together with some basic semantic text features. Moreover, the Bot-DenseNet produces a low-dimensional representation of the user account which can be used for any application within the Information Retrieval (IR) framework.","2169-3536","","10.1109/ACCESS.2021.3068659","H2020 European Project: FAke News discovery and propagation from big Data ANalysis and artificial intelliGence Operations (FANDANGO)(grant numbers:780355); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9385071","Artificial intelligence;bot detector;deep learning;feature representation;language models;misinformation detection;social media mining;transfer learning;transformers","Social networking (online);Blogs;Encoding;Metadata;Feature extraction;Deep learning;Task analysis","","36","","48","CCBY","24 Mar 2021","","","IEEE","IEEE Journals"
"Research on Topic Recognition of Network Sensitive Information Based on SW-LDA Model","G. Xu; X. Wu; H. Yao; F. Li; Z. Yu","School of Information Engineering, Minzu University of China, Beijing, China; School of Information Engineering, Minzu University of China, Beijing, China; School of Information Engineering, Minzu University of China, Beijing, China; School of Information Engineering, Minzu University of China, Beijing, China; School of Information Engineering, Minzu University of China, Beijing, China",IEEE Access,"25 Feb 2019","2019","7","","21527","21538","The mining of network sensitive information is of great significance for understanding the social stability of the network. Obtaining the network public opinion of sensitive information is helpful to master Internet users' attitudes toward important social events. The related artificial intelligence technology can achieve the topics from the network texts. At present, the current topic recognition model has a low recognition rate for sensitive information and usually generates some inaccurate topic keywords. In this paper, a topic recognition method of the network sensitive information based on a sensitive word weighted-latent Dirichlet allocation (LDA) model is proposed. First, the basic sensitive word vocabulary is constructed by manual collection, and the embedding representation of the word is obtained through the training of a large amount of network corpus based onWord2vec. The semantic similarity between the word embedding is calculated to extend the basic sensitive word vocabulary. Second, the extended sensitive word vocabulary is embedded in the LDA model. On the one hand, it can improve the semantic understanding and the recognition ability of LDA for the sensitive topic words and promote the quality of the generated topic words. On the other hand, it can also improve the relevance of the topic keywords and the related topics and find more fine-grained keywords. The experimental results show that the sensitive word weighted-LDA model can effectively improve the topic recognition quantity and quality of sensitive information. This paper is helpful to the development of artificial intelligence. The generated corpus in this paper is meaningful to the research of text classification, clustering and information retrieval, and so on.","2169-3536","","10.1109/ACCESS.2019.2897475","Project of Humanities and Social Sciences, Ministry of Education of China(grant numbers:18YJA740059); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8633906","Sensitive information;topic recognition;word embedding;artificial intelligence","Semantics;Internet;Vocabulary;Feature extraction;Data mining;Artificial intelligence;Resource management","","16","","37","OAPA","3 Feb 2019","","","IEEE","IEEE Journals"
"Make It Directly: Event Extraction Based on Tree-LSTM and Bi-GRU","W. Yu; M. Yi; X. Huang; X. Yi; Q. Yuan","PLA Information Engineering University, Zhengzhou, China; PLA Information Engineering University, Zhengzhou, China; PLA Information Engineering University, Zhengzhou, China; PLA Information Engineering University, Zhengzhou, China; PLA Information Engineering University, Zhengzhou, China",IEEE Access,"28 Jan 2020","2020","8","","14344","14354","Event extraction is an important research direction in the field of natural language processing (NLP) applications including information retrieval (IR). Traditional event extraction is realized with two methods: the pipeline and the joint extraction methods. The pipeline method determines the event by triggering word recognition to further implement event extraction and is prone to error cascading. The joint extraction method applies deep learning to implement the completion of the trigger word and the argument role classification task. Most studies with the joint extraction method adopt the CNN or RNN network structure. However, in the case of event extraction, deeper understanding of complex contexts is required. Existing studies do not make full use of syntactic relations. This paper proposes a novel event extraction model, which is built upon a Tree-LSTM network and a Bi-GRU network and carries syntactically related information. It is illustrated that this method simultaneously uses Tree-LSTM and Bi-GRU to obtain a representation of the candidate event sentence and identify the event type, which results in a better performance compared to the ones that use chain structured LSTM, CNN or only Tree-LSTM. Finally, the hidden state of each node is used in Tree-LSTM to predict a label for candidate arguments and identify/classify all arguments of an event. Lab results show that the proposed event extraction model achieves competitive results compared to previous works.","2169-3536","","10.1109/ACCESS.2020.2965964","National Basic Research Program of China (973 Program)(grant numbers:2016YFB0201402); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8957160","Event extraction;Bi-GRU;Tree-LSTM","Data mining;Task analysis;Hidden Markov models;Feature extraction;Information retrieval;Semantics;Computational modeling","","16","","35","CCBY","13 Jan 2020","","","IEEE","IEEE Journals"
"A Non-Exclusive Multi-Class Convolutional Neural Network for the Classification of Functional Requirements in AUTOSAR Software Requirement Specification Text","S. Jp; V. K. Menon; K. Soman; A. K. R. Ojha","Center for Computational Engineering and Networking, Amrita School of Engineering, Amrita Vishwa Vidyapeetham, Coimbatore, India; KeepFlying, 5 Tampines Central 6, Singapore; Center for Computational Engineering and Networking, Amrita School of Engineering, Amrita Vishwa Vidyapeetham, Coimbatore, India; Data Science Institute, National University of Ireland Galway, Galway, Ireland",IEEE Access,"15 Nov 2022","2022","10","","117707","117714","Software Requirement Specification (SRS) describes a software system to be developed that captures the functional, non-functional, and technical aspects of the stakeholder’s requirements. Retrieval and extraction of software information from SRS are essential to the development of software product line (SPL). Albeit Natural Language Processing (NLP) techniques, such as information retrieval and standard machine learning, have been advocated in the recent past as a semi-automatic means of optimising requirements specifications, they have not been widely embraced. The complexity in the organization’s information makes requirement analysis intricately a challenging task. The interdependence of subsystems and within an organisation drives this complexity. A plain multi-class classification framework may not address this issue. Hence, this paper propounds an automated non-exclusive approach for classification of functional requirements from SRS, using a deep learning framework. Specifically, Word2Vec and FastText word embeddings are utilised for document representation for training a convolutional neural network (CNN). The study was carried out by the compilation of manually categorised relevant enterprise data (AUTomotive Open System ARchitecture (AUTOSAR)), which were also employed for model training. Over a convolutional neural network, the impact of data trained with Word2Vec and FastText word embeddings from SRS documentation were compared to pre-trained word embeddings models, available online.","2169-3536","","10.1109/ACCESS.2022.3217752","Science Foundation Ireland (SFI)(grant numbers:SFI/12/RC/2289_P2 Insight_2); Panlingua Language Processing LLP; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9931679","Functional requirement;software requirement specification;convolutional neural network;multi-layer perceptron;word embedding","Software engineering;Convolutional neural networks;Documentation;Support vector machines;Semantics;Automotive engineering;Embedded systems","","2","","26","CCBY","28 Oct 2022","","","IEEE","IEEE Journals"
"Knowledge Based Deep Inception Model for Web Page Classification","A. Gupta; R. Bhatia","Department of Computer Science and Engineering, Punjab Engineering College (Deemed to be University), Chandigarh, India; Department of Computer Science and Engineering, Punjab Engineering College (Deemed to be University), Chandigarh, India",Journal of Web Engineering,"22 Sep 2023","2021","20","7","2131","2168","Web Page Classification is decisive for information retrieval and management task and plays an imperative role for natural language processing (NLP) problems in web engineering. Traditional machine learning algorithms excerpt covet features from web pages whereas deep leaning algorithms crave features as the network goes deeper. Pre-trained models such as BERT attains remarkable achievement for text classification and continue to show state-of-the-art results. Knowledge Graphs can provide rich structured factual information for better language modelling and representation. In this study, we proposed an ensemble Knowledge Based Deep Inception (KBDI) approach for web page classification by learning bidirectional contextual representation using pre-trained BERT incorporating Knowledge Graph embeddings and fine-tune the target task by applying Deep Inception network utilizing parallel multi-scale semantics. Proposed ensemble evaluates the efficacy of fusing domain specific knowledge embeddings with the pre-trained BERT model. Experimental interpretation exhibit that the proposed BERT fused KBDI model outperforms benchmark baselines and achieve better performance in contrast to other conventional approaches evaluated on web page classification datasets.","1544-5976","","10.13052/jwe1540-9589.2075","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10246785","Web page classification;transfer learning;knowledge graph embedding;pre-trained model","Machine learning algorithms;Knowledge based systems;Text categorization;Semantics;Web pages;Knowledge graphs;Information retrieval","","1","","53","","22 Sep 2023","","","River Publishers","River Publishers Journals"
"Effective Natural Language Processing and Interpretable Machine Learning for Structuring CT Liver-Tumor Reports","Y. -H. Chuang; J. -H. Su; D. -H. Han; Y. -W. Liao; Y. -C. Lee; Y. -F. Cheng; T. -P. Hong; K. S. -M. Li; H. -Y. Ou; Y. Lu; C. -C. Wang","Liver Transplantation Program, Department of Diagnostic Radiology and Surgery, Kaohsiung Chang Gung Memorial Hospital, Chang Gung University College of Medicine, Niao-Sung, Kaohsiung, Taiwan; Department of Computer Science and Information Engineering, National University of Kaohsiung, Kaohsiung, Taiwan; Department of Computer Science and Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan; Department of Intelligent Commerce, National Kaohsiung University of Science and Technology, Kaohsiung, Taiwan; Department of Information Management, Cheng Shiu University, Kaohsiung, Taiwan; Liver Transplantation Program, Department of Diagnostic Radiology and Surgery, Kaohsiung Chang Gung Memorial Hospital, Chang Gung University College of Medicine, Niao-Sung, Kaohsiung, Taiwan; Department of Computer Science and Information Engineering, National University of Kaohsiung, Kaohsiung, Taiwan; Department of Computer Science and Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan; Liver Transplantation Program, Department of Diagnostic Radiology and Surgery, Kaohsiung Chang Gung Memorial Hospital, Chang Gung University College of Medicine, Niao-Sung, Kaohsiung, Taiwan; Liver Transplantation Program, Department of Diagnostic Radiology and Surgery, Kaohsiung Chang Gung Memorial Hospital, Chang Gung University College of Medicine, Niao-Sung, Kaohsiung, Taiwan; Liver Transplantation Center and Department of Surgery, Kaohsiung Chang Gung Memorial Hospital, Niao-Sung, Kaohsiung, Taiwan",IEEE Access,"10 Nov 2022","2022","10","","116273","116286","In the past, the liver tumors were reported manually in an unstructured format. There actually exists much valuable knowledge in these reports for further disease risk assessment, disease recognition and treatment recommendation. Yet, it is not easy to read and mine knowledge from the unstructured reports. Hence, how to extract the knowledge from these biomedical reports effectively and efficiently has been a challenging issue in the past decades. Although a set of Natural Language Processing techniques were proposed for Bio-medical information retrieval, few related works were made on transforming the unstructured CT liver-tumor reports into structured ones. To aim at this issue, in this paper, we propose a two-stage report structuring method by integrating effective Natural Language Processing (NLP) and interpretable machine learning. For the first stage, the candidate keywords in unstructured reports are extracted. Next, the feature keywords are determined by the feature-selection technique. For the second stage, the well-known multi-classifiers are performed, and finally the reports are labeled in a refined structure format. Further, the factor keywords in the classification model are filtered to interpret the performance. In overall, the proposed report structuring method generates a hierarchical data structure, including the common features and refined features in the  $1^{\mathrm {st}}$  and  $2^{\mathrm {nd}}$  levels/stages, respectively. To reveal the performance of proposed method, a set of evaluations were conducted and the results show that, the proposed method is more promising than the fashion neural networks such as Bert (Bidirectional Encoder Representations from Transformers) in terms of effectiveness and efficiency.","2169-3536","","10.1109/ACCESS.2022.3218646","Ministry of Science and Technology, Republic of China(grant numbers:MOST 110-2321-B-182A-003 (NZRPG8L0031),MOST 111-2321-B-182A-003 (NZRPG8L0032)); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9933737","Structured reports;natural language processing;interpretable machine learning;CT liver-tumors;biomedical science","Natural language processing;Computed tomography;Information retrieval;Tumors;Machine learning;Visualization;Biomedical imaging;Liver cancer","","1","","48","CCBY","1 Nov 2022","","","IEEE","IEEE Journals"
"Exploring Topic Coherence with PCC-LDA and BERT for Contextual Word Generation","S. K. Rachamadugu; T. P. Pushphavathi; S. B. Khan; M. Alojail","Department of CSE, M.S.Ramaiah University of Applied Sciences, Bangalore, Karnataka, India; Department of CSE, M.S.Ramaiah University of Applied Sciences, Bangalore, Karnataka, India; School of Science, Engineering and Environment, University of Salford, United Kingdom; Management Information System Department, College of Business Administration, King Saud University, Saudi Arabia",IEEE Access,"","2024","PP","99","1","1","In the field of natural language processing (NLP), topic modeling and word generation are crucial for comprehending and producing texts that resemble human languages. Extracting key phrases is an essential task that aids document summarization, information retrieval, and topic classification. Topic modeling significantly enhances our understanding of the latent structure of textual data. Latent Dirichlet Allocation (LDA) is a popular algorithm for topic modeling, which assumes that every document is a mix of several topics, and each topic will have multiple words. A new model similar to LDA, but a better version called Probabilistic Correlated Clustering Latent Dirichlet Allocation (PCC-LDA) was recently introduced. On the other hand, BERT is an advanced bidirectional pre-trained language model that understands words in a sentence based on the full context to generate more precise and contextually correct words. Topic modeling is a useful way to discover hidden themes or topics within a range of documents aiming to tune better topics from the corpus and enhance topic modeling implementation. The experiments indicated a significant improvement in performance when using this combination approach. Coherence criteria of are utilized to judge whether the words in each topic accord with prior knowledge, which could ensure that topics are interpretable and meaningful. The above results of the topic-level analysis indicate that PCC-LDA consistency topics perform better than LDA and NMF(non-negative matrix factorization Technique) by at least 15.4%,12.9%(k=5) and up to nearly 12.5% and 11.8% ( k=10) respectively, where k represents the number of topics.","2169-3536","","10.1109/ACCESS.2024.3477992","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10713309","BERT;key phrases;LDA;topic coherence;topic modeling","Coherence;Encoding;Data models;Bidirectional control;Semantics;Data mining;Analytical models;Context modeling;Probabilistic logic;Measurement","","","","","CCBYNCND","10 Oct 2024","","","IEEE","IEEE Early Access Articles"
"MediGPT: Exploring Potentials of Conventional and Large Language Models on Medical Data","M. Abu Tareq Rony; M. Shariful Islam; T. Sultan; S. Alshathri; W. El-Shafai","Department of Statistics, Noakhali Science & Technology University, Noakhali, Bangladesh; Department of Computer Science and Telecommunication Engineering, Noakhali Science & Technology University, Noakhali, Bangladesh; Department of Computer Science, College of Computer and Information Sciences, Fordham University, Bronx, NY, USA; Department of Information Technology, College of Computer and Information Sciences, Princess Nourah Bint Abdulrahman University, P. O. Box, 84428, Riyadh, Saudi Arabia; Computer Science Department, Security Engineering Laboratory, Prince Sultan University, Riyadh, Saudi Arabia",IEEE Access,"5 Aug 2024","2024","12","","103473","103487","Medical text classification organizes medical documents into categories to streamline information retrieval and support clinical decision-making. Traditional machine learning techniques, including pre-trained language models, are effective but require extensive domain-specific training data, often underperform across languages, and are costly and complex to deploy on a large scale. In this study, we employed four datasets: Clinical trials on cancer, encompassing 6 million statements from interventional cancer clinical trial protocols; the Illness-dataset, consisting of 22,660 categorized tweets from 2018 and 2019; the Multi-View active learning for short medical text classification in user-generated data, an extended version of the Illness-dataset including 22,660 documents from the same period; and the Symptom2Disease dataset, containing 1,200 data points used to predict diseases based on symptom descriptions. This study uses ChatGPT, particularly its ChatGPT-3.5 and ChatGPT-4 versions, as a viable alternative for classifying medical texts. We investigate essential aspects, including the construction of prompts, the parsing of responses, and the various strategic use of GPT models to optimize outcomes. Through comparative analysis with established methods like pre-trained language model fine-tuning and prompt-tuning, our findings indicate that ChatGPT addresses these challenges efficiently and matches the performance of traditional methods. Furthermore, the enhanced capabilities of the proposed MediGPT (Medical Generative Pre-Trained Transformers) have led to performance improvements of 14.3%, 22.3%, 13.6%, and 13.7% across the datasets, highlighting its adaptability and robustness in diverse medical text scenarios without the need for specialized domain adjustments. This research underscores the capability of ChatGPT to facilitate a versatile AI framework in medical text processing, which could revolutionize medical informatics practices.","2169-3536","","10.1109/ACCESS.2024.3428918","Princess Nourah Bint Abdulrahman University, Riyadh, Saudi Arabia, through the Researchers Supporting Project(grant numbers:PNURSP2024R197); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10599190","Medical text;natural language processing;ChatGPT;classification;large language model","Protocols;Text categorization;Training data;Clinical trials;Chatbots;Transformers;Data models","","1","","32","CCBYNCND","16 Jul 2024","","","IEEE","IEEE Journals"
"Research on Semantic Similarity of Short Text Based on Bert and Time Warping Distance","S. Qiu; Y. Niu; J. Li; X. Li","School of Computer Science, Hubei University of Technology, Wuhan, China; School of Computer Science, Hubei University of Technology, Wuhan, China; School of Computing, Hubei University of Technology, Wuhan, China; China Communications Services Sci and Tech Co., Ltd., Wuhan, China",Journal of Web Engineering,"22 Sep 2023","2021","20","8","2521","2544","The research on semantic similarity of short text plays an important role in machine translation, emotion analysis, information retrieval and other AI business applications. However, according to existing short text similarity research, the characteristics of ambiguous vocabularies are difficult to be effectively analyzed, the solution of the problem caused by words order needs to be further optimized as well. This paper proposes a short text semantic similarity calculation method that combines BERT and time warping distance algorithm, in order to solve the problem of vocabulary ambiguity. The model first uses the pre trained Bert model to extract the semantic features of the short text from the whole level, and obtains a 768 dimensional short text feature vector. Then, it transforms the extracted feature vector into a point sequence in space, uses the CTW algorithm to calculate the time warping distance between the curves connected by the point sequence, and finally uses the weight function designed by the analysis, according to the smaller the time warpage distance is, the higher the degree of small similarity is, to calculate the similarity between short texts. The experimental results show that this model can mine the feature information of ambiguous words, and calculate the similarity of short texts with lexical ambiguity effectively. Compared with other models, it can distinguish the semantic features of ambiguous words more accurately.","1544-5976","","10.13052/jwe1540-9589.20814","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10246912","BERT;CTW;time warping distance;lexical ambiguity;semantic similarity","Training;Vocabulary;Analytical models;Semantics;Transforms;Feature extraction;Machine translation","","","","17","","22 Sep 2023","","","River Publishers","River Publishers Journals"
"Noisy Token Removal for Bug Localization: The Impact of Semantically Confusing Misguiding Terms","Y. Kim; M. Kim; E. Lee","Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon-si, Gyeonggi-do, South Korea; Department of Artificial Intelligence Convergence, Chonnam National University, Gwangju, South Korea; College of Computing and Informatics, Sungkyunkwan University, Suwon-si, Gyeonggi-do, South Korea",IEEE Access,"22 Nov 2024","2024","12","","172396","172409","A bug report is a technical document describing bugs that have occurred in the software. Finding the source code files to resolve a reported bug is a laborious task. To automate this process, information retrieval-based bug localization (IRBL) techniques have been proposed. These techniques assess the relevance between the bug report and source files, providing developers with a ranked list of source files. They rely heavily on text tokens, making it essential to remove noisy tokens from the input tokens. To address the problem of prevalent noisy tokens deteriorating IRBL performance, we define impactful noisy words as misguiding terms and investigate their prevalence and impact. We employed a deep learning model combined with explainable AI techniques to detect misguiding terms, leveraging their semantic embedding capabilities. We conducted extensive experiments on 24 open-source software projects and three IRBL models. By removing misguiding terms, the mean reciprocal rank of bug localization improved by 19%, 17%, and 27% for three models on average and up to 120%. The proposed approach effectively distinguishes between beneficial terms and noise, leading to superior IRBL performance compared to the existing noise detection approaches, with consistent improvements observed across 24 projects.","2169-3536","","10.1109/ACCESS.2024.3500367","Institute of Information & Communications Technology Planning & Evaluation (IITP); Korean Government through the Ministry of Science and ICT (MSIT), South Korea, through the Development of Software Reliability Improvement Technology through Identification of Abnormal Open Sources and Automatic Application of DevSecOps (60%)(grant numbers:RS-2024-00438686); IITP under the Artificial Intelligence Convergence Innovation Human Resources Development Program(grant numbers:IITP-2023-RS-2023-00256629); MSIT under the Information Technology Research Center (ITRC); IITP(grant numbers:IITP-2024-RS-2024-00437718); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10755074","Automated debugging;bug localization;bug report;deep learning;explainable AI;information retrieval;misguiding terms","Computer bugs;Noise measurement;Vectors;Location awareness;Data models;Software;Explainable AI;Deep learning;Training;Source coding","","","","48","CCBY","18 Nov 2024","","","IEEE","IEEE Journals"
"A History and Theory of Textual Event Detection and Recognition","Y. Chen; Z. Ding; Q. Zheng; Y. Qin; R. Huang; N. Shah","Guizhou Provincial Key Laboratory of Public Big Data, College of Computer Science and Technology, Guizhou University, Guiyang, China; Guizhou Provincial Key Laboratory of Public Big Data, College of Computer Science and Technology, Guizhou University, Guiyang, China; Department of Computer Science and Technology, Xi’an Jiaotong University, Xi’an, China; Guizhou Provincial Key Laboratory of Public Big Data, College of Computer Science and Technology, Guizhou University, Guiyang, China; Guizhou Provincial Key Laboratory of Public Big Data, College of Computer Science and Technology, Guizhou University, Guiyang, China; School of Computing, Electronics and Maths, Coventry University, Coventry, U.K.",IEEE Access,"18 Nov 2020","2020","8","","201371","201392","There is large and growing amounts of textual data that contains information about human activities. Mining interesting knowledge from this textual data is a challenging task because it consists of unstructured or semistructured text that are written in natural language. In the field of artificial intelligence, event-oriented techniques are helpful in addressing this problem, where information retrieval (IR), information extraction (IE) and graph methods (GMs) are three of the most important paradigms in supporting event-oriented processing. In recent years, due to information explosions, textual event detection and recognition have received extensive research attention and achieved great success. Many surveys have been conducted to retrospectively assess the development of event detection. However, until now, all of these surveys have focused on only a single aspect of IR, IE or GMs. There is no research that provides a complete introduction or a comparison of IR, IE, and GMs. In this article, a survey about these techniques is provided from a broader perspective, and a convenient and comprehensive comparison of these techniques is given. The hallmark of this article is that it is the first survey that combines IR, IE and GMs in a single frame and will therefore benefit researchers by acting as a reference in this field.","2169-3536","","10.1109/ACCESS.2020.3034907","National Natural Science Foundation of China through the Joint Funds(grant numbers:U1836205); National Natural Science Foundation of China through the Major Research Program(grant numbers:91746116); National Natural Science Foundation of China(grant numbers:62066007,62066008); Major Special Science and Technology Projects of Guizhou Province(grant numbers:[2017]3002); Key Projects of Science and Technology of Guizhou Province(grant numbers:[2020[ 1Z055); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9245483","Event detection;event recognition;information extraction;information retrieval","Semantics;Task analysis;Event detection;Graphics;History;Natural language processing;Linguistics","","4","","277","CCBY","30 Oct 2020","","","IEEE","IEEE Journals"
