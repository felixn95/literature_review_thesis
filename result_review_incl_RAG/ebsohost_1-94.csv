Article Title,Author,Journal Title,ISSN,ISBN,Publication Date,Volume,Issue,First Page,Page Count,Accession Number,DOI,Publisher,Doctype,Subjects,Keywords,Abstract,PLink
"A case study of duplications detection for educational domain thorough ad hoc search and identification NLP-based method.","Mikhaylov, S.N.; Chuikova, V.V.; Sokolova, Marina V.; Potapenko, A.M.","Expert Systems",="02664720",,="Aug2017","34","4","n/a","11","124518582","10.1111/exsy.12200","Wiley-Blackwell","Article","NATURAL language processing; ELECTRONIC records; WEB services; COMPUTER software; Computer, computer peripheral and pre-packaged software merchant wholesalers; Computer and Computer Peripheral Equipment and Software Merchant Wholesalers; Computer and software stores; Software publishers (except video game publishers); All Other Miscellaneous Schools and Instruction; Educational Support Services; Administration of Education Programs; Data Processing, Hosting, and Related Services; TEACHING; EDUCATION","evaluation; information resource; information retrieval; natural language processing; visual interface for knowledge representation","During the organization and planning of lecture courses for a discipline, its content may be overlapped and partially delivered in more than one course. Sometimes this action causes time loss through unnecessary repeating. This paper introduces an automated tool for duplications detections adapting methods of natural language processing used for Web search. The experiment for unstructured electronic document repositories clustering for thematic duplicate identification in different documents in the case of educational domain is presented. A prototype of this Web service-based software search engine is being designed and discussed. The experiment aimed to identify thematic duplicates of various courses within one of the teaching disciplines is also presented. [ABSTRACT FROM AUTHOR] Copyright of Expert Systems is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=124518582&site=bsi-live"
"A compressed trie structure using divided keys.","Oono, Masaki; Atlam, El-Sayed; Fuketa, Masao; Morita, Kazuhiro; Jun-ichi Aoe","International Journal of Computer Applications in Technology",="09528091",,="2009","34","2","101","7","37178004","10.1504/IJCAT.2009.023615","Inderscience Enterprises Ltd.","Article","INFORMATION retrieval; AUTOMATION; SYSTEMS engineering; SIMULATION methods & models; ARTIFICIAL intelligence; COMPUTER storage devices; Computer and peripheral equipment manufacturing; Computer Storage Device Manufacturing; MACHINE learning","data compression; dictionary; divided keys; information retrieval; natural language processing; storage capacity; trie search","A link-trie structure is an efficient data structure for collocation information using a trie structure. The link-trie stores two basic words into the trie and defines link-information by a link-function. This paper presents how to apply the link-trie into a general set of keys and compress the storage capacity. The method divides a key into several sub-keys and defines link-information between these sub-keys. From simulation results for 100,000 keys, it turns out that the presented method compresses the storage capacity by 30% smaller than the normal trie. [ABSTRACT FROM AUTHOR] Copyright of International Journal of Computer Applications in Technology is the property of Inderscience Enterprises Ltd. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=37178004&site=bsi-live"
"A hybrid approach to Arabic named entity recognition.","Shaalan, Khaled; Oudah, Mai","Journal of Information Science",="01655515",,="Feb2014","40","1","67","21","93719428","10.1177/0165551513502417","Sage Publications, Ltd.","Article","NATURAL language processing; DATA mining; MACHINE learning; INFORMATION retrieval research; ARABIC language","hybrid approach; information extraction; information retrieval; machine learning approach; named entity recognition; natural language processing; rule-based approach","In this paper, we propose a hybrid named entity recognition (NER) approach that takes the advantages of rule-based and machine learning-based approaches in order to improve the overall system performance and overcome the knowledge elicitation bottleneck and the lack of resources for underdeveloped languages that require deep language processing, such as Arabic. The complexity of Arabic poses special challenges to researchers of Arabic NER, which is essential for both monolingual and multilingual applications. We used the hybrid approach to develop an Arabic NER system that is capable of recognizing 11 types of Arabic named entities: Person, Location, Organization, Date, Time, Price, Measurement, Percent, Phone Number, ISBN and File Name. Extensive experiments were conducted using decision trees, Support Vector Machines and logistic regression classifiers to evaluate the system performance. The empirical results indicate that the hybrid approach outperforms both the rule-based and the ML-based approaches when they are processed independently. More importantly, our system outperforms the state-of-the-art of Arabic NER in terms of accuracy when applied to ANERcorp standard dataset, with F-measures 0.94 for Person, 0.90 for Location and 0.88 for Organization. [ABSTRACT FROM PUBLISHER] Copyright of Journal of Information Science is the property of Sage Publications, Ltd. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=93719428&site=bsi-live"
"A hybrid approach to fuzzy name search incorporating language-based and text-based principles.","Paul Wu Horng-Jyh; Na Jin-Cheon; Christopher Khoo Soo-Guan","Journal of Information Science",="01655515",,="2007","33","1","3","17","25142366","10.1177/0165551506068146","Sage Publications, Ltd.","Article","INFORMATION retrieval; SEARCH engines; INTERNET searching; DIGITAL libraries; INFORMATION resources management; ELECTRONIC information resource searching; Libraries and Archives; All Other Information Services; INTERNET research; DIGITIZATION of library materials; COMPUTER network resources","fuzzy name search; hybrid system; information retrieval; language and text; natural language processing","Name Search is an important search function in various types of information retrieval systems, such as online library catalogs and electronic yellow pages. It is also difficult, because of the high degree of fuzziness required in matching name variants. Previous approaches to name search systems use ad hoc combinations of search heuristics. This paper first discusses two approaches to name modeling - the natural language processing (NLP) and information retrieval (IR) models - and proposes a hybrid approach. The approach demonstrates a critical combination of complementary NILP and IR features that produces more effective fuzzy name matching. Two principles, position-as-attribute and position-transition-likelihood, are introduced as the principles for integrating the advantageous aspects of both approaches. They have been implemented in an NLP- and JR-hybrid model system called Friendly Name Search (FNS) for real world applications in multilingual directory searches on the Singapore Yellow pages website. [ABSTRACT FROM AUTHOR] Copyright of Journal of Information Science is the property of Sage Publications, Ltd. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=25142366&site=bsi-live"
"A hybrid approach to managing job offers and candidates","Kessler, Rémy; Béchet, Nicolas; Roche, Mathieu; Torres-Moreno, Juan-Manuel; El-Bèze, Marc","Information Processing & Management",="03064573",,="Nov2012","48","6","1124","12","79562574","10.1016/j.ipm.2012.03.002","Elsevier B.V.","Article","JOB offers; EMPLOYEE recruitment; INFORMATION processing; PROBABILITY theory; MATHEMATICAL models; PERFORMANCE evaluation; Administration of Human Resource Programs (except Education, Public Health, and Veterans' Affairs Programs); Human Resources Consulting Services; HYBRID systems; INFORMATION filtering systems","Automatic summarization; Human resources; Information retrieval; Natural language processing; Similarity measures; Statistical approaches","Abstract: The evolution of the job market has resulted in traditional methods of recruitment becoming insufficient. As it is now necessary to handle volumes of information (mostly in the form of free text) that are impossible to process manually, an analysis and assisted categorization are essential to address this issue. In this paper, we present a combination of the E-Gen and Cortex systems. E-Gen aims to perform analysis and categorization of job offers together with the responses given by the candidates. E-Gen system strategy is based on vectorial and probabilistic models to solve the problem of profiling applications according to a specific job offer. Cortex is a statistical automatic summarization system. In this work, E-Gen uses Cortex as a powerful filter to eliminate irrelevant information contained in candidate answers. Our main objective is to develop a system to assist a recruitment consultant and the results obtained by the proposed combination surpass those of E-Gen in standalone mode on this task. [Copyright &y& Elsevier] Copyright of Information Processing & Management is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=79562574&site=bsi-live"
"A knowledge acquisition methodology to ontology construction for information retrieval from medical documents.","Valencia-García, Rafael; Fernández-Breis, Jesualdo Tomás; Ruiz-Martínez, Juana María; García-Sánchez, Francisco; Martínez-Béjar, Rodrigo","Expert Systems",="02664720",,="Jul2008","25","3","314","21","34185282","10.1111/j.1468-0394.2008.00464.x","Wiley-Blackwell","Article","MANAGEMENT science research; INFORMATION storage & retrieval systems; MEDICAL informatics; COMPUTERS in medicine; MEDICAL records; INFORMATION retrieval software; TEXT mining","information retrieval; natural language processing; ontologies; ontology learning; UMLS","Vast amounts of medical information reside within text documents, so that the automatic retrieval of such information would certainly be beneficial for clinical activities. The need for overcoming the bottleneck provoked by the manual construction of ontologies has generated several studies and research on obtaining semi-automatic methods to build ontologies. Most techniques for learning domain ontologies from free text have important limitations. Thus, they can extract concepts so that only taxonomies are generally produced although there are other types of semantic relations relevant in knowledge modelling. This paper presents a language-independent approach for extracting knowledge from medical natural language documents. The knowledge is represented by means of ontologies that can have multiple semantic relationships among concepts. [ABSTRACT FROM AUTHOR] Copyright of Expert Systems is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=34185282&site=bsi-live"
"A knowledge management application in enterprises.","El-Korany, Abeer","International Journal of Management & Enterprise Development",="14684330",,="2007","4","6","693","10","28842930","10.1504/IJMED.2007.014989","Inderscience Enterprises Ltd.","Abstract","KNOWLEDGE management","Al; artificial intelligence; databases; enterprise competitiveness; information systems; KM; knowledge discovery; knowledge management; knowledge retrieval; product quality","An abstract of the article ""A knowledge management application in enterprises,"" by Abeer El-Korany is presented.","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=28842930&site=bsi-live"
"A lemmatization method for Mongolian and its application to indexing for information retrieval","Khaltar, Badam-Osor; Fujii, Atsushi","Information Processing & Management",="03064573",,="Jul2009","45","4","438","14","40117412","10.1016/j.ipm.2009.01.008","Elsevier B.V.","Article","INFORMATION retrieval; Book, periodical and newspaper merchant wholesalers; MONGOLIAN language; CYRILLIC alphabet; COGNITIVE processing of language; ENCYCLOPEDIAS & dictionaries; SUFFIXES & prefixes (Grammar); ALPHABET","Information retrieval; Lemmatization; Mongolian language; Natural language processing","In Mongolian, two different alphabets are used, Cyrillic and Mongolian. In this paper, we focus solely on the Mongolian language using the Cyrillic alphabet, in which a content word can be inflected when concatenated with one or more suffixes. Identifying the original form of content words is crucial for natural language processing and information retrieval. We propose a lemmatization method for Mongolian. The advantage of our lemmatization method is that it does not rely on noun dictionaries, enabling us to lemmatize out-of-dictionary words. We also apply our method to indexing for information retrieval. We use newspaper articles and technical abstracts in experiments that show the effectiveness of our method. Our research is the first significant exploration of the effectiveness of lemmatization for information retrieval in Mongolian. [Copyright &y& Elsevier] Copyright of Information Processing & Management is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=40117412&site=bsi-live"
"A Novel classification framework for the Thirukkural for building an efficient search system.","Ramalingam, Anita; Navaneethakrishnan, Subalalitha Chinnaudayar","Journal of Intelligent & Fuzzy Systems",="10641246",,="2022","42","3","2397","12","156139289","10.3233/JIFS-211667","IOS Press","Article","INFORMATION retrieval; RANDOM forest algorithms; SUPPORT vector machines; NAIVE Bayes classification; KEYWORD searching","information retrieval; morphological analysis; multinomial naive bayes classifier; Natural language processing; text classification; the Thirukkural","Thirukkural, a Tamil classic literature, which was written in 300 BCE is a didactic literature. Though Thirukkural comprises 1330 couplets which are organized into three sections and 133 chapters, in order to retrieve meaningful Thirukkural for a given query in search systems, a better organization of the Thirukkural is needed. This paper lays such a foundation by classifying the Thirukkural into ten new categories called superclasses that is helpful for building a better Information Retrieval (IR) system. The classifier is trained using Multinomial Naïve Bayes algorithm. Each superclass is further classified into two subcategories based on the didactic information. The proposed classification framework is evaluated using precision, recall and F-score metrics and achieved an overall F-score of 82.33% and a comparison analysis has been done with the Support Vector Machine, Logistic Regression and Random Forest algorithms. An IR system is built on top of the proposed system and the performance comparison has been done with the Google search and a locally built keyword search. The proposed classification framework has achieved a mean average precision score of 89%, whereas the Google search and keyword search have yielded 59% and 68% respectively. [ABSTRACT FROM AUTHOR] Copyright of Journal of Intelligent & Fuzzy Systems is the property of IOS Press and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=156139289&site=bsi-live"
"A novel method for providing relational databases with rich semantics and natural language processing.","Hamaz, Kamal; Benchikha, Fouzia","Journal of Enterprise Information Management",="17410398",,="2017","30","3","503","23","122548771","10.1108/JEIM-01-2015-0005","Emerald Publishing Limited","Article","NATURAL language processing; RELATIONAL databases; SEMANTICS","Enrichment; Information retrieval; Natural language processing; Ontologies; Relational databases; Reverse engineering","Purpose With the development of systems and applications, the number of users interacting with databases has increased considerably. The relational database model is still considered as the most used model for data storage and manipulation. However, it does not offer any semantic support for the stored data which can facilitate data access for the users. Indeed, a large number of users are intimidated when retrieving data because they are non-technical or have little technical knowledge. To overcome this problem, researchers are continuously developing new techniques for Natural Language Interfaces to Databases (NLIDB). Nowadays, the usage of existing NLIDBs is not widespread due to their deficiencies in understanding natural language (NL) queries. In this sense, the purpose of this paper is to propose a novel method for an intelligent understanding of NL queries using semantically enriched database sources.Design/methodology/approach First a reverse engineering process is applied to extract relational database hidden semantics. In the second step, the extracted semantics are enriched further using a domain ontology. After this, all semantics are stored in the same relational database. The phase of processing NL queries uses the stored semantics to generate a semantic tree.Findings The evaluation part of the work shows the advantages of using a semantically enriched database source to understand NL queries. Additionally, enriching a relational database has given more flexibility to understand contextual and synonymous words that may be used in a NL query.Originality/value Existing NLIDBs are not yet a standard option for interfacing a relational database due to their lack for understanding NL queries. Indeed, the techniques used in the literature have their limits. This paper handles those limits by identifying the NL elements by their semantic nature in order to generate a semantic tree. This last is a key solution towards an intelligent understanding of NL queries to relational databases. [ABSTRACT FROM AUTHOR] Copyright of Journal of Enterprise Information Management is the property of Emerald Publishing Limited and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=122548771&site=bsi-live"
"A Review and Future Perspectives of Arabic Question Answering Systems.","Ray, Santosh K.; Shaalan, Khaled","IEEE Transactions on Knowledge & Data Engineering",="10414347",,="Dec2016","28","12","3169","22","119353200","10.1109/TKDE.2016.2607201","IEEE","Article","NATURAL language processing; INTERNET; Wired Telecommunications Carriers; Internet Publishing and Broadcasting and Web Search Portals; QUESTION answering systems; ARABIC language; COMPUTATIONAL linguistics","Computer architecture; Document retrieval; Internet; Knowledge based systems; Knowledge discovery; literature review; Natural language processing; query expansion; question answering systems; Search engines; Standards; tools for arabic information retrieval","Question Answering Systems (QASs) have emerged as a good alternative for information seekers to retrieve precise information over the Internet. A good amount of research has been done to improve the performance of QASs across several languages, including European and Asian languages. However, Arabic, a morphologically rich Semitic language spoken by over 422 million people, has not seen similar development in the field of question answering. This article reviews the developments taking place in Arabic QASs as well as the challenges faced by researchers in developing Arabic QASs. After conducting an extensive literature survey of a number of English and Arabic QASs, this article classifies them according to several criteria. The most commonly used architecture for the development of an Arabic QAS, known as pipeline architecture, has been presented. In order to encourage and support the new researchers and scholars in conducting research in Arabic QASs, a list of techniques, tools, and computational linguistic resources, required to implement the components of the presented pipelined architecture, are described in this article in a simple and persuasive manner. Finally, the gap analysis between the research in Arabic and English QASs has been performed and accordingly, some future directions for research in Arabic QASs have been proposed. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Knowledge & Data Engineering is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=119353200&site=bsi-live"
"A Review of Modern Fashion Recommender Systems.","DELDJOO, YASHAR; NAZARY, FATEMEH; RAMISA, ARNAU; MCAULEY, JULIAN; PELLEGRINI, GIOVANNI; BELLOGIN, ALEJANDRO; DI NOIA, TOMMASO","ACM Computing Surveys",="03600300",,="Apr2024","56","4","1","37","174004917","10.1145/3624733","Association for Computing Machinery","Article",,"artificial intelligence; computer vision; e-commerce; fashion retail; information retrieval; machine learning; Recommender systems; text mining",,"https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=174004917&site=bsi-live"
"Accounting fraud detection using contextual language learning.","Bhattacharya, Indranil; Mickovic, Ana","International Journal of Accounting Information Systems",="14670895",,="Jun2024","53",,"N.PAG","1","177747955","10.1016/j.accinf.2024.100682","Elsevier B.V.","Article","ACCOUNTING fraud; FORENSIC accounting; ACCOUNTING software; UNITED States. Securities & Exchange Commission; Other Accounting Services; Regulation, Licensing, and Inspection of Miscellaneous Commercial Sectors; FRAUD investigation; CONTEXTUAL learning; LANGUAGE models","Accounting fraud detection; BERT; Information Retrieval; Natural Language Processing","• Financial reporting text is an important source of information for fraud detection. • Application of BERT in accounting fraud detection setting. • Contextual learning improves accounting fraud detection relative to benchmarks. • Final model outperforms existing textual and quantitative benchmark models. • We provide practical insights for financial investigators. Accounting fraud is a widespread problem that causes significant damage in the economic market. Detection and investigation of fraudulent firms require a large amount of time, money, and effort for corporate monitors and regulators. In this study, we explore how textual contents from financial reports help in detecting accounting fraud. Pre-trained contextual language learning models, such as BERT, have significantly advanced natural language processing in recent years. We fine-tune the BERT model on Management Discussion and Analysis (MD&A) sections of annual 10-K reports from the Securities and Exchange Commission (SEC) database. Our final model outperforms the textual benchmark model and the quantitative benchmark model from the previous literature by 15% and 12%, respectively. Further, our model identifies five times more fraudulent firm-year observations than the textual benchmark by investigating the same number of firms, and three times more than the quantitative benchmark. Optimizing this investigation process, where more fraudulent observations are detected in the same size of the investigation sample, would be of great economic significance for regulators, investors, financial analysts, and auditors. [ABSTRACT FROM AUTHOR] Copyright of International Journal of Accounting Information Systems is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=177747955&site=bsi-live"
"An augmented semantic search tool for multilingual news analytics.","Harikumar, Sandhya; Sathyajit, Rohit; Karumudi, Gnana Venkata Naga Sai Kalyan","Journal of Intelligent & Fuzzy Systems",="10641246",,="2022","43","6","8315","13","160553638","10.3233/JIFS-221184","IOS Press","Article","INFORMATION retrieval; NATURAL language processing; SEARCH engines; KEYWORD searching; ENGLISH language","Latent dirichlet allocation(LDA); multilingual; natural language processing(NLP); News analytics; semantic information retrieval","News feeds generate colossal amount of data consisting of important information hidden in the intricacies. State of the art methods are still at infancy in providing a very generic and publicly available solution to skim through the important information in the news from various sources and an ability to search using specific keywords in different languages. This paper focuses on designing a tool to extract semantic details from news articles published through various internet sources in various languages. The semantic information is stored within DBMS for ease of organizing and retrieving the data. Further, a querying facility to search through entire articles based on the keyword or date-based search is also proposed to view the crisp content. The news articles in English, and two Indian languages - Hindi and Malayalam are considered for experimentation. The proposed strategy consists of two main components namely, Generative model creation and Query engine. Generative model aims to extract important entities and keywords along with their relevance to the article and other similar articles using Latent Dirichlet Allocation(LDA) and Named Entity Recognition(NER). Query engine is to facilitate on the fly retrieval of semantic content from the database, based on user keyword. The search engine, along with database indexing, reduces the access time to the database thereby retrieving the information in less time. Experimental results show that the proposed method is effective in terms of quality of information and time consumed for information retrieval. [ABSTRACT FROM AUTHOR] Copyright of Journal of Intelligent & Fuzzy Systems is the property of IOS Press and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=160553638&site=bsi-live"
"An Automated Framework for Incorporating News into Stock Trading Strategies.","Nuij, Wijnand; Milea, Viorel; Hogenboom, Frederik; Frasincar, Flavius; Kaymak, Uzay","IEEE Transactions on Knowledge & Data Engineering",="10414347",,="Apr2014","26","4","823","13","95069368","10.1109/TKDE.2013.133","IEEE","Article","STOCKS (Finance); APPLICATION software; NATURAL language processing; WEB analytics; Software Publishers; Software publishers (except video game publishers); Custom Computer Programming Services; GENETIC algorithms; GENETIC programming","Companies; Computer applications; Context; Corporate acquisitions; evolutionary computing and genetic algorithms; Genetic programming; Indexes; Information retrieval; learning; natural language processing; Stock markets; web text analysis","In this paper we present a framework for automatic exploitation of news in stock trading strategies. Events are extracted from news messages presented in free text without annotations. We test the introduced framework by deriving trading strategies based on technical indicators and impacts of the extracted events. The strategies take the form of rules that combine technical trading indicators with a news variable, and are revealed through the use of genetic programming. We find that the news variable is often included in the optimal trading rules, indicating the added value of news for predictive purposes and validating our proposed framework for automatically incorporating news in stock trading strategies. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Knowledge & Data Engineering is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=95069368&site=bsi-live"
"An automated online crisis dispatcher.","Fitrianie, Siska; Rothkrantz, Leon J. M.","International Journal of Emergency Management",="14714825",,="2008","5","1/2","123","22","34159895","10.1504/IJEM.2008.019910","Inderscience Enterprises Ltd.","Article","HUMAN-computer interaction; NATURAL language processing; INFORMATION storage & retrieval systems; TECHNOLOGY; CRISIS management; INTERPERSONAL communication; EMERGENCY management; Other Justice, Public Order, and Safety Activities; Other federal protective services; Other provincial protective services; Other municipal protective services; Emergency and Other Relief Services","communication flow; crisis hotline dispatcher; emergency management; emergency response; HCI; human-computer dialogue; human-computer interaction; information retrieval; information systems; natural language processing; online crisis management; text-based emotion recognition","The article features the dialogue system that can play as a crisis hotline dispatcher, hence, helping human operators at the crisis centers disseminate precise information during crisis events. It notes that the dialogue system offers a natural user interaction through its ability to start a user-friendly dialogue that takes care of the content, context, and user's emotion. It also retrieves information regarding crisis situations from users while controlling the communication flow.","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=34159895&site=bsi-live"
"An English Language Question Answering System for a Large Relational Database.","Waltz, David L.; Montgomery, Christine A.","Communications of the ACM",="00010782",,="Jul1978","21","7","526","14","5236444","10.1145/359545.359550","Association for Computing Machinery","Article","RELATIONAL databases; AIRPLANES; INFORMATION storage & retrieval systems; DATABASES; Nonscheduled Chartered Passenger Air Transportation; Aerospace product and parts manufacturing; Aircraft Manufacturing; ENGLISH language; AERONAUTICS","artificial intelligence; database front end; dialogue; information retrieval; natural language; natural language programming; query generation; question answering; relational database","By typing requests in English, casual users will be able to obtain explicit answers from a large relational database of aircraft flight and maintenance data using a system called PLANES. The design and implementation of this system is described and illustrated with detailed examples of the operation of system components and examples of overall system operation. The language processing portion of the system uses a number of augmented transition networks, each of which matches phrases with a specific meaning, along with context registers (history keepers) and concept case frames; these are used for judging meaningfulness of questions, generating dialogue for clarifying partially understood questions, and resolving ellipsis and pronoun reference problems. Other system components construct a formal query for the relational database, and optimize the order of searching relations. Methods are discussed for handling vague or complex questions and for providing browsing ability. Also included are discussions of important issues in programming natural language systems for limited domains, and the relationship of this system to others. [ABSTRACT FROM AUTHOR] Copyright of Communications of the ACM is the property of Association for Computing Machinery and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=5236444&site=bsi-live"
"An ensemble clustering approach for topic discovery using implicit text segmentation.","Memon, Muhammad Qasim; Lu, Yu; Chen, Penghe; Memon, Aasma; Pathan, Muhammad Salman; Zardari, Zulfiqar Ali","Journal of Information Science",="01655515",,="Aug2021","47","4","431","27","151171329","10.1177/0165551520911590","Sage Publications, Ltd.","Article","INFORMATION retrieval; NATURAL language processing; WORD frequency","Information retrieval; natural language processing; ontological similarity; text clustering; text mining; text segmentation","Text segmentation (TS) is the process of dividing multi-topic text collections into cohesive segments using topic boundaries. Similarly, text clustering has been renowned as a major concern when it comes to multi-topic text collections, as they are distinguished by sub-topic structure and their contents are not associated with each other. Existing clustering approaches follow the TS method which relies on word frequencies and may not be suitable to cluster multi-topic text collections. In this work, we propose a new ensemble clustering approach (ECA) is a novel topic-modelling-based clustering approach, which induces the combination of TS and text clustering. We improvised a LDA-onto (LDA-ontology) is a TS-based model, which presents a deterioration of a document into segments (i.e. sub-documents), wherein each sub-document is associated with exactly one sub-topic. We deal with the problem of clustering when it comes to a document that is intrinsically related to various topics and its topical structure is missing. ECA is tested through well-known datasets in order to provide a comprehensive presentation and validation of clustering algorithms using LDA-onto. ECA exhibits the semantic relations of keywords in sub-documents and resultant clusters belong to original documents that they contain. Moreover, present research sheds the light on clustering performances and it indicates that there is no difference over performances (in terms of F -measure) when the number of topics changes. Our findings give above par results in order to analyse the problem of text clustering in a broader spectrum without applying dimension reduction techniques over high sparse data. Specifically, ECA provides an efficient and significant framework than the traditional and segment-based approach, such that achieved results are statistically significant with an average improvement of over 10.2%. For the most part, proposed framework can be evaluated in applications where meaningful data retrieval is useful, such as document summarization, text retrieval, novelty and topic detection. [ABSTRACT FROM AUTHOR] Copyright of Journal of Information Science is the property of Sage Publications, Ltd. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=151171329&site=bsi-live"
"An improved Urdu stemming algorithm for text mining based on multi-step hybrid approach.","Jabbar, Abdul; Iqbal, Sajid; Akhunzada, Adnan; Abbas, Qaisar","Journal of Experimental & Theoretical Artificial Intelligence",="0952813X",,="Sep2018","30","5","703","21","132054653","10.1080/0952813X.2018.1467495","Taylor & Francis Ltd","Article","ARTIFICIAL intelligence; ALGORITHMS; DATA mining; ELECTRONIC data processing; Data Processing, Hosting, and Related Services; MACHINE learning","affixes; information retrieval; lemmatization; natural language processing; stemming; text mining; Urdu; Urdu stemmer","Stemming is the basic operation in Natural language processing (NLP) to remove derivational and inflectional affixes without performing a morphological analysis. This practice is essential to extract the root or stem. In NLP domains, the stemmer is used to improve the process of information retrieval (IR), text classifications (TC), text mining (TM) and related applications. In particular, Urdu stemmers utilize only uni-gram words from the input text by ignoring bigrams, trigrams, and n-gram words. To improve the process and efficiency of stemming, bigrams and trigram words must be included. Despite this fact, there are a few developed methods for Urdu stemmers in the past studies. Therefore, in this paper, we proposed an improved Urdu stemmer, using hybrid approach divided into multi-step operation, to deal with unigram, bigram, and trigram features as well. To evaluate the proposed Urdu stemming method, we have used two corpora; word corpus and text corpus. Moreover, two different evaluation metrics have been applied to measure the performance of the proposed algorithm. The proposed algorithm achieved an accuracy of 92.97% and compression rate of 55%. These experimental results indicate that the proposed system can be used to increase the effectiveness and efficiency of the Urdu stemmer for better information retrieval and text mining applications. [ABSTRACT FROM AUTHOR] Copyright of Journal of Experimental & Theoretical Artificial Intelligence is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=132054653&site=bsi-live"
"An improvement key deletion method for double-array structure using single-nodes","Oono, Masaki; Fuketa, Masao; Morita, Kazuhiro; Kashiji, Shinkaku; Aoe, Jun-ichi","Information Processing & Management",="03064573",,="Jan2004","40","1","47","17","11535296","10.1016/S0306-4573(02)00090-0","Elsevier B.V.","Article","TEXT processing (Computer science); ELECTRONIC data processing; INFORMATION retrieval; Data Processing, Hosting, and Related Services; Book, periodical and newspaper merchant wholesalers; ENCYCLOPEDIAS & dictionaries","Dictionary; Double-array structure; Information retrieval; Natural language processing; Trie search","A trie is a well known method for various dictionaries, such as spelling check and morphological analysis. A double-array structure is an efficient data structure combining fast access of a matrix form with the compactness of a list form. The drawback of the double-array is that the space efficiency degrades by empty elements produced in key deletion. Morita presented a key deletion method eliminating empty elements. However, the space efficiency of this method is low for high frequent deletion and deletion takes much time because the cost depends on the number of the empty elements. This paper presents a fast and compact deletion method by using the property of nodes that have no brothers. From simulation results for 100,000 keys, the present method is about 330 times faster than Morita’s method and keeps high space efficiency. [Copyright &y& Elsevier] Copyright of Information Processing & Management is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=11535296&site=bsi-live"
"An open source and modular search engine for biomedical literature retrieval.","Almeida, Hayda; Jean‐Louis, Ludovic; Meurs, Marie‐Jean","Computational Intelligence",="08247935",,="Feb2018","34","1","200","19","128227532","10.1111/coin.12125","Wiley-Blackwell","Article","OPEN source software; SEARCH engines; NATURAL language processing; DATA mining; Research and development in the physical, engineering and life sciences; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); BIOLOGICAL research","biomedical literature, document index, full‐text search, information retrieval, natural language processing, natural language query, search engine","Abstract: This work presents the bioMine system, a full‐text natural language search engine for biomedical literature. bioMine provides search capabilities based on the full‐text content of documents belonging to a database composed of scientific articles and allows users to submit their search queries using natural language. Beyond the text content of articles, the system engine also uses article metadata, empowering the search by considering extra information from picture and table captions. bioMine is publicly released as an open‐source system under the MIT license. [ABSTRACT FROM AUTHOR] Copyright of Computational Intelligence is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=128227532&site=bsi-live"
"Applications of natural language processing in software traceability: A systematic mapping study.","Pauzi, Zaki; Capiluppi, Andrea","Journal of Systems & Software",="01641212",,="Apr2023","198",,"N.PAG","1","161845363","10.1016/j.jss.2023.111616","Elsevier B.V.","Article","NATURAL language processing; COMPUTER software; INFORMATION retrieval; Computer, computer peripheral and pre-packaged software merchant wholesalers; Computer and Computer Peripheral Equipment and Software Merchant Wholesalers; Computer and software stores; Software publishers (except video game publishers); TACIT knowledge; SOFTWARE maintenance; SOURCE code; TREND analysis","Information retrieval; Natural language processing; Software traceability","A key part of software evolution and maintenance is the continuous integration from collaborative efforts, often resulting in complex traceability challenges between software artifacts: features and modules remain scattered in the source code, and traceability links become harder to recover. In this paper, we perform a systematic mapping study dealing with recent research recovering these links through information retrieval, with a particular focus on natural language processing (NLP). Our search strategy gathered a total of 96 papers in focus of our study, covering a period from 2013 to 2021. We conducted trend analysis on NLP techniques and tools involved, and traceability efforts (applying NLP) across the software development life cycle (SDLC). Based on our study, we have identified the following key issues, barriers, and setbacks: syntax convention, configuration, translation, explainability, properties representation, tacit knowledge dependency, scalability, and data availability. Based on these, we consolidated the following open challenges: representation similarity across artifacts, the effectiveness of NLP for traceability, and achieving scalable, adaptive, and explainable models. To address these challenges, we recommend a holistic framework for NLP solutions to achieve effective traceability and efforts in achieving interoperability and explainability in NLP models for traceability. [Display omitted] • Our search strategy across multiple library databases gathered a total of 96 papers. • Trend analysis was conducted throughout the years 2013 to 2021. • Our study highlighted open challenges from key issues, barriers, and setbacks. • We proposed two key recommendations to address these open challenges. [ABSTRACT FROM AUTHOR] Copyright of Journal of Systems & Software is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=161845363&site=bsi-live"
"Automated arabic text classification with P- Stemmer, machine learning, and a tailored news article taxonomy.","Kanan, Tarek; Fox, Edward A.","Journal of the Association for Information Science & Technology",="23301635",,="Nov2016","67","11","2667","17","118731689","10.1002/asi.23609","Wiley-Blackwell","Article","NATURAL language processing; PROBABILITY theory; RESEARCH funding; MIDDLE East; Book stores and news dealers; News Dealers and Newsstands; Book, Periodical, and Newspaper Merchant Wholesalers; Book, periodical and newspaper merchant wholesalers; Newspaper Publishers; LANGUAGE classification; ARABS; ELECTRONIC journals; NEWSPAPERS; MANN Whitney U Test","digital libraries; information retrieval; natural language processing","Arabic news articles in electronic collections are difficult to study. Browsing by category is rarely supported. Although helpful machine-learning methods have been applied successfully to similar situations for English news articles, limited research has been completed to yield suitable solutions for Arabic news. In connection with a Qatar National Research Fund ( QNRF)-funded project to build digital library community and infrastructure in Qatar, we developed software for browsing a collection of about 237,000 Arabic news articles, which should be applicable to other Arabic news collections. We designed a simple taxonomy for Arabic news stories that is suitable for the needs of Qatar and other nations, is compatible with the subject codes of the International Press Telecommunications Council, and was enhanced with the aid of a librarian expert as well as five Arabic-speaking volunteers. We developed tailored stemming (i.e., a new Arabic light stemmer called P- Stemmer) and automatic classification methods (the best being binary Support Vector Machines classifiers) to work with the taxonomy. Using evaluation techniques commonly used in the information retrieval community, including 10-fold cross-validation and the Wilcoxon signed-rank test, we showed that our approach to stemming and classification is superior to state-of-the-art techniques. [ABSTRACT FROM AUTHOR] Copyright of Journal of the Association for Information Science & Technology is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=118731689&site=bsi-live"
"Automatic identification of light stop words for Persian information retrieval systems.","Sadeghi, Mohammad; Vegas, Jesús","Journal of Information Science",="01655515",,="Aug2014","40","4","476","12","96966746","10.1177/0165551514530655","Sage Publications, Ltd.","Article","TEXT processing (Computer science); INFORMATION storage & retrieval systems; STOP words; INFORMATION retrieval research; INDEXING","Information retrieval; information theory; natural language processing; Persian language; stop words","Stop word identification is one of the most important tasks for many text processing applications such as information retrieval. Stop words occur too frequently in documents in a collection and do not contribute significantly to determining the context or information about the documents. These words are worthless as index terms and should be removed during indexing as well as before querying by an information retrieval system. In this paper, we propose an automatic aggregated methodology based on term frequency, normalized inverse document frequency and information model to extract the light stop words from Persian text. We define a ‘light stop word’ as a stop word that has few letters and is not a compound word. In the Persian language, a complete stop word list can be derived by combining the light stop words. The evaluation results, using a standard corpus, show a good percentage of coincidence between the Persian and English stop words and a significant improvement in the number of index terms. Specifically, the first 32 Persian light stop words have a great impact on the index size reduction and the set of stop words can reduce the number of index terms by about 27%. [ABSTRACT FROM PUBLISHER] Copyright of Journal of Information Science is the property of Sage Publications, Ltd. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=96966746&site=bsi-live"
"Automatic Parsing for Content Analysis.","Damerau, Frederick J.; Borrow, D. G.","Communications of the ACM",="00010782",,="Jun70","13","6","356","5","5221521","10.1145/362384.362495","Association for Computing Machinery","Article","INFORMATION theory; CONTENT analysis; SEMANTICS; LANGUAGE & languages; SEMANTICS (Philosophy); COMPUTATIONAL linguistics","Content analysis; information retrieval; language analysis; natural language processing; parsing; syntactic analysis; text processing","Although automatic syntactic and semantic analysis is not yet possible for all of an unrestricted natural language text, some applications, of which content analysis is one, do not have such a stringent coverage requirement. Preliminary studies show that the Harvard Syntactic Analyzer can produce correct and unambiguous identification of the subject and object of certain verbs for approximately half of the relevant occurrences. This provides a degree of coverage for content analysis variables which compares favorably to manual methods, in which only a sample of the total available text is normally processed. [ABSTRACT FROM AUTHOR] Copyright of Communications of the ACM is the property of Association for Computing Machinery and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=5221521&site=bsi-live"
"Automatic smart contract comment generation via large language models and in-context learning.","Zhao, Junjie; Chen, Xiang; Yang, Guang; Shen, Yiheng","Information & Software Technology",="09505849",,="Apr2024","168",,"N.PAG","1","175296066","10.1016/j.infsof.2024.107405","Elsevier B.V.","Article",,"Demonstration selection; In-context learning; Information retrieval; Large language model; Smart contract comment","Designing effective automatic smart contract comment generation approaches can facilitate developers' comprehension, boosting smart contract development and improving vulnerability detection. The previous approaches can be divided into two categories: fine-tuning paradigm-based approaches and information retrieval-based approaches. However, for the fine-tuning paradigm-based approaches, the performance may be limited by the quality of the gathered dataset for the downstream task and they may have knowledge-forgetting issues, which can reduce the generality of the fine-tuned model. While for the information retrieval-based approaches, it is difficult for them to generate high-quality comments if similar code does not exist in the historical repository. Therefore we want to utilize the domain knowledge related to smart contract code comment generation in large language models (LLMs) to alleviate the disadvantages of these two types of approaches. In this study, we propose an approach SCCLLM based on LLMs and in-context learning. Specifically, in the demonstration selection phase, SCCLLM retrieves the top- k code snippets from the historical corpus by considering syntax, semantics, and lexical information. In the in-context learning phase, SCCLLM utilizes the retrieved code snippets as demonstrations for in-context learning, which can help to utilize the related knowledge for this task in the LLMs. In the LLMs inference phase, the input is the target smart contract code snippet, and the output is the corresponding comment generated by the LLMs. We select a large corpus from a smart contract community Etherscan.io as our experimental subject. Extensive experimental results show the effectiveness of SCCLLM when compared with baselines in automatic evaluation and human evaluation. We also show the rationality of our customized demonstration selection strategy in SCCLLM by ablation studies. Our study shows using LLMs and in-context learning is a promising direction for automatic smart contract comment generation, which calls for more follow-up studies. [ABSTRACT FROM AUTHOR] Copyright of Information & Software Technology is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=175296066&site=bsi-live"
"Automatically structuring domain knowledge from text: An overview of current research","Clark, Malcolm; Kim, Yunhyong; Kruschwitz, Udo; Song, Dawei; Albakour, Dyaa; Dignum, Stephen; Beresi, Ulises Cerviño; Fasli, Maria; De Roeck, Anne","Information Processing & Management",="03064573",,="May2012","48","3","552","17","73968757","10.1016/j.ipm.2011.07.002","Elsevier B.V.","Article","INFORMATION resources management; ARTIFICIAL intelligence; NATURAL language processing; INFORMATION retrieval; All Other Information Services; INFORMATION modeling; SEMANTIC Web; LITERATURE reviews","Artificial intelligence; Domain models; Information retrieval; Natural language processing","This paper presents an overview of automatic methods for building domain knowledge structures (domain models) from text collections. Applications of domain models have a long history within knowledge engineering and artificial intelligence. In the last couple of decades they have surfaced noticeably as a useful tool within natural language processing, information retrieval and semantic web technology. Inspired by the ubiquitous propagation of domain model structures that are emerging in several research disciplines, we give an overview of the current research landscape and some techniques and approaches. We will also discuss trade-offs between different approaches and point to some recent trends. [Copyright &y& Elsevier] Copyright of Information Processing & Management is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=73968757&site=bsi-live"
"BIMASR: Framework for Voice-Based BIM Information Retrieval.","Shin, Sangyun; Issa, Raja R. A.","Journal of Construction Engineering & Management",="07339364",,="Oct2021","147","10","1","18","152144732","10.1061/(ASCE)CO.1943-7862.0002138","American Society of Civil Engineers","Article","AUTOMATIC speech recognition; NATURAL language processing; INFORMATION retrieval; BUILDING information modeling; SPEECH perception","Building information modeling; Domain ontology; Information retrieval; Natural language processing; Speech recognition; Structured query language","Voice is the most convenient means for human beings to communicate with others, even if the objects of their communication are not other humans but machines or computers. Many industries, and even the architecture, engineering, construction, and operations (AECO) industry, have attempted to study and apply speech recognition systems in their operations to improve work efficiency and productivity. However, previous studies on speech recognition had two limitations: they used keywords requiring basic knowledge of building information modeling (BIM) commands for using them and in searching BIM data, they relied on the Industry Foundation Classes (IFC) format, which involves converting BIM data to IFC. Such methods did not conduce to direct retrieval in BIM software. In the latter case, data search was possible, but data manipulation was not. To improve on the limitations of previous studies, this study developed a building information modeling automatic speech recognition (BIMASR) framework that requires no knowledge of BIM commands, which allows for the input of natural language (NL)-based questions into BIM software using human voice to search and manipulate data. The framework consists of three modules: one for voice recognition, one for natural language processing (syntax and semantic analysis), and one for BIM data preprocessing and interworking with relational databases. The manipulation of BIM data with NL-based speech recognition converts the BIM operating environment from an expert-oriented into a user-oriented environment. This conversion allows for more BIM interaction and the popularization of BIM use and enhances the use of BIM in dynamic environments such as virtual reality, augmented reality, and holograms, where conventional input devices are typically absent. [ABSTRACT FROM AUTHOR] Copyright of Journal of Construction Engineering & Management is the property of American Society of Civil Engineers and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=152144732&site=bsi-live"
"Biomedical named entity recognition and linking datasets: survey and our recent development.","Huang, Ming-Siang; Lai, Po-Ting; Lin, Pei-Yen; You, Yu-Ting; Tsai, Richard Tzong-Han; Hsu, Wen-Lian","Briefings in Bioinformatics",="14675463",,="Nov2020","21","6","2219","20","147531223","10.1093/bib/bbaa054","Oxford University Press / USA","Article","NATURAL language processing; INTERNET servers; Data Processing, Hosting, and Related Services; PROTEIN-protein interactions; NAMED-entity recognition","biological information retrieval; biomedical dataset; biomedical natural language processing; named entity recognition","Natural language processing (NLP) is widely applied in biological domains to retrieve information from publications. Systems to address numerous applications exist, such as biomedical named entity recognition (BNER), named entity normalization (NEN) and protein–protein interaction extraction (PPIE). High-quality datasets can assist the development of robust and reliable systems; however, due to the endless applications and evolving techniques, the annotations of benchmark datasets may become outdated and inappropriate. In this study, we first review commonlyused BNER datasets and their potential annotation problems such as inconsistency and low portability. Then, we introduce a revised version of the JNLPBA dataset that solves potential problems in the original and use state-of-the-art named entity recognition systems to evaluate its portability to different kinds of biomedical literature, including protein–protein interaction and biology events. Lastly, we introduce an ensembled biomedical entity dataset (EBED) by extending the revised JNLPBA dataset with PubMed Central full-text paragraphs, figure captions and patent abstracts. This EBED is a multi-task dataset that covers annotations including gene, disease and chemical entities. In total, it contains 85000 entity mentions, 25000 entity mentions with database identifiers and 5000 attribute tags. To demonstrate the usage of the EBED, we review the BNER track from the AI CUP Biomedical Paper Analysis challenge. Availability: The revised JNLPBA dataset is available at https://iasl-btm.iis.sinica.edu.tw/BNER/Content/Re vised_JNLPBA.zip. The EBED dataset is available at https://iasl-btm.iis.sinica.edu.tw/BNER/Content/AICUP _EBED_dataset.rar. Contact: Email: thtsai@g.ncu.edu.tw , Tel. 886-3-4227151 ext. 35203, Fax: 886-3-422-2681 Email: hsu@iis.sinica.edu.tw , Tel. 886-2-2788-3799 ext. 2211, Fax: 886-2-2782-4814 Supplementary information: Supplementary data are available at Briefings in Bioinformatics online. [ABSTRACT FROM AUTHOR] Copyright of Briefings in Bioinformatics is the property of Oxford University Press / USA and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=147531223&site=bsi-live"
"Building siamese attention-augmented recurrent convolutional neural networks for document similarity scoring.","Han, Sifei; Shi, Lingyun; Richie, Russell; Tsui, Fuchiang R.","Information Sciences",="00200255",,="Nov2022","615",,"90","13","160251254","10.1016/j.ins.2022.10.032","Elsevier B.V.","Article","ARTIFICIAL neural networks; NATURAL language processing; JOB resumes; CONVOLUTIONAL neural networks; RECURRENT neural networks","Attention neural network; Deep learning; Information retrieval; Machine learning; Natural language processing; Text similarity","• We proposed a new architecture - the Siamese attention-augmented recurrent convolutional neural network (S-ARCNN). • We compared the performance of S-ARCNN with eight popular models for measuring document similarity. • Our model outperformed the state-of-the-art Transformer based model (Sentence BERT) by over 5% in F1. • Simply fitting an optimal decision threshold can significantly improve pre-trained BERT model for the new task. • S-ARCNN performed best in longer question pairs (length > = 50 words). Automatically measuring document similarity is imperative in natural language processing, with applications ranging from recommendation to duplicate document detection. State-of-the-art approach in document similarity commonly involves deep neural networks, yet there is little study on how different architectures may be combined. Thus, we introduce the Siamese Attention-augmented Recurrent Convolutional Neural Network (S-ARCNN) that combines multiple neural network architectures. In each subnetwork of S-ARCNN, a document passes through a bidirectional Long Short-Term Memory (bi-LSTM) layer, which sends representations to local and global document modules. A local document module uses convolution, pooling, and attention layers, whereas a global document module uses last states of the bi-LSTM. Both local and global features are concatenated to form a single document representation. Using the Quora Question Pairs dataset, we evaluated S-ARCNN, Siamese convolutional neural networks (S-CNNs), Siamese LSTM, and two BERT models. While S-CNNs (82.02% F1) outperformed S-ARCNN (79.83% F1) overall, S-ARCNN slightly outperformed S-CNN on duplicate question pairs with more than 50 words (39.96% vs. 39.42% accuracy). With the potential advantage of S-ARCNN for processing longer documents, S-ARCNN may help researchers identify collaborators with similar research interests, help editors find potential reviewers, or match resumes with job descriptions. [ABSTRACT FROM AUTHOR] Copyright of Information Sciences is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=160251254&site=bsi-live"
"Categorization of services for seeking information in biomedical literature: a typology for improvement of practice.","Jung-jae Kim; Rebholz-Schuhmann, Dietrich","Briefings in Bioinformatics",="14675463",,="Nov2008","9","6","452","14","36312647","10.1093/bib/bbn032","Oxford University Press / USA","Article","INFORMATION science; DATA mining; Research and Development in Biotechnology; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); MEDICAL literature; BIOINFORMATICS; COMPUTERS in biology","biomedical literature; information behaviour; information retrieval; literature analysis; literature mining; natural language processing; text","Biomedical researchers have to efficiently explore the scientific literature, keeping the focus on their research. This goal can only be achieved if the available means for accessing the literature meet the researchers' retrieval needs and if they understand how the tools filter the perpetually increasing number of documents. We have examined existing web-based services for information retrieval in order to give users guidance to improve their everyday practice of literature analysis. We propose two dimensions along which the services may be categorized: categories of input and output formats; and categories of behavioural usage. The categorization would be helpful for biologists to understand the differences in the input and output formats and the tasks they fulfil in information-retrieval activities. Also, they may inspire future bioinformaticians to further innovative development in this field. [ABSTRACT FROM AUTHOR] Copyright of Briefings in Bioinformatics is the property of Oxford University Press / USA and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=36312647&site=bsi-live"
"Deep supervised hashing network with integrated regularisation.","Liao, Jianxin; Li, Baoran; Yang, Di; Wang, Jingyu; Qi, Qi; Wang, Jing","IET Image Processing (Wiley-Blackwell)",="17519659",,="Oct2019","13","12","2143","9","148084114","10.1049/iet-ipr.2018.6644","Wiley-Blackwell","Article",,"approximate nearest neighbour search; binary codes; binary hash code; compact binary codes; deep hashing system; deep structures; deep supervised hashing network; DSHIR system; existing end‐to‐end deep hashing systems; existing methods; feature extraction; file organisation; good hash codes; hashing methods; image classification; image representation; image retrieval; indexing; information retrieval; integrated regularisation system; large‐scale multimedia retrieval tasks; learning (artificial intelligence); retrieval efficiency; state‐of‐the‐art systems; storage","Hashing has been widely deployed to approximate nearest neighbour search for large‐scale multimedia retrieval tasks due to storage and retrieval efficiency. State‐of‐the‐art supervised hashing methods for image retrieval construct deep structures to simultaneously learn image representation and generate good hash codes, and the key step among them is simultaneously learned feature representation and binary hash code. Existing methods use similarity and regularity loss to train deep hashing systems, but these two functions usually work together but not cooperative, which may lead to inadequate performance of the whole system. In this study, a new method for training deep hashing system to learn compact binary codes is presented. The deep supervised hashing network with integrated regularisation (DSHIR) system develop the zero division restriction as a new part of the loss function, which settles the problem of cooperatively guiding the system generate similarity preserving binary codes. DSHIR system also modifies the similarity handling loss to better extract features from image data, which promotes the performance compared to existing end‐to‐end deep hashing systems. Experiments show that DSHIR yields about 10 per cent higher mean average precision on CIFAR‐10 dataset, and also promote on other evaluation indexes compared with state‐of‐the‐art systems. [ABSTRACT FROM AUTHOR] Copyright of IET Image Processing (Wiley-Blackwell) is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=148084114&site=bsi-live"
"Descriptive Document Clustering via Discriminant Learning in a Co-Embedded Space of Multilevel Similarities.","Mu, Tingting; Goulermas, John Y.; Korkontzelos, Ioannis; Ananiadou, Sophia","Journal of the Association for Information Science & Technology",="23301635",,="Jan2016","67","1","106","28","112228408","10.1002/asi.23374","Wiley-Blackwell","Article","ALGORITHMS; INFORMATION retrieval; DATA mining; ACCESS to information","information retrieval; natural language processing; text mining","Descriptive document clustering aims at discovering clusters of semantically interrelated documents together with meaningful labels to summarize the content of each document cluster. In this work, we propose a novel descriptive clustering framework, referred to as CEDL. It relies on the formulation and generation of 2 types of heterogeneous objects, which correspond to documents and candidate phrases, using multilevel similarity information. CEDL is composed of 5 main processing stages. First, it simultaneously maps the documents and candidate phrases into a common co-embedded space that preserves higher-order, neighbor-based proximities between the combined sets of documents and phrases. Then, it discovers an approximate cluster structure of documents in the common space. The third stage extracts promising topic phrases by constructing a discriminant model where documents along with their cluster memberships are used as training instances. Subsequently, the final cluster labels are selected from the topic phrases using a ranking scheme using multiple scores based on the extracted co-embedding information and the discriminant output. The final stage polishes the initial clusters to reduce noise and accommodate the multitopic nature of documents. The effectiveness and competitiveness of CEDL is demonstrated qualitatively and quantitatively with experiments using document databases from different application fields. [ABSTRACT FROM AUTHOR] Copyright of Journal of the Association for Information Science & Technology is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=112228408&site=bsi-live"
"Detecting new Chinese words from massive domain texts with word embedding.","Qian, Yu; Du, Yang; Deng, Xiongwen; Ma, Baojun; Ye, Qiongwei; Yuan, Hua","Journal of Information Science",="01655515",,="Apr2019","45","2","196","16","135207149","10.1177/0165551518786676","Sage Publications, Ltd.","Article","INFORMATION retrieval; DATA mining; WORD recognition; VOCABULARY; CHINESE language","Natural language processing; new word detection; similarity measurement; textual information retrieval; word embedding","Textual information retrieval (TIR) is based on the relationship between word units. Traditional word segmentation techniques attempt to discern the word units accurately from texts; however, they are unable to appropriately and efficiently identify all new words. Identification of new words, especially in languages such as Chinese, remains a challenge. In recent years, word embedding methods have used numerical word vectors to retain the semantic and correlated information between words in a corpus. In this article, we propose the word-embedding-based method (WEBM), a novel method that combines word embedding and frequent n-gram string mining for discovering new words from domain corpora. First, we mapped all word units in a domain corpus to a high-dimension word vector space. Second, we used a frequent n-gram word string mining method to identify a set of candidates for new words. We designed a pruning strategy based on the word vectors to quantify the possibility of a word string being a new word, thereby allowing the evaluation of candidates based on the similarity of word units in the same string. In a comparative study, our experimental results revealed that WEBM had a great advantage in detecting new words from massive Chinese corpora. [ABSTRACT FROM AUTHOR] Copyright of Journal of Information Science is the property of Sage Publications, Ltd. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=135207149&site=bsi-live"
"Developing intuitive and explainable algorithms through inspiration from human physiology and computational biology.","Turki, Houcemeddine; Taieb, Mohamed Ali Hadj; Aouicha, Mohamed Ben","Briefings in Bioinformatics",="14675463",,="Sep2021","22","5","1","2","152975229","10.1093/bib/bbab081","Oxford University Press / USA","Article","ARTIFICIAL intelligence; ALGORITHMS; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); Research and development in the physical, engineering and life sciences; Research and Development in Biotechnology; COMPUTATIONAL biology; HUMAN physiology; INSPIRATION","biological computing; computational biology; explainable artificial intelligence; human physiology; information retrieval","In this letter, we explain how intuitive and explainable methods inspired from human physiology and computational biology can serve to simplify and ameliorate the way we process and generate knowledge resources. [ABSTRACT FROM AUTHOR] Copyright of Briefings in Bioinformatics is the property of Oxford University Press / USA and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=152975229&site=bsi-live"
"Development of ontology from Indian agricultural e-governance data using IndoWordNet: a semantic web approach.","Sinha, Bhaskar; Chandra, Somnath; Garg, Megha","Journal of Knowledge Management",="13673270",,="2015","19","1","25","20","101006312","10.1108/JKM-10-2014-0441","Emerald Publishing Limited","Article","INFORMATION theory; CULTURE; INDIA; AGRICULTURE; ELECTRONIC government information; SEMANTICS","Artificial intelligence; Domain concept; Domain ontology; Information retrieval; Information technology; Semantic web","Purpose -- The purpose of this explorative research study is to focus on the implementation of semantic Web technology on agriculture domain of e-governance data. The study contributes to an understanding of problems and difficulties in implantations of unstructured and unformatted unique datasets of multilingual local language-based electronic dictionary (IndoWordnet). Design/methodology/approach -- An approach to an implementation in the perspective of conceptual logical concept to realization of agriculture-based terms and terminology extracted from linked multilingual IndoWordNet while maintaining the support and specification of the World Wide Web Consortium (W3C) standard of semantic Web technology to generate ontology and uniform unicode structured datasets. Findings -- The findings reveal the fact about partial support of extraction of terms, relations and concepts while linking to IndoWordNet, resulting in the form of SynSets, lexical relations of Words and relations between themselves. This helped in generation of ontology, hierarchical modeling and creation of structured metadata datasets. Research limitations/implications -- IndoWordNet has limitations, as it is not fully revised version due to diversified cultural base in India, and the new version is yet to be released in due time span. As mentioned in Section 5, implications of these ideas and experiments will have good impact in doing more exploration and better applications using such wordnet. Practical implications -- Language developer tools and frameworks have been used to get tagged annotated raw data processed and get intermediate results, which provides as a source for the generation of ontology and dynamic metadata. Social implications -- The results are expected to be applied for other e-governance applications. Better use of applications in social and government departments. Originality/value -- The authors have worked out experimental facts and raw information source datasets, revealing satisfactory results such as SynSets, sensecount, semantic and lexical relations, class concepts hierarchy and other related output, which helped in developing ontology of domain interest and, hence, creation of a dynamic metadata which can be globally used to facilitate various applications support. [ABSTRACT FROM AUTHOR] Copyright of Journal of Knowledge Management is the property of Emerald Publishing Limited and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=101006312&site=bsi-live"
"Distance Learning for Similarity Estimation.","Jie Yu; Amores, Jaume; Sebe, Nicu; Radeva, Petia; Qi Tian","IEEE Transactions on Pattern Analysis & Machine Intelligence",="01628828",,="Mar2008","30","3","451","12","29986406","10.1109/TPAMI.2007.70714","IEEE","Article","ARTIFICIAL intelligence; ALGORITHMS; INFORMATION retrieval; Photographic and Photocopying Equipment Manufacturing; Computer Terminal and Other Computer Peripheral Equipment Manufacturing; PATTERN recognition systems; PATTERN perception; COMPUTER vision; MACHINE learning; IMAGE analysis; IMAGING systems","algorithms; artificial intelligence; image classification; information retrieval; pattern recognition","In this paper, we present a general guideline to find a better distance measure for similarity estimation based on statistical analysis of distribution models and distance functions. A new set of distance measures are derived from the harmonic distance, the geometric distance, and their generalized variants according to the Maximum Likelihood theory. These measures can provide a more accurate feature model than the classical euclidean and Manhattan distances. We also find that the feature elements are often from heterogeneous sources that may have different influence on similarity estimation. Therefore, the assumption of single isotropic distribution model is often inappropriate. To alleviate this problem, we use a boosted distance measure framework that finds multiple distance measures, which fit the distribution of selected feature elements best for accurate similarity estimation. The new distance measures for similarity estimation are tested on two applications: stereo matching and motion tracking in video sequences. The performance of boosted distance measure is further evaluated on several benchmark data sets from the UCI repository and two image retrieval applications. In all the experiments, robust results are obtained based on the proposed methods. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Pattern Analysis & Machine Intelligence is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=29986406&site=bsi-live"
"Editorial Artificial Intelligence and Innovation Management.","Tanev, Stoyan; Sandstrom, Gregory","Technology Innovation Management Review",="19270321",,="Dec2019","9","12","3","2","141074995","10.22215/timreview/1286","Carleton University, Talent First Network, Technology Innovation Management Review","Article","ARTIFICIAL intelligence; INNOVATION management; NATURAL language processing; AUSTRIA","3S Process; Advanced Analytics; AI; AI innovationmanagement; AI value chain; AImaturity; Artificial intelligence; Austria; Big Data; business education; connected health; data access; datamanagement; decision-making; design science; design thinking; enterprise platform; environmental scanning; front-end of innovation; governance; Harvard CaseMethod; information mobility; information processing; information retrieval; innovation; innovation search field; latent semantic indexing; opportunity; orchestration; patient- centered; SME",,"https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=141074995&site=bsi-live"
"Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs.","Malkov, Yu A.; Yashunin, D. A.","IEEE Transactions on Pattern Analysis & Machine Intelligence",="01628828",,="Apr2020","42","4","824","13","143315070","10.1109/TPAMI.2018.2889473","IEEE","Article","ALGORITHMS; NEW South Wales; METRIC spaces","approximate search; artificial intelligence; big data; data structures; Graph and tree search strategies; graphs and networks; information search and retrieval; information storage and retrieval; information technology and systems; nearest neighbor search; search process; similarity search","We present a new approach for the approximate K-nearest neighbor search based on navigable small world graphs with controllable hierarchy (Hierarchical NSW, HNSW). The proposed solution is fully graph-based, without any need for additional search structures (typically used at the coarse search stage of the most proximity graph techniques). Hierarchical NSW incrementally builds a multi-layer structure consisting of a hierarchical set of proximity graphs (layers) for nested subsets of the stored elements. The maximum layer in which an element is present is selected randomly with an exponentially decaying probability distribution. This allows producing graphs similar to the previously studied Navigable Small World (NSW) structures while additionally having the links separated by their characteristic distance scales. Starting the search from the upper layer together with utilizing the scale separation boosts the performance compared to NSW and allows a logarithmic complexity scaling. Additional employment of a heuristic for selecting proximity graph neighbors significantly increases performance at high recall and in case of highly clustered data. Performance evaluation has demonstrated that the proposed general metric space search index is able to strongly outperform previous opensource state-of-the-art vector-only approaches. Similarity of the algorithm to the skip list structure allows straightforward balanced distributed implementation. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Pattern Analysis & Machine Intelligence is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=143315070&site=bsi-live"
"Efficient feature extraction model for validation performance improvement of duplicate bug report detection in software bug triage systems.","Soleimani Neysiani, Behzad; Babamir, Seyed Morteza; Aritsugi, Masayoshi","Information & Software Technology",="09505849",,="Oct2020","126",,"N.PAG","1","145736248","10.1016/j.infsof.2020.106344","Elsevier B.V.","Article","NATURAL language processing; COMPUTER software; Software publishers (except video game publishers); Computer and software stores; Computer and Computer Peripheral Equipment and Software Merchant Wholesalers; Computer, computer peripheral and pre-packaged software merchant wholesalers; FEATURE extraction; MODEL validation; MACHINE learning; EXTRACTION techniques","Bug reports; Dimension reduction; Duplicate detection; Feature extraction; Feature selection; Information retrieval; Natural language processing; Textual similarity metric","There are many duplicate bug reports in the semi-structured software repository of various software bug triage systems. The duplicate bug report detection (DBRD) process is a significant problem in software triage systems. The DBRD problem has many issues, such as efficient feature extraction to calculate similarities between bug reports accurately, building a high-performance duplicate detector model, and handling continuous real-time queries. Feature extraction is a technique that converts unstructured data to structured data. The main objective of this study is to improve the validation performance of DBRD using a feature extraction model. This research focuses on feature extraction to build a new general model containing all types of features. Moreover, it introduces a new feature extractor method to describe a new viewpoint of similarity between texts. The proposed method introduces new textual features based on the aggregation of term frequency and inverse document frequency of text fields of bug reports in uni-gram and bi-gram forms. Further, a new hybrid measurement metric is proposed for detecting efficient features, whereby it is used to evaluate the efficiency of all features, including the proposed ones. The validation performance of DBRD was compared for the proposed features and state-of-the-art features. To show the effectiveness of our model, we applied it and other related studies to DBRD of the Android, Eclipse, Mozilla, and Open Office datasets and compared the results. The comparisons showed that our proposed model achieved (i) approximately 2% improvement for accuracy and precision and more than 4.5% and 5.9% improvement for recall and F1-measure , respectively, by applying the linear regression (LR) and decision tree (DT) classifiers and (ii) a performance of 91%−99% (average ~97%) for the four metrics, by applying the DT classifier as the best classifier. Our proposed features improved the validation performance of DBRD concerning runtime performance. The pre-processing methods (primarily stemming) could improve the validation performance of DBRD slightly (up to 0.3%), but rule-based machine learning algorithms are more useful for the DBRD problem. The results showed that our proposed model is more effective both for the datasets for which state-of-the-art approaches were effective (i.e., Mozilla Firefox) and those for which state-of-the-art approaches were less effective (i.e., Android). The results also showed that the combination of all types of features could improve the validation performance of DBRD even for the LR classifier with less validation performance, which can be implemented easily for software bug triage systems. Without using the longest common subsequence (LCS) feature, which is effective but time-consuming, our proposed features could cover the effectiveness of LCS with lower time-complexity and runtime overhead. In addition, a statistical analysis shows that the results are reliable and can be generalized to other datasets or similar classifiers. [ABSTRACT FROM AUTHOR] Copyright of Information & Software Technology is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=145736248&site=bsi-live"
"Elicitation and use of relevance feedback information","Vechtomova, Olga; Karamuftuoglu, Murat","Information Processing & Management",="03064573",,="Jan2006","42","1","191","16","18194897","10.1016/j.ipm.2004.10.006","Elsevier B.V.","Article","INFORMATION retrieval; DOCUMENTATION; INFORMATION science; QUERY (Information retrieval system); ARCHIVAL research","Information retrieval; Interactive retrieval; Natural language processing; Query expansion; Relevance feedback","Abstract: The paper presents two approaches to interactively refining user search formulations and their evaluation in the new High Accuracy Retrieval from Documents (HARD) track of TREC-12. The first method consists of asking the user to select a number of sentences that represent documents. The second method consists of showing to the user a list of noun phrases extracted from the initial document set. Both methods then expand the query based on the user feedback. The TREC results show that one of the methods is an effective means of interactive query expansion and yields significant performance improvements. The paper presents a comparison of the methods and detailed analysis of the evaluation results. [Copyright &y& Elsevier] Copyright of Information Processing & Management is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=18194897&site=bsi-live"
"Excavating the mother lode of human-generated text: A systematic review of research that uses the wikipedia corpus.","Mehdi, Mohamad; Okoli, Chitu; Mesgari, Mostafa; Nielsen, Finn Årup; Lanamäki, Arto","Information Processing & Management",="03064573",,="Mar2017","53","2","505","25","120834017","10.1016/j.ipm.2016.07.003","Elsevier B.V.","Article","INFORMATION retrieval; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); Research and development in the physical, engineering and life sciences; META-analysis; COMPUTER science research; ONTOLOGY; WIKIPEDIA","Information extraction; Information retrieval; Literature review; Natural language processing; Ontologies; Wikipedia","Although primarily an encyclopedia, Wikipedia’s expansive content provides a knowledge base that has been continuously exploited by researchers in a wide variety of domains. This article systematically reviews the scholarly studies that have used Wikipedia as a data source, and investigates the means by which Wikipedia has been employed in three main computer science research areas: information retrieval, natural language processing, and ontology building. We report and discuss the research trends of the identified and examined studies. We further identify and classify a list of tools that can be used to extract data from Wikipedia, and compile a list of currently available data sets extracted from Wikipedia. [ABSTRACT FROM AUTHOR] Copyright of Information Processing & Management is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=120834017&site=bsi-live"
"Exploiting named entity recognition for improving syntactic-based web service discovery.","Lizarralde, Ignacio; Mateos, Cristian; Rodriguez, Juan Manuel; Zunino, Alejandro","Journal of Information Science",="01655515",,="Jun2019","45","3","398","18","136492938","10.1177/0165551518793321","Sage Publications, Ltd.","Article","WEB services; INFORMATION retrieval; COMPUTER software industry; BIG data; DATA mining; Computer, computer peripheral and pre-packaged software merchant wholesalers; Computer and software stores; Computer and Computer Peripheral Equipment and Software Merchant Wholesalers; Software publishers (except video game publishers); Software Publishers; Data Processing, Hosting, and Related Services","Information retrieval; named entity recognition; natural language processing; service-oriented computing; web Services","Web Services have become essential to the software industry as they represent reusable, remotely accessible functionality and data. Since Web Services must be discovered before being consumed, many discovery approaches applying classic Information Retrieval techniques, which store and process textual service descriptions, have arisen. These efforts are affected by term mismatch: a description relevant to a query can be retrieved only if they share many words. We present an approach to improve Web Service discoverability that automatically augments Web Service descriptions and can be used on top of such existing syntactic-based approaches. We exploit Named Entity Recognition to identify entities in descriptions and expand them with information from public text corpora, for example, Wikidata, mitigating term mismatch since it exploits both synonyms and hypernyms. We evaluated our approach together with classical syntactic-based service discovery approaches using a real 1274-service dataset, achieving up to 15.06% better Recall scores, and up to 17% Precision-at-1, 8% Precision-at-2 and 4% Precision-at-3. [ABSTRACT FROM AUTHOR] Copyright of Journal of Information Science is the property of Sage Publications, Ltd. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=136492938&site=bsi-live"
"Extraction of complex index terms in non-English IR: A shallow parsing based approach","Vilares, Jesús; Alonso, Miguel A.; Vilares, Manuel","Information Processing & Management",="03064573",,="Jul2008","44","4","1517","21","32497047","10.1016/j.ipm.2007.12.005","Elsevier B.V.","Article","INFORMATION storage & retrieval systems; INFORMATION retrieval; HUMAN-computer interaction; INFORMATION resources management; All Other Information Services; KEYWORD searching; COMPUTATIONAL linguistics; ASSISTED searching (Information retrieval)","Finite-state transducers; Information retrieval; Linguistic variation; Natural language processing; Shallow parsing","The performance of information retrieval systems is limited by the linguistic variation present in natural language texts. Word-level natural language processing techniques have been shown to be useful in reducing this variation. In this article, we summarize our work on the extension of these techniques for dealing with phrase-level variation in European languages, taking Spanish as a case in point. We propose the use of syntactic dependencies as complex index terms in an attempt to solve the problems deriving from both syntactic and morpho-syntactic variation and, in this way, to obtain more precise index terms. Such dependencies are obtained through a shallow parser based on cascades of finite-state transducers in order to reduce as far as possible the overhead due to this parsing process. The use of different sources of syntactic information, queries or documents, has been also studied, as has the restriction of the dependencies applied to those obtained from noun phrases. Our approaches have been tested using the CLEF corpus, obtaining consistent improvements with regard to classical word-level non-linguistic techniques. Results show, on the one hand, that syntactic information extracted from documents is more useful than that from queries. On the other hand, it has been demonstrated that by restricting dependencies to those corresponding to noun phrases, important reductions of storage and management costs can be achieved, albeit at the expense of a slight reduction in performance. [Copyright &y& Elsevier] Copyright of Information Processing & Management is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=32497047&site=bsi-live"
"Fast and compact updating algorithms of a double-array structure","Morita, Kazuhiro; Atlam, El-Sayed; Fuketa, Masao; Tsuda, Kazuhiko; Aoe, Jun-ichi","Information Sciences",="00200255",,="Jan2004","159","1/2","53","15","11825861","10.1016/S0020-0255(03)00189-0","Elsevier B.V.","Article","DATABASE searching; ALGORITHMS; ELECTRONIC data processing; COMPUTER science; Research and development in the physical, engineering and life sciences; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); Data Processing, Hosting, and Related Services","Dictionary; Digital search; Information retrieval; Key retrieval strategies; Natural language processing","In many information retrieval applications, it is necessary to be able to adopt a trie search for looking at the input character by character. As a fast and compact data structure for a trie, a double-array is presented. However, the insertion time is not faster than other dynamic retrieval methods because the double-array is a semi-static retrieval method that cannot treat high frequent updating. Further, the space efficiency of the double-array degrades with the number of deletions because it keeps empty elements produced by deletion. This paper presents a fast insertion algorithm by linking empty elements to find inserting positions quickly and a compression algorithm by reallocating empty elements for each deletion. From the simulation results for 100 thousands keys, it turned out that the insertion time and the space efficiency are achieved. [Copyright &y& Elsevier] Copyright of Information Sciences is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=11825861&site=bsi-live"
"Feedback evaluations to promote image captioning.","He, Jun; Zhao, Yijia; Sun, Bo; Yu, Lejun","IET Image Processing (Wiley-Blackwell)",="17519659",,="Nov2020","14","13","3021","7","148084462","10.1049/iet-ipr.2019.1317","Wiley-Blackwell","Article",,"ARL; auxiliary retrieval loss; caption‐generating process; captioning model; captioning process; discriminability score; evaluation reward; feature extraction; feedback evaluation method; feedback evaluations; generated caption; image captioning evaluation metrics; image retrieval; information retrieval; learning (artificial intelligence); neural nets; policy gradient problem; retrieval model; text analysis","Image captioning can be treated as a policy gradient problem. A retrieval model to obtain the discriminability score to distinguish between two images, given the caption for one of them, has been proposed previously; the discriminability score and one of the image captioning evaluation metrics were optimised using policy gradient. Based on this, two methods to evaluate the caption and caption‐generating process, referred to as feedback evaluations, are proposed in this study. The results of the evaluations were used to improve the model. First, an auxiliary retrieval loss (ARL) is introduced to evaluate the generated caption to improve the discriminability of the model. ARL has been utilised as a feedback evaluation method because it calculates similarity between the generated caption and convolutional neural network features. With ARL, a higher similarity and better discriminability were achieved. Second, an evaluation reward is introduced to evaluate the captioning process. With ER, the overall evaluation metrics can be improved. A policy gradient was used, and a captioning model could be trained by jointly adjusting the captioning process and captioning itself. The attention long short‐term memory network was trained with ARL and ER successively and it demonstrated state‐of‐the‐art performance on the COCO database. [ABSTRACT FROM AUTHOR] Copyright of IET Image Processing (Wiley-Blackwell) is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=148084462&site=bsi-live"
"Genetic-based approaches in ranking function discovery and optimization in information retrieval — A framework","Fan, Weiguo; Pathak, Praveen; Zhou, Mi","Decision Support Systems",="01679236",,="Nov2009","47","4","398","10","44580988","10.1016/j.dss.2009.04.005","Elsevier B.V.","Article","INFORMATION retrieval; INFORMATION storage & retrieval systems; RELEVANCE ranking (Information science); QUERYING (Computer science); GENETIC algorithms; EVOLUTIONARY computation; MULTISENSOR data fusion; ROUTING (Computer network management)","Artificial intelligence; Data fusion; Evolutionary computations; Genetic algorithms; Information retrieval","An Information Retrieval (IR) system consists of document collection, queries issued by users, and the matching/ranking functions used to rank documents in the predicted order of relevance for a given query. A variety of ranking functions have been used in the literature. But studies show that these functions do not perform consistently well across different contexts. In this paper we propose a two-stage integrated framework for discovering and optimizing ranking functions used in IR. The first stage, discovery process, is accomplished by intelligently leveraging the structural and statistical information available in HTML documents by using Genetic Programming techniques to yield novel ranking functions. In the second stage, the optimization process, document retrieval scores of various well-known ranking functions are combined using Genetic Algorithms. The overall discovery and optimization framework is tested on the well-known TREC collection of web documents for both the ad-hoc retrieval task and the routing task. Utilizing our framework we observe a significant increase in retrieval performance compared to some of the well-known stand alone ranking functions. [Copyright &y& Elsevier] Copyright of Decision Support Systems is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=44580988&site=bsi-live"
"Identifying Features in Opinion Mining via Intrinsic and Extrinsic Domain Relevance.","Hai, Zhen; Chang, Kuiyu; Kim, Jung-Jae; Yang, Christopher C.","IEEE Transactions on Knowledge & Data Engineering",="10414347",,="Mar2014","26","3","623","12","94339023","10.1109/TKDE.2013.26","IEEE","Article","SENTIMENT analysis; DATA mining; FEATURE extraction; MATHEMATICAL domains; RELEVANCE ranking (Information science); HIDDEN Markov models","Batteries; Chinese; Data mining; Dispersion; Educational institutions; Feature extraction; Hidden Markov models; Information search and retrieval; natural language processing; opinion feature; opinion mining; Syntactics","The vast majority of existing approaches to opinion feature extraction rely on mining patterns only from a single review corpus, ignoring the nontrivial disparities in word distributional characteristics of opinion features across different corpora. In this paper, we propose a novel method to identify opinion features from online reviews by exploiting the difference in opinion feature statistics across two corpora, one domain-specific corpus (i.e., the given review corpus) and one domain-independent corpus (i.e., the contrasting corpus). We capture this disparity via a measure called domain relevance (DR), which characterizes the relevance of a term to a text collection. We first extract a list of candidate opinion features from the domain review corpus by defining a set of syntactic dependence rules. For each extracted candidate feature, we then estimate its intrinsic-domain relevance (IDR) and extrinsic-domain relevance (EDR) scores on the domain-dependent and domain-independent corpora, respectively. Candidate features that are less generic (EDR score less than a threshold) and more domain-specific (IDR score greater than another threshold) are then confirmed as opinion features. We call this interval thresholding approach the intrinsic and extrinsic domain relevance (IEDR) criterion. Experimental results on two real-world review domains show the proposed IEDR approach to outperform several other well-established methods in identifying opinion features. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Knowledge & Data Engineering is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=94339023&site=bsi-live"
"Improving automatic bug assignment using time‐metadata in term‐weighting.","Shokripour, Ramin; Anvik, John; Kasirun, Zarinah M.; Zamani, Sima","IET Software (Wiley-Blackwell)",="17518806",,="Dec2014","8","6","269","10","148479833","10.1049/iet-sen.2013.0150","Wiley-Blackwell","Article",,"automatic bug assignment improvement; information retrieval; information-retrieval technique; learning (artificial intelligence); machine learning technique; manual bug triage process; meta data; program debugging; software fault tolerance; statistical analysis; statistical computation technique; term frequency; term-weighting technique; time-metadata technique","Assigning newly reported bugs to project developers is a time‐consuming and tedious task for triagers using the traditional manual bug triage process. Previous efforts for creating automatic bug assignment systems use machine learning and information‐retrieval techniques. These approaches commonly use tf‐idf, a statistical computation technique for weighting terms based on term frequency. However, tf‐idf does not consider the metadata, such as the time frame at which a term was used, when calculating the weight of the terms. This study proposes an alternate term‐weighting technique to improve the accuracy of automatic bug assignment approaches that use a term‐weighting technique. This technique includes the use of metadata in addition to the statistical computation to calculate the term weights. Moreover, it restricts the set of terms used to only nouns. It was found that when using only nouns and the proposed term‐weighting technique, the accuracy of an automatic bug assignment approach improves from 12 to 49% over tf‐idf for three open‐source projects. [ABSTRACT FROM AUTHOR] Copyright of IET Software (Wiley-Blackwell) is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=148479833&site=bsi-live"
"Incorporating Deep Median Networks for Arabic Document Retrieval Using Word Embeddings-Based Query Expansion.","Farhan, Yasir Hadi; Tareq, Mustafa Abd; Shakir, Mohanaad; Shannaq, Boumedyen","Journal of Information Science Theory & Practice (JIStaP)",="22879099",,="2024","12","3","36","13","180644236","10.1633/JISTaP.2024.12.3.3","Korea Institute of Science & Technology Information","Article","NATURAL language processing; INFORMATION retrieval; INFORMATION networks; RESEARCH personnel; SYNONYMS","Arabic document retrieval; automatic query expansion; deep median networks; information retrieval; natural language processing; word embedding","The information retrieval (IR) process often encounters a challenge known as query-document vocabulary mismatch, where user queries do not align with document content, impacting search effectiveness. Automatic query expansion (AQE) techniques aim to mitigate this issue by augmenting user queries with related terms or synonyms. Word embedding, particularly Word2Vec, has gained prominence for AQE due to its ability to represent words as real-number vectors. However, AQE methods typically expand individual query terms, potentially leading to query drift if not carefully selected. To address this, researchers propose utilizing median vectors derived from deep median networks to capture query similarity comprehensively. Integrating median vectors into candidate term generation and combining them with the BM25 probabilistic model and two IR strategies (EQE1 and V2Q) yields promising results, outperforming baseline methods in experimental settings. [ABSTRACT FROM AUTHOR] Copyright of Journal of Information Science Theory & Practice (JIStaP) is the property of Korea Institute of Science & Technology Information and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=180644236&site=bsi-live"
"Independent component analysis for near-synonym choice","Yu, Liang-Chih; Chien, Wei-Nan","Decision Support Systems",="01679236",,="Apr2013","55","1","146","10","87616698","10.1016/j.dss.2012.12.038","Elsevier B.V.","Article","PERFORMANCE evaluation; DISCRIMINANT analysis; INFORMATION retrieval; INDEPENDENT component analysis; SYNONYMS; SUPPORT vector machines; LATENT semantic analysis","Independent component analysis; Information retrieval; Natural language processing; Near-synonym choice","Abstract: Despite their similar meanings, near-synonyms may have different usages in different contexts, and the development of algorithms that can verify whether near-synonyms do match their given contexts has been the focus of increasing concern. Such algorithms have many applications such as query expansion for information retrieval (IR), alternative word selection for writing support systems, and (near-)duplicate detection for text summarization. In this paper, we propose a framework that incorporates latent semantic analysis (LSA) and independent component analysis (ICA) to automatically select suitable near-synonyms according to the given context. LSA is used to discover useful latent features that do not frequently occur in the contexts of near-synonyms, and ICA is used to estimate a set of independent components by minimizing the dependence between features. An SVM classifier is then trained with the independent components for best near-synonym prediction. In experiments, we evaluate the proposed method on both Chinese and English sentences, and compare its performance to state-of-the-art supervised and unsupervised methods. Experimental results show that training on the independent components that contain useful contextual features with minimized term dependence can improve the classifiers'' ability to discriminate among near-synonyms, thus yielding better performance. [Copyright &y& Elsevier] Copyright of Decision Support Systems is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=87616698&site=bsi-live"
"INEX Tweet Contextualization task: Evaluation, results and lesson learned.","Bellot, Patrice; Moriceau, Véronique; Mothe, Josiane; SanJuan, Eric; Tannier, Xavier","Information Processing & Management",="03064573",,="Sep2016","52","5","801","19","116988472","10.1016/j.ipm.2016.03.002","Elsevier B.V.","Article","MICROBLOGS; ANAPHORA (Linguistics); PARTS of speech; SENTENCES (Grammar); WIKIPEDIA","Automatic summarization; Contextual information retrieval; Focus information retrieval; Kullback–Leibler divergence; Natural language processing; Question answering; Short text contextualization; Text informativeness; Text readability; Textual references; Tweet contextualization; Tweet understanding; Wikipedia","Microblogging platforms such as Twitter are increasingly used for on-line client and market analysis. This motivated the proposal of a new track at CLEF INEX lab of Tweet Contextualization . The objective of this task was to help a user to understand a tweet by providing him with a short explanatory summary (500 words). This summary should be built automatically using resources like Wikipedia and generated by extracting relevant passages and aggregating them into a coherent summary. Running for four years, results show that the best systems combine NLP techniques with more traditional methods. More precisely the best performing systems combine passage retrieval, sentence segmentation and scoring, named entity recognition, text part-of-speech (POS) analysis, anaphora detection, diversity content measure as well as sentence reordering. This paper provides a full summary report on the four-year long task. While yearly overviews focused on system results, in this paper we provide a detailed report on the approaches proposed by the participants and which can be considered as the state of the art for this task. As an important result from the 4 years competition, we also describe the open access resources that have been built and collected. The evaluation measures for automatic summarization designed in DUC or MUC were not appropriate to evaluate tweet contextualization, we explain why and depict in detailed the LogSim measure used to evaluate informativeness of produced contexts or summaries. Finally, we also mention the lessons we learned and that it is worth considering when designing a task. [ABSTRACT FROM AUTHOR] Copyright of Information Processing & Management is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=116988472&site=bsi-live"
"Information is essential for competitive and cost-effective public procurement.","Gorgun, Mustafa Kaan; Kutlu, Mucahid; Tas, Bedri Kamil Onur","Journal of Information Science",="01655515",,="Dec2022",,,"1",,"160812237","10.1177/01655515221141042","Sage Publications, Ltd.","Article",,"Economic analysis; information retrieval; natural language processing; public procurement","Public authorities promote transparent public procurement practices to increase competition and reduce public procurement costs. In this article, we focus on public procurement of the European Union (EU). We employ a multidisciplinary approach to analyse economic effects of information in public procurement. We quantify the information content of 2,390,630 EU public procurement notices published in 22 different languages using natural language processing techniques. Subsequently, we examine the impact of the information content on public procurement outcomes. We find that higher information levels have significant positive effects. Competition is considerably higher when notices contain more information. On average, contract prices would be 6%–8% lower if notices were to contain adequate information. EU governments could save up to € 80 billion if all public procurement notices were to have detailed information. Based on our comprehensive analysis, we believe that authorities should regulate the information content of notices to promote competition and cost-effectiveness in public procurement. [ABSTRACT FROM AUTHOR] Copyright of Journal of Information Science is the property of Sage Publications, Ltd. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=160812237&site=bsi-live"
"Information Retrieval: Searching in the 21st Century; Human Information Retrieval.","Larson, Ray R.","Journal of the American Society for Information Science & Technology",="15322882",,="Nov2010","61","11","2370","3","54336873","10.1002/asi.21399","Wiley-Blackwell","Book Review","NONFICTION; INFORMATION Retrieval: Searching in the 21st Century (Book); HUMAN Information Retrieval (Book); GOEKER, Ayse; WARNER, Julian","evaluation; information retrieval; multilingual retrieval; natural language processing; ontologies","The article reviews two books ""Information Retrieval: Searching in the 21st Century,"" by Ayşe Göker and ""Human Information Retrieval,"" by Julian Warner.","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=54336873&site=bsi-live"
"Integrating Discriminant and Descriptive Information for Dimension Reduction and Classification.","Jie Yu; Qi Tian; Ting Rui; Huang, Thomas S.","IEEE Transactions on Circuits & Systems for Video Technology",="10518215",,="Mar2007","17","3","372","6","24570410","10.1109/TCSVT.2007.890861","IEEE","Article","ARTIFICIAL intelligence; IMAGE retrieval; INFORMATION retrieval; DATA analysis; INFORMATION storage & retrieval systems; One-Hour Photofinishing; Photofinishing Laboratories (except One-Hour); DATA compression; IMAGE processing; PATTERN perception","Artificial intelligence; image classification; information retrieval; pattern recognition","In this paper, a novel hybrid dimension reduction technique for classification is proposed based on the hybrid analysis of principal component analysis (PCA) and linear discriminant analysis (LDA). LDA is known for capturing the most discriminant features of the data in the projected space while PCA is known for preserving the most descriptive ones after projection. Our hybrid technique integrates discriminant and descriptive information and finds a richer set of alternatives beyond LDA and PCA in a 2-D parametric space, which fits a specific classification task and data distribution better. Theoretical study shows that our technique also alleviates the singularity problem of scatter matrix, which is caused by small training set, and increases the effective dimension of the projected subspace. In order to find the hybrid features adaptively and avoid exhaustive parameter searching, we further propose a boosted hybrid analysis method that incorporates a nonlinear boosting process to enhance a set of hybrid classifiers and combine them into a more accurate one. Compared with the other techniques that aim at combining PCA and LDA, our approaches are novel because our method finds alternatives to LDA and PCA in a 2-D parameter space and the boosting process provides enhancement and robust combination of the classifiers. Extensive experiments are conducted on benchmark and real image databases to compare our proposed methods with the state-of-the-art linear and nonlinear discriminant analysis techniques. The results show the superior performance of our hybrid analysis methods. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Circuits & Systems for Video Technology is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=24570410&site=bsi-live"
"Integrating large language models and generative artificial intelligence tools into information literacy instruction.","Carroll, Alexander J.; Borycz, Joshua","Journal of Academic Librarianship",="00991333",,="Jul2024","50","4","N.PAG","1","177854578","10.1016/j.acalib.2024.102899","Elsevier B.V.","Article","INFORMATION science; LANGUAGE & languages; GENERATIVE artificial intelligence; INFORMATION literacy; STEM education; ENGINEERING education","Critical thinking; Generative artificial intelligence; Information literacy; Information retrieval; Large language models","Generative artificial intelligence (AI) and large language models (LLMs) have induced a mixture of excitement and panic among educators. However, there is a lack of consensus over how much experience science and engineering students have with using these tools for research-related tasks. Likewise, it is not yet known how educators and information professionals can leverage these tools to teach students strategies for information retrieval and knowledge synthesis. This study assesses the extent of students' use of AI tools in research-related tasks and if information literacy instruction could impact their perception of these tools. Responses to Likert-scale questions indicate that many students did not have extensive experience using LLMs for research-related purposes prior to the information literacy sessions. However, after participating in a didactic lecture and discussion with an engineering librarian that explored how to use these tools effectively and responsibly, many students reported viewing these tools as potentially useful for future assignments. Student responses to open-response questions suggest that librarian-led information literacy training can assist students in developing more sophisticated understandings of the limitations and use cases for artificial intelligence in inquiry-based coursework. [ABSTRACT FROM AUTHOR] Copyright of Journal of Academic Librarianship is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=177854578&site=bsi-live"
"Interactive query expansion for professional search applications.","Russell-Rose, Tony; Gooch, Philip; Kruschwitz, Udo","Business Information Review",="02663821",,="Sep2021","38","3","127","11","153606127","10.1177/02663821211034079","Sage Publications Inc.","Article","INFORMATION professionals; KNOWLEDGE workers; MEDICAL personnel; PATENT lawyers; EMPLOYEE recruitment; Human Resources Consulting Services; Administration of Human Resource Programs (except Education, Public Health, and Veterans' Affairs Programs)","Information retrieval; machine learning; natural language processing; ontologies; professional search; query expansion","Knowledge workers (such as healthcare information professionals, patent agents and recruitment professionals) undertake work tasks where search forms a core part of their duties. In these instances, the search task is often complex and time-consuming and requires specialist expert knowledge to formulate accurate search strategies. Interactive features such as query expansion can play a key role in supporting these tasks. However, generating query suggestions within a professional search context requires that consideration be given to the specialist, structured nature of the search strategies they employ. In this paper, we investigate a variety of query expansion methods applied to a collection of Boolean search strategies used in a variety of real-world professional search tasks. The results demonstrate the utility of context-free distributional language models and the value of using linguistic cues to optimise the balance between precision and recall. [ABSTRACT FROM AUTHOR] Copyright of Business Information Review is the property of Sage Publications Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=153606127&site=bsi-live"
"Introduction to the special issue on patent processing","Fujii, Atsushi; Iwayama, Makoto; Kando, Noriko","Information Processing & Management",="03064573",,="Sep2007","43","5","1149","5","24867640","10.1016/j.ipm.2006.11.004","Elsevier B.V.","Article","PATENTS; INTELLECTUAL property; PATENT law; INFORMATION services; All Other Information Services; All Other Legal Services; Offices of Lawyers","Citation analysis; Classification; Information retrieval; Natural language processing; Patent processing","Abstract: The processing of intellectual property documents, such as patents, has been important to the industry, business, and law communities. Recently, the importance of patent processing has also been recognized in academic research communities, particularly by information retrieval and natural language processing researchers. In addition, large test collections that include patents have recently become available, to enable the systematic evaluation of methodologies from a scientific point of view. In the light of these activities, this special issue is intended to collect advanced research papers on patent processing. As an introduction to the special issue on patent processing, this paper surveys the relevant literature and outlines the papers selected for the special issue. [Copyright &y& Elsevier] Copyright of Information Processing & Management is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=24867640&site=bsi-live"
"Is 1 noun worth 2 adjectives? Measuring relative feature utility","Losee, Robert M.","Information Processing & Management",="03064573",,="Sep2006","42","5","1248","12","20557435","10.1016/j.ipm.2005.11.002","Elsevier B.V.","Article","INFORMATION retrieval; NATURAL language processing; LINGUISTICS; NOUNS","Document ordering; Economic utility; Information retrieval; Natural language processing; Performance measures; Relative performance","Abstract: Are two adjectives worth the same as a single noun when documents are ordered based on decreasing topicality? We propose an easy to interpret single number Relative Feature Utility (RFU) measure of the relative worth of using specific linguistic or non-linguistic features or sets of features in computational systems that order or filter media, such as information retrieval and classification systems. This measure allows one to make easily interpreted claims about the relative utility of features such as parts-of-speech, term suffixes, phrases vs. single terms, annotations, hyperlinks, citations, index terms, and metadata when ordering natural language text or other media. Data is provided for the RFU for stemming characteristics, part-of-speech tags, and phrase lengths, as well as retrieval characteristics and procedures. Using this linear measure of the relative utility of features makes available a wide range of cost-benefit analyses and decision theoretic techniques, allowing the study of whether or not to use many different kinds of representational information or tagging systems, and for the design of indexing and metadata systems. Some characteristics of natural languages used in the spectrum from softer to harder sciences, as well as medical terminology, are studied. [Copyright &y& Elsevier] Copyright of Information Processing & Management is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=20557435&site=bsi-live"
"Karen Spärck Jones.","Robertson, Stephen; Tait, John","Journal of the American Society for Information Science & Technology",="15322882",,="Mar2008","59","5","852","3","30081006","10.1002/asi.20784","Wiley-Blackwell","Obituary","SPARCK Jones, Karen, 1935-2007","information retrieval; natural language processing; scholars","An obituary is presented for computer science researcher Karen Spärck Jones.","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=30081006&site=bsi-live"
"Knowledge powered by artificial intelligence.","Pichman, Brian","Information Services & Use",="01675265",,="Nov2024",,,"1",,"180869637","10.1177/18758789241299017","IOS Press","Article","ARTIFICIAL intelligence; LEGACY systems; KNOWLEDGE management; GENERATIVE artificial intelligence; LANGUAGE models","AI ethics; bias reduction; customer support AI; generative AI; healthcare AI; knowledge forecasting; knowledge management; large language models; legacy systems; multilingual support; personalized information retrieval; RAG; real-time collaboration; Retrieval Augmented Generation systems","Generative Artificial Intelligence (GenAI) has revolutionized knowledge management, offering unprecedented capabilities for creating, proofing, summarizing, and evaluating documentation. This paper explores how AI, particularly large language models (LLMs), and Retrieval Augmented Generation (RAG) systems, can streamline the development of knowledge articles while addressing ethical concerns such as data ownership and bias. We examine practical applications, including real-time collaboration, multilingual support, personalized information retrieval, and automated knowledge forecasting. Additionally, we explore AI’s role in bridging legacy systems, reducing biases, and enhancing decision-making. Ultimately, AI extends beyond generating content, shaping a more efficient, inclusive, and innovative approach to knowledge management. This article is based upon a presentation given at the 2024 NISO Plus Conference that was held in Baltimore, MD, USA, February 13–14, 2024. [ABSTRACT FROM AUTHOR] Copyright of Information Services & Use is the property of IOS Press and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=180869637&site=bsi-live"
"Language-modeling kernel based approach for information retrieval.","Ying Xie; Raghavan, Vijay V.","Journal of the American Society for Information Science & Technology",="15322882",,="Dec2007","58","14","2353","13","27704229","10.1002/asi.20711","Wiley-Blackwell","Article","INFORMATION resources management; INFORMATION science; INFORMATION services; INFORMATION retrieval; ACCESS to information; SEARCH engines; FUNCTIONAL analysis; All Other Information Services; INFORMATION-seeking strategies; INFORMATION filtering; MACHINE learning","concept discrimination; information retrieval; machine learning; natural language processing; vector space models","In this presentation, we propose a novel integrated information retrieval approach that provides a unified solution for two challenging problems in the field of information retrieval. The first problem is how to build an optimal vector space corresponding to users' different information needs when applying the vector space model. The second one is how to smoothly incorporate the advantages of machine learning techniques into the language modeling approach. To solve these problems, we designed the language-modeling kernel function, which has all the modeling powers provided by language modeling techniques. In addition, for each information need, this kernel function automatically determines an optimal vector space, for which a discriminative learning machine, such as the support vector machine, can be applied to find an optimal decision boundary between relevant and nonrelevant documents. Large-scale experiments on standard test-beds show that our approach makes significant improvements over other state-of-the-art information retrieval methods. [ABSTRACT FROM AUTHOR] Copyright of Journal of the American Society for Information Science & Technology is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=27704229&site=bsi-live"
"Leveraging AI-based Decision Support for Opportunity Analysis.","Groher, Wolfgang; Rademacher, Friedrich-Wilhelm; Csillaghy, André","Technology Innovation Management Review",="19270321",,="Dec2019","9","12","29","7","141074992","10.22215/timreview/1289","Carleton University, Talent First Network, Technology Innovation Management Review","Article","ARTIFICIAL intelligence; INFORMATION retrieval; DESIGN science; INFORMATION processing; LATENT semantic analysis; CONCEPTUAL models","artificial intelligence; decision-making; design science; environmental scanning; front-end of innovation; information processing; information retrieval; innovation search field; latent semantic indexing; opportunity","The dynamics and speed of change in corporate environments have increased. At the front-end of innovation, firms are challenged to evaluate growing amounts of information within shorter time frames in order to stay competitive. Either they spend significant time on structured data analysis, at the risk of delayed market launch, or they follow their intuition, at the risk of not meeting market trends. Both scenarios constitute a significant risk for a firm's continued existence.Motivated by this, a conceptual model is presented in this paper that aims at remediating these risks. Grounded on design science methodology, it concentrates on previous assessments of innovation search fields. These innovation search fields assist in environmental scanning and lay the foundation for deciding which opportunities to pursue. The model applies a novel AI-based approach, which draws on natural language processing and information retrieval. To provide decision support, the approach includes market-, technology-, and firm-related criteria. This allows us to replace intuitive decision-making by fact-based considerations. In addition, an often-iterative approach for environmental scanning is replaced by a more straightforward process. Early testing of the conceptual model has shown results of increased quality and speed of decision-making. Further testing and feedback is still required to enhance and calibrate the AI-functionality. Applied in business environments, the approach can contribute to remediate fuzziness in early front-end activities, thus helping direct innovation managers to ""do the right things"". [ABSTRACT FROM AUTHOR] Copyright of Technology Innovation Management Review is the property of Carleton University, Talent First Network, Technology Innovation Management Review and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=141074992&site=bsi-live"
"Lexical and Syntactic knowledge for Information Retrieval","Ferrández, Antonio","Information Processing & Management",="03064573",,="Sep2011","47","5","692","14","63189714","10.1016/j.ipm.2011.01.003","Elsevier B.V.","Article","INFORMATION retrieval; NATURAL language processing; TASK analysis; SEARCH engines; LEXICAL-functional grammar; SEMANTICS (Philosophy); QUERYING (Computer science)","Information Retrieval; Lexical and syntactic relationships; Natural Language Processing; Question Answering; Term Proximity","Abstract: Traditional Information Retrieval (IR) models assume that the index terms of queries and documents are statistically independent of each other, which is intuitively wrong. This paper proposes the incorporation of the lexical and syntactic knowledge generated by a POS-tagger and a syntactic Chunker into traditional IR similarity measures for including this dependency information between terms. Our proposal is based on theories of discourse structure by means of the segmentation of documents and queries into sentences and entities. Therefore, we measure dependencies between entities instead of between terms. Moreover, we handle discourse references for each entity. It has been evaluated on Spanish and English corpora as well as on Question Answering tasks obtaining significant increases. [Copyright &y& Elsevier] Copyright of Information Processing & Management is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=63189714&site=bsi-live"
"Listwise approach based on the cross‐correntropy for learning to rank.","Wu, Mintao; Zhu, Jihua; Wang, Jun; Pang, Shanmin; Li, Yaochen","Electronics Letters (Wiley-Blackwell)",="00135194",,="Jul2018","54","15","878","3","148786885","10.1049/el.2018.0815","Wiley-Blackwell","Article",,"cross‐correntropy loss; document retrieval; entropy; gradient descent algorithm; gradient methods; information retrieval; learning (artificial intelligence); learning to rank; ListCCE; listwise approach; listwise loss function; neural network model; training model","The problem of learning to rank is addressed and a novel listwise approach by taking document retrieval as an example is proposed. It first introduces the concept of cross‐correntropy into learning to rank and then proposes the listwise loss function based on the cross‐correntropy between the ranking list given by the label and the one predicted by training model. The use of the cross‐correntropy loss leads to the development of the listwise approach called ListCCE, which employs the gradient descent algorithm to train a neural network model. Experimental results tested on publicly available data sets show that the proposed approach performs better than some existing approaches. [ABSTRACT FROM AUTHOR] Copyright of Electronics Letters (Wiley-Blackwell) is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=148786885&site=bsi-live"
"Machine learning and ontology-based novel semantic document indexing for information retrieval.","Sharma, Anil; Kumar, Suresh","Computers & Industrial Engineering",="03608352",,="Feb2023","176",,"N.PAG","1","161600684","10.1016/j.cie.2022.108940","Elsevier B.V.","Article","INFORMATION retrieval; COMPUTER science; Research and development in the physical, engineering and life sciences; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); ONTOLOGIES (Information retrieval); MACHINE learning; ANALYTIC hierarchy process; INFORMATION needs","Computer science ontology; Concept extraction; Document indexing; Information retrieval; Machine learning; Natural language processing; Semantic web","• Document key phrases to ontology concept mapping with limited or no related concepts in the ontology. • Analytic hierarchy process based application specific concept feature weights. • Document's concept term variations and synonyms mapped on domain ontology. • Average F-measure enhanced by 25% compared to the state-of-the-art. The goal of information retrieval (IR) systems is to find the contents most closely related to the user's information needs from a pool of information. However, conventional IR methods neglect semantic descriptions of document contents and index documents based on the words that they include. When users and indexing systems use different terms to express the same subject, a vocabulary gap emerges. To overcome this limitation and to enhance the effectiveness of the IR systems, this paper introduced a novel hybrid semantic document indexing employing machine learning and domain ontology. The presented technique uses a skip-gram with negative sampling-based machine learning model and a domain ontology to determine the concepts for annotating unstructured documents. The proposed work also introduced multiple feature based novel concept ranking algorithm where statistical, semantic, and scientific named entity features of the concept were used to assign relevance weight to the annotations. The fuzzy analytical hierarchy process was used to derive the parameters of these feature weights. The final step is to rank the concepts according to their relevance to the document. Five benchmark publicly accessible datasets from the computer science domain were used in a series of experiments to validate the results of presented method. Experiment findings showed that the proposed method performs better than state-of-the-art techniques on these datasets, by improving average accuracy by 29%, while an improvement of 25% was recorded in F-measure. The improvement in average accuracy demonstrates that the performance of the proposed approach is better than the state-of-the-art methods in extracting document concepts accurately even when the same concept is referred to by distinct terms in the document and domain ontologies. The proposed system's ability to find similar concepts when the documents possess no concept from domain ontology is demonstrated by the improvement in F-measure, which is attributed to high recall rates of the proposed indexing scheme while maintaining high accuracy. [ABSTRACT FROM AUTHOR] Copyright of Computers & Industrial Engineering is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=161600684&site=bsi-live"
"Measuring the Semantic Uncertainty of News Events for Evolution Potential Estimation.","XIANGFENG LUO; JUNYU XUAN; JIE LU; GUANGQUAN ZHANG","ACM Transactions on Information Systems",="10468188",,="2016","34","4","1","25","116348725","10.1145/2903719","Association for Computing Machinery","Article","DECISION making; PUBLIC relations; UNCERTAINTY (Information theory); STATISTICAL correlation; Public Relations Agencies; SEMANTIC computing","Information search and retrieval; natural language processing; news event; semantic analysis; text mining","The evolution potential estimation of news events can support the decision making of both corporations and governments. For example, a corporation could manage its public relations crisis in a timely manner if a negative news event about this corporation is known with large evolution potential in advance. However, existing state-of-the-art methods are mainly based on time series historical data, which are not suitable for the news events with limited historical data and bursty properties. In this article, we propose a purely content-based method to estimate the evolution potential of the news events. The proposed method considers a news event at a given time point as a system composed of different keywords, and the uncertainty of this system is defined and measured as the Semantic Uncertainty of this news event. At the same time, an uncertainty space is constructed with two extreme states: the most uncertain state and the most certain state. We believe that the Semantic Uncertainty has correlation with the content evolution of the news events, so it can be used to estimate the evolution potential of the news events. In order to verify the proposed method, we present detailed experimental setups and results measuring the correlation of the Semantic Uncertainty with the Content Change of news events using collected news events data. The results show that the correlation does exist and is stronger than the correlation of value from the time-series-based method with the Content Change. Therefore, we can use the Semantic Uncertainty to estimate the evolution potential of news events. [ABSTRACT FROM AUTHOR] Copyright of ACM Transactions on Information Systems is the property of Association for Computing Machinery and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=116348725&site=bsi-live"
"Mining knowledge from natural language texts using fuzzy associated concept mapping","Wang, W.M.; Cheung, C.F.; Lee, W.B.; Kwok, S.K.","Information Processing & Management",="03064573",,="Sep2008","44","5","1707","13","33389491","10.1016/j.ipm.2008.05.002","Elsevier B.V.","Article","ARTIFICIAL intelligence; HUMAN-computer interaction; KNOWLEDGE management; MATHEMATICAL logic; COMPUTATIONAL linguistics; DATABASE evaluation; CITATION indexes; SET theory; FUZZY sets","Concept mapping; Fuzzy set theory; Information retrieval; Knowledge management; Knowledge mining; Natural language processing","Abstract: Natural Language Processing (NLP) techniques have been successfully used to automatically extract information from unstructured text through a detailed analysis of their content, often to satisfy particular information needs. In this paper, an automatic concept map construction technique, Fuzzy Association Concept Mapping (FACM), is proposed for the conversion of abstracted short texts into concept maps. The approach consists of a linguistic module and a recommendation module. The linguistic module is a text mining method that does not require the use to have any prior knowledge about using NLP techniques. It incorporates rule-based reasoning (RBR) and case based reasoning (CBR) for anaphoric resolution. It aims at extracting the propositions in text so as to construct a concept map automatically. The recommendation module is arrived at by adopting fuzzy set theories. It is an interactive process which provides suggestions of propositions for further human refinement of the automatically generated concept maps. The suggested propositions are relationships among the concepts which are not explicitly found in the paragraphs. This technique helps to stimulate individual reflection and generate new knowledge. Evaluation was carried out by using the Science Citation Index (SCI) abstract database and CNET News as test data, which are well known databases and the quality of the text is assured. Experimental results show that the automatically generated concept maps conform to the outputs generated manually by domain experts, since the degree of difference between them is proportionally small. The method provides users with the ability to convert scientific and short texts into a structured format which can be easily processed by computer. Moreover, it provides knowledge workers with extra time to re-think their written text and to view their knowledge from another angle. [Copyright &y& Elsevier] Copyright of Information Processing & Management is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=33389491&site=bsi-live"
"Modeling and Learning Distributed Word Representation with Metadata for Question Retrieval.","Zhou, Guangyou; Huang, Jimmy Xiangji","IEEE Transactions on Knowledge & Data Engineering",="10414347",,="Jun2017","29","6","1226","14","122814210","10.1109/TKDE.2017.2665625","IEEE","Article","METADATA; FACILITATED learning; INTERROGATIVE (Grammar); DOCUMENT type definitions; ARCS Model of Motivational Design","Aggregates; community question answering; Computational modeling; Context modeling; information retrieval; Kernel; Knowledge discovery; Metadata; Natural language processing; question retrieval; Semantics; text mining","Community question answering (cQA) has become an important issue due to the popularity of cQA archives on the Web. This paper focuses on addressing the lexical gap problem in question retrieval. Question retrieval in cQA archives aims to find the existing questions that are semantically equivalent or relevant to the queried questions. However, the lexical gap problem brings a new challenge for question retrieval in cQA. In this paper, we propose to model and learn distributed word representations with metadata of category information within cQA pages for question retrieval using two novel category powered models. One is a basic category powered model called MB-NET and the other one is an enhanced category powered model called ME-NET which can better learn the distributed word representations and alleviate the lexical gap problem. To deal with the variable size of word representation vectors, we employ the framework of fisher kernel to transform them into the fixed-length vectors. Experimental results on large-scale English and Chinese cQA data sets show that our proposed approaches can significantly outperform state-of-the-art retrieval models for question retrieval in cQA. Moreover, we further conduct our approaches on large-scale automatic evaluation experiments. The evaluation results show that promising and significant performance improvements can be achieved. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Knowledge & Data Engineering is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=122814210&site=bsi-live"
"Natural language asymmetry and internet infrastructures.","di Sciullo, Anna Maria","International Journal of Electronic Business",="14706067",,="2005","3","3/4","1","1","18845010","10.1504/IJEB.2005.007276","Inderscience Enterprises Ltd.","Abstract","INFORMATION storage & retrieval systems; NATURAL language processing; ELECTRONIC data processing; SEARCH engines; INFORMATION architecture; ONLINE information services; Data Processing, Hosting, and Related Services","asymmetric relations; communication; e-business; electronic business; information extraction; information processing; information retrieval; internet search; natural language processing internet infrastructures; search engine performance","This article presents the summary of an article published in the ""International Journal of Electronic Business,"" which examines the main features of an information retrieval and extraction system based on natural language asymmetric relations. The article illustrates that asymmetric relations help in improving the performance of search engines. An information retrieval and extraction system based on the recovery of a subset of asymmetric relations is compared with current operating search engines based on keyword search and Boolean analysis. Finally, it has been shown that natural language asymmetries are important elements of internet infrastructures and ensure greater precision to internet communication.","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=18845010&site=bsi-live"
"Natural language processing and query expansion in legal information retrieval: Challenges and a response.","Maxwell, Tamsin; Schafer, Burkhard","International Review of Law, Computers & Technology",="13600869",,="Mar2010","24","1","63","10","48361706","10.1080/13600860903570194","Taylor & Francis Ltd","Article","INFORMATION storage & retrieval systems; ABSTRACTING & indexing services; INFORMATION retrieval; NATURAL language processing; ELECTRONIC data processing; Data Processing, Hosting, and Related Services","information retrieval; law; natural language processing; query expansion","As methods in legal information retrieval (IR) evolve to meet the demands of rapidly increasing stores of electronic information, there is the intuitive appeal of capturing detail in legal queries with natural language processing (NLP). One difficulty with this approach is that incorporation of word dependencies in IR has not been shown to consistently and reliably improve results over a unigram bag-of-words approach. We consider challenges faced when incorporating NLP in IR and briefly review three proposals in this vein, highlighting how these might have responded better to requirements in legal search. We then present our novel response based on split query expansion that accounts for the way lawyers seek to apply search results whilst meeting the challenges identified in a unique and flexible manner. [ABSTRACT FROM AUTHOR] Copyright of International Review of Law, Computers & Technology is the property of Routledge and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=48361706&site=bsi-live"
"Opera Clustering: K-means on librettos datasets.","Harim Jeong; Joo Hun Yoo","Journal of Internet Computing & Services",="15980170",,="Apr2022","23","2","45","8","156816744","10.7472/jksii.2022.23.2.45","Korean Society for Internet Information","Article","ARTIFICIAL intelligence; NATURAL language processing; K-means clustering; MACHINE learning; EMOTIONS","Classification; Embedding; K-means Clustering; Music Analysis; Music Information Retrieval; Natural Language Processing","With the development of artificial intelligence analysis methods, especially machine learning, various fields are widely expanding their application ranges. However, in the case of classical music, there still remain some difficulties in applying machine learning techniques. Genre classification or music recommendation systems generated by deep learning algorithms are actively used in general music, but not in classical music. In this paper, we attempted to classify opera among classical music. To this end, an experiment was conducted to determine which criteria are most suitable among, composer, period of composition, and emotional atmosphere, which are the basic features of music. To generate emotional labels, we adopted zero-shot classification with four basic emotions, ‘happiness’, ‘sadness’, ‘anger’, and ‘fear.’ After embedding the opera libretto with the doc2vec processing model, the optimal number of clusters is computed based on the result of the elbow method. Decided four centroids are then adopted in k-means clustering to classify unsupervised libretto datasets. We were able to get optimized clustering based on the result of adjusted rand index scores. With these results, we compared them with notated variables of music. As a result, it was confirmed that the four clusterings calculated by machine after training were most similar to the grouping result by period. Additionally, we were able to verify that the emotional similarity between composer and period did not appear significantly. At the end of the study, by knowing the period is the right criteria, we hope that it makes easier for music listeners to find music that suits their tastes. [ABSTRACT FROM AUTHOR] Copyright of Journal of Internet Computing & Services is the property of Korean Society for Internet Information and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=156816744&site=bsi-live"
"Predicting taxi demand hotspots using automated Internet Search Queries.","Markou, Ioulia; Kaiser, Kevin; Pereira, Francisco C.","Transportation Research Part C: Emerging Technologies",="0968090X",,="May2019","102",,"73","14","135915002","10.1016/j.trc.2019.03.001","Elsevier B.V.","Article","INTERNET searching; NATURAL language processing; GLOBALIZATION","Demand prediction; Information retrieval; Natural language processing; Query expansion; Special events","Highlights • Popular events can cause distinct taxi demand hotspots. • Internet search queries are proven useful for the prediction of demand hotspots. • Queries expansion with two new terms return more representative results. • MedLDA performs better than an independent topic modelling and classification process. • Terms with time expressions seem to be good prediction indicators in topics. Abstract Disruptions due to special events are a well-known challenge in transport operations, since the transport system is typically designed for habitual demand. Part of the problem relates to the difficulty in collecting comprehensive and reliable information early enough to prepare mitigation measures. A tool that automatically scans the internet for events and predicts their impact would strongly support transport management in many cities in the world. This study addresses the challenges related to retrieving and analyzing web documents about real world events, and using them for demand explanation (if related to a past event) and prediction (if a future one). Transport demand is predicted with a supervised topic modeling algorithm by utilizing information about social events retrieved using various strategies, which made use of search aggregation, natural language processing, and query expansion. It was found that a two-step process produced the highest accuracy for transport demand prediction, where different (but related) queries are used to retrieve an initial set of documents, and then, based on these documents, a final query is constructed that obtains the set of predictive documents. These are then used to model the most discriminating topics related to the transport demand. A framework was proposed that sequentially handles all stages of data gathering, enrichment, and prediction with the intention of generating automated search queries. [ABSTRACT FROM AUTHOR] Copyright of Transportation Research Part C: Emerging Technologies is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=135915002&site=bsi-live"
"Predictive hierarchical human augmented map generation for itinerary perception.","Kaur, B.; Bhattacharya, J.","Electronics Letters (Wiley-Blackwell)",="00135194",,="Aug2016","52","16","1381","3","148785116","10.1049/el.2016.0397","Wiley-Blackwell","Article",,"augmented reality; cartography; environmental conditions; Gaussian process; Gaussian processes; image sensors; information retrieval; itinerary perception; learning (artificial intelligence); machine learning technique; mobile robots; path planning; predictive hierarchical human augmented map generation; region‐of‐interest input acquisition; robot vision; service robot; service robots; timing events; traffic density distribution database; traffic engineering computing; traffic related information extraction; vision sensor","A new class of augmented map application is introduced which can provide detailed knowledge about any area, to a user. This brief particularly focuses on obtaining itinerary perception subject to different environmental conditions. This refers to extraction of traffic related information from an augmented map. The problem is modelled as a machine learning technique where the traffic distribution at different times (including same days, different days and different weather) are observed continuously using a service robot. This data is posed as a Gaussian process for post‐estimation. Our system consists of a vision sensor which will acquire the region of interest input, queried to a database of traffic density distributions, learned from the scenes at different points of time. The user interacting with the system will obtain an information pertaining to the region conditioned on environmental and timing events. [ABSTRACT FROM AUTHOR] Copyright of Electronics Letters (Wiley-Blackwell) is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=148785116&site=bsi-live"
"Processing social media in real-time.","Spina, Damiano; Zubiaga, Arkaitz; Sheth, Amit; Strohmaier, Markus","Information Processing & Management",="03064573",,="May2019","56","3","1081","3","135137886","10.1016/j.ipm.2018.06.006","Elsevier B.V.","Article","DATA mining; NATURAL language processing; INFORMATION retrieval; SOCIAL media; SOCIAL context","Data mining; Information retrieval; Natural language processing; Social media mining",,"https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=135137886&site=bsi-live"
"Project-Based As-Needed Information Retrieval from Unstructured AEC Documents.","Fan, Hongqin; Xue, Fan; Li, Heng","Journal of Management in Engineering",="0742597X",,="Jan2015","31","1","-1","1","99974277","10.1061/(ASCE)ME.1943-5479.0000341","American Society of Civil Engineers","Article","PROJECT management; ENGINEERING; RECORDS management; Residential building construction; Process, Physical Distribution, and Logistics Consulting Services; Administrative Management and General Management Consulting Services; Other management consulting services; Other business support services; ARCHITECTURAL research; CONSTRUCTION industry research","Decision support; Document management; Information retrieval; Natural language processing; Project management","With the increasing complexity of architecture, engineering, and construction (AEC) projects and fast track execution of project works, written documents are becoming more and more important for project coordination, communication, and works control. Finding all the relevant information from unstructured construction documents is critical to various management tasks such as work planning, progress control, and claims. A framework is proposed in this research to retrieve project-wide as-needed information from AEC documents. Through this framework, improvement in the levels of precision and recall in the information retrieval process can be made effective through the use of a project-specific term dictionary and dependency grammar parsing information of textual documents. Their effectiveness is demonstrated through a series of experimental tests conducted on a real life building redevelopment project with different information retrieval and ranking strategies. The results and findings are presented in this paper along with discussion on the related issues on research and system development. [ABSTRACT FROM AUTHOR] Copyright of Journal of Management in Engineering is the property of American Society of Civil Engineers and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=99974277&site=bsi-live"
"Recognition of cursive video text using a deep learning framework.","Mirza, Ali; Siddiqi, Imran","IET Image Processing (Wiley-Blackwell)",="17519659",,="Dec2020","14","14","3444","12","148084505","10.1049/iet-ipr.2019.1070","Wiley-Blackwell","Article",,"background segmentation; bi‐directional recurrent neural networks; character recognition rate; complex ligatures; content‐based retrieval; context‐dependent shape variations; convolutional networks; convolutional neural network; cursive caption text; cursive scripts; cursive video text; deep learning; end‐to‐end framework; feature extraction; feature sequence extraction; image segmentation; information retrieval; learning (artificial intelligence); mature V‐OCRs; News channel videos; noncursive scripts; optical character recognition; overlapping ligatures; recurrent neural nets; sequence‐to‐sequence mapping; text analysis; text lines extraction; text regions; textual content‐based retrieval system; Urdu text; video frames; video optical character recognition systems; video retrieval; video signal processing; video text recognition","This study focuses on cursive text recognition appearing in videos, using a complete framework of deep neural networks. While mature video optical character recognition systems (V‐OCRs) are available for text in non‐cursive scripts, recognition of cursive scripts is marked by many challenges. These include complex and overlapping ligatures, context‐dependent shape variations and presence of a large number of dots and diacritics. The authors present an analytical technique for recognition of cursive caption text that relies on a combination of convolutional and recurrent neural networks trained in an end‐to‐end framework. Text lines extracted from video frames are preprocessed to segment the background and are fed to a convolutional neural network for feature extraction. The extracted feature sequences are fed to different variants of bi‐directional recurrent neural networks along with the ground truth transcription to learn sequence‐to‐sequence mapping. Finally, a connectionist temporal classification layer is employed to produce the final transcription. Experiments on a data set of more than 40,000 text lines from 11,192 video frames of various News channel videos reported an overall character recognition rate of 97.63%. The proposed work employs Urdu text as a case study but the findings can be generalised to other cursive scripts as well. [ABSTRACT FROM AUTHOR] Copyright of IET Image Processing (Wiley-Blackwell) is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=148084505&site=bsi-live"
"Retrieval Contrastive Learning for Aspect-Level Sentiment Classification.","Jian, Zhongquan; Li, Jiajian; Wu, Qingqiang; Yao, Junfeng","Information Processing & Management",="03064573",,="Jan2024","61","1","N.PAG","1","173784683","10.1016/j.ipm.2023.103539","Elsevier B.V.","Article",,"Aspect-level sentiment classification; Contrastive learning; Information retrieval; Natural language processing","Aspect-Level Sentiment Classification (ALSC) aims to assign specific sentiments to a sentence toward different aspects, which is one of the crucial challenges in the field of Natural Language Processing (NLP). Despite numerous approaches being proposed and obtaining prominent results, the majority of them focus on leveraging the relationships between the aspect and opinion words in a single instance while ignoring correlations with other instances, which will make models inevitably become trapped in local optima due to the absence of a global viewpoint. Instance representation derived from a single instance, on the one hand, the contained information is insufficient due to the lack of descriptions from other perspectives; on the other hand, its stored knowledge is redundant since the inability to filter extraneous content. To obtain a polished instance representation, we developed a Retrieval Contrastive Learning (RCL) framework to subtly extract intrinsic knowledge across instances. RCL consists of two modules: (a) obtaining retrieval instances by sparse retriever and dense retriever, and (b) extracting and learning the knowledge of the retrieval instances by using Contrastive Learning (CL). To demonstrate the superiority of RCL, five ALSC models are employed to conduct comprehensive experiments on three widely-known benchmarks. Compared with the baselines, ALSC models achieve substantial improvements when trained with RCL. Especially, ABSA-DeBERTa with RCL obtains new state-of-the-art results, which outperform the advanced methods by 0.92%, 0.23%, and 0.47% in terms of Macro F1 gains on Laptops, Restaurants, and Twitter, respectively. • We proposed RCL to enable ALSC models to generate polished representations. • RCL has two modules: obtain retrieval instances and learn common features by CL. • The sparse and dense retrievers are used to obtain high-quality retrieval instances. • The ALSC model can be enhanced greatly by training with RCL. • ABSA-DeBERTa obtains new state-of-the-art results by being trained with RCL. [ABSTRACT FROM AUTHOR] Copyright of Information Processing & Management is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=173784683&site=bsi-live"
"Retrieve–Revise–Refine: A novel framework for retrieval of concise entailing legal article set.","Nguyen, Chau; Nguyen, Phuong; Nguyen, Le-Minh","Information Processing & Management",="03064573",,="Jan2025","62","1","N.PAG","1","181036359","10.1016/j.ipm.2024.103949","Elsevier B.V.","Article",,"COLIEE competition; Information retrieval; Large language models; Legal article set retrieval; Retrieval–Revise–Refine framework","The retrieval of entailing legal article sets aims to identify a concise set of legal articles that holds an entailment relationship with a legal query or its negation. Unlike traditional information retrieval that focuses on relevance ranking, this task demands conciseness. However, prior research has inadequately addressed this need by employing traditional methods. To bridge this gap, we propose a three-stage Retrieve–Revise–Refine framework which explicitly addresses the need for conciseness by utilizing both small and large language models (LMs) in distinct yet complementary roles. Empirical evaluations on the COLIEE 2022 and 2023 datasets demonstrate that our framework significantly enhances performance, achieving absolute increases in the macro F2 score by 3.17% and 4.24% over previous state-of-the-art methods, respectively. Specifically, our Retrieve stage, employing various tailored fine-tuning strategies for small LMs, achieved a recall rate exceeding 0.90 in the top-5 results alone—ensuring comprehensive coverage of entailing articles. In the subsequent Revise stage, large LMs narrow this set, improving precision while sacrificing minimal coverage. The Refine stage further enhances precision by leveraging specialized insights from small LMs, resulting in a relative improvement of up to 19.15% in the number of concise article sets retrieved compared to previous methods. Our framework offers a promising direction for further research on specialized methods for retrieving concise sets of entailing legal articles, thereby more effectively meeting the task's demands. • Propose a novel three-stage Retrieve–Revise–Refine framework for concise legal article set retrieval. • Achieve 3.17% and 4.24% higher macro F2 scores on two evaluated datasets. • Retrieve 19.15% more concise sets of legal articles compared to previous methods. • Ablation studies show over 90% coverage within top-5 results in the Retrieval stage. [ABSTRACT FROM AUTHOR] Copyright of Information Processing & Management is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=181036359&site=bsi-live"
"Reviewer assignment algorithms for peer review automation: A survey.","Zhao, Xiquan; Zhang, Yangsen","Information Processing & Management",="03064573",,="Sep2022","59","5","N.PAG","1","158915546","10.1016/j.ipm.2022.103028","Elsevier B.V.","Article","MATHEMATICAL optimization; ALGORITHMS; NATURAL language processing; ASSIGNMENT problems (Programming)","Information retrieval; Matching degree; Natural language processing; Optimization algorithm; Peer review; Reviewer assignment problem","Assigning paper to suitable reviewers is of great significance to ensure the accuracy and fairness of peer review results. In the past three decades, many researchers have made a wealth of achievements on the reviewer assignment problem (RAP). In this survey, we provide a comprehensive review of the primary research achievements on reviewer assignment algorithm from 1992 to 2022. Specially, this survey first discusses the background and necessity of automatic reviewer assignment, and then systematically summarize the existing research work from three aspects, i.e., construction of candidate reviewer database, computation of matching degree between reviewers and papers, and reviewer assignment optimization algorithm, with objective comments on the advantages and disadvantages of the current algorithms. Afterwards, the evaluation metrics and datasets of reviewer assignment algorithm are summarized. To conclude, we prospect the potential research directions of RAP. Since there are few comprehensive survey papers on reviewer assignment algorithm in the past ten years, this survey can serve as a valuable reference for the related researchers and peer review organizers. [ABSTRACT FROM AUTHOR] Copyright of Information Processing & Management is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=158915546&site=bsi-live"
"Roundtable: What's Next in Software Analytics.","Hassan, Ahmed E.; Hindle, Abram; Runeson, Per; Shepperd, Martin; Devanbu, Prem; Kim, Sunghun","IEEE Software",="07407459",,="Jul2013","30","4","53","4","88781445","10.1109/MS.2013.85","IEEE","Article","COMPUTER software; COMPUTER software development; COMPUTER software developers; Software publishers (except video game publishers); Computer and Computer Peripheral Equipment and Software Merchant Wholesalers; Computer, computer peripheral and pre-packaged software merchant wholesalers; Computer and software stores; Computer systems design and related services (except video game design and development); Custom Computer Programming Services; DEVANBU, Prem; SHEPPERD, Martin; HASSAN, Ahmed; O'BRIAN, Chloe; RUNESON, Per","Analytical models; Data mining; decision support; information search and retrieval; management; metrics; Natural language processing; Software development; Software engineering; software/program verification; statistical methods; testing and debugging","For this special issue, the guest editors asked a panel of six established experts in software analytics to highlight what they thought were the most important, or overlooked, aspect of this field. They all pleaded for a much broader view of analytics than seen in current practice: software analytics should go beyond developers (Ahmed Hassan) and numbers (Per Runeson). Analytics should also prove its relevance to practitioners (Abram Hindle, Martin Shepperd). There are now opportunities for ""natural"" software analytics based on statistical natural language processing (Prem Devanbu). Lastly, software analytics needs information analysts and field agents like Chloe O'Brian and Jack Bauer in the TV show 24 (Sung Kim). [ABSTRACT FROM PUBLISHER] Copyright of IEEE Software is the property of IEEE Computer Society and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=88781445&site=bsi-live"
"RUBRIC: A System for Rule-Based Information Retrieval.","McCune, Brian P.; Tong, Richard M.; Dean, Jeffrey S.; Shapiro, Daniel G.","IEEE Transactions on Software Engineering",="00985589",,="Sep85","11","9","939","7","14281470",,"IEEE","Article","INFORMATION retrieval; COMPUTER software; ARTIFICIAL intelligence; EXPERT systems; INFORMATION science; Computer, computer peripheral and pre-packaged software merchant wholesalers; Computer and Computer Peripheral Equipment and Software Merchant Wholesalers; Computer and software stores; Software publishers (except video game publishers); FUZZY logic","Artificial intelligence; evidential reasoning; expert systems; information retrieval","A research prototype software system for conceptual information retrieval has been developed. The goal of the system, called RUBRIC, is to provide more automated and relevant access to unformatted textual databases. The approach is to use production rules from artificial intelligence to define a hierarchy of retrieval subtopics, with fuzzy context expressions and specific word phrases at the bottom. RUBRIC allows the definition of detailed queries starting at a conceptual level, partial matching of a query and a document, selection of only the highest ranked documents for presentation to the user, and detailed explanation of how and why a particular document was selected. Initial experiments indicate that a RUBRIC rule set better matches human retrieval judgment than a standard Boolean keyword expression, given equal amounts of effort in defining each. The techniques presented may be useful in stand-alone retrieval systems, front-ends to existing information retrieval systems, or real-time document filtering and routing. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Software Engineering is the property of IEEE Computer Society and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=14281470&site=bsi-live"
"Sistemas de recuperación de información adaptados al dominio biomédico.","Marrero, Mónica; Sánchez-Cuadrado, Sonia; Urbano, Julián; Morato, Jorge; Moreiro, José-Antonio","El Profesional de la Información",="13866710",,="may/jun2010","19","3","246","9","52819191","10.3145/epi.2010.may.04","EPI SCP","Article","INFORMATION resources management; SQL; SEARCH engines; INFORMATION storage & retrieval systems; ELECTRONIC data processing; HUMAN-computer interaction; NATURAL language processing; INFORMATION retrieval; Data Processing, Hosting, and Related Services; All Other Information Services; COMPUTATIONAL linguistics","Biomedicine; BioNER; BioNLP; Information retrieval; Natural Language Processing; NLP; Text-mining; Biomedicina; BioNER; BioNLP; NLP; Proceso del lenguaje natural; Recuperación de información; Text-mining","The terminology used in biomedicine has lexical characteristics that have required the elaboration of terminological resources and information retrieval systems with specific functionalities. The main characteristics are the high rates of synonymy and homonymy, due to phenomena such as the proliferation of polysemic acronyms and their interaction with common language. Information retrieval systems in the biomedical domain use techniques oriented to the treatment of these lexical peculiarities. In this paper we review some of these techniques, such as the application of Natural Language Processing (BioNLP), the incorporation of lexical-semantic resources, and the application of Named Entity Recognition (BioNER). Finally, we present the evaluation methods adopted to assess the suitability of these techniques for retrieving biomedical resources. (English) [ABSTRACT FROM AUTHOR] La terminología usada en biomedicina tiene rasgos léxicos que han requerido la elaboración de recursos terminológicos y sistemas de recuperación de información con funciones específicas. Las principales características son las elevadas tasas de sinonimia y homonimia, debidas a fenómenos como la proliferación de siglas polisémicas y su interacción con el lenguaje común. Los sistemas de recuperación de información en el dominio biomédico utilizan técnicas orientadas al tratamiento de estas peculiaridades léxicas. Se revisan algunas de estas técnicas, como la aplicación de Procesamiento del Lenguaje Natural (BioNLP), la incorporación de recursos léxico-semánticos, y la aplicación de Reconocimiento de Entidades (BioNER). Se presentan los métodos de evaluación adoptados para comprobar la adecuación de estas técnicas en la recuperación de recursos biomédicos. (Spanish) [ABSTRACT FROM AUTHOR] Copyright of El Profesional de la Información is the property of EPI SCP and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=52819191&site=bsi-live"
"Spatially Aware Term Selection for Geotagging.","Van Laere, Olivier; Quinn, Jonathan; Schockaert, Steven; Dhoedt, Bart","IEEE Transactions on Knowledge & Data Engineering",="10414347",,="Jan2014","26","1","221","14","92680834","10.1109/TKDE.2013.42","IEEE","Article","ELECTRONIC publishing; KNOWLEDGE management; ARTIFICIAL intelligence; INFORMATION retrieval; Book, periodical and newspaper merchant wholesalers; TAGS (Metadata); FEATURE extraction; ENCYCLOPEDIAS & dictionaries","artificial intelligence; classification; Context; Electronic publishing; Encyclopedias; Estimation; feature extraction; geographic information retrieval; Information search and retrieval; Internet; knowledge management; metadata; semi-structured data; Standards; text mining","The task of assigning geographic coordinates to textual resources plays an increasingly central role in geographic information retrieval. The ability to select those terms from a given collection that are most indicative of geographic location is of key importance in successfully addressing this task. However, this process of selecting spatially relevant terms is at present not well understood, and the majority of current systems are based on standard term selection techniques, such as $(\chi^2)$ or information gain, and thus fail to exploit the spatial nature of the domain. In this paper, we propose two classes of term selection techniques based on standard geostatistical methods. First, to implement the idea of spatial smoothing of term occurrences, we investigate the use of kernel density estimation (KDE) to model each term as a two-dimensional probability distribution over the surface of the Earth. The second class of term selection methods we consider is based on Ripley's K statistic, which measures the deviation of a point set from spatial homogeneity. We provide experimental results which compare these classes of methods against existing baseline techniques on the tasks of assigning coordinates to Flickr photos and to Wikipedia articles, revealing marked improvements in cases where only a relatively small number of terms can be selected. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Knowledge & Data Engineering is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=92680834&site=bsi-live"
"Structuring Tweets for Improving Twitter Search.","Luo, Zhunchen; Yu, Yang; Osborne, Miles; Wang, Ting","Journal of the Association for Information Science & Technology",="23301635",,="Dec2015","66","12","2522","18","111519802","10.1002/asi.23332","Wiley-Blackwell","Article","INFORMATION retrieval; PUBLIC opinion; ACCESS to information; SOCIAL media","information retrieval; natural language processing; text mining","Spam and wildly varying documents make searching in Twitter challenging. Most Twitter search systems generally treat a Tweet as a plain text when modeling relevance. However, a series of conventions allows users to Tweet in structural ways using a combination of different blocks of texts. These blocks include plain texts, hashtags, links, mentions, etc. Each block encodes a variety of communicative intent and the sequence of these blocks captures changing discourse. Previous work shows that exploiting the structural information can improve the structured documents (e.g., web pages) retrieval. In this study we utilize the structure of Tweets, induced by these blocks, for Twitter retrieval and Twitter opinion retrieval. For Twitter retrieval, a set of features, derived from the blocks of text and their combinations, is used into a learning-to-rank scenario. We show that structuring Tweets can achieve state-of-the-art performance. Our approach does not rely on social media features, but when we do add this additional information, performance improves significantly. For Twitter opinion retrieval, we explore the question of whether structural information derived from the body of Tweets and opinionatedness ratings of Tweets can improve performance. Experimental results show that retrieval using a novel unsupervised opinionatedness feature based on structuring Tweets achieves comparable performance with a supervised method using manually tagged Tweets. Topic-related specific structured Tweet sets are shown to help with query-dependent opinion retrieval. [ABSTRACT FROM AUTHOR] Copyright of Journal of the Association for Information Science & Technology is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=111519802&site=bsi-live"
"Swat: A system for detecting salient Wikipedia entities in texts.","Ponza, Marco; Ferragina, Paolo; Piccinno, Francesco","Computational Intelligence",="08247935",,="Nov2019","35","4","858","33","139349679","10.1111/coin.12216","Wiley-Blackwell","Article","NATURAL language processing; LATENT semantic analysis; WIKIPEDIA","entity linking; entity salience; information retrieval; machine learning; natural language processing; Wikipedia","We study the problem of entity salience by proposing the design and implementation of Swat, a system that identifies the salient Wikipedia entities occurring in an input document. Swat consists of several modules that are able to detect and classify on‐the‐fly Wikipedia entities as salient or not, based on a large number of syntactic, semantic, and latent features properly extracted via a supervised process, which has been trained over millions of examples drawn from the New York Times corpus. The validation process is performed through a large experimental assessment, eventually showing that Swat improves known solutions over all publicly available datasets. We release Swat via an API that we describe and comment in the paper to ease its use in other software. [ABSTRACT FROM AUTHOR] Copyright of Computational Intelligence is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=139349679&site=bsi-live"
"Systematic mapping study on question answering frameworks over linked data.","Tasar, Ceren Ocal; Komesli, Murat; Unalir, Murat Osman","IET Software (Wiley-Blackwell)",="17518806",,="Dec2018","12","6","461","12","148479975","10.1049/iet-sen.2018.5105","Wiley-Blackwell","Article",,"database management systems; exclusion criteria; inclusion criteria; input questions; Linked Data; linked data technologies; natural language processing; ontologies (artificial intelligence); question answering (information retrieval); question answering frameworks; research questions; semantic analysis; semantic endpoints; sentence-level recognition; systematic mapping","Employing linked data technologies and semantic endpoints for question answering systems are expanding approaches among the researchers. Therefore, systems that combine syntactic and semantic analysis and enrich input questions by sentence‐level recognition are examined. A systematic mapping study is conducted to identify and analyse the studies from major databases, journals and proceedings of conferences or workshops published between 2010 and 2017. With a set of 14 research questions, inclusion and exclusion criteria are specified. 53 studies are selected as primary studies from an initial set of 845 papers. This study provides a mapping while focusing on the methods and identifying the gaps between required and existing approaches. Popular approaches which have gained the most attention among researchers are given as a conclusion. Moreover, a comparison between the authors' study and related work in the literature is given to point out the differences and the contributions of their study. As the result of the comparison, it is concluded that the study is a novel and original topic on question answering frameworks. [ABSTRACT FROM AUTHOR] Copyright of IET Software (Wiley-Blackwell) is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=148479975&site=bsi-live"
"Text mining approaches for dealing with the rapidly expanding literature on COVID-19.","Wang, Lucy Lu; Lo, Kyle","Briefings in Bioinformatics",="14675463",,="Mar2021","22","2","781","19","149507131","10.1093/bib/bbaa296","Oxford University Press / USA","Article","COVID-19; MEDICAL personnel; INFORMATION overload; SHARED housing","CORD-19; information extraction; information retrieval; natural language processing; question answering; shared tasks; summarization; text mining","More than 50 000 papers have been published about COVID-19 since the beginning of 2020 and several hundred new papers continue to be published every day. This incredible rate of scientific productivity leads to information overload, making it difficult for researchers, clinicians and public health officials to keep up with the latest findings. Automated text mining techniques for searching, reading and summarizing papers are helpful for addressing information overload. In this review, we describe the many resources that have been introduced to support text mining applications over the COVID-19 literature; specifically, we discuss the corpora, modeling resources, systems and shared tasks that have been introduced for COVID-19. We compile a list of 39 systems that provide functionality such as search, discovery, visualization and summarization over the COVID-19 literature. For each system, we provide a qualitative description and assessment of the system's performance, unique data or user interface features and modeling decisions. Many systems focus on search and discovery, though several systems provide novel features, such as the ability to summarize findings over multiple documents or linking between scientific articles and clinical trials. We also describe the public corpora, models and shared tasks that have been introduced to help reduce repeated effort among community members; some of these resources (especially shared tasks) can provide a basis for comparing the performance of different systems. Finally, we summarize promising results and open challenges for text mining the COVID-19 literature. [ABSTRACT FROM AUTHOR] Copyright of Briefings in Bioinformatics is the property of Oxford University Press / USA and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=149507131&site=bsi-live"
"The “General Problem Solver” Does Not Exist: Mortimer Taube and the Art of AI Criticism.","Garvey, Shunryu Colin","IEEE Annals of the History of Computing",="10586180",,="Jan-Mar2021","43","1","60","14","149121999","10.1109/MAHC.2021.3051686","IEEE","Article","ARTIFICIAL intelligence; LIBRARY science; ART criticism; COMMON sense; MUSICAL criticism; GLOBAL Positioning System","Artificial intelligence; Artificial Intelligence (AI); Cognitive Simulation; Computer Systems; Computers; Global Positioning System; History; Indexing; Information Processing; Information retrieval; Libraries; Social Criticism","This article reconfigures the history of artificial intelligence (AI) and its accompanying tradition of criticism by excavating the work of Mortimer Taube, a pioneer in information and library sciences, whose magnum opus, Computers and Common Sense: The Myth of Thinking Machines (1961), has been mostly forgotten. To convey the essence of his distinctive critique, the article focuses on Taube's attack on the general problem solver (GPS), the second major AI program. After examining his analysis of the social construction of this and other ""thinking machines,"" it concludes that, despite technical changes in AI, much of Taube's criticism remains relevant today. Moreover, his status as an ""information processing"" insider who criticized AI on behalf of the public good challenges the boundaries and focus of most critiques of AI from the past half-century. In sum, Taube's work offers an alternative model from which contemporary AI workers and critics can learn much. [ABSTRACT FROM AUTHOR] Copyright of IEEE Annals of the History of Computing is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=149121999&site=bsi-live"
"Turning WordNet into an Information Retrieval Resource: Systematic Polysemy and Conversion to Hierarchical Codes.","Mihalcea, Rada","International Journal of Pattern Recognition & Artificial Intelligence",="02180014",,="Aug2003","17","5","689","16","10405776","10.1142/S0218001403002605","World Scientific Publishing Company","Article","INFORMATION storage & retrieval systems; INFORMATION resources; SEMANTICS; POLYSEMY","coarse grained dictionary; hierarchical encoding; Information Retrieval (IR); Natural Language Processing (NLP); systematic polysemy; WordNet","This paper addresses the problem of transforming WordNet into a resource tailored to Information Retrieval (IR) applications. We address two of the major drawbacks pointed out in previous literature in relation to this semantic network. One is the fine granularity of senses defined in WordNet, which proves useless from an IR perspective. To solve this problem, we propose a set of methods that enable the automatic transformation of WordNet into a coarse grained dictionary. The other drawback is the encoding used in this resource, and the methods for accessing related words across the semantic net. Due to the high number of connections among concepts, the simple computation of a path in this net, or the generation of related concepts may become a computationally intensive process. This effect is highly undesirable in time sensitive applications such as IR applications. We propose a methodology for hierarchical encoding that enables increased efficiency in WordNet-based IR systems. [ABSTRACT FROM AUTHOR] Copyright of International Journal of Pattern Recognition & Artificial Intelligence is the property of World Scientific Publishing Company and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=10405776&site=bsi-live"
"Unlocking maintenance insights in industrial text through semantic search.","Naqvi, Syed Meesam Raza; Ghufran, Mohammad; Varnier, Christophe; Nicod, Jean-Marc; Javed, Kamran; Zerhouni, Noureddine","Computers in Industry",="01663615",,="May2024","157",,"N.PAG","1","176499791","10.1016/j.compind.2024.104083","Elsevier B.V.","Article",,"Industrial information retrieval; Large Language Models; Maintenance decision support; Navigating human knowledge; Semantic search; Technical Language Processing","Maintenance records in Computerized Maintenance Management Systems (CMMS) contain valuable human knowledge on maintenance activities. These records primarily consist of noisy and unstructured texts written by maintenance experts. The technical nature of the text, combined with a concise writing style and frequent use of abbreviations, makes it difficult to be processed through classical Natural Language Processing (NLP) pipelines. Due to these complexities, this text must be normalized before feeding to classical machine learning models. Developing these custom normalization pipelines requires manual labor and domain expertise and is a time-consuming process that demands constant updates. This leads to the under-utilization of this valuable source of information to generate insights to help with maintenance decision support. This study proposes a Technical Language Processing (TLP) pipeline for semantic search in industrial text using BERT (Bidirectional Encoder Representations), a transformer-based Large Language Model (LLM). The proposed pipeline can automatically process complex unstructured industrial text and does not require custom preprocessing. To adapt the BERT model for the target domain, three unsupervised domain fine-tuning techniques are compared to identify the best strategy for leveraging available tacit knowledge in industrial text. The proposed approach is validated on two industrial maintenance records from the mining and aviation domains. Semantic search results are analyzed from a quantitative and qualitative perspective. Analysis shows that TSDAE, a state-of-the-art unsupervised domain fine-tuning technique, can efficiently identify intricate patterns in the industrial text regardless of associated complexities. BERT model fine-tuned with TSDAE on industrial text achieved a precision of 0.94 and 0.97 for mining excavators and aviation maintenance records, respectively. • Industrial text stores crucial human knowledge about various assets. • The free-form nature of the industrial text makes it difficult to process. • Transformer-based models can leverage tacit knowledge in the industrial text. • LLMs can enable semantic search in complex industrial text with high precision. [ABSTRACT FROM AUTHOR] Copyright of Computers in Industry is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=176499791&site=bsi-live"
"User satisfaction with Arabic COVID-19 apps: Sentiment analysis of users' reviews using machine learning techniques.","Ramzy, Mina; Ibrahim, Bahaa","Information Processing & Management",="03064573",,="May2024","61","3","N.PAG","1","175964535","10.1016/j.ipm.2024.103644","Elsevier B.V.","Article",,"Arabic mHealth apps; COVID-19 apps; Deep learning; Information retrieval; Machine learning; Natural language processing; Neural networks; Sentiment analysis; User's reviews","• We provide a benchmark dataset composed of 114,499 reviews from 18 Arabic COVID-19 Apps. • The ANN algorithm provides the best performance with 89 % accuracy and 89 % F1. • The proposed model can be generalized for Arab sentiment analysis to mobile apps for new COVID-19 strains that may appear in the future. • There are positive sentiments (71 %) among most users of Arabic COVID-19 apps, which reflects its positive role in infection control. Digital technologies such as mobile health (mHealth) apps with a variety of features can be essential tools for controlling pandemics. Therefore, many Arab countries have launched COVID-19 mHealth apps to reduce the spread of infection among their citizens. Recently, empirical studies have shown that user reviews include useful details to develop apps. However, Arab citizens' satisfaction with the COVID-19 mHealth apps has not been examined yet. Our study aims to provide Arabic sentiment analysis of users' reviews to explore their satisfaction with Arabic Covid-19 apps. To achieve this goal, we have provided a benchmark dataset composed of 114,499 reviews from 18 Arabic COVID-19 Apps. Six machine learning (ML) models were tested and compared (Support Vector Machine (SVM), K-Nearest Neighbor (KNN), Naive Bayes (NB), Logistic Regression (LR), Random Forest (RF), and Artificial Neural Network (ANN)) using a representative sample of 8220 reviews, which were annotated manually. Then, the best-performing algorithms were applied to the benchmark dataset to explore the polarity of Arab sentiment toward the apps. In a later step, we conducted a thematic analysis of both positive and negative reviews to determine which factors positively and negatively influence the effectiveness of apps. The findings show that the ANN algorithm provides the best performance with 89 % accuracy and 89 % F1. 71 % of user reviews include positive sentiments, while only 21 % include negative sentiments. Frequently crashes, update issues, and bugs were among the most prominent negative factors that affected the effectiveness of apps from the users' point of view. Finally, we presented a set of recommendations to address the negative factors and improve the effectiveness of Arabic COVID-19 apps. [ABSTRACT FROM AUTHOR] Copyright of Information Processing & Management is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=175964535&site=bsi-live"
"Utilizing BERT for Information Retrieval: Survey, Applications, Resources, and Challenges.","Wang, Jiajia; Huang, Jimmy Xiangji; Tu, Xinhui; Wang, Junmei; Huang, Angela Jennifer; Laskar, Md Tahmid Rahman; Bhuiyan, Amran","ACM Computing Surveys",="03600300",,="Jul2024","56","7","1","33","176628823","10.1145/3648471","Association for Computing Machinery","Article",,"artificial intelligence; BERT; information retrieval; natural language processing",,"https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=176628823&site=bsi-live"
"Why are these similar? Investigating item similarity types in a large digital library.","Gonzalez ‐ Agirre, Aitor; Rigau, German; Agirre, Eneko; Aletras, Nikolaos; Stevenson, Mark","Journal of the Association for Information Science & Technology",="23301635",,="Jul2016","67","7","1624","14","115995683","10.1002/asi.23482","Wiley-Blackwell","Article","STATISTICAL correlation; DIGITAL libraries; INFORMATION retrieval; RESEARCH funding; CROWDSOURCING; EUROPE; Libraries and Archives; METADATA","information retrieval; natural language processing; similarity","We introduce a new problem, identifying the type of relation that holds between a pair of similar items in a digital library. Being able to provide a reason why items are similar has applications in recommendation, personalization, and search. We investigate the problem within the context of Europeana, a large digital library containing items related to cultural heritage. A range of types of similarity in this collection were identified. A set of 1,500 pairs of items from the collection were annotated using crowdsourcing. A high intertagger agreement (average 71.5 Pearson correlation) was obtained and demonstrates that the task is well defined. We also present several approaches to automatically identifying the type of similarity. The best system applies linear regression and achieves a mean Pearson correlation of 71.3, close to human performance. The problem formulation and data set described here were used in a public evaluation exercise, the * SEM shared task on Semantic Textual Similarity. The task attracted the participation of 6 teams, who submitted 14 system runs. All annotations, evaluation scripts, and system runs are freely available. [ABSTRACT FROM AUTHOR] Copyright of Journal of the Association for Information Science & Technology is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=115995683&site=bsi-live"
