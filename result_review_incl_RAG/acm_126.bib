@article{10.1145/3648471,
author = {Wang, Jiajia and Huang, Jimmy Xiangji and Tu, Xinhui and Wang, Junmei and Huang, Angela Jennifer and Laskar, Md Tahmid Rahman and Bhuiyan, Amran},
title = {Utilizing BERT for Information Retrieval: Survey, Applications, Resources, and Challenges},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {7},
issn = {0360-0300},
url = {https://doi.org/10.1145/3648471},
doi = {10.1145/3648471},
abstract = {Recent years have witnessed a substantial increase in the use of deep learning to solve various natural language processing (NLP) problems. Early deep learning models were constrained by their sequential or unidirectional nature, such that they struggled to capture the contextual relationships across text inputs. The introduction of bidirectional encoder representations from transformers (BERT) leads to a robust encoder for the transformer model that can understand the broader context and deliver state-of-the-art performance across various NLP tasks. This has inspired researchers and practitioners to apply BERT to practical problems, such as information retrieval (IR). A survey that focuses on a comprehensive analysis of prevalent approaches that apply pretrained transformer encoders like BERT to IR can thus be useful for academia and the industry. In light of this, we revisit a variety of BERT-based methods in this survey, cover a wide range of techniques of IR, and group them into six high-level categories: (i) handling long documents, (ii) integrating semantic information, (iii) balancing effectiveness and efficiency, (iv) predicting the weights of terms, (v) query expansion, and (vi) document expansion. We also provide links to resources, including datasets and toolkits, for BERT-based IR systems. Additionally, we highlight the advantages of employing encoder-based BERT models in contrast to recent large language models like ChatGPT, which are decoder-based and demand extensive computational resources. Finally, we summarize the comprehensive outcomes of the survey and suggest directions for future research in the area.},
journal = {ACM Comput. Surv.},
month = apr,
articleno = {185},
numpages = {33},
keywords = {BERT, information retrieval, natural language processing, artificial intelligence}
}

@article{10.1145/3699953,
author = {Dadure, Pankaj and Pakray, Partha and Bandyopadhyay, Sivaji},
title = {Mathematical Information Retrieval: A Review},
year = {2024},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3699953},
doi = {10.1145/3699953},
abstract = {Mathematical formulas are commonly used to demonstrate theories and basic fundamentals in the Science, Technology, Engineering, and Mathematics (STEM) domain. The burgeoning research in the STEM domain results in the mass production of scientific documents that contain both textual and mathematical terms. In scientific information, the definition of mathematical formulas is expressed through context and symbolic structure that adheres to strong domain-specific notions. Whereas the retrieval of textual information is well-researched, and numerous text-based search engines are present. However, textual information retrieval systems are inadequate for searching scientific information containing mathematical formulas, including simple symbols to complicated mathematical structures. The retrieval of mathematical information is in its infancy, and it requires the inclusion of new technologies and tools to promote the retrieval of scientific information and the management of digital libraries. This article provides a comprehensive study of mathematical information retrieval and highlights their challenges and future opportunities.},
journal = {ACM Comput. Surv.},
month = nov,
articleno = {61},
numpages = {34},
keywords = {Artificial intelligence, natural language processing, information retrieval, formula retrieval, mathematical knowledge discovery, digital libraries}
}

@inproceedings{10.1145/3627673.3680275,
author = {Xu, Han},
title = {Towards Seamless User Query to REST API Conversion},
year = {2024},
isbn = {9798400704369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627673.3680275},
doi = {10.1145/3627673.3680275},
abstract = {Integrating Large Language Models (LLMs) with external tools and APIs is essential for fields such as information retrieval and knowledge management. While LLMs have made significant strides, their effective integration with external APIs-essential for real-world applications-remains challenging. This paper introduces RESTful-Llama, a novel method designed to empower open-source LLMs to accurately convert natural language instructions into well-formed RESTful API calls. Moreover, RESTful-Llama utilizes DOC-Prompt, a newly proposed technique for generating fine-tuning datasets from publicly available API documentation. Initial experiments demonstrate that RESTful-Llama significantly enhances the accuracy of generated REST API requests.},
booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management},
pages = {5495–5498},
numpages = {4},
keywords = {information retrieval, natural language processing},
location = {Boise, ID, USA},
series = {CIKM '24}
}

@inproceedings{10.1145/3627508.3638293,
author = {Odede, Julius and Frommholz, Ingo},
title = {JayBot -- Aiding University Students and Admission with an LLM-based Chatbot},
year = {2024},
isbn = {9798400704345},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627508.3638293},
doi = {10.1145/3627508.3638293},
abstract = {This demo paper presents JayBot, an LLM-based chatbot system aimed at enhancing the user experience of prospective and current students, faculty, and staff at a UK university. The objective of JayBot is to provide information to users on general enquiries regarding course modules, duration, fees, entry requirements, lecturers, internship, career paths, course employability and other related aspects. Leveraging the use cases of generative artificial intelligence (AI), the chatbot application was built using OpenAI’s advanced large language model (GPT-3.5 turbo); to tackle issues such as hallucination as well as focus and timeliness of results, an embedding transformer model has been combined with a vector database and vector search. Prompt engineering techniques were employed to enhance the chatbot’s response abilities. Preliminary user studies indicate JayBot’s effectiveness and efficiency. The demo will showcase JayBot in a university admission use case and discuss further application scenarios.},
booktitle = {Proceedings of the 2024 Conference on Human Information Interaction and Retrieval},
pages = {391–395},
numpages = {5},
keywords = {Artificial Intelligence, Chatbot, Interactive Information Retrieval, Large Language Models, Machine Learning, Retrieval Augmented Generation, Vector Database},
location = {Sheffield, United Kingdom},
series = {CHIIR '24}
}

@inproceedings{10.1145/3637528.3671458,
author = {Dai, Sunhao and Xu, Chen and Xu, Shicheng and Pang, Liang and Dong, Zhenhua and Xu, Jun},
title = {Bias and Unfairness in Information Retrieval Systems: New Challenges in the LLM Era},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671458},
doi = {10.1145/3637528.3671458},
abstract = {With the rapid advancements of large language models (LLMs), information retrieval (IR) systems, such as search engines and recommender systems, have undergone a significant paradigm shift. This evolution, while heralding new opportunities, introduces emerging challenges, particularly in terms of biases and unfairness, which may threaten the information ecosystem. In this paper, we present a comprehensive survey of existing works on emerging and pressing bias and unfairness issues in IR systems when the integration of LLMs. We first unify bias and unfairness issues as distribution mismatch problems, providing a groundwork for categorizing various mitigation strategies through distribution alignment. Subsequently, we systematically delve into the specific bias and unfairness issues arising from three critical stages of LLMs integration into IR systems: data collection, model development, and result evaluation. In doing so, we meticulously review and analyze recent literature, focusing on the definitions, characteristics, and corresponding mitigation strategies associated with these issues. Finally, we identify and highlight some open problems and challenges for future work, aiming to inspire researchers and stakeholders in the IR field and beyond to better understand and mitigate bias and unfairness issues of IR in this LLM era. We also consistently maintain a GitHub repository for the relevant papers and resources in this rising direction at https://github.com/KID-22/LLM-IR-Bias-Fairness-Survey.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {6437–6447},
numpages = {11},
keywords = {bias, fairness, information retrieval, large language model},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3644713.3644763,
author = {Alghamdi, Muath and Abushawarib, Mohammed and Ellouh, Mahmoud and Ghaleb, Mustafa and Felemban, Muhamad},
title = {Enhancing Arabic Information Retrieval for Question Answering},
year = {2024},
isbn = {9798400709036},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644713.3644763},
doi = {10.1145/3644713.3644763},
abstract = {In the modern landscape of Natural Language Processing (NLP), intelligent chatbots like ChatGPT 3.5 and Google’s Bard have shown remarkable competence in generic question-answering (QA) tasks. However, their performance falters when navigating domain-specific QA, particularly in the Arabic language, which is celebrated for its complex morphology and syntax. This paper presents a comprehensive approach to address these issues. The aim of this research is to build a chatbot tailored for a university community. We first create an extensive Arabic Q&amp;A dataset by extracting data from academic documents, employing state-of-the-art Optical Character Recognition (OCR) tools. Then, we evaluate multiple text similarity measures like Pooled FastText Word embedding, BM25 ranking functions, and various semantic sentence embedding models. A thorough performance assessment reveals that the domain-specific model excels at both sentence-level similarity and context-relevance tasks. The developed web application chatbot, leveraging LangChain library and Retrieval Augmented Generation (RAG) methods, outperforms existing chatbots in domain-specific, Arabic language QA scenarios.},
booktitle = {Proceedings of the 7th International Conference on Future Networks and Distributed Systems},
pages = {366–371},
numpages = {6},
keywords = {Information Retrieval, Natural Language Processing},
location = {Dubai, United Arab Emirates},
series = {ICFNDS '23}
}

@inproceedings{10.1145/3627673.3679099,
author = {Xu, Da and Zhang, Danqing and Zheng, Lingling and Yang, Bo and Yang, Guangyu and Xu, Shuyuan and Liang, Cindy},
title = {Tutorial on Landing Generative AI in Industrial Social and E-commerce Recsys},
year = {2024},
isbn = {9798400704369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627673.3679099},
doi = {10.1145/3627673.3679099},
abstract = {Over the past two years, GAI has evolved rapidly, influencing various fields including social and e-commerce Recsys. Despite exciting advances, landing these innovations in real-world Recsys remains challenging due to the sophistication of modern industrial product and systems. Our tutorial begins with a brief overview of building industrial Recsys and GAI fundamentals, followed by the ongoing efforts and opportunities to enhance personalized recommendations with foundation models.We then explore the integration of curation capabilities into Recsys, such as repurposing raw content, incorporating external knowledge, and generating personalized insights/explanations to foster transparency and trust. Next, the tutorial illustrates how AI agents can transform Recsys through interactive reasoning and action loops, shifting away from traditional passive feedback models. Finally, we shed insights on real-world solutions for human-AI alignment and responsible GAI practices.A critical component of the tutorial is detailing the AI, Infrastructure, LLMOps, and Product roadmap (including the evaluation and responsible AI practices) derived from the production solutions in LinkedIn, Amazon, TikTok, and Microsoft. While GAI in Recsys is still in its early stages, this tutorial provides valuable insights and practical solutions for the Recsys and GAI communities.},
booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management},
pages = {5538–5542},
numpages = {5},
keywords = {generative AI, industrial system, information retrieval, large language model, recommender system},
location = {Boise, ID, USA},
series = {CIKM '24}
}

@article{10.1145/3695995,
author = {Huang, Yuekai and Wang, Junjie and Wang, Song and Wei, Moshi and Shi, Lin and Liu, Zhe and Wang, Qing},
title = {Deep API Sequence Generation via Golden Solution Samples and API Seeds},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3695995},
doi = {10.1145/3695995},
abstract = {Automatic API recommendation can accelerate developers’ programming, and has been studied for years. There are two orthogonal lines of approaches for this task, i.e., information retrieval-based (IR-based) approaches and sequence to sequence (seq2seq) model based approaches. Although these approaches were reported to have remarkable performance, our observation finds two major drawbacks, i.e., IR-based approaches lack the consideration of relations among the recommended APIs, and seq2seq models do not model the API’s semantic meaning. To alleviate the above two problems, we propose APIGens, which is a retrieval-enhanced large language model (LLM) based API recommendation approach to recommend an API sequence for a natural language query. The approach first retrieves similar programming questions in history based on the input natural language query, and then scores the results based on API documents via a scorer model. Finally, these results are used as samples for few-shot learning of LLM. To reduce the risk of encountering local optima, we also extract API seeds from the retrieved results to increase the search scope during the LLM generation process. The results show that our approach can achieve 48.41% ROUGE@10 on API sequence recommendation and the 82.61% MAP on API set recommendation, largely outperforming the state-of-the-art baselines.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
keywords = {API recommendation, deep learning, information retrieval, sequence generation, large language model}
}

@inproceedings{10.1145/3626772.3657788,
author = {Zhou, Ruiwen and Yang, Yingxuan and Wen, Muning and Wen, Ying and Wang, Wenhao and Xi, Chunling and Xu, Guoqiang and Yu, Yong and Zhang, Weinan},
title = {TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657788},
doi = {10.1145/3626772.3657788},
abstract = {Several large language model (LLM) agents have been constructed for diverse purposes such as web navigation and online shopping, leveraging the broad knowledge and text comprehension capabilities of LLMs. Many of these works rely on in-context examples to achieve generalization without requiring fine-tuning. However, few have addressed the challenge of selecting and effectively utilizing these examples. Recent approaches have introduced trajectory-level retrieval with task meta-data and the use of trajectories as in-context examples to enhance overall performance in some sequential decision making tasks like computer control. Nevertheless, these methods face issues like plausible examples retrieved without task-specific state transition dynamics and long input with plenty of irrelevant context due to using complete trajectories. In this paper, we propose a novel framework (TRAD) to tackle these problems. TRAD first employs Thought Retrieval for step-level demonstration selection through thought matching, enhancing the quality of demonstrations and reducing irrelevant input noise. Then, Aligned Decision is introduced to complement retrieved demonstration steps with their preceding or subsequent steps, providing tolerance for imperfect thought and offering a balance between more context and less noise. Extensive experiments on ALFWorld and Mind2Web benchmarks demonstrate that TRAD not only surpasses state-of-the-art models but also effectively reduces noise and promotes generalization. Furthermore, TRAD has been deployed in real-world scenarios of a global business insurance company and yields an improved success rate of robotic process automation. Our codes are available at: https://github.com/skyriver-2000/TRAD-Official.},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {3–13},
numpages = {11},
keywords = {information retrieval, large language model, llm agent, llm reasoning, sequential decision making},
location = {Washington DC, USA},
series = {SIGIR '24}
}

@article{10.1145/3580496,
author = {Ahmed, Usman and Lin, Jerry Chun-Wei and Srivastava, Gautam},
title = {Emotional Intelligence Attention Unsupervised Learning Using Lexicon Analysis for Irony-based Advertising},
year = {2024},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3580496},
doi = {10.1145/3580496},
abstract = {Social media platforms have made increasing use of irony in recent years. Users can express their ironic thoughts with audio, video, and images attached to text content. When you use irony, you are making fun of a situation or trying to make a point. It can also express frustration or highlight the absurdity of a situation. The use of irony in social media is likely to continue to increase, no matter the reason. By using syntactic information in conjunction with semantic exploration, we show that attention networks can be enhanced. Using learned embedding, unsupervised learning encodes word order into a joint space. By evaluating the entropy of an example class and adding instances, the active learning method uses the shared representation as a query to retrieve semantically similar sentences from a knowledge base. In this way, the algorithm can identify the instance with the maximum uncertainty and extract the most informative example from the training set. An ironic network trained for each labelled record is used to train a classifier (model). The partial training model and the original labelled data generate pseudo-labels for the unlabeled data. To correctly predict the label of a dataset, a classifier (attention network) updates the pseudo-labels for the remaining datasets. After the experimental evaluation of the 1,021 annotated texts, the proposed model performed better than the baseline models, achieving an F1 score of 0.63 on ironic tasks and 0.59 on non-ironic tasks. We also found that the proposed model generalized well to new instances of datasets.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {1},
numpages = {19},
keywords = {Information retrieval, Emotional Intelligence, Natural Language Processing, irony detection, unsupervised learning}
}

@inproceedings{10.1145/3616855.3636452,
author = {Alonso, Omar and Church, Kenneth},
title = {Some Useful Things to Know When Combining IR and NLP: The Easy, the Hard and the Ugly},
year = {2024},
isbn = {9798400703713},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3616855.3636452},
doi = {10.1145/3616855.3636452},
abstract = {Deep nets such as GPT are at the core of the current advances in many systems and applications. Things are moving fast; techniques become obsolete quickly (within weeks). How can we take advantage of new discoveries and incorporate them into our existing work? Are new developments radical improvements, or incremental repetitions of established concepts, or combinations of both?In this tutorial, we aim to bring interested researchers and practitioners up to speed on the recent and ongoing techniques around ML and Deep learning in the context of IR and NLP. Additionally, our goal is to clarify terminology, emphasize fundamentals, and outline problems and new research opportunities.},
booktitle = {Proceedings of the 17th ACM International Conference on Web Search and Data Mining},
pages = {1110–1113},
numpages = {4},
keywords = {evaluation and benchmarks, fine-tuning, information retrieval, natural language processing, prompt engineering},
location = {Merida, Mexico},
series = {WSDM '24}
}

@inproceedings{10.1145/3589335.3641299,
author = {Liu, Zheng and Zhou, Yujia and Zhu, Yutao and Lian, Jianxun and Li, Chaozhuo and Dou, Zhicheng and Lian, Defu and Nie, Jian-Yun},
title = {Information Retrieval Meets Large Language Models},
year = {2024},
isbn = {9798400701726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589335.3641299},
doi = {10.1145/3589335.3641299},
abstract = {The advent of large language models (LLMs) presents both opportunities and challenges for the information retrieval (IR) community. On one hand, LLMs will revolutionize how people access information, meanwhile the retrieval techniques can play a crucial role in addressing many inherent limitations of LLMs. On the other hand, there are open problems regarding the collaboration of retrieval and generation, the potential risks of misinformation, and the concerns about cost-effectiveness. To seize the critical moment for development, it calls for the joint effort from academia and industry on many key issues, including identification of new research problems, proposal of new techniques, and creation of new evaluation protocols. It has been one year since the launch of ChatGPT in November last year, and the entire community is currently undergoing a profound transformation in techniques. Therefore, this workshop will be a timely venue to exchange ideas and forge collaborations. The organizers, committee members, and invited speakers are composed of a diverse group of researchers coming from leading institutions in the world. This event will be made up of multiple sessions, including invited talks, paper presentations, hands-on tutorials, and panel discussions. All the materials collected for this workshop will be archived and shared publicly, which will present a long-term value to the community.},
booktitle = {Companion Proceedings of the ACM Web Conference 2024},
pages = {1586–1589},
numpages = {4},
keywords = {information retrieval, large language models, question answering, ranking, retrieval-augmented generation, search},
location = {Singapore, Singapore},
series = {WWW '24}
}

@inproceedings{10.1145/3539618.3591789,
author = {Nilles, Markus},
title = {Conversational Bibliographic Search},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591789},
doi = {10.1145/3539618.3591789},
abstract = {In almost every area of research, it is necessary to find experts and publications on a topic. However, finding experts and publications is a difficult task not only for computers, but also for humans. For example, searching for experts, a user often enters a topic into a search engine, which then checks which people have published on that topic. A problem arises when a user does not make their query specific enough which can happen intentionally, e.g. when the user is doing a navigational search, or unintentionally, e.g., when the user lacks knowledge. As a result, the quality of the search results may not be very high and the best results may not be found. Current and widely used search engines for bibliographic metadata, such as dblp[2], ResearchGate, Google Scholar or Semantic Scholar allow only keyword-based searches. Kreutz et al.[1] presented SchenQL, a query language for bibliographic metadata that allows users to formulate their queries more easily and precisely than SQL. However, it requires training to understand the language and is not as easy for non-experts to use e.g. Google Scholar.To address the limitations of insufficient attention to the user's search intent and lack of search support, we aim to develop a conversational retrieval system in the domain of bibliographic metadata. This conversational search system assists users in achieving their search intent through a natural language dialog. It should be possible not only to find experts, but also to search for bibliographic metadata with the help of the system without prior knowledge.In this work, we aim to answer the research question: How beneficial is a conversational information retrieval system for searching bibliographic data? To address the research question, our contribution is threefold. First, we present an architecture for such a conversational information retrieval system for bibliographic metadata. Second, we will implement all the components of this system and evaluate our system by comparing it to existing bibliographic data search engines in terms of effectiveness, efficiency, and user satisfaction. Third, we will create and publish a dataset consisting of user queries that we will use to train our system.The architecture we propose consists of five main components: i) user intent classification, ii) a keyword extractor, iii) a search module, iv) a conversational module and v) the conversation history.i) The task of user intent classification is to determine the goal the user wants to achieve with their search query. The user intent classification consists of a set of corresponding user intent classifiers, each of which is responsible for one intent. If no classifier can match the user's query to their intent, or multiple classifiers conclude that the query matches their intents, the system asks the user to specify the query accordingly. ii) If the intent is correctly determined, a keyword extractor extracts the actual search term from the query. iii) After the intent and the search term have been determined, the actual search takes place in the search module. The user's query could be reformulated into a SchenQL query (Kreutz et al.[1] and sent to the database. iv) In the conversational module, the results are converted into a natural language response and the user is given suggestions for further queries related to the previous ones. v) The conversation history stores the user's queries and the system's responses, both to consider the entire session when determining intent and to improve the system's components. For example, new question formulations could improve the accuracy of the user intent classifiers as well as reveal what new intents a user of such a conversational search system might have that have not yet been implemented.In first experiments, we defined four user intents and already evaluated the user intent classification. The four user intents are: (1) searching for persons/authors/experts on a topic, (2) searching for publications by author name, (3) searching for publications on a topic and (4) searching for similar topics of a topic. Our classifiers achieved an accuracy of 0.998 in correctly determining user intent.In the future, we not only want to evaluate the individual components of a conversational information retrieval system for bibliographic data, but also want to work out the advantages and disadvantages of the conversational information retrieval system for bibliographic data, in comparison to already existing systems that do not support the user in their search process via natural language conversations.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {3491},
numpages = {1},
keywords = {bibliographic metadata, conversational information retrieval, natural language processing},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@article{10.1145/3702647,
author = {Wang, Yuqi and Chen, Qiuyi and Zhang, Haiyang and Wang, Wei and Wang, Qiufeng and Pan, Yushan and Xie, Liangru and Huang, Kaizhu and Nguyen, Anh},
title = {Biomedical Information Retrieval with Positive-Unlabeled Learning and Knowledge Graphs},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2157-6904},
url = {https://doi.org/10.1145/3702647},
doi = {10.1145/3702647},
abstract = {The rapid growth of biomedical publications has presented significant challenges in the field of information retrieval. Most existing work focuses on document retrieval given explicit queries. However, in real applications such as curated biomedical database maintenance, explicit queries are missing. In this paper, we propose a two-step model for biomedical information retrieval in the case that only a small set of example documents is available without explicit queries. Initially, we extract keywords from the observed documents using large pre-trained language models and biomedical knowledge graphs. These keywords are then enriched with domain-specific entities. Information retrieval techniques can subsequently use the collected entities to rank the documents. Following this, we introduce an iterative Positive-Unlabeled learning method to classify all unlabeled documents. Experiments conducted on the PubMed dataset demonstrate that the proposed technique outperforms the state-of-the-art positive-unlabeled learning methods. The results underscore the effectiveness of integrating large language models and biomedical knowledge graphs in improving zero-shot information retrieval performance in the biomedical domain.},
note = {Just Accepted},
journal = {ACM Trans. Intell. Syst. Technol.},
month = nov,
keywords = {Knowledge graph embedding, pre-trained large language models, positive-unlabeled learning, text classification, natural language processing, information retrieval}
}

@inproceedings{10.1145/3539618.3591911,
author = {Kaur, Simerjot and Smiley, Charese and Gupta, Akshat and Sain, Joy and Wang, Dongsheng and Siddagangappa, Suchetha and Aguda, Toyin and Shah, Sameena},
title = {REFinD: Relation Extraction Financial Dataset},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591911},
doi = {10.1145/3539618.3591911},
abstract = {A number of datasets for Relation Extraction (RE) have been created to aide downstream tasks such as information retrieval, semantic search, question answering and textual entailment. However, these datasets fail to capture financial-domain specific challenges since most of these datasets are compiled using general knowledge sources such as Wikipedia, web-based text and news articles, hindering real-life progress and adoption within the financial world. To address this limitation, we propose REFinD, the first large-scale annotated dataset of relations, with ~29K instances and 22 relations amongst 8 types of entity pairs, generated entirely over financial documents. We also provide an empirical evaluation with various state-of-the-art models as benchmarks for the RE task and highlight the challenges posed by our dataset. We observed that various state-of-the-art deep learning models struggle with numeric inference, relational and directional ambiguity. To encourage further research in this direction, REFinD is available at https://www.jpmorgan.com/technology/artificial-intelligence/initiatives/refind-dataset/problem-motivation-outcome.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {3054–3063},
numpages = {10},
keywords = {annotation datasets, benchmarking, finance, information retrieval, natural language processing, relation extraction},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@article{10.1145/3615668,
author = {Hemberg, Erik and Turner, Matthew J. and Rutar, Nick and O’reilly, Una-May},
title = {Enhancements to Threat, Vulnerability, and Mitigation Knowledge for Cyber Analytics, Hunting, and Simulations},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
url = {https://doi.org/10.1145/3615668},
doi = {10.1145/3615668},
abstract = {Cross-linked threat, vulnerability, and defensive mitigation knowledge is critical in defending against diverse and dynamic cyber threats. Cyber analysts consult it by deductively or inductively creating a chain of reasoning to identify a threat starting from indicators they observe or vice versa. Cyber hunters use it abductively to reason when hypothesizing specific threats. Threat modelers use it to explore threat postures. We aggregate five public sources of threat knowledge and three public sources of knowledge that describe cyber defensive mitigations, analytics, and engagements and which share some unidirectional links between them. We unify the sources into a graph, and in the graph, we make all unidirectional cross-source links bidirectional. This enhancement of the knowledge makes the questions that analysts and automated systems formulate easier to answer. We demonstrate this in the context of various cyber analytic and hunting tasks as well as modeling and simulations. Because the number of linked entries is very sparse, to further increase the analytic utility of the data, we use natural language processing and supervised machine learning to identify new links. These two contributions demonstrably increase the value of the knowledge sources for cyber security activities.},
journal = {Digital Threats},
month = mar,
articleno = {8},
numpages = {33},
keywords = {Cyber security, threat hunting, machine learning, natural language processing, information retrieval, reinforcement learning, coevolutionary algorithm}
}

@inproceedings{10.1145/3583780.3614736,
author = {Kusa, Wojciech and Knoth, Petr and Hanbury, Allan},
title = {CRUISE-Screening: Living Literature Reviews Toolbox},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3614736},
doi = {10.1145/3583780.3614736},
abstract = {Keeping up with research and finding related work is still a time-consuming task for academics. Researchers sift through thousands of studies to identify a few relevant ones. Automation techniques can help by increasing the efficiency and effectiveness of this task. To this end, we developed CRUISE-Screening, a web-based application for conducting living literature reviews -- a type of literature review that is continuously updated to reflect the latest research in a particular field. CRUISE-Screening is connected to several search engines via an API, which allows for updating the search results periodically. Moreover, it can facilitate the process of screening for relevant publications by using text classification and question answering models. CRUISE-Screening can be used both by researchers conducting literature reviews and by those working on automating the citation screening process to validate their algorithms. The application is open-source, and a demo is available under this URL: https://citation-screening.ec.tuwien.ac.at.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {5071–5075},
numpages = {5},
keywords = {citation screening, information retrieval, literature reviews, living reviews, natural language processing, systematic reviews},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@inproceedings{10.1145/3582768.3582796,
author = {Faccia, Alessio and Petratos, Pythagoras},
title = {NLP And IR Applications For Financial Reporting And Non-Financial Disclosure. Framework Implementation And Roadmap For Feasible Integration With The Accounting Process},
year = {2023},
isbn = {9781450397629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582768.3582796},
doi = {10.1145/3582768.3582796},
abstract = {Corporations produce financial and non-financial reports containing structured and unstructured data. In general, all organisations report information of some kind. Natural Language Processing (NLP) and Information Retrieval (IR) were fields developed from approximately the 1950s and have presented important applications, especially in the last three decades. Nevertheless, applications in accounting and finance have not developed accordingly, and a comprehensive framework is missing in the existing literature. This paper examines how NLP and IR can facilitate reporting and disclosure, both Financial and Non-Financial. The paper provides a brief literature review on NLP/IR applications in accounting and finance. It better informs and expands on the discussion of NLP/IR applications in academic research, professional organisations (i.e., IFRS), and industry. It explores some innovative applications of NLP/IR in unstructured data and its use in reporting and disclosure and FinTech applications. The main contribution is the definition of a complete framework that consistently analyses the possible NLP/IR applications in the accounting processes. We find that there can be many more applications of NLP/IR in accounting and finance and suggest future directions for research.},
booktitle = {Proceedings of the 2022 6th International Conference on Natural Language Processing and Information Retrieval},
pages = {117–124},
numpages = {8},
keywords = {FinTech, Financial Reporting, Information Retrieval, Natural Language Processing, Non-Financial Disclosure},
location = {Bangkok, Thailand},
series = {NLPIR '22}
}

@inproceedings{10.1145/3637528.3671882,
author = {Dai, Sunhao and Zhou, Yuqi and Pang, Liang and Liu, Weihao and Hu, Xiaolin and Liu, Yong and Zhang, Xiao and Wang, Gang and Xu, Jun},
title = {Neural Retrievers are Biased Towards LLM-Generated Content},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671882},
doi = {10.1145/3637528.3671882},
abstract = {Recently, the emergence of large language models (LLMs) has revolutionized the paradigm of information retrieval (IR) applications, especially in web search, by generating vast amounts of human-like texts on the Internet. As a result, IR systems in the LLM era are facing a new challenge: the indexed documents are now not only written by human beings but also automatically generated by the LLMs. How these LLM-generated documents influence the IR systems is a pressing and still unexplored question. In this work, we conduct a quantitative evaluation of IR models in scenarios where both human-written and LLM-generated texts are involved. Surprisingly, our findings indicate that neural retrieval models tend to rank LLM-generated documents higher. We refer to this category of biases in neural retrievers towards the LLM-generated content as the source bias. Moreover, we discover that this bias is not confined to the first-stage neural retrievers, but extends to the second-stage neural re-rankers. Then, in-depth analyses from the perspective of text compression indicate that LLM-generated texts exhibit more focused semantics with less noise, making it easier for neural retrieval models to semantic match. To mitigate the source bias, we also propose a plug-and-play debiased constraint for the optimization objective, and experimental results show its effectiveness. Finally, we discuss the potential severe concerns stemming from the observed source bias and hope our findings can serve as a critical wake-up call to the IR community and beyond. To facilitate future explorations of IR in the LLM era, the constructed two new benchmarks are available at https://github.com/KID-22/Source-Bias.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {526–537},
numpages = {12},
keywords = {LLM-generated texts, artificial intelligence generated content, information retrieval, source bias},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3616855.3635722,
author = {Schedl, Markus and Moscati, Marta and Sguerra, Bruno and Hennequin, Romain and Lex, Elisabeth},
title = {Psychology-informed Information Access Systems Workshop},
year = {2024},
isbn = {9798400703713},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3616855.3635722},
doi = {10.1145/3616855.3635722},
abstract = {The Psychology-informed Information Access Systems (PsyIAS) workshop bridges the fields of machine learning and psychology, aiming to connect the research communities of information retrieval, recommender systems, natural language processing, as well as cognitive and behavioral psychology. It serves as a forum for multidisciplinary discussions about the use of psychological constructs, theories, and empirical findings for modeling and predicting user preferences, intents, and behaviors. PsyIAS particularly focuses on research that incorporates such psychology-inspired models into the search, retrieval, and recommendation processes, creates corresponding algorithms and systems, or looks into the role of cognitive processes underlying human information access. More information can be found at https://sites.google.com/view/psyias.},
booktitle = {Proceedings of the 17th ACM International Conference on Web Search and Data Mining},
pages = {1216–1217},
numpages = {2},
keywords = {affect, cognition, explainability, human decision-making, information retrieval, learning to rank, natural language processing, personality, psychology, recommender systems, user-centric evaluation},
location = {Merida, Mexico},
series = {WSDM '24}
}

@inproceedings{10.1145/3606040.3617440,
author = {Gong, Yan and Cosma, Georgina},
title = {Boon: A Neural Search Engine for Cross-Modal Information Retrieval},
year = {2023},
isbn = {9798400702716},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3606040.3617440},
doi = {10.1145/3606040.3617440},
abstract = {Visual-Semantic Embedding (VSE) networks can help search engines understand the meaning behind visual content and associate it with relevant textual information, leading to accurate search results. VSE networks can be used in cross-modal search engines to embed image and textual descriptions in a shared space, enabling image-to-text and text-to-image retrieval tasks. However, the full potential of VSE networks for search engines has yet to be fully explored. This paper presents Boon, a novel cross-modal search engine that combines two state-of-the-art networks: the GPT-3.5-turbo large language model, and the VSE network VITR (VIsion Transformers with Relation-focused learning) to enhance the engine's capabilities in extracting and reasoning with regional relationships in images. VITR employs encoders from CLIP that were trained with 400 million image-description pairs and it was fine-turned on the RefCOCOg dataset. Boon's neural-based components serve as its main functionalities: 1) a 'cross-modal search engine' that enables end-users to perform image-to-text and text-to-image retrieval. 2) a 'multi-lingual conversational AI' component that enables the end-user to converse about one or more images selected by the end-user. Such a feature makes the search engine accessible to a wide audience, including those with visual impairments. 3) Boon is multi-lingual and can take queries and handle conversations about images in multiple languages. Boon was implemented using the Django and PyTorch frameworks. The interface and capabilities of the Boon search engine are demonstrated using the RefCOCOg dataset, and the engine's ability to search for multimedia through the web is facilitated by Google's API.},
booktitle = {Proceedings of the 1st International Workshop on Deep Multimodal Learning for Information Retrieval},
pages = {29–37},
numpages = {9},
keywords = {cross-modal information retrieval, large language model, neural networks, search engine, visual-semantic embedding},
location = {Ottawa ON, Canada},
series = {MMIR '23}
}

@inproceedings{10.1145/3501409.3501541,
author = {Nie, Liangyu and Ye, Jiacheng},
title = {Question answering algorithm based on deep learning},
year = {2022},
isbn = {9781450384322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501409.3501541},
doi = {10.1145/3501409.3501541},
abstract = {With the rapid development of Internet technology, people are no longer limited to simply obtaining information from the Internet and then manually screening, but prefer to use high-tech to retrieve information quickly and accurately. Therefore, this paper designs an intelligent question answering system to solve and apply the above problems. At present, intelligent question answering system has become a hot direction in the field of natural language processing. The purpose of this paper is to build a deep learning-based question answering algorithm based on deep learning technology and lay a foundation for its future application.},
booktitle = {Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering},
pages = {731–734},
numpages = {4},
keywords = {answer deep learning, information retrieval, intelligent question, natural language processing},
location = {Xiamen, China},
series = {EITCE '21}
}

@inproceedings{10.1145/3486622.3493930,
author = {Azevedo, Pedro and Rocha, Gil and Esteves, Diego and Cardoso, Henrique Lopes},
title = {Towards Better Evidence Extraction Methods for Fact-Checking Systems},
year = {2022},
isbn = {9781450391153},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486622.3493930},
doi = {10.1145/3486622.3493930},
abstract = {Given current levels of misinformation spread, never before have fact-checking frameworks been so critical. Unfortunately, the performance of Automated Fact-checking systems is still poor due to the complexity of the task. In this paper, we present an ablation study of a framework submitted to the FEVER 1.0 challenge. Based on our findings, we explore how triple-based information retrieval, coreference resolution, and recent language model representations can impact the performance of each subtask. We show the importance of recall and precision in the retrieval of documents and sentences that can be provided to justify the veracity of a given claim. We reach state-of-the-art results in the Document Retrieval task and we show promising results when using coreference resolution to improve the Sentence Retrieval task.},
booktitle = {IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology},
pages = {277–284},
numpages = {8},
keywords = {Fact Extraction and Verification, Information Retrieval, Natural Language Processing, Open Information Extraction},
location = {Melbourne, VIC, Australia},
series = {WI-IAT '21}
}

@inproceedings{10.1145/3415958.3433087,
author = {Gomes, Thiago and Ladeira, Marcelo},
title = {A new conceptual framework for enhancing legal information retrieval at the Brazilian Superior Court of Justice},
year = {2020},
isbn = {9781450381154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3415958.3433087},
doi = {10.1145/3415958.3433087},
abstract = {Effective retrieval of jurisprudence (case-law) is imperative to achieve consistency and predictability for any legal system. In this work, we propose and proceed to an empirical evaluation of a framework for jurisprudence retrieval of the Brazilian Superior Court of Justice in order to ease the task of retrieval of other decisions with the same legal opinion. The experimental results shown that our approach based on text similarity performs better than the legacy system of the Court based on Boolean queries. The building of complex Boolean queries is very specialized and we aim to offer a tool able to use free text as queries without any operator. With the legacy system as baseline, we compare the TF-IDF traditional retrieval model, the BM25 probabilistic model and the Word2Vec model. Our results indicate that the Word2Vec Skip-Gram model, trained on a specialized legal corpus and BM25 yield similar performance and surpasses the legacy system. Combining BM25 model with embedding models improved the performance up to 19%.},
booktitle = {Proceedings of the 12th International Conference on Management of Digital EcoSystems},
pages = {26–29},
numpages = {4},
keywords = {E-Discovery, Information Retrieval, Natural Language Processing, Word Embedding},
location = {Virtual Event, United Arab Emirates},
series = {MEDES '20}
}

@article{10.1145/3624733,
author = {Deldjoo, Yashar and Nazary, Fatemeh and Ramisa, Arnau and McAuley, Julian and Pellegrini, Giovanni and Bellogin, Alejandro and Noia, Tommaso Di},
title = {A Review of Modern Fashion Recommender Systems},
year = {2023},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3624733},
doi = {10.1145/3624733},
abstract = {The textile and apparel industries have grown tremendously over the past few years. Customers no longer have to visit many stores, stand in long queues, or try on garments in dressing rooms, as millions of products are now available in online catalogs. However, given the plethora of options available, an effective recommendation system is necessary to properly sort, order, and communicate relevant product material or information to users. Effective fashion recommender systems (RSs) can have a noticeable impact on billions of customers’ shopping experiences and increase sales and revenues on the provider side.The goal of this survey is to provide a review of RSs that operate in the specific vertical domain of garment and fashion products. We have identified the most pressing challenges in fashion RS research and created a taxonomy that categorizes the literature according to the objective they are trying to accomplish (e.g., item or outfit recommendation, size recommendation, and explainability, among others) and type of side information (users, items, context). We have also identified the most important evaluation goals and perspectives (outfit generation, outfit recommendation, pairing recommendation, and fill-in-the-blank outfit compatibility prediction) and the most commonly used datasets and evaluation metrics.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {87},
numpages = {37},
keywords = {Recommender systems, information retrieval, fashion retail, machine learning, artificial intelligence, computer vision, text mining, e-commerce}
}

@inproceedings{10.1145/3639856.3639916,
author = {Purwar, Anupam and Sundar, Rahul},
title = {Keyword Augmented Retrieval: Novel framework for Information Retrieval integrated with speech interface},
year = {2024},
isbn = {9798400716492},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639856.3639916},
doi = {10.1145/3639856.3639916},
abstract = {Retrieving answers in a quick and low cost manner without hallucinations from a combination of structured and unstructured data using Language models is a major hurdle. This is what prevents employment of Language models in knowledge retrieval automation. This becomes accentuated when one wants to integrate a speech interface on top of a text based knowledge retrieval system. Besides, for commercial search and chat-bot applications, complete reliance on commercial large language models (LLMs) like GPT 3.5 etc. can be very costly. In the present study, the authors have addressed the aforementioned problem by first developing a keyword based search framework which augments discovery of the context from the document to be provided to the LLM. The keywords in turn are generated by a relatively smaller LLM and cached for comparison with keywords generated by the same smaller LLM against the query raised. This significantly reduces time and cost to find the context within documents. Once the context is set, a larger LLM uses that to provide answers based on a prompt tailored for Q&amp;A. This research work demonstrates that use of keywords in context identification reduces the overall inference time and cost of information retrieval. Given this reduction in inference time and cost with the keyword augmented retrieval framework, a speech based interface for user input and response readout was integrated. This allowed a seamless interaction with the language model.},
booktitle = {Proceedings of the Third International Conference on AI-ML Systems},
articleno = {58},
numpages = {5},
keywords = {Large Language model (LLM), information retrieval (IR), keyBERT, keyword augmented retrieval (KAR)., prompt},
location = {Bangalore, India},
series = {AIMLSystems '23}
}

@inproceedings{10.1145/3582768.3582798,
author = {Lim, Lecia Kai Heng and Agarwal, Samarth and Zhang, Xuejie and Lu, John Jianan},
title = {False Positive Intent Detection Framework for Chatbot Annotation},
year = {2023},
isbn = {9781450397629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582768.3582798},
doi = {10.1145/3582768.3582798},
abstract = {For chatbots answering thousands of user queries daily, it requires huge annotation efforts or explicit signals from users to identify incorrect chatbot predictions. Identification of such False Positives is key to improving chatbot accuracy and is a challenging problem due to the high cost and limited explicit signals from users. In this paper, we present a framework for automatically detecting False Positive intents in an employee chatbot through implicit feedback by capturing specific user behavior using techniques such as detection of repeated queries and leveraging on active learning sampling strategies to find cases where the chatbot might have provided an incorrect response. Using this approach within the bank, annotators can prioritize their efforts and detect False Positive intent approximately three times better than manual screening of random chatbot dialogues. This framework can be reused across different chatbot applications.},
booktitle = {Proceedings of the 2022 6th International Conference on Natural Language Processing and Information Retrieval},
pages = {187–194},
numpages = {8},
keywords = {active learning query strategies, banking, chatbot, digital transformation, duplicate detection, false positive, implicit feedback, information retrieval, natural language processing},
location = {Bangkok, Thailand},
series = {NLPIR '22}
}

@inproceedings{10.1145/3616855.3635724,
author = {Dave, Vachik S. and Pang, Linsey and Cui, Xiquan and Luo, Chen and Zamani, Hamed and Wu, Lingfei and Karypis, George},
title = {The 3rd International Workshop on Interactive and Scalable Information Retrieval Methods for eCommerce (ISIR-eCom 2024)},
year = {2024},
isbn = {9798400703713},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3616855.3635724},
doi = {10.1145/3616855.3635724},
abstract = {Over the past few years, consumer behavior has shifted from traditional in-store shopping to online shopping. For example, eCommerce sales have grown from around 5% of total US sales in 2012 to around 15.4% in year 2023. This rapid growth of eCommerce has created new challenges and vital new requirements for intelligent information retrieval systems. Which lead to the primary motivations of this workshop:(1) Since the pandemic hit, eCommerce became an important part of people's routine and they started using online shop- ping for smallest grocery items to big electronics as well as cars. With such a large assortment of products and millions of users, achieving higher scalability without losing accuracy is a leading concern for information retrieval systems for eCommerce.(2) The diverse buyers make the relevance of the results highly subjective, because relevance varies for different buyers. The most suitable and intuitive solution to this problem is to make the system interactive and provide correct relevance for different users. Hence, interactive information retrieval systems are becoming necessity in eCommerce.(3) To handle sudden change in buyers' behavior, industries adopted existing sub-optimal information retrieval techniques for various eCommerce tasks. Parallelly, they also started exploring/researching for better solutions and in dire need of help from research community.This workshop will provide a forum to discuss and learn the latest trends for interactive and scalable information retrieval approaches for eCommerce. It will provide academic and industrial researchers a platform to present their latest works, share research ideas, present and discuss various challenges, and identify the areas where further research is needed. It will foster the development of a strong research community focused on solving eCommerce-related information retrieval problems that provide superior eCommerce experience to all users.},
booktitle = {Proceedings of the 17th ACM International Conference on Web Search and Data Mining},
pages = {1208–1209},
numpages = {2},
keywords = {ecommerce search, information retrieval, interactive systems, large language models (llms) in ecommerce, natural language processing (nlp) for ecommerce, ranking models, recommender systems},
location = {Merida, Mexico},
series = {WSDM '24}
}

@inproceedings{10.1145/3583780.3615993,
author = {Zhang, Dell and Petrova, Alina and Trautmann, Dietrich and Schilder, Frank},
title = {Unleashing the Power of Large Language Models for Legal Applications},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3615993},
doi = {10.1145/3583780.3615993},
abstract = {The use of Large Language Models (LLMs) is revolutionizing the legal industry. In this technical talk, we would like to explore the various use cases of LLMs in legal tasks, discuss the best practices, investigate the available resources, examine the ethical concerns, and suggest promising research directions.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {5257–5258},
numpages = {2},
keywords = {large language models, legal data mining, legal information retrieval, legal knowledge management, legal natural language processing},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@inproceedings{10.1145/3463945.3469058,
author = {Cheema, Gullal S. and Hakimov, Sherzod and M\"{u}ller-Budack, Eric and Ewerth, Ralph},
title = {A Fair and Comprehensive Comparison of Multimodal Tweet Sentiment Analysis Methods},
year = {2021},
isbn = {9781450385305},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3463945.3469058},
doi = {10.1145/3463945.3469058},
abstract = {Opinion and sentiment analysis is a vital task to characterize subjective information in social media posts. In this paper, we present a comprehensive experimental evaluation and comparison with six state-of-the-art methods, from which we have re-implemented one of them. In addition, we investigate different textual and visual feature embeddings that cover different aspects of the content, as well as the recently introduced multimodal CLIP embeddings. Experimental results are presented for two different publicly available benchmark datasets of tweets and corresponding images. In contrast to the evaluation methodology of previous work, we introduce a reproducible and fair evaluation scheme to make results comparable. Finally, we conduct an error analysis to outline the limitations of the methods and possibilities for the future work.},
booktitle = {Proceedings of the 2021 Workshop on Multi-Modal Pre-Training for Multimedia Understanding},
pages = {37–45},
numpages = {9},
keywords = {computer vision, information retrieval, multimodal sentiment analysis, natural language processing, social media, transformer models},
location = {Taipei, Taiwan},
series = {MMPT '21}
}

@inproceedings{10.1145/3583780.3615308,
author = {Makrehchi, Masoud and Zhang, Dell and Petrova, Alina and Armour, John},
title = {The 3rd International Workshop on Mining and Learning in the Legal Domain},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3615308},
doi = {10.1145/3583780.3615308},
abstract = {The increasing accessibility of legal corpora and databases create opportunities to develop data-driven techniques and advanced tools that can facilitate a variety of tasks in the legal domain, such as legal search and research, legal document review and summary, legal contract drafting, and legal outcome prediction. Compared with other application domains, the legal domain is characterized by the huge scale of natural language text data, the high complexity of specialist knowledge, and the critical importance of ethical considerations. The MLLD workshop aims to bring together researchers and practitioners to share the latest research findings and innovative approaches in employing data mining, machine learning, information retrieval, and knowledge management techniques to transform the legal sector. Building upon the previous successes, the third edition of the MLLD workshop will emphasize the exploration of new research opportunities brought about by recent rapid advances in Large Language Models and Generative AI. We encourage submissions that intersect computer science and law, from both academia and industry, embodying the interdisciplinary spirit of CIKM.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {5277–5280},
numpages = {4},
keywords = {large language models, legal data mining, legal information retrieval, legal knowledge management, legal natural language processing},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@inproceedings{10.1145/3357384.3358148,
author = {Yoon, Seunghyun and Dernoncourt, Franck and Kim, Doo Soon and Bui, Trung and Jung, Kyomin},
title = {A Compare-Aggregate Model with Latent Clustering for Answer Selection},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3358148},
doi = {10.1145/3357384.3358148},
abstract = {In this paper, we propose a novel method for a sentence-level answer-selection task that is a fundamental problem in natural language processing. First, we explore the effect of additional information by adopting a pretrained language model to compute the vector representation of the input text and by applying transfer learning from a large-scale corpus. Second, we enhance the compare-aggregate model by proposing a novel latent clustering method to compute additional information within the target corpus and by changing the objective function from listwise to pointwise. To evaluate the performance of the proposed approaches, experiments are performed with the WikiQA and TREC-QA datasets. The empirical results demonstrate the superiority of our proposed approach, which achieve state-of-the-art performance for both datasets.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {2093–2096},
numpages = {4},
keywords = {deep learning, information retrieval, natural language processing, question answering},
location = {Beijing, China},
series = {CIKM '19}
}

@inproceedings{10.1145/3302425.3302437,
author = {Cao, Jiarun and Wang, Chongwen and Gao, Liming},
title = {A Joint Model for Text and Image Semantic Feature Extraction},
year = {2018},
isbn = {9781450366250},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302425.3302437},
doi = {10.1145/3302425.3302437},
abstract = {Most of the current information retrieval are based on keyword information appearing in the text or statistical information according to the number of vocabulary words. It is also possible to add additional semantic information by using synonyms, polysemous words, etc. to increase the accuracy of similarity and screening. However, in the current network, in addition to generate a large number of new words every day, pictures, audio, video and other information will appear too. Therefore, the manual features are difficult to express on this kind of newly appearing data, and the low-dimensional feature abstraction is very difficult to represent the overall semantics of text and images. In this paper, we propose a semantic feature extraction algorithm based on deep network, which applies the local attention mechanism to the feature generation model of pictures and texts. The retrieval of text and image information is converted into the similarity calculation of the vector, which improves the retrieval speed and ensures the semantic relevance of the result. Through the compilation of many years of news text and image data to complete the training and testing of text and image feature extraction models, the results show that the depth feature model has great advantages in semantic expression and feature extraction. On the other hand, add the similarity calculation to the training processing also improve the retrieval accuracy.},
booktitle = {Proceedings of the 2018 International Conference on Algorithms, Computing and Artificial Intelligence},
articleno = {63},
numpages = {8},
keywords = {Information retrieval, Natural language processing, Similarity Calculation},
location = {Sanya, China},
series = {ACAI '18}
}

@article{10.1145/3349527,
author = {Tamine, Lynda and Soulier, Laure and Nguyen, Gia-Hung and Souf, Nathalie},
title = {Offline versus Online Representation Learning of Documents Using External Knowledge},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3349527},
doi = {10.1145/3349527},
abstract = {An intensive recent research work investigated the combined use of hand-curated knowledge resources and corpus-driven resources to learn effective text representations. The overall learning process could be run by online revising the learning objective or by offline refining an original learned representation. The differentiated impact of each of the learning approaches on the quality of the learned representations has not been studied so far in the literature. This article focuses on the design of comparable offline vs. online knowledge-enhanced document representation learning models and the comparison of their effectiveness using a set of standard IR and NLP downstream tasks. The results of quantitative and qualitative analyses show that (1) offline vs. online learning approaches have dissimilar result trends regarding the task as well as the dataset distribution counts with regard to domain application; (2) while considering external knowledge resources is undoubtedly beneficial, the way used to express relational constraints could affect semantic inference effectiveness. The findings of this work present opportunities for the design of future representation learning models, but also for providing insights about the evaluation of such models.},
journal = {ACM Trans. Inf. Syst.},
month = sep,
articleno = {42},
numpages = {34},
keywords = {Representation learning, information retrieval, knowledge resources, natural language processing}
}

@article{10.1145/3404995,
author = {Wang, Wei and Gong, Zhiguo and Ren, Jing and Xia, Feng and Lv, Zhihan and Wei, Wei},
title = {Venue Topic Model–enhanced Joint Graph Modelling for Citation Recommendation in Scholarly Big Data},
year = {2020},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3404995},
doi = {10.1145/3404995},
abstract = {Natural language processing technologies, such as topic models, have been proven to be effective for scholarly recommendation tasks with the ability to deal with content information. Recently, venue recommendation is becoming an increasingly important research task due to the unprecedented number of publication venues. However, traditional methods focus on either the author’s local network or author-venue similarity, where the multiple relationships between scholars and venues are overlooked, especially the venue–venue interaction. To solve this problem, we propose an author topic model–enhanced joint graph modeling approach that consists of venue topic modeling, venue-specific topic influence modeling, and scholar preference modeling. We first model the venue topic with Latent Dirichlet Allocation. Then, we model the venue-specific topic influence in an asymmetric and low-dimensional way by considering the topic similarity between venues, the top-influence of venues, and the top-susceptibility of venues. The top-influence characterizes venues’ capacity of exerting topic influence on other venues. The top-susceptibility captures venues’ propensity of being topically influenced by other venues. Extensive experiments on two real-world datasets show that our proposed joint graph modeling approach outperforms the state-of-the-art methods.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = dec,
articleno = {4},
numpages = {15},
keywords = {Network embedding, academic information retrieval, natural language processing, scientific collaboration}
}

@inproceedings{10.1145/3368567.3368577,
author = {Mahato, Devasish and Dudhal, Disha and Revagade, Dhanashree and Bhargava, Yesoda},
title = {A Method to Detect Inconsistent Annotations in a Medical Document using UMLS},
year = {2019},
isbn = {9781450377508},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368567.3368577},
doi = {10.1145/3368567.3368577},
abstract = {Information retrieval from clinical documents relies heavily on annotated corpus. Any inconsistency in annotations, in form of heterogeneous annotations for similar concepts, could be detrimental to the quality of information retrieved. In extreme cases, this may lead to incorrect deductions about patient's medical history and result in erroneous decisions. In the present work, a complete end-to-end system that identifies inconsistencies in a clinically annotated document is presented and analysed. Unified Medical Language System(UMLS) is used to identify medical concepts in the clinical document. The output is presented in a clustered format wherein, each cluster identifies a unique medical concept and contains its semantic synonyms. For each semantic synonym, inconsistent annotations and the sentences in which they occur in the document are listed. The work could be useful for annotation experts who need automated tools to verify their work.},
booktitle = {Proceedings of the 11th Annual Meeting of the Forum for Information Retrieval Evaluation},
pages = {47–51},
numpages = {5},
keywords = {Inconsistency Detection, Information Retrieval, UMLS, Natural Language Processing},
location = {Kolkata, India},
series = {FIRE '19}
}

@inproceedings{10.1145/3397271.3401266,
author = {Matsubara, Yoshitomo and Vu, Thuy and Moschitti, Alessandro},
title = {Reranking for Efficient Transformer-based Answer Selection},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401266},
doi = {10.1145/3397271.3401266},
abstract = {IR-based Question Answering (QA) systems typically use a sentence selector to extract the answer from retrieved documents. Recent studies have shown that powerful neural models based on the Transformer can provide an accurate solution to Answer Sentence Selection (AS2). Unfortunately, their computation cost prevents their use in real-world applications. In this paper, we show that standard and efficient neural rerankers can be used to reduce the amount of sentence candidates fed to Transformer models without hurting Accuracy, thus improving efficiency up to four times. This is an important finding as the internal representation of shallower neural models is dramatically different from the one used by a Transformer model, e.g., word vs. contextual embeddings.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1577–1580},
numpages = {4},
keywords = {information retrieval, natural language processing, neural networks, question answering, reranking, transformer models},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@inproceedings{10.1145/3336191.3371870,
author = {Chien, Jen-Tzung},
title = {Deep Bayesian Data Mining},
year = {2020},
isbn = {9781450368223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336191.3371870},
doi = {10.1145/3336191.3371870},
abstract = {This tutorial addresses the fundamentals and advances in deep Bayesian mining and learning for natural language with ubiquitous applications ranging from speech recognition to document summarization, text classification, text segmentation, information extraction, image caption generation, sentence generation, dialogue control, sentiment classification, recommendation system, question answering and machine translation, to name a few. Traditionally, "deep learning" is taken to be a learning process where the inference or optimization is based on the real-valued deterministic model. The "semantic structure" in words, sentences, entities, actions and documents drawn from a large vocabulary may not be well expressed or correctly optimized in mathematical logic or computer programs. The "distribution function" in discrete or continuous latent variable model for natural language may not be properly decomposed or estimated. This tutorial addresses the fundamentals of statistical models and neural networks, and focus on a series of advanced Bayesian models and deep models including hierarchical Dirichlet process, Chinese restaurant process, hierarchical Pitman-Yor process, Indian buffet process, recurrent neural network (RNN), long short-term memory, sequence-to-sequence model, variational auto-encoder (VAE), generative adversarial network (GAN), attention mechanism, memory-augmented neural network, skip neural network, temporal difference VAE, stochastic neural network, stochastic temporal convolutional network, predictive state neural network, and policy neural network. Enhancing the prior/posterior representation is addressed. We present how these models are connected and why they work for a variety of applications on symbolic and complex patterns in natural language. The variational inference and sampling method are formulated to tackle the optimization for complicated models. The word and sentence embeddings, clustering and co-clustering are merged with linguistic and semantic constraints. A series of case studies, tasks and applications are presented to tackle different issues in deep Bayesian mining, searching, learning and understanding. At last, we will point out a number of directions and outlooks for future studies. This tutorial serves the objectives to introduce novices to major topics within deep Bayesian learning, motivate and explain a topic of emerging importance for data mining and natural language understanding, and present a novel synthesis combining distinct lines of machine learning work.},
booktitle = {Proceedings of the 13th International Conference on Web Search and Data Mining},
pages = {865–868},
numpages = {4},
keywords = {bayesian learning, data mining, deep learning, information retrieval, natural language processing},
location = {Houston, TX, USA},
series = {WSDM '20}
}

@inproceedings{10.1145/3380688.3380701,
author = {Sinha, Ankita and Subrahmaniam, Vignesh},
title = {Cerebro: Novelty Detection in Product Reviews},
year = {2020},
isbn = {9781450376310},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3380688.3380701},
doi = {10.1145/3380688.3380701},
abstract = {The recent boom in e-commerce has created active electronic communities where consumers share their thoughts about the product and the company. These reviews play a very important part in building customer opinion about the said item. For a popular product or service, there might be thousands of reviews, making it difficult for the customer to make an informed decision about the product. In this paper, we present a way to surface only those reviews that contain information relevant to the user. To address this problem, we try to surface out the reviews that are outliers to the general cluster of reviews during a particular time period.We are leveraging anomaly detection algorithms to achieve this.},
booktitle = {Proceedings of the 4th International Conference on Machine Learning and Soft Computing},
pages = {140–143},
numpages = {4},
keywords = {Anomaly detection, Information Retrieval, Natural language processing, Novelty detection, Outlier Detection, Reviews},
location = {Haiphong City, Viet Nam},
series = {ICMLSC '20}
}

@inproceedings{10.1145/3308558.3313746,
author = {Han, Fred.X and Niu, Di and Lai, Kunfeng and Guo, Weidong and He, Yancheng and Xu, Yu},
title = {Inferring Search Queries from Web Documents via a Graph-Augmented Sequence to Attention Network},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313746},
doi = {10.1145/3308558.3313746},
abstract = {We study the problem of search query inference from web documents, where a short, comprehensive natural language query is inferred from a long article. Search query generation or inference is of great value to search engines and recommenders in terms of locating potential target users and ranking content. Despite being closely related to other NLP tasks like abstract generation and keyword extraction, we point out that search query inference is, in fact, a new problem, in that the generated natural language query, which consists of a few words, is expected to be comprehensive enough to lead to the click-through of the corresponding document. Therefore, query generation requires an accurate inference of query words, as well as a deeper level of understanding on document semantic structures. Toward this end, we propose a novel generative model called the Graph-augmented Sequence to Attention (G-S2A) network. Adopting an Encoder-Decoder architecture, G-S2A incorporates a sentence-level Graph Convolutional Network (GCN), a keyword-level GCN, as well as a hierarchical recurrent neural network (RNN) into the encoder to generate structural document representations. An attentional Transformer decoder is then applied to combine different types of encoded features to generate a target query. On a query-document dataset from a real-world search engine, our model outperforms several neural generative models on a wide range of metrics.},
booktitle = {The World Wide Web Conference},
pages = {2792–2798},
numpages = {7},
keywords = {information retrieval, natural language processing, query suggestion, search query generation},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3341981.3344217,
author = {Purpura, Alberto and Maggipinto, Marco and Silvello, Gianmaria and Susto, Gian Antonio},
title = {Probabilistic Word Embeddings in Neural IR: A Promising Model That Does Not Work as Expected (For Now)},
year = {2019},
isbn = {9781450368810},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341981.3344217},
doi = {10.1145/3341981.3344217},
abstract = {In this paper, we discuss how a promising word vector representation based on Probabilistic Word Embeddings (PWE) can be applied to Neural Information Retrieval (NeuIR). We illustrate PWE pros for text retrieval, and identify the core issues which prevent a full exploitation of their potential. In particular, we focus on the application of elliptical probabilistic embeddings, a type of PWE, to a NeuIR system (i.e., MatchPyramid). The main contributions of this paper are: (i) an analysis of the pros and cons of PWE in NeuIR; (ii) an in-depth comparison of PWE against pre-trained Word2Vec, FastText and WordNet word embeddings; (iii) an extension of the MatchPyramid model to take advantage of broader word relations information from WordNet; (iv) a topic-level evaluation of the MatchPyramid ranking models employing the considered word embeddings. Finally, we discuss some lessons learned and outline some open research problems to employ PWE in NeuIR systems more effectively.},
booktitle = {Proceedings of the 2019 ACM SIGIR International Conference on Theory of Information Retrieval},
pages = {3–10},
numpages = {8},
keywords = {natural language processing, neural information retrieval, probabilistic word embedding},
location = {Santa Clara, CA, USA},
series = {ICTIR '19}
}

@inproceedings{10.1145/3136040.3136052,
author = {Lape\~{n}a, Ra\'{u}l and Font, Jaime and Pastor, \'{O}scar and Cetina, Carlos},
title = {Analyzing the impact of natural language processing over feature location in models},
year = {2017},
isbn = {9781450355247},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3136040.3136052},
doi = {10.1145/3136040.3136052},
abstract = {Feature Location (FL) is a common task in the Software Engineering field, specially in maintenance and evolution of software products. The results of FL depend in a great manner in the style in which Feature Descriptions and software artifacts are written. Therefore, Natural Language Processing (NLP) techniques are used to process them. Through this paper, we analyze the influence of the most common NLP techniques over FL in Conceptual Models through Latent Semantic Indexing, and the influence of human participation when embedding domain knowledge in the process. We evaluated the techniques in a real-world industrial case study in the rolling stocks domain.},
booktitle = {Proceedings of the 16th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {63–76},
numpages = {14},
keywords = {Feature Location, Information Retrieval, Natural Language Processing},
location = {Vancouver, BC, Canada},
series = {GPCE 2017}
}

@article{10.1145/3091995,
author = {Zhou, Guang-You and Huang, Jimmy Xiangji},
title = {Modeling and Mining Domain Shared Knowledge for Sentiment Analysis},
year = {2017},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3091995},
doi = {10.1145/3091995},
abstract = {Sentiment classification aims to automatically predict sentiment polarity (e.g., positive or negative) of user generated sentiment data (e.g., reviews, blogs). In real applications, these user-generated sentiment data can span so many different domains that it is difficult to label the training data for all of them. Therefore, we study the problem of sentiment classification adaptation task in this article. That is, a system is trained to label reviews from one source domain but is meant to be used on the target domain. One of the biggest challenges for sentiment classification adaptation task is how to deal with the problem when two data distributions between the source domain and target domain are significantly different from one another. However, our observation is that there might exist some domain shared knowledge among certain input dimensions of different domains. In this article, we present a novel method for modeling and mining the domain shared knowledge from different sentiment review domains via a joint non-negative matrix factorization–based framework. In this proposed framework, we attempt to learn the domain shared knowledge and the domain-specific information from different sentiment review domains with several various regularization constraints. The advantage of the proposed method can promote the correspondence under the topic space between the source domain and the target domain, which can significantly reduce the data distribution gap across two domains. We conduct extensive experiments on two real-world balanced data sets from Amazon product reviews for sentence-level and document-level binary sentiment classification. Experimental results show that our proposed approach significantly outperforms several strong baselines and achieves an accuracy that is competitive with the most well-known methods for sentiment classification adaptation.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {18},
numpages = {36},
keywords = {Information Retrieval, Natural Language Processing, Sentiment Analysis}
}

@inproceedings{10.1145/3534678.3542919,
author = {Nenkova, Ani and Burdick, Douglas and Han, Benjamin and Lewis, Dave and Tata, Sandeep and Tecuci, Dan},
title = {DI-2022: The Third Document Intelligence Workshop},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3542919},
doi = {10.1145/3534678.3542919},
abstract = {Business documents are central to the operation of all organizations, and they come in all shapes and sizes: project reports, planning documents, technical specifications, financial statements, meeting minutes, legal agreements, contracts, resumes, purchase orders, invoices, and many more. The ability to read, understand and interpret these documents, referred to here as Document Intelligence (DI), is challenging due to not only many domains of knowledge involved, but also their complex formats and structures, internal and external cross references deployed, and even less-than-ideal quality of scans and OCR oftentimes performed on them. This workshop aims to explore and advance the current state of research and practice in answering these challenges.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4890–4891},
numpages = {2},
keywords = {computer vision, data mining, information retrieval., knowledge discovery, knowledge representation and reasoning, layout understanding, natural language processing, natural language understanding},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.1145/3447548.3469454,
author = {Han, Benjamin and Burdick, Douglas and Lewis, Dave and Lu, Yijuan and Motahari, Hamid and Tata, Sandeep},
title = {DI-2021: The Second Document Intelligence Workshop},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3469454},
doi = {10.1145/3447548.3469454},
abstract = {Business documents are central to the operation of all organizations, and they come in all shapes and sizes: project reports, planning documents, technical specifications, financial statements, meeting minutes, legal agreements, contracts, resumes, purchase orders, invoices, and many more. The ability to read, understand and interpret these documents, referred to here as Document Intelligence (DI), is challenging due to not only many domains of knowledge involved, but also their complex formats and structures, internal and external cross references deployed, and even less-than-ideal quality of scans and OCR oftentimes performed on them. This workshop aims to explore and advance the current state of research and practice in answering these challenges.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {4127–4128},
numpages = {2},
keywords = {computer vision, data mining, information retrieval, knowledge discovery, knowledge representation and reasoning, layout understanding, natural language processing, natural language understanding},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.1145/3511808.3557656,
author = {Moscati, Marta and Parada-Cabaleiro, Emilia and Deldjoo, Yashar and Zangerle, Eva and Schedl, Markus},
title = {Music4All-Onion -- A Large-Scale Multi-faceted Content-Centric Music Recommendation Dataset},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557656},
doi = {10.1145/3511808.3557656},
abstract = {When we appreciate a piece of music, it is most naturally because of its content, including rhythmic, tonal, and timbral elements as well as its lyrics and semantics. This suggests that the human affinity for music is inherently content-driven. This kind of information is, however, still frequently neglected by mainstream recommendation models based on collaborative filtering that rely solely on user-item interactions to recommend items to users. A major reason for this neglect is the lack of standardized datasets that provide both collaborative and content information. The work at hand addresses this shortcoming by introducing Music4All-Onion, a large-scale, multi-modal music dataset. The dataset expands the Music4All dataset by including 26 additional audio, video, and metadata characteristics for 109,269 music pieces. In addition, it provides a set of 252,984,396 listening records of 119,140 users, extracted from the online music platform Last.fm, which allows leveraging user-item interactions as well. We organize distinct item content features in an onion model according to their semantics, and perform a comprehensive examination of the impact of different layers of this model (e.g., audio features, user-generated content, and derivative content) on content-driven music recommendation, demonstrating how various content features influence accuracy, novelty, and fairness of music recommendation systems. In summary, with Music4All-Onion, we seek to bridge the gap between collaborative filtering music recommender systems and content-centric music recommendation requirements.},
booktitle = {Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management},
pages = {4339–4343},
numpages = {5},
keywords = {video, onion model, natural language processing, music recommender systems, music information retrieval, multimedia content analysis, lyrics, image, audio signal, audio features},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{10.1145/3539781.3539795,
author = {Satheesan, Sandeep Puthanveetil and Bhavya and Davies, Adam and Craig, Alan B. and Zhang, Yu and Zhai, ChengXiang},
title = {Toward a big data analysis system for historical newspaper collections research},
year = {2022},
isbn = {9781450394109},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539781.3539795},
doi = {10.1145/3539781.3539795},
abstract = {The availability and generation of digitized newspaper collections have provided researchers in several domains with a powerful tool to advance their research. More specifically, digitized historical newspapers give us a magnifying glass into the past. In this paper, we propose a scalable and customizable big data analysis system that enables researchers to study complex questions about our society as depicted in news media for the past few centuries by applying cutting-edge text analysis tools to large historical newspaper collections. We discuss our experience with building a preliminary version of such a system, including how we have addressed the following challenges: processing millions of digitized newspaper pages from various publications worldwide, which amount to hundreds of terabytes of data; applying article segmentation and Optical Character Recognition (OCR) to historical newspapers, which vary between and within publications over time; retrieving relevant information to answer research questions from such data collections by applying human-in-the-loop machine learning; and enabling users to analyze topic evolution and semantic dynamics with multiple compatible analysis operators. We also present some preliminary results of using the proposed system to study the social construction of juvenile delinquency in the United States and discuss important remaining challenges to be tackled in the future.},
booktitle = {Proceedings of the Platform for Advanced Scientific Computing Conference},
articleno = {12},
numpages = {11},
keywords = {big data analysis system, data visualization, historical newspapers, image analysis, information retrieval, juvenile delinquency, natural language processing, newspaper article segmentation, social construction, social science research, text analysis},
location = {Basel, Switzerland},
series = {PASC '22}
}

@inproceedings{10.1145/3003733.3003792,
author = {Karanikolas, Nikitas N.},
title = {Building Stemmers for the Polish Language},
year = {2016},
isbn = {9781450347891},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3003733.3003792},
doi = {10.1145/3003733.3003792},
abstract = {In this paper we examine the applicability of our "supervised learning methodology that builds stemmers given minor target language knowledge" (shortly: stemmer builder) for building a stemmer for the Polish language. We examine if the existing features of our stemmer builder can support the necessities of the Polish language for building an acceptable stemmer.},
booktitle = {Proceedings of the 20th Pan-Hellenic Conference on Informatics},
articleno = {77},
numpages = {4},
keywords = {Information Retrieval, Natural Language Processing, Stemming Algorithms},
location = {Patras, Greece},
series = {PCI '16}
}

@inproceedings{10.1145/3345252.3345275,
author = {Trandafili, Evis and Paci, Hakik and Karaj, Elona},
title = {A Novel Document Summarization System for Albanian Language},
year = {2019},
isbn = {9781450371490},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3345252.3345275},
doi = {10.1145/3345252.3345275},
abstract = {Summarization is a Natural Language Processing application that may seem trivial to a person, but in a time where the quantity of information provided is continuously growing, the possibility of implementing a "helper" in order to summarize it, has become a necessity. Most of the existing scientific studies in automatic text summarization has been paying attention primarily to English with only some recent attempts in other major languages. To the best of our knowledge, no prior approaches handle automatic summarization for Albanian documents. This paper is proposed to fill this gap by implementing a novel extractive summarization system, designed specifically for Albanian Language. We showed experimentally that the enrichment of the summarization system with language-dependent elements improves the systems' performance and the compression rate.},
booktitle = {Proceedings of the 20th International Conference on Computer Systems and Technologies},
pages = {273–277},
numpages = {5},
keywords = {Albanian language, Extractive Summarization, Information Retrieval and Extraction, Natural Language Processing},
location = {Ruse, Bulgaria},
series = {CompSysTech '19}
}

@inproceedings{10.1145/3010089.3010101,
author = {Wallace, Duncan and Kechadi, M-Tahar},
title = {Retrieval and Clustering of Medicines Within Healthcare Data Records},
year = {2016},
isbn = {9781450347792},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3010089.3010101},
doi = {10.1145/3010089.3010101},
abstract = {Electronic Health Records (EHRs) are typically designed to electronically document all information that is administratively and clinically relevant in a patient's use of a healthcare facility. This paper intends to improve discoverability of medications which may exist within the narrative-based free-text notes of patients' health care data records. This led us to introduce a context sensitive approach to retrieve candidate pharmaceuticals. Additionally, a combination of contraction promotion and clustering based upon edit distance will be utilised to increase the precision of this process.},
booktitle = {Proceedings of the International Conference on Big Data and Advanced Wireless Technologies},
articleno = {16},
numpages = {6},
keywords = {Information Retrieval, Natural language processing, clustering, medical-informatics},
location = {Blagoevgrad, Bulgaria},
series = {BDAW '16}
}

@inproceedings{10.1145/3018661.3018691,
author = {Vinayakarao, Venkatesh and Sarma, Anita and Purandare, Rahul and Jain, Shuktika and Jain, Saumya},
title = {ANNE: Improving Source Code Search using Entity Retrieval Approach},
year = {2017},
isbn = {9781450346757},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018661.3018691},
doi = {10.1145/3018661.3018691},
abstract = {Code search with natural language terms performs poorly because programming concepts do not always lexically match their syntactic forms. For example, in Java, the programming concept "array" does not match with its syntactic representation of "[ ]". Code search engines can assist developers more effectively over natural language queries if such mappings existed for a variety of programming languages. In this work, we present a programming language agnostic technique to discover such mappings between syntactic forms and natural language terms representing programming concepts. We use the questions and answers in Stack Overflow to create this mapping. We implement our approach in a tool called ANNE. To evaluate its effectiveness, we conduct a user study in an academic setting in which teaching assistants use ANNE to search for code snippets in student submissions. With the use of ANNE, we find that the participants are 29% quicker with no significant drop in correctness and completeness.},
booktitle = {Proceedings of the Tenth ACM International Conference on Web Search and Data Mining},
pages = {211–220},
numpages = {10},
keywords = {assignment grading, code search, information retrieval, natural language processing},
location = {Cambridge, United Kingdom},
series = {WSDM '17}
}

@inproceedings{10.1145/3086512.3086549,
author = {Heo, Seongwan and Hong, Kihyun and Rhim, Young-Yik},
title = {Legal content fusion for legal information retrieval},
year = {2017},
isbn = {9781450348911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3086512.3086549},
doi = {10.1145/3086512.3086549},
abstract = {With recent increasing attention to legal information processing, legal information retrieval (IR) has become one of the active research fields. However, there are still many hindrances obtaining rigorous results in legal IR applications in comparison with IR applications for general document retrieval. It is mainly due to the characteristics of legal information such as the complicated structure of legal contents and usage of legal jargon. In this paper, we present a legal IR method, which is a structure-wise IR approach. e presented method in this study focuses on analyzing the contents of legal documents and applying the content contributions to the IR processing. We demonstrate the performance of the proposed IR method with the COILEE data set, which are derived from Japanese bar exams.},
booktitle = {Proceedings of the 16th Edition of the International Conference on Articial Intelligence and Law},
pages = {277–281},
numpages = {5},
keywords = {information retrieval, legal text mining, machine learning, natural language processing},
location = {London, United Kingdom},
series = {ICAIL '17}
}

@inproceedings{10.1145/3022198.3024948,
author = {Boon, Miriam L.},
title = {Augmenting Media Literacy with Automatic Characterization of News along Pragmatic Dimensions},
year = {2017},
isbn = {9781450346887},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3022198.3024948},
doi = {10.1145/3022198.3024948},
abstract = {Media literacy allows individuals to better interpret the information they need to absorb to contribute to our democratic, knowledge-based society. I propose that by automatically notifying readers of an article's problematic pragmatic characteristics, an application could augment their media literacy. I describe two characteristics for which I have been building automatic classifiers -- factiness and tropes as narrative frames -- and discuss the status of each project.},
booktitle = {Companion of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing},
pages = {49–52},
numpages = {4},
keywords = {computational journalism, information retrieval, machine learning, natural language processing},
location = {Portland, Oregon, USA},
series = {CSCW '17 Companion}
}

@inproceedings{10.1145/3240117.3240123,
author = {Bertin, Marc and Atanassova, Iana},
title = {Recommending Scientific Papers: The Role of Citation Contexts},
year = {2018},
isbn = {9781450364515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240117.3240123},
doi = {10.1145/3240117.3240123},
abstract = {This paper addresses the problem of building recommender systems for scientific papers based on the linguistic and contextual analysis of citation contexts. We explain the importance of taking into consideration citation contexts and the different methodologies that exist as well as the ways that citations impact recommender systems. We also discuss the limits of using citation contexts to generate recommendations.},
booktitle = {Proceedings of the 1st International Conference on Digital Tools &amp; Uses Congress},
articleno = {6},
numpages = {4},
keywords = {Bibliometrics, Citation Context Analysis, Information Retrieval, Natural Language Processing, Recommender Systems, Scientific Papers},
location = {Paris, France},
series = {DTUC '18}
}

@inproceedings{10.1145/3283812.3283818,
author = {Yu, Zhe and Menzies, Tim},
title = {Total recall, language processing, and software engineering},
year = {2018},
isbn = {9781450360555},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3283812.3283818},
doi = {10.1145/3283812.3283818},
abstract = {A broad class of software engineering problems can be generalized as the "total recall problem". This short paper claims that identifying and exploring the total recall problems in software engineering is an important task with wide applicability.  To make that case, we show that by applying and adapting the state of the art active learning and natural language processing algorithms for solving the total recall problem, two important software engineering tasks can also be addressed : (a) supporting large literature reviews and (b) identifying software security vulnerabilities. Furthermore, we conjecture that (c) test case prioritization and (d) static warning identification can also be generalized as and benefit from the total recall problem.  The widespread applicability of "total recall" to software engineering suggests that there exists some underlying framework that encompasses not just natural language processing, but a wide range of important software engineering tasks.},
booktitle = {Proceedings of the 4th ACM SIGSOFT International Workshop on NLP for Software Engineering},
pages = {10–13},
numpages = {4},
keywords = {Software engineering, active learning, information retrieval, literature review, natural language processing, total recall, vulnerabilities},
location = {Lake Buena Vista, FL, USA},
series = {NL4SE 2018}
}

@inproceedings{10.1145/3006299.3006315,
author = {Lioma, Christina and Larsen, Birger and Lu, Wei and Huang, Yong},
title = {A study of factuality, objectivity and relevance: three desiderata in large-scale information retrieval?},
year = {2016},
isbn = {9781450346177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3006299.3006315},
doi = {10.1145/3006299.3006315},
abstract = {Much of the information processed by Information Retrieval (IR) systems is unreliable, biased, and generally untrust-worthy [15, 45, 48]. Yet, factuality &amp; objectivity detection is not a standard component of IR systems, even though it has been possible in Natural Language Processing (NLP) in the last decade. Motivated by this, we ask if and how factuality &amp; objectivity detection may benefit IR. We answer this in two parts. First, we use state-of-the-art NLP to compute the probability of document factuality &amp; objectivity in two TREC collections, and analyse its relation to document relevance. We find that factuality is strongly and positively correlated to document relevance, but objectivity is not. Second, we study the impact of factuality &amp; objectivity to retrieval effectiveness by treating them as query independent features that we combine with a competitive language modelling baseline. Experiments with 450 TREC queries show that factuality improves precision by more than 10% over strong baselines, especially for the type of uncurated data typically used in web search; objectivity gives mixed results. An overall clear trend is that document factuality &amp; objectivity is much more beneficial to IR when searching uncurated (e.g. web) documents vs. curated (e.g. state documentation and newswire articles).To our knowledge, this is the first study of factuality &amp; objectivity for back-end IR, contributing novel findings about the relation between relevance and factuality/objectivity, and statistically significant gains to retrieval effectiveness in the competitive web search task.},
booktitle = {Proceedings of the 3rd IEEE/ACM International Conference on Big Data Computing, Applications and Technologies},
pages = {107–117},
numpages = {11},
keywords = {information retrieval, large-scale content analysis, natural language processing},
location = {Shanghai, China},
series = {BDCAT '16}
}

@inproceedings{10.1145/3397482.3450721,
author = {Perez-Ortiz, Maria and Dormann, Claire and Rogers, Yvonne and Bulathwela, Sahan and Kreitmayer, Stefan and Yilmaz, Emine and Noss, Richard and Shawe-Taylor, John},
title = {X5Learn: A Personalised Learning Companion at the Intersection of AI and HCI},
year = {2021},
isbn = {9781450380188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397482.3450721},
doi = {10.1145/3397482.3450721},
abstract = {X5Learn (available at https://x5learn.org ) is a human-centered AI-powered platform for supporting access to free online educational resources. X5Learn provides users with a number of educational tools for interacting with open educational videos, and a set of tools adapted to suit the pedagogical preferences of users. It is intended to support both teachers and students, alike. For teachers, it provides a powerful platform to reuse, revise, remix, and redistribute open courseware produced by others. These can be videos, pdfs, exercises and other online material. For students, it provides a scaffolded and informative interface to select content to watch, read, make notes and write reviews, as well as a powerful personalised recommendation system that can optimise learning paths and adjust to the user’s learning preferences. What makes X5Learn stand out from other educational platforms, is how it combines human-centered design with AI algorithms and software tools with the goal of making it intuitive and easy to use, as well as making the AI transparent to the user. We present the core search tool of X5Learn, intended to support exploring open educational materials.},
booktitle = {Companion Proceedings of the 26th International Conference on Intelligent User Interfaces},
pages = {70–74},
numpages = {5},
keywords = {Artificial Intelligence, Future User Interfaces, Human Computer Interaction, Information Retrieval, Open Education, Recommender Systems, User Modeling},
location = {College Station, TX, USA},
series = {IUI '21 Companion}
}

@inproceedings{10.1145/3372278.3388041,
author = {Dao, Minh-Son and Fjeld, Morten and Biljecki, Filip and Yavanoglu, Uraz and Dong, Mianxiong},
title = {ICDAR'20: Intelligent Cross-Data Analysis and Retrieval},
year = {2020},
isbn = {9781450370875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372278.3388041},
doi = {10.1145/3372278.3388041},
abstract = {The First International Workshop on "Intelligence Cross-Data Analytics and Retrieval" (ICDAR'20) welcomes any theoretical and practical works on intelligence cross-data analytics and retrieval to bring the smart-sustainable society to human beings. We have witnessed the era of big data where almost any event that happens is recorded and stored either distributedly or centrally. The utmost requirement here is that data came from different sources, and various domains must be harmonically analyzed to get their insights immediately towards giving the ability to be retrieved thoroughly. These emerging requirements lead to the need for interdisciplinary and multidisciplinary contributions that address different aspects of the problem, such as data collection, storage, protection, processing, and transmission, as well as knowledge discovery, retrieval, and security and privacy. Hence, the goal of the workshop is to attract researchers and experts in the areas of multimedia information retrieval, machine learning, AI, data science, event-based processing and analysis, multimodal multimedia content analysis, lifelog data analysis, urban computing, environmental science, atmospheric science, and security and privacy to tackle the issues as mentioned earlier.},
booktitle = {Proceedings of the 2020 International Conference on Multimedia Retrieval},
pages = {580–581},
numpages = {2},
keywords = {security and privacy, knowledge discovery, information retrieval, data analytic, cross-data, artificial intelligence},
location = {Dublin, Ireland},
series = {ICMR '20}
}

@inproceedings{10.1145/3003464.3003468,
author = {Golubovic, Nevena and Krintz, Chandra and Wolski, Rich and Lafia, Sara and Hervey, Thomas and Kuhn, Werner},
title = {Extracting spatial information from social media in support of agricultural management decisions},
year = {2016},
isbn = {9781450345880},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3003464.3003468},
doi = {10.1145/3003464.3003468},
abstract = {Farmers face pressure to respond to unpredictable weather, the spread of pests, and other variable events on their farms. This paper proposes a framework for data aggregation from diverse sources that extracts named places impacted by events relevant to agricultural practices. Our vision is to couple natural language processing, geocoding, and existing geographic information retrieval techniques to increase the value of already-available data through aggregation, filtering, validation, and notifications, helping farmers make timely and informed decisions with greater ease.},
booktitle = {Proceedings of the 10th Workshop on Geographic Information Retrieval},
articleno = {4},
numpages = {2},
keywords = {agricultural analytics, gazetteer, information retrieval, natural language processing, social media},
location = {Burlingame, California},
series = {GIR '16}
}

@inproceedings{10.1145/3209978.3210194,
author = {Kumar Chandrasekaran, Muthu and Jaidka, Kokil and Mayr, Philipp},
title = {Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2018)},
year = {2018},
isbn = {9781450356572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209978.3210194},
doi = {10.1145/3209978.3210194},
abstract = {The large scale of scholarly publications poses a challenge for scholars in information seeking and sensemaking. Information retrieval~(IR), bibliometric and natural language processing (NLP) techniques could enhance scholarly search, retrieval and user experience but are not yet widely used. To this purpose, we propose the third iteration of the Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL). The workshop is intended to stimulate IR, NLP researchers and Digital Library professionals to elaborate on new approaches in natural language processing, information retrieval, scientometrics, text mining and recommendation techniques that can advance the state-of-the-art in scholarly document understanding, analysis, and retrieval at scale. The BIRNDL workshop will incorporate multiple invited talks, paper sessions, a poster session and the 4th edition of the Computational Linguistics (CL) Scientific Summarization Shared Task.},
booktitle = {The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval},
pages = {1415–1418},
numpages = {4},
keywords = {text mining, natural language processing, information retrieval, information extraction, digital libraries, citation analysis, bibliometrics},
location = {Ann Arbor, MI, USA},
series = {SIGIR '18}
}

@inproceedings{10.1145/3331184.3331444,
author = {Leidner, Jochen L.},
title = {Nobody Said it Would be Easy: A Decade of R&amp;D Projects in Information Access from Thomson over Reuters to Refinitiv},
year = {2019},
isbn = {9781450361729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331184.3331444},
doi = {10.1145/3331184.3331444},
abstract = {In this talk, I survey a small, non-random sample of research projects in information access carried out as part of the Thomson Reuters family of companies over the course of a 10+-year period. I analyse into how these projects are similar and different when compared to academic research efforts and attempt a critical (and personal, so certainly subjective) assessment of what academia can do for industry, and what industry can do for research in terms of R&amp;D efforts. I will conclude with some advice for academic-industry collaboration initiatives in several areas of vertical information services (legal, finance, pharma and regulatory/compliance) as well as news.},
booktitle = {Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1387–1388},
numpages = {2},
keywords = {academic collaboration, corporate research &amp; development, information extraction, information retrieval, innovation, machine learning, natural language processing, professional information services},
location = {Paris, France},
series = {SIGIR'19}
}

@inproceedings{10.1145/2910896.2926734,
author = {Cabanac, Guillaume and Chandrasekaran, Muthu Kumar and Frommholz, Ingo and Jaidka, Kokil and Kan, Min-Yen and Mayr, Philipp and Wolfram, Dietmar},
title = {Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2016)},
year = {2016},
isbn = {9781450342292},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2910896.2926734},
doi = {10.1145/2910896.2926734},
abstract = {The large scale of scholarly publications poses a challenge for scholars in information-seeking and sensemaking. Bibliometric, information retrieval~(IR), text mining and NLP techniques could help in these activities, but are not yet widely used in digital libraries. This workshop is intended to stimulate IR researchers and digital library professionals to elaborate on new approaches in natural language processing, information retrieval, scientometric and recommendation techniques which can advance the state-of-the-art in scholarly document understanding, analysis and retrieval at scale.},
booktitle = {Proceedings of the 16th ACM/IEEE-CS on Joint Conference on Digital Libraries},
pages = {299–300},
numpages = {2},
keywords = {bibliometrics, digital libraries, information retrieval, natural language processing, text mining},
location = {Newark, New Jersey, USA},
series = {JCDL '16}
}

@inproceedings{10.1145/3077136.3084370,
author = {Chandrasekaran, Muthu Kumar and Jaidka, Kokil and Mayr, Philipp},
title = {Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2017)},
year = {2017},
isbn = {9781450350228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077136.3084370},
doi = {10.1145/3077136.3084370},
abstract = {The large scale of scholarly publications poses a challenge for scholars in information seeking and sensemaking. Bibliometrics, information retrieval (IR), text mining and NLP techniques could help in these search and look-up activities, but are not yet widely used. This workshop is intended to stimulate IR researchers and digital library professionals to elaborate on new approaches in natural language processing, information retrieval, scientometrics, text mining and recommendation techniques that can advance the state-of-the-art in scholarly document understanding, analysis, and retrieval at scale. The BIRNDL workshop at SIGIR 2017 will incorporate an invited talk, paper sessions and the third edition of the Computational Linguistics (CL) Scientific Summarization Shared Task.},
booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1421–1422},
numpages = {2},
keywords = {citation analysis, digital libraries, information extraction, information retrieval, natural language processing, scientometrics, summarization},
location = {Shinjuku, Tokyo, Japan},
series = {SIGIR '17}
}

@inproceedings{10.1145/2939672.2939679,
author = {Malmi, Eric and Takala, Pyry and Toivonen, Hannu and Raiko, Tapani and Gionis, Aristides},
title = {DopeLearning: A Computational Approach to Rap Lyrics Generation},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939679},
doi = {10.1145/2939672.2939679},
abstract = {Writing rap lyrics requires both creativity to construct a meaningful, interesting story and lyrical skills to produce complex rhyme patterns, which form the cornerstone of good flow. We present a rap lyrics generation method that captures both of these aspects. First, we develop a prediction model to identify the next line of existing lyrics from a set of candidate next lines. This model is based on two machine-learning techniques: the RankSVM algorithm and a deep neural network model with a novel structure. Results show that the prediction model can identify the true next line among 299 randomly selected lines with an accuracy of 17%, i.e., over 50 times more likely than by random. Second, we employ the prediction model to combine lines from existing songs, producing lyrics with rhyme and a meaning. An evaluation of the produced lyrics shows that in terms of quantitative rhyme density, the method outperforms the best human rappers by 21%. The rap lyrics generator has been deployed as an online tool called DeepBeat, and the performance of the tool has been assessed by analyzing its usage logs. This analysis shows that machine-learned rankings correlate with user preferences.},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {195–204},
numpages = {10},
keywords = {deep learning, information retrieval, lyrics generation, natural language processing, rankSVM, rap},
location = {San Francisco, California, USA},
series = {KDD '16}
}

@inproceedings{10.1145/2837689.2837703,
author = {Brun, Geoffrey and Domingu\`{e}s, Catherine and Van Damme, M.-D.},
title = {TEXTOMAP: determining geographical window for texts},
year = {2015},
isbn = {9781450339377},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2837689.2837703},
doi = {10.1145/2837689.2837703},
abstract = {In newspapers or scholar manuals, numerous texts are accompanied by maps. In these map/text couples, maps give a spatial portrayal of the text issues, thus they make the spatial issues easier to understand. TEXTOMAP aims to design the geographical window of the text, based on the notion of important toponyms according to text issues. The important toponym selection is based on indicators which may be spatial, linguistic or semantic. Examples of geographical window calculation are shown and compared with the corresponding CLAVIN geographical focus. The work is in progress and perspectives are offered.},
booktitle = {Proceedings of the 9th Workshop on Geographic Information Retrieval},
articleno = {17},
numpages = {2},
keywords = {gazetteer, geographic tagging, geographical window, information retrieval, natural language processing},
location = {Paris, France},
series = {GIR '15}
}

@inproceedings{10.1109/WI-IAT.2013.168,
author = {Fernandes, Paulo and Furquim, Luis O. C. and Lopes, Lucelene},
title = {A Supervised Method to Enhance Vocabulary with the Creation of Domain Specific Lexica},
year = {2013},
isbn = {9780769551456},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI-IAT.2013.168},
doi = {10.1109/WI-IAT.2013.168},
abstract = {This paper proposes a method to enhance lexica by processing domain specific corpora. The proposed method relies on the identification of the more relevant unknown terms in each domain corpus. The innovative points of the proposed approach is to automatically detect unknown terms using MTMDD technology to handle lexical structures, and to automatically rank and identify domain specific terms using gini and tf-dcf indices. The proposed method is experimented in six corpora in order to illustrate its benefits.},
booktitle = {Proceedings of the 2013 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT) - Volume 03},
pages = {139–142},
numpages = {4},
keywords = {information retrieval, natural language processing, term extraction},
series = {WI-IAT '13}
}

@article{10.1109/TASLP.2016.2544661,
author = {Zhou, Guangyou and Xie, Zhiwen and He, Tingting and Zhao, Jun and Hu, Xiaohua Tony},
title = {Learning the multilingual translation representations for question retrieval in community question answering via non-negative matrix factorization},
year = {2016},
issue_date = {July 2016},
publisher = {IEEE Press},
volume = {24},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2544661},
doi = {10.1109/TASLP.2016.2544661},
abstract = {Community question answering (CQA) has become an increasingly popular research topic. In this paper, we focus on the problem of question retrieval. Question retrieval in CQA can automatically find the most relevant and recent questions that have been solved by other users. However, the word ambiguity and word mismatch problems bring about new challenges for question retrieval in CQA. State-of-the-art approaches address these issues by implicitly expanding the queried questions with additional words or phrases using monolingual translation models. While useful, the effectiveness of these models is highly dependent on the availability of quality parallel monolingual corpora (e.g., question--answer pairs) in the absence of which they are troubled by noise issues. In this work, we propose an alternative way to address the word ambiguity and word mismatch problems by taking advantage of potentially rich semantic information drawn from other languages. Our proposed method employs statistical machine translation to improve question retrieval and enriches the question representation with the translated words from other languages via non-negative matrix factorization. Experiments conducted on real CQA data sets show that our proposed approach is promising.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1305–1314},
numpages = {10},
keywords = {text mining, question retrieval, natural language processing, information retrieval, community question answering}
}

@inproceedings{10.1145/2663712.2666187,
author = {Cotelo, Santiago and Makowski, Alejandro and Chiruzzo, Luis and Wonsever, Dina},
title = {Documents Search Using Semantics Criteria},
year = {2014},
isbn = {9781450313650},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2663712.2666187},
doi = {10.1145/2663712.2666187},
abstract = {Current Information Retrieval systems generally search documents using a keywords model, which is often not expressive enough for the user. In this paper we describe some directions for improving an Information Retrieval system by letting the user specify different semantics constraints in her query, using a language based on a simplified version of first-order logic. The user can write queries that express the association between objects and attributes, temporal constraints and negation of attributes, and also perform synonyms expansion of queries. In order to evaluate the relevance of a candidate document with respect to the query, the dependency parse tree of the document is used, as well as other linguistic resources. The system was evaluated using a set of queries and a corpus extracted from the British newspaper The Times. The results are compared against the newspaper's own search engine and they look promising, showing an important improvement in precision in the first documents returned by the query.},
booktitle = {Proceedings of the 7th International Workshop on Exploiting Semantic Annotations in Information Retrieval},
pages = {5–7},
numpages = {3},
keywords = {semantics, query language, natural language processing, information retrieval, dependency parsing},
location = {Shanghai, China},
series = {ESAIR '14}
}

@inproceedings{10.1145/3269206.3274273,
author = {Vazirgiannis, Michalis and Malliaros, Fragkiskos D. and Nikolentzos, Giannis},
title = {GraphRep: Boosting Text Mining, NLP and Information Retrieval with Graphs},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3274273},
doi = {10.1145/3269206.3274273},
abstract = {Graphs have been widely used as modeling tools in Natural Language Processing (NLP), Text Mining (TM) and Information Retrieval (IR). Traditionally, the unigram bag-of-words representation is applied; that way, a document is represented as a multiset of its terms, disregarding dependencies between the terms. Although several variants and extensions of this modeling approach have been proposed, the main weakness comes from the underlying term independence assumption; the order of the terms within a document is completely disregarded and any relationship between terms is not taken into account in the final task. To deal with this problem, the research community has explored various representations, and to this direction, graphs constitute a well-developed model for text representation. The goal of this tutorial is to offer a comprehensive presentation of recent methods that rely on graph-based text representations to deal with various tasks in Text Mining, NLP and IR.},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {2295–2296},
numpages = {2},
keywords = {text streams, text categorisation, natural language processing, information retrieval, graph of words, graph mining, extractive summarisation, event detection twitter, deep learning},
location = {Torino, Italy},
series = {CIKM '18}
}

@inproceedings{10.1145/3219819.3220086,
author = {Tay, Yi and Luu, Anh Tuan and Hui, Siu Cheung},
title = {Multi-Pointer Co-Attention Networks for Recommendation},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3220086},
doi = {10.1145/3219819.3220086},
abstract = {Many recent state-of-the-art recommender systems such as D-ATT, TransNet and DeepCoNN exploit reviews for representation learning. This paper proposes a new neural architecture for recommendation with reviews. Our model operates on a multi-hierarchical paradigm and is based on the intuition that not all reviews are created equal, i.e., only a selected few are important. The importance, however, should be dynamically inferred depending on the current target. To this end, we propose a review-by-review pointer-based learning scheme that extracts important reviews from user and item reviews and subsequently matches them in a word-by-word fashion. This enables not only the most informative reviews to be utilized for prediction but also a deeper word-level interaction. Our pointer-based method operates with a gumbel-softmax based pointer mechanism that enables the incorporation of discrete vectors within differentiable neural architectures. Our pointer mechanism is co-attentive in nature, learning pointers which are co-dependent on user-item relationships. Finally, we propose a multi-pointer learning scheme that learns to combine multiple views of user-item interactions. We demonstrate the effectiveness of our proposed model via extensive experiments on 24 benchmark datasets from Amazon and Yelp. Empirical results show that our approach significantly outperforms existing state-of-the-art models, with up to 19% and 71% relative improvement when compared to TransNet and DeepCoNN respectively. We study the behavior of our multi-pointer learning mechanism, shedding light on 'evidence aggregation' patterns in review-based recommender systems.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {2309–2318},
numpages = {10},
keywords = {review-based recommender systems, review rating prediction, recommendation, pointer networks, natural language processing, information retrieval, deep learning, collaborative filtering, attention mechanism},
location = {London, United Kingdom},
series = {KDD '18}
}

@inproceedings{10.1145/3123779.3123792,
author = {Berta, Peter and Bystrick\'{y}, Michal and Krempask\'{y}, Michal and Vrani\'{c}, Valentino},
title = {Employing issues and commits for in-code sentence based use case identification and remodularization},
year = {2017},
isbn = {9781450348430},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3123779.3123792},
doi = {10.1145/3123779.3123792},
abstract = {Use case driven modularization improves code comprehension and maintenance and provides another view on software alongside object-oriented modularization. However, approaches enabling use case driven modularization require to modularize code manually. In this paper, we propose an approach to employing issues and commits for in-code sentence based use case identification and remodularization. The approach aims at providing use case based perspective on the existing code. The sentences of use case steps are compared to sentences of issue descriptions, while the sentences generated from the source code of issue commits are compared to sentences generated from the corresponding methods in source code in order to quantify the similarity between use case steps and methods in source code using different similarity calculation algorithms. The resulting level of similarity is used to remodularize source code according to use cases. We conducted a study on the OpenCart open source e-shop employing 16 use cases. The approach achieved the recall of 3.37% and precision of 75%. The success of the approach strongly depends on issues and commits assigned to them. The results would be better especially for the code that natively employs use case driven modularization.},
booktitle = {Proceedings of the Fifth European Conference on the Engineering of Computer-Based Systems},
articleno = {1},
numpages = {8},
keywords = {use case, traceability links, text similarity, remodularization, natural language processing, modularization, intent, information retrieval, aspect-oriented programming, DCI},
location = {Larnaca, Cyprus},
series = {ECBS '17}
}

@inproceedings{10.1145/2505515.2505810,
author = {Liu, Xiaozhong and Chen, Miao and Ding, Ying and Song, Min},
title = {Workshop summary for the 2013 international workshop on mining unstructured big data using natural language processing},
year = {2013},
isbn = {9781450322638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2505515.2505810},
doi = {10.1145/2505515.2505810},
booktitle = {Proceedings of the 22nd ACM International Conference on Information &amp; Knowledge Management},
pages = {2547–2548},
numpages = {2},
keywords = {big data, information retrieval, natural language processing, text mining},
location = {San Francisco, California, USA},
series = {CIKM '13}
}

@inproceedings{10.1145/2505515.2508215,
author = {Gubanov, Michael and Pyayt, Anna},
title = {READFAST: high-relevance search-engine for big text},
year = {2013},
isbn = {9781450322638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2505515.2508215},
doi = {10.1145/2505515.2508215},
abstract = {Relevance of search-results is a key factor for any search engine. In order to return and rank the Web-pages that are most relevant to the query, contemporary search engines use complex ranking functions that depend on hundreds of features. For example, presence or absence of the query keywords on the page, their proximity, frequencies, HTML markup are just a few to name. Additional features might include fonts, tags, hyperlinks, metadata, and parts of the Web-page description. All this information is used by the search-engine to rank HTML Web pages returned to the user, but is unfortunately absent in free text that has no HTML markup, tags, hyperlinks, and any other metadata, except implicit natural language structure.Here we demonstrate one of the first Big text search engines that leverages hidden structure of the natural language sentences in order to process user queries and return more relevant search-results than a standard keyword-search. It provides a structured index extracted from the text using Natural Language Processing (NLP) that can be used to browse and query free text.},
booktitle = {Proceedings of the 22nd ACM International Conference on Information &amp; Knowledge Management},
pages = {2465–2468},
numpages = {4},
keywords = {data integration, information retrieval, natural language processing, search, structure extraction},
location = {San Francisco, California, USA},
series = {CIKM '13}
}

@inproceedings{10.1145/3342827.3342830,
author = {Lin, Nay and A, Kudinov Vitaly and Soe, Yan Naing},
title = {Text Compression for Myanmar Information Retrieval},
year = {2019},
isbn = {9781450362795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342827.3342830},
doi = {10.1145/3342827.3342830},
abstract = {Myanmar word segmentation is an important task for construction of dictionary file for Myanmar information retrieval and Myanmar text compression. Although Myanmar word segmentation using dictionary and orthography has been existed for Myanmar language, the performance of word segmentation depends on the coverage of the dictionary and training dataset and can cause out of vocabulary (OOV) problem, leading to lower precision and recall in information retrieval. And to compress Myanmar text, words in text needs to be recognized first. In this paper, we propose a new method for Myanmar word segmentation by local statistical dataset without the use of any additional data (e.g., training corpus) and new compressed Myanmar Information Retrieval (MIR) model which used End Tagged Dense Code (ETDC) text compressed method. The experimental results showed that the method can improve evaluation of vocabulary file with precision 75%, recall 87%, F-measure 80% and average compression ratio is 32% of texts for Myanmar language.},
booktitle = {Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval},
pages = {62–67},
numpages = {6},
keywords = {vocabulary file, indexing, Text Compression, Myanmar information retrieval, Myanmar Natural Language Processing, ETDC, Boyer Moore pattern matching},
location = {Tokushima, Japan},
series = {NLPIR '19}
}

@article{10.1145/2522920.2522930,
author = {Mcmillan, Collin and Poshyvanyk, Denys and Grechanik, Mark and Xie, Qing and Fu, Chen},
title = {Portfolio: Searching for relevant functions and their usages in millions of lines of code},
year = {2013},
issue_date = {October 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/2522920.2522930},
doi = {10.1145/2522920.2522930},
abstract = {Different studies show that programmers are more interested in finding definitions of functions and their uses than variables, statements, or ordinary code fragments. Therefore, developers require support in finding relevant functions and determining how these functions are used. Unfortunately, existing code search engines do not provide enough of this support to developers, thus reducing the effectiveness of code reuse. We provide this support to programmers in a code search system called Portfolio that retrieves and visualizes relevant functions and their usages. We have built Portfolio using a combination of models that address surfing behavior of programmers and sharing related concepts among functions. We conducted two experiments: first, an experiment with 49 C/C++ programmers to compare Portfolio to Google Code Search and Koders using a standard methodology for evaluating information-retrieval-based engines; and second, an experiment with 19 Java programmers to compare Portfolio to Koders. The results show with strong statistical significance that users find more relevant functions with higher precision with Portfolio than with Google Code Search and Koders. We also show that by using PageRank, Portfolio is able to rank returned relevant functions more efficiently.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = oct,
articleno = {37},
numpages = {30},
keywords = {user studies, natural language processing, information retrieval, Source-code search, Pagerank}
}

@inproceedings{10.1145/1882992.1883065,
author = {D'Avolio, Leonard W. and Nguyen, Thien and Fiore, Louis},
title = {The automated retrieval console (ARC): open source software for streamlining the process of natural language processing},
year = {2010},
isbn = {9781450300308},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1882992.1883065},
doi = {10.1145/1882992.1883065},
abstract = {Open source natural language processing (NLP) frameworks have made it easier for NLP developers and researchers to develop more reusable and modular components and to capitalize on the work of others. With the Automated Retrieval Console (ARC) we attempt to build upon this foundation by streamlining the many processes surrounding the development, evaluation, and deployment of natural language processing technologies. Toward this end, ARC offers graphical user interfaces to facilitate corpus import, reference set creation, annotation, and inter-annotator agreement calculation. To speed task-specific information extraction development, ARC combines NLP-generated features from UIMA pipelines with machine learning classifiers and calculates performance statistics against a reference set. We also use ARC to explore automated algorithm creation for specific information extraction tasks in an effort to reduce the need for custom code and rules development. We present a detailed description of the ideas implemented in this proof-of-concept and a brief overview of two empirical evaluations.},
booktitle = {Proceedings of the 1st ACM International Health Informatics Symposium},
pages = {469–473},
numpages = {5},
keywords = {natural language processing, medical informatics, information retrieval},
location = {Arlington, Virginia, USA},
series = {IHI '10}
}

@inproceedings{10.1145/3339252.3339282,
author = {Mahaini, Mohamad Imad and Li, Shujun and Sa\u{g}lam, Rahime Belen},
title = {Building Taxonomies based on Human-Machine Teaming: Cyber Security as an Example},
year = {2019},
isbn = {9781450371643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3339252.3339282},
doi = {10.1145/3339252.3339282},
abstract = {Taxonomies and ontologies are handy tools in many application domains such as knowledge systematization and automatic reasoning. In the cyber security field, many researchers have proposed such taxonomies and ontologies, most of which were built based on manual work. Some researchers proposed the use of computing tools to automate the building process, but mainly on very narrow sub-areas of cyber security. Thus, there is a lack of general cyber security taxonomies and ontologies, possibly due to the difficulties of manually curating keywords and concepts for such a diverse, inter-disciplinary and dynamically evolving field.This paper presents a new human-machine teaming based process to build taxonomies, which allows human experts to work with automated natural language processing (NLP) and information retrieval (IR) tools to co-develop a taxonomy from a set of relevant textual documents. The proposed process could be generalized to support non-textual documents and to build (more complicated) ontologies as well. Using the cyber security as an example, we demonstrate how the proposed taxonomy building process has allowed us to build a general cyber security taxonomy covering a wide range of data-driven keywords (topics) with a reasonable amount of human effort.},
booktitle = {Proceedings of the 14th International Conference on Availability, Reliability and Security},
articleno = {30},
numpages = {9},
keywords = {visualization, taxonomy, ontology, online social network (OSN), natural language processing (NLP), knowledge representation, information retrieval (IR), cyber security, Twitter},
location = {Canterbury, CA, United Kingdom},
series = {ARES '19}
}

@inproceedings{10.1145/1900008.1900068,
author = {Wilson, Dale-Marie and Martin, Aqueasha M. and Gilbert, Juan E.},
title = { 'How may I help you'-spoken queries for technical assistance},
year = {2010},
isbn = {9781450300643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1900008.1900068},
doi = {10.1145/1900008.1900068},
abstract = {Spoken dialog systems, including interactive assistants, have emerged as a viable option for presenting technical communication. Thus has contributed to interests in improving the effectiveness and design of such systems through natural language. Traditional methods of natural language processing include parts-of-speech tagging, syntactic parsing, and statistical models. This paper introduces a new conversational question answering methodology, Answer First (A1) that bypasses traditional methods and removes the need for preprocessing of queries.},
booktitle = {Proceedings of the 48th Annual ACM Southeast Conference},
articleno = {43},
numpages = {6},
keywords = {information retrieval, natural language processing, technical communication},
location = {Oxford, Mississippi},
series = {ACMSE '10}
}

@inproceedings{10.1145/2533888.2533942,
author = {Karimzadeh, Morteza and Huang, Wenyi and Banerjee, Siddhartha and Wallgr\"{u}n, Jan Oliver and Hardisty, Frank and Pezanowski, Scott and Mitra, Prasenjit and MacEachren, Alan M.},
title = {GeoTxt: a web API to leverage place references in text},
year = {2013},
isbn = {9781450322416},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2533888.2533942},
doi = {10.1145/2533888.2533942},
abstract = {Associating place name mentions in unstructured text with their actual references in geographic space is vital to enable spatial queries and analysis. In this paper, we introduce GeoTxt, a web API plus human-usable web tool designed and implemented to tackle three components of place-reference processing from text, namely: extraction, disambiguation, and geolocation of place names mentioned in unstructured text. Current GeoTxt development is focused particularly on support for processing short microblog posts.},
booktitle = {Proceedings of the 7th Workshop on Geographic Information Retrieval},
pages = {72–73},
numpages = {2},
keywords = {natural language processing, geographic information systems, geographic information retrieval, geocoding},
location = {Orlando, Florida},
series = {GIR '13}
}

@inproceedings{10.1145/2389686.2389706,
author = {Gal\'{a}n Garc\'{\i}a, Patxi and Laorden Gom\'{e}z, Carlos and Garc\'{\i}a Bringas, Pablo},
title = {Towards a more efficient and personalised advertisement content in on-line social networks},
year = {2012},
isbn = {9781450317191},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2389686.2389706},
doi = {10.1145/2389686.2389706},
abstract = {Knowing what potential clients want, is the most important issue for companies. The current situation of social communication is generating a lot of information about users, such as favourite sites, food, politic tendencies, hopes or needs. This information has an incalculable value for marketing interests but, unfortunately, it is not trivial to process it. One way to obtain this information without disturbing the users is to store their searches, used words on the Internet and clicked items, to construct specific profiles. But, the problem with these techniques is that they do not retrieve current information about the targeted user because they gather information that may or may not be up to date. In light of this background, we propose a methodology to obtain up to date information about user's interests, likes and needs by analysing users conversations in social networks and instant messaging systems to generate personalised and interesting advertisement with better impact and higher success rates.},
booktitle = {Proceedings of the 5th Ph.D. Workshop on Information and Knowledge},
pages = {95–98},
numpages = {4},
keywords = {social networks, persoanlised advertisement, natural language processing, marketing, information retrieval, conversations},
location = {Maui, Hawaii, USA},
series = {PIKM '12}
}

@inproceedings{10.1109/WI-IAT.2011.187,
author = {Adindla, Suma and Kruschwitz, Udo},
title = {Combining the Best of Two Worlds: NLP and IR for Intranet Search},
year = {2011},
isbn = {9780769545134},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI-IAT.2011.187},
doi = {10.1109/WI-IAT.2011.187},
abstract = {Natural language processing (NLP) is becoming much more robust and applicable in realistic applications. One area in which NLP has still not been fully exploited is information retrieval (IR). In particular we are interested in search over intranets and other local Web sites. We see dialogue-driven search which is based on a largely automated knowledge extraction process as one of the next big steps. Instead of replying with a set of documents for a user query the system would allow the user to navigate through the extracted knowledge base by making use of a simple dialogue manager. Here we support this idea with a first task-based evaluation that we conducted on a university intranet. We automatically extracted entities like person names, organizations and locations as well as relations between entities and added visual graphs to the search results whenever a user query could be mapped into this knowledge base. We found that users are willing to interact and use those visual interfaces. We also found that users preferred such a system that guides a user through the result set over a baseline approach. The results represent an important first step towards full NLP-driven intranet search.},
booktitle = {Proceedings of the 2011 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology - Volume 01},
pages = {483–488},
numpages = {6},
keywords = {visualization, natural language processing, information retrieval, domain knowledge, dialogue},
series = {WI-IAT '11}
}

@inproceedings{10.1145/2479787.2479796,
author = {Tahrat, Sabiha and Kergosien, Eric and Bringay, Sandra and Roche, Mathieu and Teisseire, Maguelonne},
title = {Text2Geo: from textual data to geospatial information},
year = {2013},
isbn = {9781450318501},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2479787.2479796},
doi = {10.1145/2479787.2479796},
abstract = {In this paper, we focus on methods for extracting spatial information in text documents. After presenting textual description of space and manual annotation of named entities, mainly location and organization, we present our proposal Text2Geo. It is a hybrid method which combines information extraction approach based on patterns with a supervised classification approach to explore context. We discuss some results obtained on the dataset of Thau lagoon.},
booktitle = {Proceedings of the 3rd International Conference on Web Intelligence, Mining and Semantics},
articleno = {23},
numpages = {4},
keywords = {supervised classification, spatial information, natural language processing, geographic information retrieval},
location = {Madrid, Spain},
series = {WIMS '13}
}

@inproceedings{10.1145/1316874.1316876,
author = {Lease, Matthew},
title = {Natural language processing for information retrieval: the time is ripe (again)},
year = {2007},
isbn = {9781595938329},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1316874.1316876},
doi = {10.1145/1316874.1316876},
abstract = {Paraphrasing van Rijsbergen [37], the time is ripe for another attempt at using natural language processing (NLP) for information retrieval (IR). This paper introduces my dissertation study, which will explore methods for integrating modern NLP with state-of-the-art IR techniques. In addition to text, I will also apply retrieval to conversational speech data, which poses a unique set of considerations in comparison to text. Greater use of NLP has potential to improve both text and speech retrieval.},
booktitle = {Proceedings of the ACM First Ph.D. Workshop in CIKM},
pages = {1–8},
numpages = {8},
keywords = {natural language processing, information retrieval},
location = {Lisbon, Portugal},
series = {PIKM '07}
}

@inproceedings{10.1145/2533888.2533941,
author = {Wang, Wei and Stewart, Kathleen},
title = {Extracting spatiotemporal and semantic events from documents},
year = {2013},
isbn = {9781450322416},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2533888.2533941},
doi = {10.1145/2533888.2533941},
abstract = {The methods demonstrated through this work support automated annotation of spatiotemporal semantic event information associated with natural hazards, such as power outages, cancellations, and emergency response associated with hurricanes, floods, or drought as reported in web news reports.},
booktitle = {Proceedings of the 7th Workshop on Geographic Information Retrieval},
pages = {17–18},
numpages = {2},
keywords = {spatiotemporal information retrieval, semantics, natural language processing, hazard ontology, gazetteers, GIS},
location = {Orlando, Florida},
series = {GIR '13}
}

@inproceedings{10.1145/1871888.1871898,
author = {Andersson, Linda},
title = {A vector space analysis of swedish patent claims with different linguistic indices},
year = {2010},
isbn = {9781450303842},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1871888.1871898},
doi = {10.1145/1871888.1871898},
abstract = {The purpose of this study was twofold, first to examine if it is possible to use a general automatic retrieval model, the Vector Space Model (VSM), in order to discover similarities between Swedish patent claims; and second to examine whether an addition morphological decompounding module at the pre-processing level improves the result. In the present study, a comparison between three different topic sets consisting of patent claims was compared against an entire collection of 30,117 claims. The VSM was evaluated with and without additional morphological decompounding modules. The results indicate that decompounding will influence the performance of the retrieval model in a positive way. However, the sublanguage of patent claims and the errors made during the Optical Character Recognition (OCR) process were harmful towards the overall performance of the Natural Language Processing (NLP) applications as well as for the retrieval model.},
booktitle = {Proceedings of the 3rd International Workshop on Patent Information Retrieval},
pages = {47–56},
numpages = {10},
keywords = {patent retrieval, natural language processing, lemmatization, information retrieval, decompounding},
location = {Toronto, ON, Canada},
series = {PaIR '10}
}

@inproceedings{10.1109/WI-IAT.2011.226,
author = {Bonnefoy, Ludovic and Bellot, Patrice and Benoit, Michel},
title = {The Web as a Source of Evidence for Filtering Candidate Answers to Natural Language Questions},
year = {2011},
isbn = {9780769545134},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI-IAT.2011.226},
doi = {10.1109/WI-IAT.2011.226},
abstract = {Identifying and extracting named entities from web pages has been the subject of many researches. In this paper, we propose and evaluate some new unsupervised language modeling approaches to determine the membership level of a candidate answer, a named entity, to a natural language question to a very fine-grained conceptual class of entity. We propose to address this issue by using the Web or DBPedia hierarchy as sources of evidence. Then, this level of membership can be used to improve the ranking of candidate answers in a question-answering task. Lastly, we present the results we obtained by participating in TREC 2010 Entity track.},
booktitle = {Proceedings of the 2011 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology - Volume 01},
pages = {63–66},
numpages = {4},
keywords = {TREC Entity, Question-Answering, Natural language processing, Information retrieval, Information filtering},
series = {WI-IAT '11}
}

@inproceedings{10.1109/WIIAT.2008.278,
author = {Ruiz-Mart\'{\i}nez, Juana M. and Mi\~{n}arro-Gim\'{e}nez, Jos\'{e} A. and Guill\'{e}n-C\'{a}rceles, Laura and Castellanos-Nieves, Dagoberto and Valencia-Garc\'{\i}a, Rafael and Garc\'{\i}a-S\'{a}nchez, Francisco and Fern\'{a}ndez-Breis, Jesualdo T. and Mart\'{\i}nez-B\'{e}jar, Rodrigo},
title = {Populating Ontologies in the eTourism Domain},
year = {2008},
isbn = {9780769534961},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WIIAT.2008.278},
doi = {10.1109/WIIAT.2008.278},
abstract = {The Semantic Web vision is based on structuring the knowledge that is present in the current Web so that it is understandable by machines without human intervention. Ontologies are the backbone technology for the Semantic Web. Thus, the realization of the Semantic Web vision largely depends on the design and instantiation of ontologies. While several methodologies for designing ontologies and automating ontology learning have been proposed, ontology population has not received much attention so far. This paper presents a methodology for populating ontologies from natural language web documents. For this, Semantic Web Technologies and Natural Language Technologies have been used. This approach has been applied in the eTourism domain.},
booktitle = {Proceedings of the 2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology - Volume 03},
pages = {316–319},
numpages = {4},
keywords = {ontology population, natural language processing, information retrieval},
series = {WI-IAT '08}
}

@inproceedings{10.1145/2110363.2110467,
author = {Zhang, Rui and Pakhomov, Serguei and Melton, Genevieve B.},
title = {Automated identification of relevant new information in clinical narrative},
year = {2012},
isbn = {9781450307819},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2110363.2110467},
doi = {10.1145/2110363.2110467},
abstract = {The ability to explore and visualize clinical information is important for clinicians when reviewing and cognitively synthesizing electronic clinical documents for new patients contained in electronic health record (EHR) systems. In this study, we explore the use of language models for detecting new and potentially relevant information within an individual patient's collection of clinical documents using an expert-based reference standard for evaluation. We achieved good accuracy with a heterogeneous system based on a modified n-gram language model with statistically-derived and classic stop word removal and lexical normalization, as well as heuristic rules. This technique also identified relevant new information not identified with the expert-derived reference standard. These methods appear promising for providing an automated means to improve the use of electronic documents by clinicians.},
booktitle = {Proceedings of the 2nd ACM SIGHIT International Health Informatics Symposium},
pages = {837–842},
numpages = {6},
keywords = {natural language processing, n-gram model, information retrieval, information redundancy, electronic health record},
location = {Miami, Florida, USA},
series = {IHI '12}
}

@inproceedings{10.1145/3003733.3003775,
author = {Ntais, Georgios and Saroukos, Spyridon and Berki, Eleni and Dalianis, Hercules},
title = {Development and Enhancement of a Stemmer for the Greek Language},
year = {2016},
isbn = {9781450347891},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3003733.3003775},
doi = {10.1145/3003733.3003775},
abstract = {Although there are three stemmers published for the Greek language, only the one presented in this paper and called Ntais' stemmer is freely open and available, together with its enhancements and extensions according to Saroukos' algorithm. The primary algorithm (Ntais' algorithm) uses only capital letters and works with better performance than other past stemming algorithms for the Greek language, giving 92.1 percent correct results. Further extensions of the proposed stemming system (e.g. from capital to small letters) and more evaluation methods are presented according to a new and improved algorithm, Saroukos' algorithm. Stemmer performance metrics are further used for evaluating the existing stemming system and algorithm and show how its accuracy and completeness are enhanced. The improvements were possible by providing an alternative implementation in the programming language PHP, which offers more syntactical rules and exceptions. The two versions of the stemming algorithm are tested and compared.},
booktitle = {Proceedings of the 20th Pan-Hellenic Conference on Informatics},
articleno = {3},
numpages = {4},
keywords = {stemmer metrics, performance evaluation metrics, Stemming algorithm, Natural Language Processing (NLP), Information Retrieval (IR), Greek language},
location = {Patras, Greece},
series = {PCI '16}
}

@inproceedings{10.1145/1523103.1523117,
author = {Dwivedi, S. K. and Rastogi, Parul},
title = {Critical analysis of WSD algorithms},
year = {2009},
isbn = {9781605583518},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1523103.1523117},
doi = {10.1145/1523103.1523117},
abstract = {Word Sense Disambiguation is the core of many natural language processing (NLP) tasks. WSD is a significant technique and has been an interest and concern for the various researchers from the long time. WSD is considered as a successful technique in the last several years. Though success rate of WSD technique is quiet high it has certain limitation of unavailability of standardized evaluation system. The basic aim of the paper is to investigate the critical aspects of various WSD approaches. The paper will also provide a detail of the success rate of the various approaches and also their usage in various application areas.},
booktitle = {Proceedings of the International Conference on Advances in Computing, Communication and Control},
pages = {62–67},
numpages = {6},
keywords = {word sense disambiguation, synsets, natural language processing, information retrieval},
location = {Mumbai, India},
series = {ICAC3 '09}
}

@inproceedings{10.1145/2640087.2644192,
author = {Lu, Yang and Fang, Xing and Zhan, Justin},
title = {Data Readiness Level for Unstructured Data},
year = {2014},
isbn = {9781450328913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2640087.2644192},
doi = {10.1145/2640087.2644192},
abstract = {When time or computational resources is a constraint, dealing with large amount of data can be painful for many organizations. In this paper, we proposed a new concept called Data Readiness Level(DRL). It will measures the readiness of a file very quickly. We define readiness as ready for immediate analytical purposes. DRL is pair-wised measure, one is PKSS value, another is PVI value. One can see PKSS as the degree of similarity to the objective. PVI exhibits how much valuable information the file has. In the real word, not every data contains valuable information. Even if it is, it is not guaranteed that it will be relevant to the objective. With the aid of DRL, users can simply analyze those data with higher DRL values when time is a constraint. We collected 40,000 PDF files by hand from IEEE Xplore digital library and spriger. The experimental result was quite encouraging.},
booktitle = {Proceedings of the 2014 International Conference on Big Data Science and Computing},
articleno = {36},
numpages = {2},
keywords = {WordNet, Unstructured Data, Semantic Similarity, Normalization, Natural Language Processing(NLP), NLTK, Information Retrieval, Data Readiness Level},
location = {Beijing, China},
series = {BigDataScience '14}
}

@inproceedings{10.1145/2640087.2644160,
author = {Lu, Yang and Fang, Xing and Zhan, Justin},
title = {Data Readiness Level For Unstructured Data With A Focus On Unindexed Text Data},
year = {2014},
isbn = {9781450328913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2640087.2644160},
doi = {10.1145/2640087.2644160},
abstract = {As we entered into the so called big data era, the amount of data organizations are dealing with are greater than ever before. When time or computational resources is a constraint, dealing with large amount of data can be painful for many organizations. In this paper, we proposed a new concept called Data Readiness Level(DRL). It will measures the readiness of a file very quickly. We define readiness as ready for immediate analytical purposes. DRL is pair-wised measure, one is PKSS value, another is PVI value. One can see PKSS as the degree of similarity to the objective. PVI exhibits how much valuable information the file has. In the real word, not every data contains valuable information. Even if it is, it is not guaranteed that it will be relevant the objective. For example, an atmospheric data from NOAA often times contain many valuable information. However, if our objective is to search psychology literatures, then the atmospheric data is not what we really want. With the aid of DRL, users can simply analyze those data with higher DRL values when time is a constraint. We collected 40,000 PDF files by hand from IEEE Xplore digital library. Overall, the experimental result was quite remarkable.},
booktitle = {Proceedings of the 2014 International Conference on Big Data Science and Computing},
articleno = {15},
numpages = {4},
keywords = {WordNet, Unstructured Data, Semantic Similarity, Normalization, Natural Language Processing(NLP), NLTK, Information Retrieval, Data Readiness Level},
location = {Beijing, China},
series = {BigDataScience '14}
}

@inproceedings{10.1145/1980022.1980174,
author = {Modi, A. and Bhandari, A. and Desai, K. and Shah, N.},
title = {Smart search engine using artificial intelligence},
year = {2011},
isbn = {9781450304498},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1980022.1980174},
doi = {10.1145/1980022.1980174},
abstract = {Due to the increasing information on the Web it becomes very difficult for a single search engine to provide comprehensive Web coverage, thus increasing the difficulty for an average user to search information on the web. We have proposed an idea for smart Meta search engine which will have a more comprehensive coverage of the web by targeting different search engines for different search categories. The user's search query will be modified using a knowledge base and appropriate search engine will be selected. The results returned from these search engines will be merged and ranked. These results will be classified into various categories using a classification algorithm and then the result will be displayed sorted into categories. It will be beneficial to the user who has an obscure idea about the information he/she wanted to search.},
booktitle = {Proceedings of the International Conference &amp; Workshop on Emerging Trends in Technology},
pages = {707–710},
numpages = {4},
keywords = {meta search engine, information retrieval, classification, artificial intelligence},
location = {Mumbai, Maharashtra, India},
series = {ICWET '11}
}

@inproceedings{10.1145/1568296.1568315,
author = {Subramaniam, L. Venkata and Roy, Shourya and Faruquie, Tanveer A. and Negi, Sumit},
title = {A survey of types of text noise and techniques to handle noisy text},
year = {2009},
isbn = {9781605584966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1568296.1568315},
doi = {10.1145/1568296.1568315},
abstract = {Often, in the real world noise is ubiquitous in text communications. Text produced by processing signals intended for human use are often noisy for automated computer processing. Automatic speech recognition, optical character recognition and machine translation all introduce processing noise. Also digital text produced in informal settings such as online chat, SMS, emails, message boards, newsgroups, blogs, wikis and web pages contain considerable noise. In this paper, we present a survey of the existing measures for noise in text. We also cover application areas that ingest this noisy text for various tasks like Information Retrieval and Information Extraction.},
booktitle = {Proceedings of The Third Workshop on Analytics for Noisy Unstructured Text Data},
pages = {115–122},
numpages = {8},
keywords = {text mining, noisy text, natural language processing, information retrieval, information extraction},
location = {Barcelona, Spain},
series = {AND '09}
}

@inproceedings{10.1145/1096601.1096643,
author = {Vilares, Jes\'{u}s and G\'{o}mez-Rodr\'{\i}guez, Carlos and Alonso, Miguel A.},
title = {Managing syntactic variation in text retrieval},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096643},
doi = {10.1145/1096601.1096643},
abstract = {Information Retrieval systems are limited by the linguistic variation of language. The use of Natural Language Processing techniques to manage this problem has been studied for a long time, but mainly focusing on English. In this paper we deal with European languages, taking Spanish as a case in point. Two different sources of syntactic information, queries and documents, are studied in order to increase the performance of Information Retrieval systems.},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {162–164},
numpages = {3},
keywords = {shallow parsing, natural language processing, information retrieval},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/2824864.2824873,
author = {Mukherjee, Abhinav and Ravi, Anirudh and Datta, Kaustav},
title = {Mixed-script query labelling using supervised learning and ad hoc retrieval using sub word indexing},
year = {2014},
isbn = {9781450337557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2824864.2824873},
doi = {10.1145/2824864.2824873},
abstract = {Much of the user generated content on the internet is written in their transliterated form instead of in their indigenous script. Due to this search engines receive a large number of transliterated search queries.This paper presents our approach to handle labelling of queries and ad hoc retrieval of documents based on these queries, as part of the FIRE2014 shared task on transliterated search. The content of each document is written in either the native Devanagari script or its transliterated form in Roman script or a combination of both. The queries to retrieve these documents can also be in mixed script. The task is challenging primarily due to the spelling variations that occur in the transliterated form of search queries. This particular problem is addressed by using back transliteration to reduce spelling variations, and a set of hand-tailored rules for consonant mapping. Sub-word indexing is done to take care of breaking and joining of transliterated words. Implementation of query labelling of the mixed script content was done using a supervised learning approach where an SVM classifier was trained using character n-grams as features for language identification. A Na\"{\i}ve Bayes classifier was used for classifying transliterated words that can belong to both Hindi and English when looked at individually.The 2 runs submitted by our team (BITS-Lipyantaran) performs best across all metrics for Subtask 2 among all the teams that participated, with a MRR score of 0.8171 and MAP score of 0.6421.},
booktitle = {Proceedings of the 6th Annual Meeting of the Forum for Information Retrieval Evaluation},
pages = {86–90},
numpages = {5},
keywords = {Support Vector Machines, Supervised learning, Sub-word Indexing, Natural Language Processing, Na\"{\i}ve Bayes, Mixed-script information retrieval, Machine Learning, Language Modelling, Language Identification},
location = {Bangalore, India},
series = {FIRE '14}
}

@inproceedings{10.1145/956863.956913,
author = {Amitay, Einat and Nelken, Rani and Niblack, Wayne and Sivan, Ron and Soffer, Aya},
title = {Multi-resolution disambiguation of term occurrences},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956913},
doi = {10.1145/956863.956913},
abstract = {We describe a system for extracting mentions of terms such as company and product names, in a large and noisy corpus of documents, such as the World Wide Web. Since natural language terms are highly ambiguous, a significant challenge in this task is disambiguating which occurrences of each term are truly related to the right meaning, and which are not. We describe our approach for disambiguation, and show that it achieves very high accuracy with only limited training. This serves as a necessary first step for applications that strive to do analytics on term mentions.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {255–262},
numpages = {8},
keywords = {text mining, natural language processing, information retrieval, disambiguation},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/1283880.1283882,
author = {Roussey, Catherine and Calabretto, Sylvie and Harrathi, Farah},
title = {Multilingual extraction of semantic indexes},
year = {2007},
isbn = {9781595936684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1283880.1283882},
doi = {10.1145/1283880.1283882},
abstract = {This article deals with multilingual document indexing. We propose an indexing method based on several stages. First of all the most important terms of the document are extracted using general characteristics of languages and statistical methods. Thus, term extraction stages can be applied to any document whatever the document language is. Secondly, our indexing method uses a multilingual ontology in order to find the most relevant concepts representing the document content. Our method can be applied to a multilingual corpus containing document written in different languages. This indexing procedure is part of a Multilingual Document System untitled SyDoM, which manages XML documents.},
booktitle = {Proceedings of the 2007 International Workshop on Semantically Aware Document Processing and Indexing},
pages = {1–6},
numpages = {6},
keywords = {statistical analysis, natural language processing, multilingual corpora, information retrieval, domain ontology, automatic indexing},
location = {Montpellier, France},
series = {SADPI '07}
}

@article{10.1145/1194936.1194938,
author = {Oh, Jong-Hoon and Choi, Key-Sun and Isahara, Hitoshi},
title = {A machine transliteration model based on correspondence between graphemes and phonemes},
year = {2006},
issue_date = {September 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/1194936.1194938},
doi = {10.1145/1194936.1194938},
abstract = {Machine transliteration is an automatic method for converting words in one language into phonetically equivalent ones in another language. There has been growing interest in the use of machine transliteration to assist machine translation and information retrieval. Three types of machine transliteration models---grapheme-based, phoneme-based, and hybrid---have been proposed. Surprisingly, there have been few reports of efforts to utilize the correspondence between source graphemes and source phonemes, although this correspondence plays an important role in machine transliteration. Furthermore, little work has been reported on ways to dynamically handle source graphemes and phonemes. In this paper, we propose a transliteration model that dynamically uses both graphemes and phonemes, particularly the correspondence between them. With this model, we have achieved better performance---improvements of about 15 to 41% in English-to-Korean transliteration and about 16 to 44% in English-to-Japanese transliteration---than has been reported for other models.},
journal = {ACM Transactions on Asian Language Information Processing},
month = sep,
pages = {185–208},
numpages = {24},
keywords = {natural language processing, machine translation, information retrieval, grapheme and phoneme, Machine transliteration}
}

@inproceedings{10.1145/956863.956952,
author = {Srikanth, Munirathnam and Srihari, Rohini},
title = {Exploiting syntactic structure of queries in a language modeling approach to IR},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956952},
doi = {10.1145/956863.956952},
abstract = {Natural Language Processing (NLP) techniques have been explored to enhance the performance of Information Retrieval (IR) methods with varied results. Most efforts in using NLP techniques have been to identify better index terms for representing documents. This use in the indexing phase of IR has implicit effect on retrieval performance. However, the explicit use of NLP techniques during the retrieval or information seeking phase has been restricted to interactive or dialogue systems. Recent advances in IR are based on using Statistical Language Models (SLM) to represent documents and ranking them based on their model generating a given user query. This paper presents a novel method for using NLP techniques on user queries, specifically, a syntactic parse of a query, in the statistical language modeling approach to IR. In the proposed method, named Concept Language Models, a query is viewed as a sequence of concepts and a concept as a sequence terms. The paper presents different approximations to estimate the concept and term probabilities and compute the query likelihood estimate for documents. Some empirical results on TREC test collections comparing Concept Language Models with smoothed N-gram language models are presented.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {476–483},
numpages = {8},
keywords = {query processing, natural language processing, language models, information retrieval},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/2479787.2479829,
author = {Dal Mas, Massimiliano},
title = {Intelligent interactive multimedia system: layered ontological video contexts in a folksonomy driven environment},
year = {2013},
isbn = {9781450318501},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2479787.2479829},
doi = {10.1145/2479787.2479829},
abstract = {This paper describes a partially developed method on going project for enabling an Intelligent Interactive Multimedia System based on ontological interaction on video clip shown on ubiquitous systems as a computer monitor, mobile or tablet. The paper aims to sketch a theoretical framework on a method for extracting and tracking objects in videos, based on various semantic attributes. It tackles a novel space (cyber-physical) of interaction between human and machine, in a creative way. We use a layered representation based on semantics-driven information to obtain spatiotemporal attributes of objects. The interface is created by extracting object information from the video with a Human Based Computation to obtain a richer semantics of attribute to bridge the semantic gap between words describing an image and its visual features. Users can navigate and manipulate objects displayed on video by associating semantic attributes and comments evaluated by the data and sentiment extraction. Folksonomy tags are extracted from users' comments to be used in a dynamical driven system (Folksodriven). We show some example applications of the proposed method like: advertisement inside the objects displayed on a video, an interface based on objects of interest video navigation, mask layer on an object of interest and a visual interaction for Smart City.},
booktitle = {Proceedings of the 3rd International Conference on Web Intelligence, Mining and Semantics},
articleno = {42},
numpages = {9},
keywords = {semantics-driven information retrieval, recommender systems, natural language processing, knowledge-based application, intelligent information systems, human-oriented knowledge, human-computer interaction},
location = {Madrid, Spain},
series = {WIMS '13}
}

@inproceedings{10.1145/354756.354850,
author = {Elworthy, David},
title = {Retrieval from captioned image databases using natural language processing},
year = {2000},
isbn = {1581133200},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/354756.354850},
doi = {10.1145/354756.354850},
booktitle = {Proceedings of the Ninth International Conference on Information and Knowledge Management},
pages = {430–437},
numpages = {8},
keywords = {natural language processing, information retrieval, image databases},
location = {McLean, Virginia, USA},
series = {CIKM '00}
}

@inproceedings{10.1145/1008992.1009040,
author = {Amitay, Einat and Har'El, Nadav and Sivan, Ron and Soffer, Aya},
title = {Web-a-where: geotagging web content},
year = {2004},
isbn = {1581138814},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1008992.1009040},
doi = {10.1145/1008992.1009040},
abstract = {We describe Web-a-Where, a system for associating geography with Web pages. Web-a-Where locates mentions of places and determines the place each name refers to. In addition, it assigns to each page a geographic focus --- a locality that the page discusses as a whole. The tagging process is simple and fast, aimed to be applied to large collections of Web pages and to facilitate a variety of location-based applications and data analyses.Geotagging involves arbitrating two types of ambiguities: geo/non-geo and geo/geo. A geo/non-geo ambiguity occurs when a place name also has a non-geographic meaning, such as a person name (e.g., Berlin) or a common word (Turkey). Geo/geo ambiguity arises when distinct places have the same name, as in London, England vs. London, Ontario.An implementation of the tagger within the framework of the WebFountain data mining system is described, and evaluated on several corpora of real Web pages. Precision of up to 82% on individual geotags is achieved. We also evaluate the relative contribution of various heuristics the tagger employs, and evaluate the focus-finding algorithm using a corpus pretagged with localities, showing that as many as 91% of the foci reported are correct up to the country level.},
booktitle = {Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {273–280},
numpages = {8},
keywords = {text mining, natural language processing, information retrieval, geographic tagging, gazetteer, disambiguation},
location = {Sheffield, United Kingdom},
series = {SIGIR '04}
}

@inproceedings{10.5555/1065226.1065248,
author = {Shulman, Stuart and Hovy, Eduard and Callan, Jamie and Zavestoski, Stephen},
title = {Language processing technologies for electronic rulemaking: a project highlight},
year = {2005},
publisher = {Digital Government Society of North America},
abstract = {In this project, we are developing new text processing tools that help people perform advanced analysis of large collections of text commentary. This problem is increasingly faced by the United States federal government's regulation writers who formulate the rules and regulations that define the details of laws enacted by Congress. Our research focuses on text clustering, text searching using information retrieval, near-duplicate detection, opinion identification, stakeholder characterization, and extractive summarization, as well as the impact of such tools on the process of rulemaking itself. Versions of a Rule-Writer's Workbench will be built by Computer Science researchers at ISI and CMU, deployed annually for experimental use by our government partners, and evaluated by social science researchers from the Library and Information Science and Sociology departments at the Universities of Pittsburgh and San Francisco respectively. This three-year project started in October 2004 and is funded under the National Science Foundation's Digital Government program.},
booktitle = {Proceedings of the 2005 National Conference on Digital Government Research},
pages = {87–88},
numpages = {2},
keywords = {federal government, information retrieval, natural language processing, near-duplicate detection, opinion recognition, regulations, rulemaking, text annotation},
location = {Atlanta, Georgia, USA},
series = {dg.o '05}
}

@article{10.1145/1066078.1066079,
author = {Wu, Chung-Hsien and Yeh, Jui-Feng and Chen, Ming-Jun},
title = {Domain-specific FAQ retrieval using independent aspects},
year = {2005},
issue_date = {March 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/1066078.1066079},
doi = {10.1145/1066078.1066079},
abstract = {This investigation presents an approach to domain-specific FAQ (frequently-asked question) retrieval using independent aspects. The data analysis classifies the questions in the collected QA (question-answer) pairs into ten question types in accordance with question stems. The answers in the QA pairs are then paragraphed and clustered using latent semantic analysis and the K-means algorithm. For semantic representation of the aspects, a domain-specific ontology is constructed based on WordNet and HowNet. A probabilistic mixture model is then used to interpret the query and QA pairs based on independent aspects; hence the retrieval process can be viewed as the maximum likelihood estimation problem. The expectation-maximization (EM) algorithm is employed to estimate the optimal mixing weights in the probabilistic mixture model. Experimental results indicate that the proposed approach outperformed the FAQ-Finder system in medical FAQ retrieval.},
journal = {ACM Transactions on Asian Language Information Processing},
month = mar,
pages = {1–17},
numpages = {17},
keywords = {question-answering, probabilistic mixture model, ontology, natural language processing, latent semantic analysis, information retrieval, FAQ retrieval}
}

@inproceedings{10.1145/379437.379675,
author = {Klavans, Judith and Whitman, Brian},
title = {Extracting taxonomic relationships from on-line definitional sources using LEXING},
year = {2001},
isbn = {1581133456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/379437.379675},
doi = {10.1145/379437.379675},
abstract = {We present a system which extracts the genus word and phrase from free -form definition text, entitled LEXING, for Lexical Information from Glossaries. The extractions will be used to build automatically a lexical knowledge base from on-line domain specific glossary sources. We combine statistical and semantic processes to extract these terms, and  demonstrate that this combination allows us to predict the genus even in difficult situations such as empty head definitions or verb definitions. We also discuss the use of “linking prepositions” for use in skipping past empty head genus phrases. This system is part of a project to extract ontological information for energy glossary information.},
booktitle = {Proceedings of the 1st ACM/IEEE-CS Joint Conference on Digital Libraries},
pages = {257–258},
numpages = {2},
keywords = {ontologies, natural language processing, lexical knowledge bases, information retrieval, glossaries, definitions},
location = {Roanoke, Virginia, USA},
series = {JCDL '01}
}

@inproceedings{10.1145/291080.291119,
author = {Normore, Lorraine and Bendig, Mark and Godby, Carol Jean},
title = {WordView: understanding words in context},
year = {1998},
isbn = {1581130988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/291080.291119},
doi = {10.1145/291080.291119},
booktitle = {Proceedings of the 4th International Conference on Intelligent User Interfaces},
pages = {194},
keywords = {natural language processing, information visualization, information retrieval, compound nominals},
location = {Los Angeles, California, USA},
series = {IUI '99}
}

@inproceedings{10.1145/238218.238332,
author = {Isahara, Hitoshi and Ozaku, Hiromi},
title = {Intelligent network news reader},
year = {1997},
isbn = {0897918398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/238218.238332},
doi = {10.1145/238218.238332},
booktitle = {Proceedings of the 2nd International Conference on Intelligent User Interfaces},
pages = {237–240},
numpages = {4},
keywords = {network news reader, natural language processing, information retrieval},
location = {Orlando, Florida, USA},
series = {IUI '97}
}

@inproceedings{10.1145/1031171.1031268,
author = {Sano, Makoto and Evans, David A.},
title = {Circumstance-based categorization analysis of knowledge management systems for the japanese market},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031268},
doi = {10.1145/1031171.1031268},
abstract = {We conducted a survey of thirty of the approximately 1,700 customers of Justsystem Corporation's knowledge-management applications. Our goal was to discover the kinds of functions that customers hoped to address in their next-generation use of knowledge management technology and to assess the core processes that we will need to deploy in our products to address their desired solutions. In particular, we sought to analyze our customers' requirements along dimensions that take account of both the context of use of the application and its stage in the cycle of knowledge creation and use. As part of our analysis, we were able to classify all customer cases as focused by one or more of three Goals, supported by one or more of eleven technology Means. To establish appropriate categories of use, we exploited the stages of the &lt;i&gt;SECI Model&lt;/i&gt;, several other transactional categories of knowledge use, and whether activities were targeted at internal or external users. Through the analysis, we found the typical technology components (Means) for each stage of knowledge creation and use associated with each set of goals. We consider such analysis essential to the task of designing next-generation knowledge-management applications and critical to overcoming the unfortunate tendency of developers to devise solutions that bear little relation to the true needs of users.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {507–514},
numpages = {8},
keywords = {ontology, natural language processing, knowledge management technology, knowledge management, information retrieval, decision support, affect analysis, SECI model},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@article{10.1145/380995.381051,
author = {Grobelnik, Marko and Mladenic, Dunja and Milic-Frayling, Natasa},
title = {Text mining as integration of several related research areas: report on KDD's workshop on text mining 2000},
year = {2000},
issue_date = {Dec. 2000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {1931-0145},
url = {https://doi.org/10.1145/380995.381051},
doi = {10.1145/380995.381051},
journal = {SIGKDD Explor. Newsl.},
month = dec,
pages = {99–102},
numpages = {4},
keywords = {text mining, natural language processing, information retrieval, information extraction, KDD workshop report}
}

@inproceedings{10.1145/325737.325763,
author = {Bergstr\"{o}m, Agneta and Jaksetic, Patricija and Nordin, Peter},
title = {Enhancing information retrieval by automatic acquisition of textual relations using genetic programming},
year = {2000},
isbn = {1581131348},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/325737.325763},
doi = {10.1145/325737.325763},
abstract = {We have explored a novel method to find textual relations in electronic documents using genetic programming and semantic networks. This can be used for enhancing information retrieval and simplifying user interfaces. The automatic extraction of relations from text enables easier updating of electronic dictionaries and may reduce interface area both for search input and hit output on small screens such as cell phones and PDAs (Personal Digital Assistants).},
booktitle = {Proceedings of the 5th International Conference on Intelligent User Interfaces},
pages = {29–32},
numpages = {4},
keywords = {semantic networks, natural language processing, machine learning, information retrieval, genecic programming},
location = {New Orleans, Louisiana, USA},
series = {IUI '00}
}

@article{10.1145/1194936.1194942,
author = {Matsumura, Atsushi and Takasu, Atsuhiro and Adachi, Jun},
title = {Effect of relationships between words on Japanese information retrieval},
year = {2006},
issue_date = {September 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {1530-0226},
url = {https://doi.org/10.1145/1194936.1194942},
doi = {10.1145/1194936.1194942},
abstract = {Two Japanese-language information retrieval (IR) methods that enhance retrieval effectiveness by utilizing the relationships between words are proposed. The first method uses dependency relationships between words in a sentence. The second method uses proximity relationships, particularly information about the ordered co-occurrence of words in a sentence, to approximate the dependency relationships between them. A Structured Index has been constructed for these two methods, which represents the dependency relationships between words in a sentence as a set of binary trees. The Structured Index is created by morphological analysis and dependency analysis based on simple template matching and compound noun analysis derived from word statistics. Through retrieval experiments using the Japanese test collection for information retrieval systems (NTCIR-1, the NACSIS Test Collection for IR systems), it is shown that these two methods offer superior retrieval effectiveness compared with the TF--IDF method, and are effective with different databases and diverse search topics sets. There is little difference in retrieval effectiveness between these two methods.},
journal = {ACM Transactions on Asian Language Information Processing},
month = sep,
pages = {264–289},
numpages = {26},
keywords = {test collection, proximity operation, phrases, natural language processing, morphological analysis, information retrieval, dependency relationships, co-occurrence, Structured Index, NTCIR, Compound noun analysis}
}

@inproceedings{10.1145/511446.511500,
author = {Radev, Dragomir and Fan, Weiguo and Qi, Hong and Wu, Harris and Grewal, Amardeep},
title = {Probabilistic question answering on the web},
year = {2002},
isbn = {1581134495},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/511446.511500},
doi = {10.1145/511446.511500},
abstract = {Web-based search engines such as Google and NorthernLight return documents that are relevant to a user query, not answers to user questions. We have developed an architecture that augments existing search engines so that they support natural language question answering. The process entails five steps: query modulation, document retrieval, passage extraction, phrase extraction, and answer ranking. In this paper we describe some probabilistic approaches to the last three of these stages. We show how our techniques apply to a number of existing search engines and we also present results contrasting three different methods for question answering. Our algorithm, probabilistic phrase reranking (PPR) using proximity and question type features achieves a total reciprocal document rank of .20 on the TREC 8 corpus. Our techniques have been implemented as a Web-accessible system, called NSIR.},
booktitle = {Proceedings of the 11th International Conference on World Wide Web},
pages = {408–419},
numpages = {12},
keywords = {search engines, question answering, query modulation, natural language processing, information retrieval, answer selection, answer extraction},
location = {Honolulu, Hawaii, USA},
series = {WWW '02}
}

@inproceedings{10.1145/1809980.1810073,
author = {Borges, Thyago Bohrer and Gonzalez, Marco and Lima, Vera L\'{u}cia Strube},
title = {Expans\~{a}o de consulta por pseudo realimenta\c{c}\~{a}o no modelo TR+ para recupera\c{c}\~{a}o de informa\c{c}\~{a}o},
year = {2008},
isbn = {9788576691990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1809980.1810073},
doi = {10.1145/1809980.1810073},
abstract = {This work presents the specification of experiments that apply query expansion techniques with pseudo relevance feedback to TR+ Model in information retrieval. The TR+ Model uses terms and binary lexical relations (BLRs) for indexing and searching of texts in Portuguese. The experiments add (or remove) new terms or BLRs to the original query in order to study the effects of those changes in document retrieval.},
booktitle = {Companion Proceedings of the XIV Brazilian Symposium on Multimedia and the Web},
pages = {381–383},
numpages = {3},
keywords = {relevance feedback, rela\c{c}\~{o}es lexicais bin\'{a}rias, recupera\c{c}\~{a}o de informa\c{c}\~{a}o, realimenta\c{c}\~{a}o de relevantes, pseudo relevance feedback, natural language processing, modelo TR+, information retrieval, expansion query, expans\~{a}o de consultas, binary lexical relations, TR+ Model},
location = {Vila Velha, Esp\'{\i}rito Santo, Brazil},
series = {WebMedia '08}
}

@inproceedings{10.1145/1364742.1364746,
author = {Kraft, Donald H. and Pasi, Gabriella and Bordogna, Gloria},
title = {Vagueness and uncertainty in information retrieval: how can fuzzy sets help?},
year = {2006},
isbn = {1595936084},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1364742.1364746},
doi = {10.1145/1364742.1364746},
abstract = {The field of fuzzy information systems has grown and is maturing. In this paper, some applications of fuzzy set theory to information retrieval are described, as well as the more recent outcomes of research in this field. Fuzzy set theory is applied to information retrieval with the main aim being to define flexible systems, i.e., systems that can represent and manage the vagueness and subjectivity which characterizes the process of information representation and retrieval, one of the main objectives of artificial intelligence.},
booktitle = {Proceedings of the 2006 International Workshop on Research Issues in Digital Libraries},
articleno = {3},
numpages = {10},
keywords = {vagueness, uncertainty, rough sets, information retrieval, imprecision, genetic programming, fuzzy set theory, fuzzy set, fuzzy indexing, clustering, artificial intelligence},
location = {Kolkata, India},
series = {IWRIDL '06}
}

@inproceedings{10.5555/1289189.1289237,
author = {Valderr\'{a}banos, Antonio S. and Belskis, Alexander and Iraola, Luis},
title = {TExtractor: a multilingual terminology extraction tool},
year = {2002},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {This demonstration presents a tool (TExtractor) employed for enriching terminology sets in four languages: English, French, German and Spanish. We present the associated linguistic resources and the experimental results obtained in the medical domain. TExtractor has been developed within project LIQUID (IST-2000-25324), which aims at developing a cost-effective solution for the problem of cross-language information retrieval (CLIR) in multilingual document bases in technical and scientific domains.},
booktitle = {Proceedings of the Second International Conference on Human Language Technology Research},
pages = {393–398},
numpages = {6},
keywords = {terminology extraction, natural language processing, medical texts, gastroenterology, cross-language information retrieval, content-based indexing and retrieval},
location = {San Diego, California},
series = {HLT '02}
}

@inproceedings{10.1145/1062745.1062765,
author = {Ntoulas, Alexandros and Chao, Gerald and Cho, Junghoo},
title = {The infocious web search engine: improving web searching through linguistic analysis},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062765},
doi = {10.1145/1062745.1062765},
abstract = {In this paper we present the Infocious Web search engine [23]. Our goal in creating Infocious is to improve the way people find information on the Web by resolving ambiguities present in natural language text. This is achieved by performing linguistic analysis on the content of the Web pages we index, which is a departure from existing Web search engines that return results mainly based on keyword matching. This additional step of linguistic processing gives Infocious two main advantages. First, Infocious gains a deeper understanding of the content of Web pages so it can better match users' queries with indexed documents and therefore can improve relevancy of the returned results. Second, based on its linguistic processing, Infocious can organize and present the results to the user in more intuitive ways. In this paper we present the linguistic processing technologies that we incorporated in Infocious and how they are applied in helping users find information on the Web more efficiently. We discuss the various components in the architecture of Infocious and how each of them benefits from the added linguistic processing. Finally, we experimentally evaluate the performance of a component which leverages linguistic information in order to categorize Web pages.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {840–849},
numpages = {10},
keywords = {word sense disambiguation, web searching, web search engine, phrase identification, part-of-speech tagging, natural language processing, linguistic analysis of web text, language analysis, information retrieval, indexing, crawling, concept extraction},
location = {Chiba, Japan},
series = {WWW '05}
}

@article{10.1145/1105696.1105701,
author = {Savoy, Jacques},
title = {Comparative study of monolingual and multilingual search models for use with asian languages},
year = {2005},
issue_date = {June 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/1105696.1105701},
doi = {10.1145/1105696.1105701},
abstract = {Based on the NTCIR-4 test-collection, our first objective is to present an overview of the retrieval effectiveness of nine vector-space and two probabilistic models that perform monolingual searches in the Chinese, Japanese, Korean, and English languages. Our second goal is to analyze the relative merits of the various automated and freely available toolsto translate the English-language topics into Chinese, Japanese, or Korean, and then submit the resultant query in order to retrieve pertinent documents written in one of the three Asian languages. We also demonstrate how bilingual searches could be improved by applying both the combined query translation strategies and data-fusion approaches. Finally, we address basic problems related to multilingual searches, in which queries written in English are used to search documents written in the English, Chinese, Japanese, and Korean languages.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
pages = {163–189},
numpages = {27},
keywords = {search engines with Asian languages, results-merging, natural language processing with Asian languages, cross-language information retrieval, Multilingual information retrieval, Korean language, Japanese language, Chinese language}
}

@inproceedings{10.1145/313238.313416,
author = {Rocha, Luis Mateus},
title = {TalkMine and the adaptive recommendation project},
year = {1999},
isbn = {1581131453},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/313238.313416},
doi = {10.1145/313238.313416},
booktitle = {Proceedings of the Fourth ACM Conference on Digital Libraries},
pages = {242–243},
numpages = {2},
keywords = {recommendation systems, knowledge management, information retrieval, human-machine interaction, fuzzy sets, evolutionary systems, evidence sets, distributed information systems, artificial intelligence},
location = {Berkeley, California, USA},
series = {DL '99}
}

@inproceedings{10.1145/800184.810472,
author = {Linn, William E. and Reitman, Walter},
title = {Referential communication in AUTONOTE, a personal information retrieval system},
year = {1971},
isbn = {9781450374842},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800184.810472},
doi = {10.1145/800184.810472},
abstract = {This paper is concerned with the mechanisms of human intelligence and natural language communication, and with the design of an interactive computer program that incorporates and utilizes analogous mechanisms to improve man-machine communication. AUTONOTE2 is an improved personal information retrieval system. It includes, in addition to AUTONOTE (a presently running system), (1) mechanisms allowing the user to employ certain kinds of noun phrases to describe the items he wishes to store and retrieve; and (2) mechanisms enabling the system to maintain a map of what the user is referring to. Though extremely limited by comparison with the analogous human capabilities, these mechanisms add significantly to the descriptive power available to the user, and to the ease and efficiency of communication with the system. Furthermore, because AUTONOTE is a file oriented system, these additions add little to the present low cost of using the system. AUTONOTE2 also suggests new directions for software development for man-machine interaction, and it provides a practical testing ground for ideas about intelligence derived from observation of natural intelligence.},
booktitle = {Proceedings of the 1971 26th Annual Conference},
pages = {67–81},
numpages = {15},
keywords = {Referential communication, Natural language processing, Man-machine communication, Information retrieval, Computer understanding, Artificial intelligence, AUTONOTE2, AUTONOTE},
series = {ACM '71}
}

@inproceedings{10.1145/800179.810186,
author = {McKinney, M. H.},
title = {Query using inferential processing implemented with inverted hashed files},
year = {1977},
isbn = {9781450339216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800179.810186},
doi = {10.1145/800179.810186},
abstract = {A methodology is presented for a prototype query processor that can infer the semantics of a query and thus allow a very natural-language-like request from a user. The inference is accomplished by fully inverting all attribute values, attribute names, and system keywords. All atoms of a query are treated in the same manner until the semantics are either positively inferred by the system or explicitly declared by the user. The implementation approach uses a common hash area containing: the attribute names, the inverted attribute values, the key words allowed by the query processor, and any synonyms that have been declared by the user. The methodology includes provision for performing updates, adds, and deletions as well as query. Advantages of the approach are the simplicity of the query language for the user and the elegance of the semantic interpretation by the system.},
booktitle = {Proceedings of the 1977 Annual Conference},
pages = {86–89},
numpages = {4},
keywords = {Semantics, Query processing, Natural language processing, Multi-attribute files, Information retrieval},
location = {Seattle, Washington},
series = {ACM '77}
}

@inproceedings{10.1145/1120212.1120214,
author = {Hauptmann, Alexander G. and Witbrock, Michael J. and Christel, Michael G.},
title = {Artificial intelligence techniques in the interface to a Digital Video Library},
year = {1997},
isbn = {0897919262},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1120212.1120214},
doi = {10.1145/1120212.1120214},
abstract = {For the huge amounts of audio and video material that could usefully be included in digital libraries, the cost of producing human-generated annotations and meta-data is prohibitive. In the Informedia Digital Video Library, the production of meta-data supporting the library interface is automated using techniques from Artificial Intelligence (AI). By applying speech recognition, natural language processing and image analysis, the interface helps users locate the information they want and navigate or browse the digital video library more effectively. Specific AI-based interface components include automatic titles, filmstrips, video skims, word location marking and representative frames for shots.},
booktitle = {CHI '97 Extended Abstracts on Human Factors in Computing Systems},
pages = {2–3},
numpages = {2},
keywords = {video summarization, video browsing, speech recognition, news-on-demand, multimedia indexing and search, informedia, information retrieval interfaces, digital library, automatic text summarization, artificial intelligence},
location = {Atlanta, Georgia},
series = {CHI EA '97}
}

@article{10.1145/362384.362495,
author = {Damerau, Frederick J.},
title = {Automatic parsing for content analysis},
year = {1970},
issue_date = {June 1970},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {6},
issn = {0001-0782},
url = {https://doi.org/10.1145/362384.362495},
doi = {10.1145/362384.362495},
abstract = {Although automatic syntactic and semantic analysis is not yet possible for all of an unrestricted natural language text, some applications, of which content analysis is one, do not have such a stringent coverage requirement. Preliminary studies show that the Harvard Syntactic Analyzer can produce correct and unambiguous identification of the subject and object of certain verbs for approximately half of the relevant occurences. This provides a degree of coverage for content analysis variables which compares favorably to manual methods, in which only a sample of the total available text is normally processed.},
journal = {Commun. ACM},
month = jun,
pages = {356–360},
numpages = {5},
keywords = {text processing, syntactic analysis, parsing, natural language processing, language analysis, information retrieval, content analysis}
}

@article{10.1145/359545.359550,
author = {Waltz, David L.},
title = {An English language question answering system for a large relational database},
year = {1978},
issue_date = {July 1978},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {7},
issn = {0001-0782},
url = {https://doi.org/10.1145/359545.359550},
doi = {10.1145/359545.359550},
abstract = {By typing requests in English, casual users will be able to obtain explicit answers from a large relational database of aircraft flight and maintenance data using a system called PLANES. The design and implementation of this system is described and illustrated with detailed examples of the operation of system components and examples of overall system operation. The language processing portion of the system uses a number of augmented transition networks, each of which matches phrases with a specific meaning, along with context registers (history keepers) and concept case frames; these are used for judging meaningfulness of questions, generating dialogue for clarifying partially understood questions, and resolving ellipsis and pronoun reference problems. Other system components construct a formal query for the relational database, and optimize the order of searching relations. Methods are discussed for handling vague or complex questions and for providing browsing ability. Also included are discussions of important issues in programming natural language systems for limited domains, and the relationship of this system to others.},
journal = {Commun. ACM},
month = jul,
pages = {526–539},
numpages = {14},
keywords = {relational database, question answering, query generation, natural language programming, natural language, information retrieval, dialogue, database front end, artificial intelligence}
}

@inproceedings{10.1145/511285.511290,
author = {Kellogg, Charles and Burger, John and Diller, Timothy and Fogt, Kenneth},
title = {The converse natural language data management system: current status and plans},
year = {1971},
isbn = {9781450373449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/511285.511290},
doi = {10.1145/511285.511290},
abstract = {This paper presents an overview of research in progress in which the principal aim is the achievement of more natural and expressive modes of on-line communication with complexly structured data bases. A natural-language compiler has been constructed that accepts sentences in a user-extendable English subset, produces surface and deep-structure syntactic analyses, and uses a network of concepts to construct semantic interpretations formalized as computable procedures. The procedures are evaluated by a data management system that updates, modifies, and searches data bases that can be formalized as finite models of states of affairs. The system has been designed and programmed to handle large vocabularies and large collections of facts efficiently. Plans for extending the research vehicle to interface with a deductive inference component and a voice input-output effort are briefly described.},
booktitle = {Proceedings of the 1971 International ACM SIGIR Conference on Information Storage and Retrieval},
pages = {33–46},
numpages = {14},
keywords = {syntax analysis, semantic interpretation, relational data files, question-answering system, on-line communication, natural language, language processing, information retrieval, fact retrieval, deductive inference, data management, concept network, artificial intelligence},
location = {College Park, Maryland},
series = {SIGIR '71}
}

@article{10.1145/366532.366554,
author = {Ross, Douglas T.},
title = {Computer-aided design},
year = {1961},
issue_date = {May 1961},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {5},
issn = {0001-0782},
url = {https://doi.org/10.1145/366532.366554},
doi = {10.1145/366532.366554},
abstract = {This project is engaged in (a) a program of research into the application of the concepts and techniques of modern data processing to the design of mechanical parts, and (b) the further development of automatic programming systems for numerically controlled machine tools. The project is a cooperative venture between the Computer Applications Group of the Electronic Systems Laboratory and the Design and Graphics Division of the Mechanical Engineering Department, and is sponsored by the Manufacturing Methods Division of the USAF Air Material Command through Contract AF-33(600)-40604.},
journal = {Commun. ACM},
month = may,
pages = {235},
numpages = {2},
keywords = {techniques, symbol manipulation, problem solving, problem formulation, operating and debugging systems, numerical control processes, manual intervention techniques and equipment, language design, information retrieval, efficient computation of very large complex problems, compiler techniques, automatic programming, artificial intelligence}
}

